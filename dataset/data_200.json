{"content":"from dataclasses import dataclass, field\n\nfrom pytest import raises\n\nfrom apischema import ValidationError, dependent_required, deserialize\nfrom apischema.json_schema import deserialization_schema\nfrom apischema.skip import NotNull\n\n\n@dataclass\nclass Billing:\n    name: str\n    # Fields used in dependencies MUST be declared with `field`\n    credit_card: NotNull[int] = field(default=None)\n    billing_address: NotNull[str] = field(default=None)\n\n    dependencies = dependent_required({credit_card: [billing_address]})\n\n\n# it can also be done outside the class with\n# dependent_required({\"credit_card\": [\"billing_address\"]}, owner=Billing)\n\n\nassert deserialization_schema(Billing) == {\n    \"$schema\": \"http:\/\/json-schema.org\/draft\/2020-12\/schema#\",\n    \"additionalProperties\": False,\n    \"dependentRequired\": {\"credit_card\": [\"billing_address\"]},\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"credit_card\": {\"type\": \"integer\"},\n        \"billing_address\": {\"type\": \"string\"},\n    },\n    \"required\": [\"name\"],\n    \"type\": \"object\",\n}\n\nwith raises(ValidationError) as err:\n    deserialize(Billing, {\"name\": \"Anonymous\", \"credit_card\": 1234_5678_9012_3456})\nassert err.value.errors == [\n    {\n        \"loc\": [\"billing_address\"],\n        \"msg\": \"missing property (required by ['credit_card'])\",\n    }\n]\n","label":1}
{"content":"# -*- coding: UTF-8 -*-\r\n# !\/usr\/bin\/python\r\n# @time     :2019\/6\/22 19:35\r\n# @author   :Mo\r\n# @function :Attention of itself\r\n\r\n\r\nfrom keras.regularizers import L1L2, Regularizer\r\n# from keras.engine.topology import Layer\r\nfrom keras.layers import Layer\r\nfrom keras import backend as K\r\n\r\n\r\nclass AttentionSelf(Layer):\r\n    \"\"\"\r\n        self attention,\r\n        codes from:  https:\/\/mp.weixin.qq.com\/s\/qmJnyFMkXVjYBwoR_AQLVA\r\n    \"\"\"\r\n    def __init__(self, output_dim, **kwargs):\r\n        self.output_dim = output_dim\r\n        super().__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        # W\u3001K and V\r\n        self.kernel = self.add_weight(name='WKV',\r\n                                        shape=(3, input_shape[2], self.output_dim),\r\n                                        initializer='uniform',\r\n                                        regularizer=L1L2(0.0000032),\r\n                                        trainable=True)\r\n        super().build(input_shape)\r\n\r\n    def call(self, x):\r\n        WQ = K.dot(x, self.kernel[0])\r\n        WK = K.dot(x, self.kernel[1])\r\n        WV = K.dot(x, self.kernel[2])\r\n        # print(\"WQ.shape\",WQ.shape)\r\n        # print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\",K.permute_dimensions(WK, [0, 2, 1]).shape)\r\n        QK = K.batch_dot(WQ, K.permute_dimensions(WK, [0, 2, 1]))\r\n        QK = QK \/ (64**0.5)\r\n        QK = K.softmax(QK)\r\n        # print(\"QK.shape\",QK.shape)\r\n        V = K.batch_dot(QK, WV)\r\n        return V\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], input_shape[1], self.output_dim)\r\n\r\n\r\nif __name__==\"__main__\":\r\n    att = AttentionSelf(300)\r\n\r\n","label":1}
{"content":"#!\/usr\/bin\/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nProvides the base DataSource class for pypsych data sources.\n\"\"\"\n\nimport pandas as pd\n\n\nclass DataSource(object):\n    \"\"\"\n    DataSource base class.\n    \"\"\"\n\n    def __init__(self, config, schedule):\n        self.config = self._validate_config(config)\n        self.schedule = self._validate_schedule(schedule)\n        self.output = pd.Panel()\n        self.data = {}\n\n    def load(self, file_paths):\n        \"\"\"By default, loads all files as TSV.\"\"\"\n        for file_type, file_path in file_paths.iteritems():\n            self.data[file_type] = pd.read_csv(file_path,\n                                               comment=\"#\",\n                                               delimiter=\"\\t\",\n                                               skipinitialspace=True)\n\n    def process(self):\n        \"\"\".\"\"\"\n        self.merge_data()\n        self.bin_data()\n\n    def bin_data(self):\n        \"\"\"Makes a dict of dicts of pd.Panels at self.output.\"\"\"\n        label_bins = self.create_label_bins(self.data['labels'])\n        major_axis = label_bins.index.values\n        minor_axis = label_bins.drop(['Start_Time', 'End_Time'], axis=1).columns\n        minor_axis = minor_axis.append(pd.Index(['stat']))\n\n        raw = self.data['samples']\n\n        output = {channel: pd.Panel(items=statistics.keys(),\n                                    major_axis=major_axis,\n                                    minor_axis=minor_axis)\n                  for channel, statistics in self.panels.iteritems()}\n\n        for channel, statistics in self.panels.iteritems():\n            for stat_name, stat_fun in statistics.iteritems():\n                stats = []\n                new_panel = label_bins.copy(deep=True)\n                new_panel.drop(['Start_Time', 'End_Time'], axis=1, inplace=True)\n                for _, label_bin in label_bins.iterrows():\n                    selector = (raw.index.values >= label_bin['Start_Time']) \\\n                        & (raw.index.values < label_bin['End_Time'])\n                    samples = raw[selector][channel]\n                    pos = raw.loc[selector, 'pos']\n                    stats.append(stat_fun(samples, pos, label_bin))\n\n                new_panel['stat'] = stats\n                output[channel][stat_name] = \\\n                    new_panel.sort_values(by='Bin_Order', axis=0)\n\n        self.output = output\n\n    def validate_data(self):\n        \"\"\"Check that each data file has at least one record.\"\"\"\n        return {f: len(d) > 1 for f, d in self.data.iteritems()}\n\n    @staticmethod\n    def _validate_config(raw):\n        \"\"\"Placeholder method for validating configuration dicts.\"\"\"\n        return raw\n\n    @staticmethod\n    def _validate_schedule(raw):\n        \"\"\"Placeholder method for validating schedule configuration dicts.\"\"\"\n        return raw\n","label":1}
{"content":"#!\/usr\/bin\/python3\n\nimport model1\n\ns = 'a'+'b'\n\nprint(s)\n\n\n\n","label":1}
{"content":"import uuid\n\nfrom django.core.cache import cache\nfrom django.db import models\nfrom thenewboston.models.created_modified import CreatedModified\n\n\nclass Roadmap(CreatedModified):\n    uuid = models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True)\n    team = models.ForeignKey('teams.CoreTeam', on_delete=models.CASCADE)\n    task_title = models.CharField(max_length=255)\n    estimated_completion_date = models.DateField()\n    is_complete = models.BooleanField(default=False)\n\n    def __str__(self):\n        return f'#{self.pk}: {self.task_title}'\n\n    class Meta:\n        ordering = ('estimated_completion_date',)\n\n    def save(self, *args, **kwargs):\n        cache.delete_pattern('views.decorators.cache.cache*')\n        return super(Roadmap, self).save(*args, **kwargs)\n","label":1}
{"content":"#!\/usr\/bin\/env python\n\n\"\"\"Command-line interface.\"\"\"\n\nimport sys\n\nfrom .types import Config\n\n\ndef main(args=None):\n    \"\"\"Process command-line arguments and run the program.\"\"\"\n    if args is None:\n        args = sys.argv\n    assert len(args) == 2\n    path = args[1]\n\n    run(path)\n\n\ndef run(path):\n    \"\"\"Run the program.\"\"\"\n    config = Config(path)\n    print(repr(config))\n    for game in config.games:\n        print(repr(game))\n\n\nif __name__ == '__main__':  # pragma: no cover (manual test)\n    main()\n","label":1}
{"content":"from django import template\nfrom django.utils.safestring import mark_safe\n\nregister = template.Library()\n\n\n@register.simple_tag(takes_context=True)\ndef assignee(context, ticket):\n    return me_or_user(context, ticket.assignee)\n\n\n@register.simple_tag(takes_context=True)\ndef me_or_user(context, user):\n    if user == context['request'].user:\n        return mark_safe('<strong>me<\/strong>')\n    return user\n","label":1}
{"content":"\"\"\"project URL Configuration\n\nThe `urlpatterns` list routes URLs to views. For more information please see:\n    https:\/\/docs.djangoproject.com\/en\/2.2\/topics\/http\/urls\/\nExamples:\nFunction views\n    1. Add an import:  from my_app import views\n    2. Add a URL to urlpatterns:  path('', views.home, name='home')\nClass-based views\n    1. Add an import:  from other_app.views import Home\n    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')\nIncluding another URLconf\n    1. Import the include() function: from django.urls import include, path\n    2. Add a URL to urlpatterns:  path('blog\/', include('blog.urls'))\n\"\"\"\nfrom django.contrib import admin\nfrom django.urls import path, include\nfrom django.conf import settings\nfrom django.conf.urls.static import static\n\nurlpatterns = [\n    path('admin\/', admin.site.urls),\n    path('meals\/', include('meals.urls', namespace='meals')),\n    path('blog\/', include('blog.urls', namespace='blog')),\n    path('reserve_table\/', include('reservation.urls', namespace='reservation')),\n    path('about-us\/', include('aboutus.urls', namespace='aboutus')),\n    path('', include('home.urls', namespace='home')),\n]\n\n\n\n\nurlpatterns +=static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\nurlpatterns +=static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n","label":1}
{"content":"import pytest\n\nimport polars as pl\n\n\ndef test_compare_series_value_mismatch() -> None:\n    srs1 = pl.Series([1, 2, 3])\n    srs2 = pl.Series([2, 3, 4])\n    with pytest.raises(AssertionError, match=\"Series are different\\n\\nValue mismatch\"):\n        pl.testing.assert_series_equal(srs1, srs2)\n\n\ndef test_compare_series_nulls_are_equal() -> None:\n    srs1 = pl.Series([1, 2, None])\n    srs2 = pl.Series([1, 2, None])\n    pl.testing.assert_series_equal(srs1, srs2)\n\n\ndef test_compare_series_value_mismatch_string() -> None:\n    srs1 = pl.Series([\"hello\", \"no\"])\n    srs2 = pl.Series([\"hello\", \"yes\"])\n    with pytest.raises(\n        AssertionError, match=\"Series are different\\n\\nExact value mismatch\"\n    ):\n        pl.testing.assert_series_equal(srs1, srs2)\n\n\ndef test_compare_series_type_mismatch() -> None:\n    srs1 = pl.Series([1, 2, 3])\n    srs2 = pl.DataFrame({\"col1\": [2, 3, 4]})\n    with pytest.raises(AssertionError, match=\"Series are different\\n\\nType mismatch\"):\n        pl.testing.assert_series_equal(srs1, srs2)  # type: ignore\n\n    srs3 = pl.Series([1.0, 2.0, 3.0])\n    with pytest.raises(AssertionError, match=\"Series are different\\n\\nDtype mismatch\"):\n        pl.testing.assert_series_equal(srs1, srs3)\n\n\ndef test_compare_series_name_mismatch() -> None:\n    srs1 = pl.Series(values=[1, 2, 3], name=\"srs1\")\n    srs2 = pl.Series(values=[1, 2, 3], name=\"srs2\")\n    with pytest.raises(AssertionError, match=\"Series are different\\n\\nName mismatch\"):\n        pl.testing.assert_series_equal(srs1, srs2)\n\n\ndef test_compare_series_shape_mismatch() -> None:\n    srs1 = pl.Series(values=[1, 2, 3, 4], name=\"srs1\")\n    srs2 = pl.Series(values=[1, 2, 3], name=\"srs2\")\n    with pytest.raises(AssertionError, match=\"Series are different\\n\\nShape mismatch\"):\n        pl.testing.assert_series_equal(srs1, srs2)\n\n\ndef test_compare_series_value_exact_mismatch() -> None:\n    srs1 = pl.Series([1.0, 2.0, 3.0])\n    srs2 = pl.Series([1.0, 2.0 + 1e-7, 3.0])\n    with pytest.raises(\n        AssertionError, match=\"Series are different\\n\\nExact value mismatch\"\n    ):\n        pl.testing.assert_series_equal(srs1, srs2, check_exact=True)\n\n\ndef test_assert_frame_equal_pass() -> None:\n    df1 = pl.DataFrame({\"a\": [1, 2]})\n    df2 = pl.DataFrame({\"a\": [1, 2]})\n    pl.testing.assert_frame_equal(df1, df2)\n\n\ndef test_assert_frame_equal_types() -> None:\n    df1 = pl.DataFrame({\"a\": [1, 2]})\n    srs1 = pl.Series(values=[1, 2], name=\"a\")\n    with pytest.raises(AssertionError):\n        pl.testing.assert_frame_equal(df1, srs1)  # type: ignore\n\n\ndef test_assert_frame_equal_length_mismatch() -> None:\n    df1 = pl.DataFrame({\"a\": [1, 2]})\n    df2 = pl.DataFrame({\"a\": [1, 2, 3]})\n    with pytest.raises(AssertionError):\n        pl.testing.assert_frame_equal(df1, df2)\n\n\ndef test_assert_frame_equal_column_mismatch() -> None:\n    df1 = pl.DataFrame({\"a\": [1, 2]})\n    df2 = pl.DataFrame({\"b\": [1, 2]})\n    with pytest.raises(AssertionError):\n        pl.testing.assert_frame_equal(df1, df2)\n\n\ndef test_assert_frame_equal_column_mismatch2() -> None:\n    df1 = pl.DataFrame({\"a\": [1, 2]})\n    df2 = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n    with pytest.raises(AssertionError):\n        pl.testing.assert_frame_equal(df1, df2)\n\n\ndef test_assert_frame_equal_column_mismatch_order() -> None:\n    df1 = pl.DataFrame({\"b\": [3, 4], \"a\": [1, 2]})\n    df2 = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n    with pytest.raises(AssertionError):\n        pl.testing.assert_frame_equal(df1, df2)\n","label":1}
{"content":"import bisect\n\nimport numpy as np\nfrom . import FairseqDataset\n\n\nclass ConcatDataset(FairseqDataset):\n\n    @staticmethod\n    def cumsum(sequence, sample_ratios):\n        r, s = [], 0\n        for e, ratio in zip(sequence, sample_ratios):\n            l = ratio * len(e)\n            r.append(l + s)\n            s += l\n        return r\n\n    def __init__(self, datasets, sample_ratios=1):\n        super(ConcatDataset, self).__init__()\n        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n        self.datasets = list(datasets)\n        if isinstance(sample_ratios, int):\n            sample_ratios = [sample_ratios] * len(self.datasets)\n        self.sample_ratios = sample_ratios\n        self.cummulative_sizes = self.cumsum(self.datasets, sample_ratios)\n        self.real_sizes = [len(d) for d in self.datasets]\n\n    def __len__(self):\n        return self.cummulative_sizes[-1]\n\n    def __getitem__(self, idx):\n        dataset_idx = bisect.bisect_right(self.cummulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cummulative_sizes[dataset_idx - 1]\n        sample_idx = sample_idx % self.real_sizes[dataset_idx]\n        return self.datasets[dataset_idx][sample_idx]\n\n    @property\n    def sizes(self):\n        return np.concatenate([np.tile(ds.sizes, sr) for ds, sr in zip(self.datasets, self.sample_ratios)])\n\n    @property\n    def supports_prefetch(self):\n        return all([d.supports_prefetch for d in self.datasets])\n\n    def prefetch(self, indices):\n        frm = 0\n        for to, ds in zip(self.cummulative_sizes, self.datasets):\n            real_size = len(ds)\n            ds.prefetch([(i - frm) % real_size for i in indices if frm <= i < to])\n            frm = to\n","label":1}
{"content":"#!\/usr\/bin\/python -tt\n# Copyright 2010 Google Inc.\n# Licensed under the Apache License, Version 2.0\n# http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n# Google's Python Class\n# http:\/\/code.google.com\/edu\/languages\/google-python-class\/\n\n# Basic list exercises\n# Fill in the code for the functions below. main() is already set up\n# to call the functions with a few different inputs,\n# printing 'OK' when each function is correct.\n# The starter code for each function includes a 'return'\n# which is just a placeholder for your code.\n# It's ok if you do not complete all the functions, and there\n# are some additional functions to try in list2.py.\n\n\n# A. match_ends\n# Given a list of strings, return the count of the number of\n# strings where the string length is 2 or more and the first\n# and last chars of the string are the same.\n# Note: python does not have a ++ operator, but += works.\ndef match_ends(words):\n    count = 0\n    for string in words:\n        if len(string) >= 2 and string[0] == string[-1]:\n            count += 1\n\n    return count\n\n\n# B. front_x\n# Given a list of strings, return a list with the strings\n# in sorted order, except group all the strings that begin with 'x' first.\n# e.g. ['mix', 'xyz', 'apple', 'xanadu', 'aardvark'] yields\n# ['xanadu', 'xyz', 'aardvark', 'apple', 'mix']\n# Hint: this can be done by making 2 lists and sorting each of them\n# before combining them.\ndef front_x(words):\n    ordernedList = []\n    listWithoutX = []\n    temp = []\n    for word in words:\n        if word[0] == 'x':\n            ordernedList.append(word)\n        else:\n            listWithoutX.append(word)\n    ordernedList.sort()\n    listWithoutX.sort()\n    ordernedList.extend(listWithoutX)\n\n    return ordernedList\n\n\n# C. sort_last\n# Given a list of non-empty tuples, return a list sorted in increasing\n# order by the last element in each tuple.\n# e.g. [(1, 7), (1, 3), (3, 4, 5), (2, 2)] yields\n# [(2, 2), (1, 3), (3, 4, 5), (1, 7)]\n# Hint: use a custom key= function to extract the last element form each tuple.\ndef sort_last(tuples):\n    # +++your code here+++\n    return\n\n\n# Simple provided test() function used in main() to print\n# what each function returns vs. what it's supposed to return.\ndef test(got, expected):\n    if got == expected:\n        prefix = ' OK '\n    else:\n        prefix = '  X '\n    print('%s got: %s expected: %s' % (prefix, repr(got), repr(expected)))\n\n\n# Calls the above functions with interesting inputs.\ndef main():\n    print('match_ends')\n    test(match_ends(['aba', 'xyz', 'aa', 'x', 'bbb']), 3)\n    test(match_ends(['', 'x', 'xy', 'xyx', 'xx']), 2)\n    test(match_ends(['aaa', 'be', 'abc', 'hello']), 1)\n\n    print()\n    print('front_x')\n    test(front_x(['bbb', 'ccc', 'axx', 'xzz', 'xaa']),\n         ['xaa', 'xzz', 'axx', 'bbb', 'ccc'])\n    test(front_x(['ccc', 'bbb', 'aaa', 'xcc', 'xaa']),\n         ['xaa', 'xcc', 'aaa', 'bbb', 'ccc'])\n    test(front_x(['mix', 'xyz', 'apple', 'xanadu', 'aardvark']),\n         ['xanadu', 'xyz', 'aardvark', 'apple', 'mix'])\n\n    print()\n    print('sort_last')\n    test(sort_last([(1, 3), (3, 2), (2, 1)]),\n         [(2, 1), (3, 2), (1, 3)])\n    test(sort_last([(2, 3), (1, 2), (3, 1)]),\n         [(3, 1), (1, 2), (2, 3)])\n    test(sort_last([(1, 7), (1, 3), (3, 4, 5), (2, 2)]),\n         [(2, 2), (1, 3), (3, 4, 5), (1, 7)])\n\n\nif __name__ == '__main__':\n    main()\n","label":1}
{"content":"from threading import Thread\nimport PyQt5.QtCore as core\nimport PyQt5.QtWidgets as core_widgets\nimport PyQt5.QtWebEngine as web_engine\nimport PyQt5.QtWebEngineWidgets as web_widgets\nimport PyQt5.QtGui as gui\nfrom flask import current_app, _app_ctx_stack\n\nclass FlaskQT(object):\n    def __init__(\n                self,\n                app          = None,\n                url          = '127.0.0.1',\n                port         = 5000,\n                debug        = False,\n                using_win32  = False,\n                icon_path    = None,\n                window_title = None\n                ):\n        self.flask_app = app\n        if app is not None:\n            self.init_app(app)\n        self.flask_thread = Thread(target=self._run_flask,\n                                   args=(url, port, debug, using_win32))\n        self.flask_thread.daemon = True\n        self.debug = debug\n\n        self.url = \"http:\/\/{}:{}\".format(url, port)\n        self.app = core_widgets.QApplication([])\n        self.app.setWindowIcon(gui.QIcon(icon_path))\n        self.app.setApplicationName(window_title)\n        self.view = web_widgets.QWebEngineView(self.app.activeModalWidget())\n        self.page = CustomWebEnginePage(self.view)\n        self.view.setPage(self.page)\n\n    def init_app(self, app):\n        pass\n\n    def run(self):\n        self.run_flask()\n        self.run_gui()\n\n    def run_flask(self):\n        self.flask_thread.start()\n\n    def run_gui(self):\n        self.view.load(core.QUrl(self.url))\n\n        change_setting = self.view.page().settings().setAttribute\n        settings = web_widgets.QWebEngineSettings\n        change_setting(settings.LocalStorageEnabled, True)\n        change_setting(settings.PluginsEnabled, True)\n\n        # TODO: These settings aren't implemented in QWebEngineSettings (yet)\n        #change_setting(settings.DeveloperExtrasEnabled, True)\n        #change_setting(settings.OfflineStorageDatabaseEnabled, True)\n        #change_setting(settings.OfflineWebApplicationCacheEnabled, True)\n\n        self.view.showMaximized()\n\n        self.app.exec_()\n\n    def _run_flask(self, host, port, debug=False, using_win32=False):\n        print(host)\n        if using_win32:\n            import pythoncom\n            pythoncom.CoInitialize()\n        self.flask_app.run(debug=debug, host=host, port=port, use_reloader=False)\n\nclass CustomWebEnginePage(web_widgets.QWebEnginePage):\n    def createWindow(self, _type):\n        page = CustomWebEnginePage(self)\n        page.urlChanged.connect(self.open_browser)\n        return page\n\n    def open_browser(self, url):\n        page = self.sender()\n        gui.QDesktopServices.openUrl(url)\n        page.deleteLater()","label":1}
{"content":"import asyncio\nimport datetime\nimport math\n\nimport aiohttp\nimport requests\n\n\ndef main():\n    t0 = datetime.datetime.now()\n\n    loop = asyncio.get_event_loop()\n\n    tasks = [\n        loop.create_task(compute_some()),\n        loop.create_task(compute_some()),\n        loop.create_task(compute_some()),\n        loop.create_task(download_some()),\n        loop.create_task(download_some()),\n        loop.create_task(download_some_more()),\n        loop.create_task(download_some_more()),\n        loop.create_task(wait_some()),\n        loop.create_task(wait_some()),\n        loop.create_task(wait_some()),\n        loop.create_task(wait_some()),\n    ]\n\n    loop.run_until_complete(asyncio.gather(*tasks))\n\n    dt = datetime.datetime.now() - t0\n    print(\"Synchronous version done in {:,.2f} seconds.\".format(dt.total_seconds()))\n\n\nasync def compute_some():\n    # no awaiting possible\n    print(\"Computing...\")\n    for _ in range(1, 10_000_000):\n        math.sqrt(25 ** 25 + .01)\n\n\nasync def download_some():\n    # uses async version of requests\n    print(\"Downloading...\")\n    url = 'https:\/\/talkpython.fm\/episodes\/show\/174\/coming-into-python-from-another-industry-part-2'\n    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:\n        async with session.get(url) as resp:\n            resp.raise_for_status()\n\n            text = await resp.text()\n\n    print(\"Downloaded (more) {:,} characters.\".format(len(text)))\n\n\nasync def download_some_more():\n    # no await so no advantage of async (threading would help)\n    print(\"Downloading more ...\")\n    url = 'https:\/\/pythonbytes.fm\/episodes\/show\/92\/will-your-python-be-compiled'\n    resp = requests.get(url)\n    resp.raise_for_status()\n\n    text = resp.text\n\n    print(\"Downloaded {:,} characters.\".format(len(text)))\n\n\nasync def wait_some():\n    print(\"Waiting...\")\n    for _ in range(1, 1000):\n        await asyncio.sleep(.001)\n\n\nif __name__ == '__main__':\n    main()\n","label":1}
{"content":"# Fill here your own Twitter Application Credentials (see https:\/\/apps.twitter.com\/)\n\nACCESS_TOKEN =      ''\nACCESS_SECRET =     ''\nCONSUMER_KEY =      ''\nCONSUMER_SECRET =   ''\n","label":1}
{"content":"# File: D (Python 2.4)\n\nimport math\nfrom pandac.PandaModules import *\nfrom direct.gui.DirectGui import *\nfrom direct.directnotify import DirectNotifyGlobal\nfrom direct.interval.IntervalGlobal import *\nfrom direct.task import Task\nfrom direct.showbase.PythonUtil import quickProfile\nfrom pirates.distributed.DistributedInteractive import DistributedInteractive\nfrom pirates.piratesgui import PiratesGuiGlobals\nfrom pirates.piratesbase import PiratesGlobals\nfrom pirates.piratesbase import PLocalizer\ncontainerCache = { }\n\nclass DistributedSearchableContainer(DistributedInteractive):\n    notify = DirectNotifyGlobal.directNotify.newCategory('DistributedSearchableContainer')\n    deferrable = True\n    \n    def __init__(self, cr):\n        NodePath.__init__(self, 'DistributedSearchableContainer')\n        DistributedInteractive.__init__(self, cr)\n        self.searchTime = None\n        self.type = None\n        self.containerColorR = 1.0\n        self.containerColorG = 1.0\n        self.containerColorB = 1.0\n        self.containerColorA = 1.0\n        self.sphereScale = 10\n        self.container = None\n        self.startSearchTime = 0.0\n\n    \n    def setSearchTime(self, t):\n        self.searchTime = t\n\n    \n    def setType(self, type):\n        self.type = type\n\n    \n    def getType(self):\n        return self.type\n\n    \n    def setSphereScale(self, sphereScale):\n        self.sphereScale = sphereScale\n\n    \n    def getSphereScale(self):\n        return self.sphereScale\n\n    \n    def setVisZone(self, zone):\n        self.visZone = zone\n\n    \n    def getVisZone(self):\n        return self.visZone\n\n    \n    def announceGenerate(self):\n        self.setInteractOptions(proximityText = PLocalizer.InteractSearchableContainer, sphereScale = self.getSphereScale(), diskRadius = 10, exclusive = 0)\n        DistributedInteractive.announceGenerate(self)\n        self.loadContainer()\n        self.getParentObj().builder.addSectionObj(self.container, self.visZone)\n\n    \n    def disable(self):\n        DistributedInteractive.disable(self)\n        self.getParentObj().builder.removeSectionObj(self.container, self.visZone)\n        if self.container:\n            self.container.removeNode()\n            self.container = None\n        \n\n    \n    def loadContainer(self):\n        if self.container:\n            return None\n        \n        modelPath = PiratesGlobals.SearchableModels.get(self.type, 'models\/props\/crate_04')\n        container = self.getContainerModel(modelPath)\n        containerColor = self.getContainerColor()\n        container.setColorScale(containerColor[0], containerColor[1], containerColor[2], containerColor[3])\n        container.reparentTo(self)\n        self.container = container\n\n    \n    def getContainerModel(self, name):\n        model = containerCache.get(name)\n        if model:\n            return model.copyTo(NodePath())\n        else:\n            model = loader.loadModel(name)\n            model.flattenStrong()\n            containerCache[name] = model\n            return model.copyTo(NodePath())\n\n    \n    def requestInteraction(self, avId, interactType = 0):\n        localAvatar.motionFSM.off()\n        DistributedInteractive.requestInteraction(self, avId, interactType)\n\n    \n    def rejectInteraction(self):\n        localAvatar.guiMgr.createWarning(PLocalizer.AlreadySearched)\n        localAvatar.motionFSM.on()\n        DistributedInteractive.rejectInteraction(self)\n\n    \n    def startSearching(self):\n        self.acceptInteraction()\n        localAvatar.guiMgr.workMeter.updateText(PLocalizer.InteractSearching)\n        localAvatar.guiMgr.workMeter.startTimer(self.searchTime)\n        localAvatar.b_setGameState('Searching')\n        pos = localAvatar.getPos(self)\n        angle = math.atan2(pos[0], pos[1])\n        radius = 4\n        localAvatar.setPos(self, math.sin(angle) * radius, math.cos(angle) * radius, 0)\n        localAvatar.headsUp(self)\n        localAvatar.setH(localAvatar, 0)\n\n    \n    def stopSearching(self, questProgress):\n        localAvatar.guiMgr.workMeter.stopTimer()\n        localAvatar.guiMgr.showQuestProgress(questProgress)\n        if localAvatar.getGameState() == 'Searching':\n            localAvatar.b_setGameState(localAvatar.gameFSM.defaultState)\n        \n        self.refreshState()\n\n    \n    def requestExit(self):\n        DistributedInteractive.requestExit(self)\n        self.stopSearching(0)\n\n    \n    def setContainerColor(self, r, g, b, a):\n        self.containerColorR = r\n        self.containerColorG = g\n        self.containerColorB = b\n        self.containerColorA = a\n\n    \n    def getContainerColor(self):\n        return (self.containerColorR, self.containerColorG, self.containerColorB, self.containerColorA)\n\n\n","label":1}
{"content":"# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Utilities similar to tf.python.platform.resource_loader.\"\"\"\n\nfrom distutils.version import LooseVersion\nimport os\nimport warnings\n\nimport tensorflow as tf\n\nINCLUSIVE_MIN_TF_VERSION_FOR_ABI_COMPATIBILITY = \"2.4.0\"\nEXCLUSIVE_MAX_TF_VERSION_FOR_ABI_COMPATIBILITY = \"2.5.0\"\nabi_warning_already_raised = False\nSKIP_CUSTOM_OPS = False\n\n\ndef get_project_root():\n    \"\"\"Returns project root folder.\"\"\"\n    return os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\ndef get_path_to_datafile(path, is_so=False):\n    \"\"\"Get the path to the specified file in the data dependencies.\n\n    The path is relative to tensorflow_addons\/\n\n    Args:\n      path: a string resource path relative to tensorflow_addons\/\n    Returns:\n      The path to the specified data file\n    \"\"\"\n    root_dir = get_project_root()\n    if is_so:\n        bazel_bin_dir = os.path.join(os.path.dirname(root_dir), \"bazel-bin\")\n        if os.path.isdir(bazel_bin_dir):\n            root_dir = os.path.join(bazel_bin_dir, \"tensorflow_addons\")\n    return os.path.join(root_dir, path.replace(\"\/\", os.sep))\n\n\nclass LazySO:\n    def __init__(self, relative_path):\n        self.relative_path = relative_path\n        self._ops = None\n\n    @property\n    def ops(self):\n        if SKIP_CUSTOM_OPS:\n            import pytest\n\n            pytest.skip(\n                \"Skipping the test because a custom ops \"\n                \"was being loaded while --skip-custom-ops was set.\"\n            )\n        if self._ops is None:\n            self.display_warning_if_incompatible()\n            self._ops = tf.load_op_library(\n                get_path_to_datafile(self.relative_path, is_so=True)\n            )\n        return self._ops\n\n    def display_warning_if_incompatible(self):\n        global abi_warning_already_raised\n        if abi_is_compatible() or abi_warning_already_raised:\n            return\n\n        warnings.warn(\n            \"You are currently using TensorFlow {} and trying to load a custom op ({}).\"\n            \"\\n\"\n            \"TensorFlow Addons has compiled its custom ops against TensorFlow {}, \"\n            \"and there are no compatibility guarantees between the two versions. \"\n            \"\\n\"\n            \"This means that you might get segfaults when loading the custom op, \"\n            \"or other kind of low-level errors.\\n If you do, do not file an issue \"\n            \"on Github. This is a known limitation.\"\n            \"\\n\\n\"\n            \"It might help you to fallback to pure Python \"\n            \"ops by setting environment variable `TF_ADDONS_PY_OPS=1` or using `tfa.options.disable_custom_kernel()` in your code. \"\n            \"To do that, see \"\n            \"https:\/\/github.com\/tensorflow\/addons#gpucpu-custom-ops \"\n            \"\\n\\n\"\n            \"You can also change the TensorFlow version installed on your system. \"\n            \"You would need a TensorFlow version equal to or above {} and strictly \"\n            \"below {}.\\n Note that nightly versions of TensorFlow, \"\n            \"as well as non-pip TensorFlow like `conda install tensorflow` or compiled \"\n            \"from source are not supported.\"\n            \"\\n\\n\"\n            \"The last solution is to find the TensorFlow Addons version that has \"\n            \"custom ops compatible with the TensorFlow installed on your \"\n            \"system. To do that, refer to the readme: \"\n            \"https:\/\/github.com\/tensorflow\/addons\"\n            \"\".format(\n                tf.__version__,\n                self.relative_path,\n                INCLUSIVE_MIN_TF_VERSION_FOR_ABI_COMPATIBILITY,\n                INCLUSIVE_MIN_TF_VERSION_FOR_ABI_COMPATIBILITY,\n                EXCLUSIVE_MAX_TF_VERSION_FOR_ABI_COMPATIBILITY,\n            ),\n            UserWarning,\n        )\n        abi_warning_already_raised = True\n\n\ndef abi_is_compatible():\n    if \"dev\" in tf.__version__:\n        # tf-nightly\n        return False\n\n    min_version = LooseVersion(INCLUSIVE_MIN_TF_VERSION_FOR_ABI_COMPATIBILITY)\n    max_version = LooseVersion(EXCLUSIVE_MAX_TF_VERSION_FOR_ABI_COMPATIBILITY)\n    return min_version <= LooseVersion(tf.__version__) < max_version\n","label":1}
{"content":"import asyncio\nimport ssl\nimport warnings\n\nimport aiohttp\n\nfrom . import CloudStack, CloudStackException\nfrom .client import transform\n\n\nclass AIOCloudStack(CloudStack):\n    def __getattr__(self, command):\n        def handler(**kwargs):\n            return self._request(command, **kwargs)\n        return handler\n\n    async def _request(self, command, json=True, opcode_name='command',\n                       fetch_list=False, headers=None, **params):\n        if \"fetch_result\" not in params:\n            warnings.warn(\"implicit job polling is deprecated. To pull the \"\n                          \"job result in future releases, please pass \"\n                          \"fetch_result=True\")\n        fetch_result = params.pop(\"fetch_result\", True)\n        kwarg, kwargs = self._prepare_request(command, json, opcode_name,\n                                              fetch_list, **params)\n\n        ssl_context = None\n        if self.cert:\n            ssl_context = ssl.create_default_context(cafile=self.cert)\n        connector = aiohttp.TCPConnector(verify_ssl=self.verify,\n                                         ssl_context=ssl_context)\n\n        async with aiohttp.ClientSession(read_timeout=self.timeout,\n                                         conn_timeout=self.timeout,\n                                         connector=connector) as session:\n            handler = getattr(session, self.method)\n\n            done = False\n            final_data = []\n            page = 1\n            while not done:\n                if fetch_list:\n                    kwargs['page'] = page\n\n                transform(kwargs)\n                kwargs.pop('signature', None)\n                kwargs['signature'] = self._sign(kwargs)\n                response = await handler(self.endpoint,\n                                         headers=headers,\n                                         **{kwarg: kwargs})\n\n                ctype = response.headers['content-type'].split(';')[0]\n                try:\n                    data = await response.json(content_type=ctype)\n                except ValueError as e:\n                    msg = \"Make sure endpoint URL {!r} is correct.\".format(\n                        self.endpoint)\n                    raise CloudStackException(\n                        \"HTTP {0} response from CloudStack\".format(\n                            response.status),\n                        response,\n                        \"{}. {}\".format(e, msg))\n\n                [key] = data.keys()\n                data = data[key]\n                if response.status != 200:\n                    raise CloudStackException(\n                        \"HTTP {0} response from CloudStack\".format(\n                            response.status), response, data)\n                if fetch_list:\n                    try:\n                        [key] = [k for k in data.keys() if k != 'count']\n                    except ValueError:\n                        done = True\n                    else:\n                        final_data.extend(data[key])\n                        page += 1\n                elif fetch_result and 'jobid' in data:\n                    try:\n                        final_data = await asyncio.wait_for(\n                            self._jobresult(data['jobid']),\n                            self.job_timeout)\n                    except asyncio.TimeoutError:\n                        raise CloudStackException(\n                            \"Timeout waiting for async job result\",\n                            data['jobid'])\n                    done = True\n                else:\n                    final_data = data\n                    done = True\n            return final_data\n\n    async def _jobresult(self, jobid):\n        failures = 0\n        while True:\n            try:\n                j = await self.queryAsyncJobResult(jobid=jobid)\n                failures = 0\n                if j['jobstatus'] != 0:\n                    if j['jobresultcode'] != 0 or j['jobstatus'] != 1:\n                        raise CloudStackException(\"Job failure\", j)\n                    if 'jobresult' not in j:\n                        raise CloudStackException(\"Unknown job result\", j)\n                    return j['jobresult']\n\n            except CloudStackException:\n                raise\n\n            except Exception:\n                failures += 1\n                if failures > 10:\n                    raise\n\n            await asyncio.sleep(self.poll_interval)\n","label":1}
{"content":"# -*- coding: utf-8 -*-\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport abc\nfrom typing import Awaitable, Callable, Dict, Optional, Sequence, Union\nimport pkg_resources\n\nimport google.auth  # type: ignore\nimport google.api_core\nfrom google.api_core import exceptions as core_exceptions\nfrom google.api_core import gapic_v1\nfrom google.api_core import retry as retries\nfrom google.api_core import operations_v1\nfrom google.auth import credentials as ga_credentials  # type: ignore\nfrom google.oauth2 import service_account  # type: ignore\n\nfrom google.cloud.artifactregistry_v1beta2.types import file\nfrom google.cloud.artifactregistry_v1beta2.types import package\nfrom google.cloud.artifactregistry_v1beta2.types import repository\nfrom google.cloud.artifactregistry_v1beta2.types import repository as gda_repository\nfrom google.cloud.artifactregistry_v1beta2.types import tag\nfrom google.cloud.artifactregistry_v1beta2.types import tag as gda_tag\nfrom google.cloud.artifactregistry_v1beta2.types import version\nfrom google.iam.v1 import iam_policy_pb2  # type: ignore\nfrom google.iam.v1 import policy_pb2  # type: ignore\nfrom google.longrunning import operations_pb2  # type: ignore\nfrom google.protobuf import empty_pb2  # type: ignore\n\ntry:\n    DEFAULT_CLIENT_INFO = gapic_v1.client_info.ClientInfo(\n        gapic_version=pkg_resources.get_distribution(\n            \"google-cloud-artifact-registry\",\n        ).version,\n    )\nexcept pkg_resources.DistributionNotFound:\n    DEFAULT_CLIENT_INFO = gapic_v1.client_info.ClientInfo()\n\n\nclass ArtifactRegistryTransport(abc.ABC):\n    \"\"\"Abstract transport class for ArtifactRegistry.\"\"\"\n\n    AUTH_SCOPES = (\n        \"https:\/\/www.googleapis.com\/auth\/cloud-platform\",\n        \"https:\/\/www.googleapis.com\/auth\/cloud-platform.read-only\",\n    )\n\n    DEFAULT_HOST: str = \"artifactregistry.googleapis.com\"\n\n    def __init__(\n        self,\n        *,\n        host: str = DEFAULT_HOST,\n        credentials: ga_credentials.Credentials = None,\n        credentials_file: Optional[str] = None,\n        scopes: Optional[Sequence[str]] = None,\n        quota_project_id: Optional[str] = None,\n        client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,\n        always_use_jwt_access: Optional[bool] = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Instantiate the transport.\n\n        Args:\n            host (Optional[str]):\n                 The hostname to connect to.\n            credentials (Optional[google.auth.credentials.Credentials]): The\n                authorization credentials to attach to requests. These\n                credentials identify the application to the service; if none\n                are specified, the client will attempt to ascertain the\n                credentials from the environment.\n            credentials_file (Optional[str]): A file with credentials that can\n                be loaded with :func:`google.auth.load_credentials_from_file`.\n                This argument is mutually exclusive with credentials.\n            scopes (Optional[Sequence[str]]): A list of scopes.\n            quota_project_id (Optional[str]): An optional project to use for billing\n                and quota.\n            client_info (google.api_core.gapic_v1.client_info.ClientInfo):\n                The client info used to send a user-agent string along with\n                API requests. If ``None``, then default info will be used.\n                Generally, you only need to set this if you're developing\n                your own client library.\n            always_use_jwt_access (Optional[bool]): Whether self signed JWT should\n                be used for service account credentials.\n        \"\"\"\n        # Save the hostname. Default to port 443 (HTTPS) if none is specified.\n        if \":\" not in host:\n            host += \":443\"\n        self._host = host\n\n        scopes_kwargs = {\"scopes\": scopes, \"default_scopes\": self.AUTH_SCOPES}\n\n        # Save the scopes.\n        self._scopes = scopes\n\n        # If no credentials are provided, then determine the appropriate\n        # defaults.\n        if credentials and credentials_file:\n            raise core_exceptions.DuplicateCredentialArgs(\n                \"'credentials_file' and 'credentials' are mutually exclusive\"\n            )\n\n        if credentials_file is not None:\n            credentials, _ = google.auth.load_credentials_from_file(\n                credentials_file, **scopes_kwargs, quota_project_id=quota_project_id\n            )\n        elif credentials is None:\n            credentials, _ = google.auth.default(\n                **scopes_kwargs, quota_project_id=quota_project_id\n            )\n\n        # If the credentials are service account credentials, then always try to use self signed JWT.\n        if (\n            always_use_jwt_access\n            and isinstance(credentials, service_account.Credentials)\n            and hasattr(service_account.Credentials, \"with_always_use_jwt_access\")\n        ):\n            credentials = credentials.with_always_use_jwt_access(True)\n\n        # Save the credentials.\n        self._credentials = credentials\n\n    def _prep_wrapped_messages(self, client_info):\n        # Precompute the wrapped methods.\n        self._wrapped_methods = {\n            self.list_repositories: gapic_v1.method.wrap_method(\n                self.list_repositories,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.get_repository: gapic_v1.method.wrap_method(\n                self.get_repository,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.create_repository: gapic_v1.method.wrap_method(\n                self.create_repository, default_timeout=30.0, client_info=client_info,\n            ),\n            self.update_repository: gapic_v1.method.wrap_method(\n                self.update_repository, default_timeout=30.0, client_info=client_info,\n            ),\n            self.delete_repository: gapic_v1.method.wrap_method(\n                self.delete_repository,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.list_packages: gapic_v1.method.wrap_method(\n                self.list_packages,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.get_package: gapic_v1.method.wrap_method(\n                self.get_package,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.delete_package: gapic_v1.method.wrap_method(\n                self.delete_package,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.list_versions: gapic_v1.method.wrap_method(\n                self.list_versions,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.get_version: gapic_v1.method.wrap_method(\n                self.get_version,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.delete_version: gapic_v1.method.wrap_method(\n                self.delete_version,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.list_files: gapic_v1.method.wrap_method(\n                self.list_files,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.get_file: gapic_v1.method.wrap_method(\n                self.get_file,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.list_tags: gapic_v1.method.wrap_method(\n                self.list_tags,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.get_tag: gapic_v1.method.wrap_method(\n                self.get_tag,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.create_tag: gapic_v1.method.wrap_method(\n                self.create_tag, default_timeout=30.0, client_info=client_info,\n            ),\n            self.update_tag: gapic_v1.method.wrap_method(\n                self.update_tag, default_timeout=30.0, client_info=client_info,\n            ),\n            self.delete_tag: gapic_v1.method.wrap_method(\n                self.delete_tag,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.set_iam_policy: gapic_v1.method.wrap_method(\n                self.set_iam_policy, default_timeout=None, client_info=client_info,\n            ),\n            self.get_iam_policy: gapic_v1.method.wrap_method(\n                self.get_iam_policy,\n                default_retry=retries.Retry(\n                    initial=0.1,\n                    maximum=60.0,\n                    multiplier=1.3,\n                    predicate=retries.if_exception_type(\n                        core_exceptions.ServiceUnavailable,\n                    ),\n                    deadline=30.0,\n                ),\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n            self.test_iam_permissions: gapic_v1.method.wrap_method(\n                self.test_iam_permissions,\n                default_timeout=30.0,\n                client_info=client_info,\n            ),\n        }\n\n    def close(self):\n        \"\"\"Closes resources associated with the transport.\n\n       .. warning::\n            Only call this method if the transport is NOT shared\n            with other clients - this may cause errors in other clients!\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def operations_client(self):\n        \"\"\"Return the client designed to process long-running operations.\"\"\"\n        raise NotImplementedError()\n\n    @property\n    def list_repositories(\n        self,\n    ) -> Callable[\n        [repository.ListRepositoriesRequest],\n        Union[\n            repository.ListRepositoriesResponse,\n            Awaitable[repository.ListRepositoriesResponse],\n        ],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def get_repository(\n        self,\n    ) -> Callable[\n        [repository.GetRepositoryRequest],\n        Union[repository.Repository, Awaitable[repository.Repository]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def create_repository(\n        self,\n    ) -> Callable[\n        [gda_repository.CreateRepositoryRequest],\n        Union[operations_pb2.Operation, Awaitable[operations_pb2.Operation]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def update_repository(\n        self,\n    ) -> Callable[\n        [gda_repository.UpdateRepositoryRequest],\n        Union[gda_repository.Repository, Awaitable[gda_repository.Repository]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def delete_repository(\n        self,\n    ) -> Callable[\n        [repository.DeleteRepositoryRequest],\n        Union[operations_pb2.Operation, Awaitable[operations_pb2.Operation]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def list_packages(\n        self,\n    ) -> Callable[\n        [package.ListPackagesRequest],\n        Union[package.ListPackagesResponse, Awaitable[package.ListPackagesResponse]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def get_package(\n        self,\n    ) -> Callable[\n        [package.GetPackageRequest], Union[package.Package, Awaitable[package.Package]]\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def delete_package(\n        self,\n    ) -> Callable[\n        [package.DeletePackageRequest],\n        Union[operations_pb2.Operation, Awaitable[operations_pb2.Operation]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def list_versions(\n        self,\n    ) -> Callable[\n        [version.ListVersionsRequest],\n        Union[version.ListVersionsResponse, Awaitable[version.ListVersionsResponse]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def get_version(\n        self,\n    ) -> Callable[\n        [version.GetVersionRequest], Union[version.Version, Awaitable[version.Version]]\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def delete_version(\n        self,\n    ) -> Callable[\n        [version.DeleteVersionRequest],\n        Union[operations_pb2.Operation, Awaitable[operations_pb2.Operation]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def list_files(\n        self,\n    ) -> Callable[\n        [file.ListFilesRequest],\n        Union[file.ListFilesResponse, Awaitable[file.ListFilesResponse]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def get_file(\n        self,\n    ) -> Callable[[file.GetFileRequest], Union[file.File, Awaitable[file.File]]]:\n        raise NotImplementedError()\n\n    @property\n    def list_tags(\n        self,\n    ) -> Callable[\n        [tag.ListTagsRequest],\n        Union[tag.ListTagsResponse, Awaitable[tag.ListTagsResponse]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def get_tag(\n        self,\n    ) -> Callable[[tag.GetTagRequest], Union[tag.Tag, Awaitable[tag.Tag]]]:\n        raise NotImplementedError()\n\n    @property\n    def create_tag(\n        self,\n    ) -> Callable[\n        [gda_tag.CreateTagRequest], Union[gda_tag.Tag, Awaitable[gda_tag.Tag]]\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def update_tag(\n        self,\n    ) -> Callable[\n        [gda_tag.UpdateTagRequest], Union[gda_tag.Tag, Awaitable[gda_tag.Tag]]\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def delete_tag(\n        self,\n    ) -> Callable[\n        [tag.DeleteTagRequest], Union[empty_pb2.Empty, Awaitable[empty_pb2.Empty]]\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def set_iam_policy(\n        self,\n    ) -> Callable[\n        [iam_policy_pb2.SetIamPolicyRequest],\n        Union[policy_pb2.Policy, Awaitable[policy_pb2.Policy]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def get_iam_policy(\n        self,\n    ) -> Callable[\n        [iam_policy_pb2.GetIamPolicyRequest],\n        Union[policy_pb2.Policy, Awaitable[policy_pb2.Policy]],\n    ]:\n        raise NotImplementedError()\n\n    @property\n    def test_iam_permissions(\n        self,\n    ) -> Callable[\n        [iam_policy_pb2.TestIamPermissionsRequest],\n        Union[\n            iam_policy_pb2.TestIamPermissionsResponse,\n            Awaitable[iam_policy_pb2.TestIamPermissionsResponse],\n        ],\n    ]:\n        raise NotImplementedError()\n\n\n__all__ = (\"ArtifactRegistryTransport\",)\n","label":1}
{"content":"import os\nfrom io import StringIO\n\nfrom django.core.management import call_command\nfrom django.test import RequestFactory, TestCase\n\nfrom rr.models.attribute import Attribute\nfrom rr.models.serviceprovider import ServiceProvider, SPAttribute\n\nVALID_DATA = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<AttributeFilterPolicyGroup xmlns=\"urn:mace:shibboleth:2.0:afp\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" id=\"urn:mace:funet.fi:haka\" xmlns:basic=\"urn:mace:shibboleth:2.0:afp:mf:basic\" xmlns:saml=\"urn:mace:shibboleth:2.0:afp:mf:saml\" xsi:schemaLocation=\"urn:mace:shibboleth:2.0:afp classpath:\/schema\/shibboleth-2.0-afp.xsd urn:mace:shibboleth:2.0:afp:mf:basic classpath:\/schema\/shibboleth-2.0-afp-mf-basic.xsd urn:mace:shibboleth:2.0:afp:mf:saml classpath:\/schema\/shibboleth-2.0-afp-mf-saml.xsd\">\n  <AttributeFilterPolicy id=\"hy-default-test:entity:1\">\n    <PolicyRequirementRule value=\"test:entity:1\" xsi:type=\"basic:AttributeRequesterString\"\/>\n    <AttributeRule attributeID=\"id-urn:mace:dir:attribute-def:eduPersonPrincipalName\">\n      <PermitValueRule xsi:type=\"basic:ANY\"\/>\n    <\/AttributeRule>\n    <AttributeRule attributeID=\"id-urn:mace:terena.org:schac:attribute-def:schacPersonalUniqueID\">\n      <PermitValueRule xsi:type=\"basic:ANY\"\/>\n    <\/AttributeRule>\n  <\/AttributeFilterPolicy>\n<\/AttributeFilterPolicyGroup>\n\"\"\"\n\n\nclass ExportMetadataTest(TestCase):\n    def setUp(self):\n        self.factory = RequestFactory()\n        self.sp = ServiceProvider.objects.create(entity_id='test:entity:1', service_type='saml', production=True)\n        self.attr_eppn = Attribute.objects.create(friendlyname='eduPersonPrincipalName',\n                                                  name='urn:oid:1.3.6.1.4.1.5923.1.1.1.6',\n                                                  attributeid='id-urn:mace:dir:attribute-def:eduPersonPrincipalName',\n                                                  nameformat='urn:oasis:names:tc:SAML:2.0:attrname-format:uri',\n                                                  public_saml=True,\n                                                  public_ldap=False)\n        self.attr_uniqueid = Attribute.objects.create(friendlyname='schacPersonalUniqueID',\n                                        name='urn:oid:1.3.6.1.4.1.25178.1.2.15',\n                                        attributeid='id-urn:mace:terena.org:schac:attribute-def:schacPersonalUniqueID',\n                                        nameformat='urn:oasis:names:tc:SAML:2.0:attrname-format:uri',\n                                        public_saml=False,\n                                        public_ldap=False)\n        SPAttribute.objects.create(attribute=self.attr_eppn, sp=self.sp, reason='User identification')\n        SPAttribute.objects.create(attribute=self.attr_uniqueid, sp=self.sp, reason='User identification')\n\n    def test_exportmetadata(self):\n        out = StringIO()\n        call_command('exportattributefilter', '-p', '-u', stdout=out)\n        self.assertEqual(VALID_DATA, out.getvalue())\n","label":1}
{"content":"#!\/usr\/bin\/env python\n# -*- coding: utf-8 -*-\n# @Time    : 5\/15\/20 4:49 PM\n# @File    : grover.py\n\n# qubit number=4\n# total number=15\nimport cirq\nimport cirq.google as cg\nfrom typing import Optional\nimport sys\nfrom math import log2\nimport numpy as np\n\n#thatsNoCode\n\ndef make_circuit(n: int, input_qubit):\n    c = cirq.Circuit()  # circuit begin\n\n    c.append(cirq.H.on(input_qubit[0])) # number=1\n    c.append(cirq.H.on(input_qubit[1]))  # number=2\n    c.append(cirq.H.on(input_qubit[1])) # number=7\n    c.append(cirq.H.on(input_qubit[2]))  # number=3\n    c.append(cirq.H.on(input_qubit[3]))  # number=4\n    c.append(cirq.CNOT.on(input_qubit[3],input_qubit[0])) # number=5\n    c.append(cirq.H.on(input_qubit[0])) # number=10\n    c.append(cirq.CZ.on(input_qubit[3],input_qubit[0])) # number=11\n    c.append(cirq.H.on(input_qubit[0])) # number=12\n    c.append(cirq.CNOT.on(input_qubit[2],input_qubit[0])) # number=8\n    c.append(cirq.CNOT.on(input_qubit[2],input_qubit[0])) # number=9\n    c.append(cirq.Y.on(input_qubit[2])) # number=13\n    c.append(cirq.Y.on(input_qubit[2])) # number=14\n    # circuit end\n\n\n    return c\n\ndef bitstring(bits):\n    return ''.join(str(int(b)) for b in bits)\n\nif __name__ == '__main__':\n    qubit_count = 4\n\n    input_qubits = [cirq.GridQubit(i, 0) for i in range(qubit_count)]\n    circuit = make_circuit(qubit_count,input_qubits)\n    circuit = cg.optimized_for_sycamore(circuit, optimizer_type='sqrt_iswap')\n\n    circuit_sample_count =0\n\n    info = cirq.final_state_vector(circuit)\n\n    qubits = round(log2(len(info)))\n    frequencies = {\n        np.binary_repr(i, qubits): round((info[i]*(info[i].conjugate())).real,3)\n        for i in range(2 ** qubits)\n    }\n    writefile = open(\"..\/data\/startCirq_Class204.csv\",\"w+\")\n\n    print(format(frequencies),file=writefile)\n    print(\"results end\", file=writefile)\n\n    print(circuit.__len__(), file=writefile)\n    print(circuit,file=writefile)\n\n\n    writefile.close()","label":1}
{"content":"from typing import Optional\n\nfrom botapi import Field\n\nfrom .keyboard import Keyboard\nfrom .message import Message\n\n\nclass VideoMessage(Message):\n    \"\"\"Represents a Viber video message object\n\n    Params:\n    https:\/\/developers.viber.com\/docs\/api\/rest-bot-api\/#video-message\n    \"\"\"\n\n    message_type = Field(default='video', alias='type')\n    media = Field(base=str)\n    size = Field(base=int)\n    duration = Field(base=int)\n    thumbnail = Field(base=str)\n\n    def __init__(\n        self,\n        media: str,\n        size: int,\n        duration: Optional[int] = None,\n        thumbnail: Optional[str] = None,\n        tracking_data: Optional[str] = None,\n        keyboard: Optional[Keyboard] = None,\n        **kwargs\n    ):\n        super().__init__(\n            media=media,\n            size=size,\n            duration=duration,\n            thumbnail=thumbnail,\n            tracking_data=tracking_data,\n            keyboard=keyboard,\n            **kwargs\n        )\n","label":1}
{"content":"import sys\nimport numpy as np\n\ndef main():\n    script = sys.argv[0]\n    action = sys.argv[1]\n    filenames = sys.argv[2:]\n    assert action in ['--min', '--mean', '--max'], \\\n           'Action is not one of --min, --mean, or --max: ' + action\n    if len(filenames) == 0:\n        process(sys.stdin, action)\n    else:\n        for f in filenames:\n            process(f, action)\n\ndef process(filename, action):\n    data = np.loadtxt(filename, delimiter=',')\n\n    if action == '--min':\n        values = data.min(axis=1)\n    elif action == '--mean':\n        values = data.mean(axis=1)\n    elif action == '--max':\n        values = data.max(axis=1)\n\n    for m in values:\n        print m\n\nmain()\n","label":1}
{"content":"import threading\nimport datetime\nfrom queue import Queue\nfrom random import randint\nimport re\nimport sys\nimport traceback\nimport inspect\nfrom datetime import timedelta\nimport logging\n\nfrom appdaemon import utils as utils\nfrom appdaemon.appdaemon import AppDaemon\n\nclass Threading:\n\n    def __init__(self, ad: AppDaemon, kwargs):\n\n        self.AD = ad\n        self.kwargs = kwargs\n\n        self.logger = ad.logging.get_child(\"_threading\")\n        self.diag = ad.logging.get_diag()\n        self.thread_count = 0\n\n        self.threads = {}\n\n        # A few shortcuts\n\n        self.add_entity = ad.state.add_entity\n        self.get_state = ad.state.get_state\n        self.set_state = ad.state.set_state\n        self.add_to_state = ad.state.add_to_state\n        self.add_to_attr = ad.state.add_to_attr\n\n        self.auto_pin = True\n        self.pin_threads = 0\n        self.total_threads = 0\n\n        # Setup stats\n\n        self.current_callbacks_executed = 0\n        self.current_callbacks_fired = 0\n\n        self.last_stats_time = datetime.datetime(1970, 1, 1, 0, 0, 0, 0)\n        self.callback_list = []\n\n    async def get_callback_update(self):\n        now = datetime.datetime.now()\n        self.callback_list.append(\n            {\n                \"fired\": self.current_callbacks_fired,\n                \"executed\": self.current_callbacks_executed,\n                \"ts\": now\n            })\n\n        if len(self.callback_list) > 10:\n            self.callback_list.pop(0)\n\n        fired_sum = 0\n        executed_sum = 0\n        for item in self.callback_list:\n            fired_sum += item[\"fired\"]\n            executed_sum += item[\"executed\"]\n\n        total_duration = (self.callback_list[len(self.callback_list) -1][\"ts\"] - self.callback_list[0][\"ts\"]).total_seconds()\n\n        if total_duration == 0:\n            fired_avg = 0\n            executed_avg = 0\n        else:\n            fired_avg = round(fired_sum \/ total_duration, 1)\n            executed_avg = round(executed_sum \/ total_duration, 1)\n\n        await self.set_state(\"_threading\", \"admin\", \"sensor.callbacks_average_fired\", state=fired_avg)\n        await self.set_state(\"_threading\", \"admin\", \"sensor.callbacks_average_executed\", state=executed_avg)\n\n        self.last_stats_time = now\n        self.current_callbacks_executed = 0\n        self.current_callbacks_fired = 0\n\n    async def init_admin_stats(self):\n\n        # Initialize admin stats\n\n        await self.add_entity(\"admin\", \"sensor.callbacks_total_fired\", 0)\n        await self.add_entity(\"admin\", \"sensor.callbacks_average_fired\", 0)\n        await self.add_entity(\"admin\", \"sensor.callbacks_total_executed\", 0)\n        await self.add_entity(\"admin\", \"sensor.callbacks_average_executed\", 0)\n        await self.add_entity(\"admin\", \"sensor.threads_current_busy\", 0)\n        await self.add_entity(\"admin\", \"sensor.threads_max_busy\", 0)\n        await self.add_entity(\"admin\", \"sensor.threads_max_busy_time\", utils.dt_to_str(datetime.datetime(1970, 1, 1, 0, 0, 0, 0)))\n        await self.add_entity(\"admin\", \"sensor.threads_last_action_time\", utils.dt_to_str(datetime.datetime(1970, 1, 1, 0, 0, 0, 0)))\n\n    async def create_initial_threads(self):\n        kwargs = self.kwargs\n\n        if \"threads\" in kwargs:\n            self.logger.warning(\n                     \"Threads directive is deprecated apps - will be pinned. Use total_threads if you want to unpin your apps\")\n\n        if \"total_threads\" in kwargs:\n            self.total_threads = kwargs[\"total_threads\"]\n            self.auto_pin = False\n        else:\n            apps = await self.AD.app_management.check_config(True, False)\n            self.total_threads = int(apps[\"active\"])\n\n        self.pin_apps = True\n        utils.process_arg(self, \"pin_apps\", kwargs)\n\n        if self.pin_apps is True:\n            self.pin_threads = self.total_threads\n        else:\n            self.auto_pin = False\n            self.pin_threads = 0\n            if \"total_threads\" not in kwargs:\n                self.total_threads = 10\n\n        utils.process_arg(self, \"pin_threads\", kwargs, int=True)\n\n        if self.pin_threads > self.total_threads:\n            raise ValueError(\"pin_threads cannot be > total_threads\")\n\n        if self.pin_threads < 0:\n            raise ValueError(\"pin_threads cannot be < 0\")\n\n        self.logger.info(\"Starting Apps with %s workers and %s pins\", self.total_threads, self.pin_threads)\n\n        self.next_thread = self.pin_threads\n\n        self.thread_count = 0\n        for i in range(self.total_threads):\n            await self.add_thread(True)\n\n    def get_q(self, thread_id):\n        return self.threads[thread_id][\"queue\"]\n\n    @staticmethod\n    def atoi(text):\n        return int(text) if text.isdigit() else text\n\n    def natural_keys(self, text):\n        return [self.atoi(c) for c in re.split('(\\d+)', text)]\n\n    # Diagnostics\n\n    def total_q_size(self):\n        qsize = 0\n        for thread in self.threads:\n            qsize += self.threads[thread][\"queue\"].qsize()\n        return qsize\n\n    def min_q_id(self):\n        id = 0\n        i = 0\n        qsize = sys.maxsize\n        for thread in self.threads:\n            if self.threads[thread][\"queue\"].qsize() < qsize:\n                qsize = self.threads[thread][\"queue\"].qsize()\n                id = i\n            i += 1\n        return id\n\n    async def dump_threads(self):\n        self.diag.info(\"--------------------------------------------------\")\n        self.diag.info(\"Threads\")\n        self.diag.info(\"--------------------------------------------------\")\n        current_busy = await self.get_state(\"_threading\", \"admin\", \"sensor.threads_current_busy\")\n        max_busy = await self.get_state(\"_threading\", \"admin\", \"sensor.threads_max_busy\")\n        max_busy_time = utils.str_to_dt(await self.get_state(\"_threading\", \"admin\", \"sensor.threads_max_busy_time\"))\n        last_action_time = await self.get_state(\"_threading\", \"admin\", \"sensor.threads_last_action_time\")\n        self.diag.info(\"Currently busy threads: %s\", current_busy)\n        self.diag.info(\"Most used threads: %s at %s\", max_busy, max_busy_time)\n        self.diag.info(\"Last activity: %s\", last_action_time)\n        self.diag.info(\"Total Q Entries: %s\", self.total_q_size())\n        self.diag.info(\"--------------------------------------------------\")\n        for thread in sorted(self.threads, key=self.natural_keys):\n            t = await self.get_state(\"_threading\", \"admin\", \"thread.{}\".format(thread), attribute=\"all\")\n            print(\"thread.{}\".format(thread), t)\n            self.diag.info(\n                     \"%s - qsize: %s | current callback: %s | since %s, | alive: %s, | pinned apps: %s\",\n                         thread,\n                         t[\"attributes\"][\"q\"],\n                         t[\"state\"],\n                         t[\"attributes\"][\"time_called\"],\n                         t[\"attributes\"][\"is_alive\"],\n                         await self.get_pinned_apps(thread)\n                     )\n        self.diag.info(\"--------------------------------------------------\")\n\n    #\n    # Thread Management\n    #\n\n    def select_q(self, args):\n        #\n        # Select Q based on distribution method:\n        #   Round Robin\n        #   Random\n        #   Load distribution\n        #\n\n        # Check for pinned app and if so figure correct thread for app\n\n        if args[\"pin_app\"] is True:\n            thread = args[\"pin_thread\"]\n            # Handle the case where an App is unpinned but selects a pinned callback without specifying a thread\n            # If this happens a lot, thread 0 might get congested but the alternatives are worse!\n            if thread == -1:\n                self.logger.warning(\"Invalid thread ID for pinned thread in app: %s - assigning to thread 0\", args[\"name\"])\n                thread = 0\n        else:\n            if self.thread_count == self.pin_threads:\n                raise ValueError(\"pin_threads must be set lower than threads if unpinned_apps are in use\")\n            if self.AD.load_distribution == \"load\":\n                thread = self.min_q_id()\n            elif self.AD.load_distribution == \"random\":\n                thread = randint(self.pin_threads, self.thread_count - 1)\n            else:\n                # Round Robin is the catch all\n                thread = self.next_thread\n                self.next_thread += 1\n                if self.next_thread == self.thread_count:\n                    self.next_thread = self.pin_threads\n\n        if thread < 0 or thread >= self.thread_count:\n            raise ValueError(\"invalid thread id: {} in app {}\".format(thread, args[\"name\"]))\n\n        id = \"thread-{}\".format(thread)\n        q = self.threads[id][\"queue\"]\n\n        q.put_nowait(args)\n\n    async def check_overdue_and_dead_threads(self):\n        if self.AD.sched.realtime is True and self.AD.thread_duration_warning_threshold != 0:\n            for thread_id in self.threads:\n                if self.threads[thread_id][\"thread\"].isAlive() is not True:\n                    self.logger.critical(\"Thread %s has died\", thread_id)\n                    self.logger.critical(\"Pinned apps were: %s\", await self.get_pinned_apps(thread_id))\n                    self.logger.critical(\"Thread will be restarted\")\n                    id=thread_id.split(\"-\")[1]\n                    await self.add_thread(silent=False, pinthread=False, id=id)\n                if await self.get_state(\"_threading\", \"admin\", \"thread.{}\".format(thread_id)) != \"idle\":\n                    start = utils.str_to_dt(await self.get_state(\"_threading\", \"admin\", \"thread.{}\".format(thread_id), attribute=\"time_called\"))\n                    dur = (await self.AD.sched.get_now() - start).total_seconds()\n                    if dur >= self.AD.thread_duration_warning_threshold and dur % self.AD.thread_duration_warning_threshold == 0:\n                        self.logger.warning(\"Excessive time spent in callback: %s - %s\",\n                                            await self.get_state(\"_threading\", \"admin\", \"thread.{}\".format(thread_id),\n                                                           attribute=\"callback\")\n                                            , dur)\n\n    async def check_q_size(self, warning_step, warning_iterations):\n        if self.total_q_size() > self.AD.qsize_warning_threshold:\n            if (warning_step == 0 and warning_iterations >= self.AD.qsize_warning_iterations) or warning_iterations == self.AD.qsize_warning_iterations:\n                self.logger.warning(\"Queue size is %s, suspect thread starvation\", self.total_q_size())\n                await self.dump_threads()\n                warning_step = 0\n            warning_step += 1\n            warning_iterations += 1\n            if warning_step >= self.AD.qsize_warning_step:\n                warning_step = 0\n        else:\n            warning_step = 0\n            warning_iterations = 0\n\n        return warning_step, warning_iterations\n\n    async def update_thread_info(self, thread_id, callback, app, type, uuid):\n        self.logger.debug(\"Update thread info: %s\", thread_id)\n        if self.AD.log_thread_actions:\n            if callback == \"idle\":\n                self.diag.info(\n                         \"%s done\", thread_id)\n            else:\n                self.diag.info(\n                         \"%s calling %s callback %s\", thread_id, type, callback)\n\n        now = await self.AD.sched.get_now()\n        if callback == \"idle\":\n            start = utils.str_to_dt(await self.get_state(\"_threading\", \"admin\", \"thread.{}\".format(thread_id), attribute=\"time_called\"))\n            if self.AD.sched.realtime is True and (now - start).total_seconds() >= self.AD.thread_duration_warning_threshold:\n                self.logger.warning(\"callback %s has now completed\", await self.get_state(\"_threading\", \"admin\", \"thread.{}\".format(thread_id)))\n            await self.add_to_state(\"_threading\", \"admin\", \"sensor.threads_current_busy\", -1)\n            await self.add_to_attr(\"_threading\", \"admin\", \"app.{}\".format(app), \"callbacks\", 1)\n            await self.add_to_attr(\"_threading\", \"admin\", \"{}_callback.{}\".format(type, uuid), \"executed\", 1)\n            await self.add_to_state(\"_threading\", \"admin\", \"sensor.callbacks_total_executed\", 1)\n            self.current_callbacks_executed += 1\n        else:\n            await self.add_to_state(\"_threading\", \"admin\", \"sensor.threads_current_busy\", 1)\n            self.current_callbacks_fired += 1\n\n        current_busy = await self.get_state(\"_threading\", \"admin\", \"sensor.threads_current_busy\")\n        max_busy = await self.get_state(\"_threading\", \"admin\", \"sensor.threads_max_busy\")\n        if current_busy > max_busy:\n            await self.set_state(\"_threading\", \"admin\", \"sensor.threads_max_busy\" , state=current_busy)\n            await self.set_state(\"_threading\", \"admin\", \"sensor.threads_max_busy_time\", state=utils.dt_to_str((await self.AD.sched.get_now()).replace(microsecond=0), self.AD.tz))\n\n            await self.set_state(\"_threading\", \"admin\", \"sensor.threads_last_action_time\", state=utils.dt_to_str((await self.AD.sched.get_now()).replace(microsecond=0), self.AD.tz))\n\n        # Update thread info\n\n        await self.set_state(\"_threading\", \"admin\", \"thread.{}\".format(thread_id),\n                             q=self.threads[thread_id][\"queue\"].qsize(),\n                             state=callback,\n                             time_called=utils.dt_to_str(now.replace(microsecond=0), self.AD.tz),\n                             is_alive = self.threads[thread_id][\"thread\"].is_alive(),\n                             pinned_apps=await self.get_pinned_apps(thread_id)\n                             )\n        await self.set_state(\"_threading\", \"admin\", \"app.{}\".format(app), state=callback)\n\n    #\n    # Pinning\n    #\n\n    async def add_thread(self, silent=False, pinthread=False, id=None):\n        if id is None:\n            tid = self.thread_count\n        else:\n            tid = id\n        if silent is False:\n            self.logger.info(\"Adding thread %s\", tid)\n        t = threading.Thread(target=self.worker)\n        t.daemon = True\n        name = \"thread-{}\".format(tid)\n        t.setName(name)\n        if id is None:\n            await self.add_entity(\"admin\", \"thread.{}\".format(name), \"idle\",\n                                 {\n                                     \"q\": 0,\n                                     \"is_alive\": True,\n                                     \"time_called\": utils.dt_to_str(datetime.datetime(1970, 1, 1, 0, 0, 0, 0)),\n                                 }\n                                 )\n            self.threads[name] = {}\n            self.threads[name][\"queue\"] = Queue(maxsize=0)\n            t.start()\n            self.thread_count += 1\n            if pinthread is True:\n                self.pin_threads += 1\n        else:\n            await self.set_state(\"_threading\", \"admin\", \"thread.{}\".format(name), state=\"idle\", is_alive=True)\n\n        self.threads[name][\"thread\"] = t\n\n\n    async def calculate_pin_threads(self):\n\n        if self.pin_threads == 0:\n            return\n\n        thread_pins = [0] * self.pin_threads\n        for name in self.AD.app_management.objects:\n            # Looking for apps that already have a thread pin value\n            if await self.get_app_pin(name) and await self.get_pin_thread(name) != -1:\n                thread = await self.get_pin_thread(name)\n                if thread >= self.thread_count:\n                    raise ValueError(\"Pinned thread out of range - check apps.yaml for 'pin_thread' or app code for 'set_pin_thread()'\")\n                # Ignore anything outside the pin range as it will have been set by the user\n                if thread < self.pin_threads:\n                    thread_pins[thread] += 1\n\n        # Now we know the numbers, go fill in the gaps\n\n        for name in self.AD.app_management.objects:\n            if await self.get_app_pin(name) and await self.get_pin_thread(name) == -1:\n                thread = thread_pins.index(min(thread_pins))\n                await self.set_pin_thread(name, thread)\n                thread_pins[thread] += 1\n\n        for thread in self.threads:\n            pinned_apps = await self.get_pinned_apps(thread)\n            await self.set_state(\"_threading\", \"admin\", \"thread.{}\".format(thread), pinned_apps=pinned_apps)\n\n    def app_should_be_pinned(self, name):\n        # Check apps.yaml first - allow override\n        app = self.AD.app_management.app_config[name]\n        if \"pin_app\" in app:\n            return app[\"pin_app\"]\n\n        # if not, go with the global default\n        return self.pin_apps\n\n    async def get_app_pin(self, name):\n        return self.AD.app_management.objects[name][\"pin_app\"]\n\n    async def set_app_pin(self, name, pin):\n        self.AD.app_management.objects[name][\"pin_app\"] = pin\n        if pin is True:\n            # May need to set this app up with a pinned thread\n            await self.calculate_pin_threads()\n\n    async def get_pin_thread(self, name):\n        return self.AD.app_management.objects[name][\"pin_thread\"]\n\n    async def set_pin_thread(self, name, thread):\n        self.AD.app_management.objects[name][\"pin_thread\"] = thread\n\n    def validate_pin(self, name, kwargs):\n        if \"pin_thread\" in kwargs:\n            if kwargs[\"pin_thread\"] < 0 or kwargs[\"pin_thread\"] >= self.thread_count:\n                self.logger.warning(\"Invalid value for pin_thread (%s) in app: %s - discarding callback\", kwargs[\"pin_thread\"], name)\n                return False\n        else:\n            return True\n\n    async def get_pinned_apps(self, thread):\n        id = int(thread.split(\"-\")[1])\n        apps = []\n        for obj in self.AD.app_management.objects:\n            if self.AD.app_management.objects[obj][\"pin_thread\"] == id:\n                apps.append(obj)\n        return apps\n\n    #\n    # Constraints\n    #\n\n    async def check_constraint(self, key, value, app):\n        unconstrained = True\n        if key in app.list_constraints():\n            method = getattr(app, key)\n            unconstrained = await utils.run_in_executor(self, method, value)\n\n        return unconstrained\n\n    async def check_time_constraint(self, args, name):\n        unconstrained = True\n        if \"constrain_start_time\" in args or \"constrain_end_time\" in args:\n            if \"constrain_start_time\" not in args:\n                start_time = \"00:00:00\"\n            else:\n                start_time = args[\"constrain_start_time\"]\n            if \"constrain_end_time\" not in args:\n                end_time = \"23:59:59\"\n            else:\n                end_time = args[\"constrain_end_time\"]\n            if await self.AD.sched.now_is_between(start_time, end_time, name) is False:\n                unconstrained = False\n\n        return unconstrained\n\n    #\n    # Workers\n    #\n\n    async def check_and_dispatch_state(self, name, funcref, entity, attribute, new_state,\n                                 old_state, cold, cnew, kwargs, uuid_, pin_app, pin_thread):\n        executed = False\n        #kwargs[\"handle\"] = uuid_\n        if attribute == \"all\":\n            executed = await self.dispatch_worker(name, {\n                \"id\": uuid_,\n                \"name\": name,\n                \"objectid\": self.AD.app_management.objects[name][\"id\"],\n                \"type\": \"state\",\n                \"function\": funcref,\n                \"attribute\": attribute,\n                \"entity\": entity,\n                \"new_state\": new_state,\n                \"old_state\": old_state,\n                \"pin_app\": pin_app,\n                \"pin_thread\": pin_thread,\n                \"kwargs\": kwargs,\n            })\n        else:\n            if old_state is None:\n                old = None\n            else:\n                if attribute in old_state:\n                    old = old_state[attribute]\n                elif 'attributes' in old_state and attribute in old_state['attributes']:\n                    old = old_state['attributes'][attribute]\n                else:\n                    old = None\n            if new_state is None:\n                new = None\n            else:\n                if attribute in new_state:\n                    new = new_state[attribute]\n                elif 'attributes' in new_state and attribute in new_state['attributes']:\n                    new = new_state['attributes'][attribute]\n                else:\n                    new = None\n\n            if (cold is None or cold == old) and (cnew is None or cnew == new):\n                if \"duration\" in kwargs:\n                    # Set a timer\n                    exec_time = await self.AD.sched.get_now() + timedelta(seconds=int(kwargs[\"duration\"]))\n                    kwargs[\"__duration\"] = await self.AD.sched.insert_schedule(\n                        name, exec_time, funcref, False, None,\n                        __entity=entity,\n                        __attribute=attribute,\n                        __old_state=old,\n                        __new_state=new, **kwargs\n                    )\n                else:\n                    # Do it now\n                    executed = await self.dispatch_worker(name, {\n                        \"id\": uuid_,\n                        \"name\": name,\n                        \"objectid\": self.AD.app_management.objects[name][\"id\"],\n                        \"type\": \"state\",\n                        \"function\": funcref,\n                        \"attribute\": attribute,\n                        \"entity\": entity,\n                        \"new_state\": new,\n                        \"old_state\": old,\n                        \"pin_app\": pin_app,\n                        \"pin_thread\": pin_thread,\n                        \"kwargs\": kwargs\n                    })\n            else:\n                if \"__duration\" in kwargs:\n                    # cancel timer\n                    await self.AD.sched.cancel_timer(name, kwargs[\"__duration\"])\n\n        return executed\n\n    async def dispatch_worker(self, name, args):\n        unconstrained = True\n        #\n        # Argument Constraints\n        #\n        for arg in self.AD.app_management.app_config[name].keys():\n            constrained = await self.check_constraint(arg, self.AD.app_management.app_config[name][arg], self.AD.app_management.objects[name][\"object\"])\n            if not constrained:\n                unconstrained = False\n        if not await self.check_time_constraint(self.AD.app_management.app_config[name], name):\n            unconstrained = False\n        #\n        # Callback level constraints\n        #\n        myargs = utils.deepcopy(args)\n        if \"kwargs\" in myargs:\n            for arg in myargs[\"kwargs\"].keys():\n                constrained = await self.check_constraint(arg, myargs[\"kwargs\"][arg], self.AD.app_management.objects[name][\"object\"])\n                if not constrained:\n                    unconstrained = False\n            if not await self.check_time_constraint(myargs[\"kwargs\"], name):\n                unconstrained = False\n\n        if unconstrained:\n            #\n            # It's going to happen\n            #\n            await self.add_to_state(\"_threading\", \"admin\", \"sensor.callbacks_total_fired\", 1)\n            await self.add_to_attr(\"_threading\", \"admin\", \"{}_callback.{}\".format(myargs[\"type\"], myargs[\"id\"]), \"fired\", 1)\n            #\n            # And Q\n            #\n            self.select_q(myargs)\n            return True\n        else:\n            return False\n\n    # noinspection PyBroadException\n    def worker(self):\n        thread_id = threading.current_thread().name\n        q = self.get_q(thread_id)\n        while True:\n            args = q.get()\n            _type = args[\"type\"]\n            funcref = args[\"function\"]\n            _id = args[\"id\"]\n            objectid = args[\"objectid\"]\n            name = args[\"name\"]\n            error_logger = logging.getLogger(\"Error.{}\".format(name))\n            args[\"kwargs\"][\"__thread_id\"] = thread_id\n            callback = \"{}() in {}\".format(funcref.__name__, name)\n            app = utils.run_coroutine_threadsafe(self, self.AD.app_management.get_app_instance(name, objectid))\n            if app is not None:\n                try:\n                    if _type == \"scheduler\":\n                        if self.validate_callback_sig(name, \"scheduler\", funcref):\n                            utils.run_coroutine_threadsafe(self, self.update_thread_info(thread_id, callback, name, _type, _id))\n                            funcref(self.AD.sched.sanitize_timer_kwargs(app, args[\"kwargs\"]))\n                    elif _type == \"state\":\n                        if self.validate_callback_sig(name, \"state\", funcref):\n                            entity = args[\"entity\"]\n                            attr = args[\"attribute\"]\n                            old_state = args[\"old_state\"]\n                            new_state = args[\"new_state\"]\n                            utils.run_coroutine_threadsafe(self, self.update_thread_info(thread_id, callback, name, _type, _id))\n                            funcref(entity, attr, old_state, new_state,\n                                    self.AD.state.sanitize_state_kwargs(app, args[\"kwargs\"]))\n                    elif _type == \"event\":\n                        data = args[\"data\"]\n                        if args[\"event\"] == \"__AD_LOG_EVENT\":\n                            if self.validate_callback_sig(name, \"log_event\", funcref):\n                                utils.run_coroutine_threadsafe(self, self.update_thread_info(thread_id, callback, name, _type, _id))\n                                funcref(data[\"app_name\"], data[\"ts\"], data[\"level\"], data[\"type\"], data[\"message\"], args[\"kwargs\"])\n                        else:\n                            if self.validate_callback_sig(name, \"event\", funcref):\n                                utils.run_coroutine_threadsafe(self, self.update_thread_info(thread_id, callback, name, _type, _id))\n                                funcref(args[\"event\"], data, args[\"kwargs\"])\n                except:\n                    error_logger.warning('-' * 60,)\n                    error_logger.warning(\"Unexpected error in worker for App %s:\", name)\n                    error_logger.warning( \"Worker Ags: %s\", args)\n                    error_logger.warning('-' * 60)\n                    error_logger.warning(traceback.format_exc())\n                    error_logger.warning('-' * 60)\n                    if self.AD.logging.separate_error_log() is True:\n                        self.logger.warning(\"Logged an error to %s\", self.AD.logging.get_filename(\"error_log\"))\n                finally:\n                    utils.run_coroutine_threadsafe(self, self.update_thread_info(thread_id, \"idle\", name, _type, _id))\n\n            else:\n                if not self.AD.stopping:\n                    self.logger.warning(\"Found stale callback for %s - discarding\", name)\n\n            q.task_done()\n\n    def validate_callback_sig(self, name, type, funcref):\n\n        callback_args = {\n            \"scheduler\": {\"count\": 1, \"signature\": \"f(self, kwargs)\"},\n            \"state\": {\"count\": 5, \"signature\": \"f(self, entity, attribute, old, new, kwargs)\"},\n            \"event\": {\"count\": 3, \"signature\": \"f(self, event, data, kwargs)\"},\n            \"log_event\": {\"count\": 6, \"signature\": \"f(self, name, ts, level, type, message, kwargs)\"},\n            \"initialize\": {\"count\": 0, \"signature\": \"initialize()\"}\n        }\n\n        sig = inspect.signature(funcref)\n\n        if type in callback_args:\n            if len(sig.parameters) != callback_args[type][\"count\"]:\n                self.logger.warning(\"Incorrect signature type for callback %s(), should be %s - discarding\", funcref.__name__, callback_args[type][\"signature\"])\n                return False\n            else:\n                return True\n        else:\n            self.logger.error(\"Unknown callback type: %s\", type)\n\n        return False\n\n","label":1}
{"content":"#!\/usr\/bin\/env python\n#\n# Copyright 2001 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Script that generates the build.ninja for ninja itself.\n\nProjects that use ninja themselves should either write a similar script\nor use a meta-build system that supports Ninja output.\"\"\"\n\nfrom __future__ import print_function\n\nfrom optparse import OptionParser\nimport os\nimport pipes\nimport string\nimport subprocess\nimport sys\n\nsourcedir = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(sourcedir, 'misc'))\nimport ninja_syntax\n\n\nclass Platform(object):\n    \"\"\"Represents a host\/target platform and its specific build attributes.\"\"\"\n    def __init__(self, platform):\n        self._platform = platform\n        if self._platform is not None:\n            return\n        self._platform = sys.platform\n        if self._platform.startswith('linux'):\n            self._platform = 'linux'\n        elif self._platform.startswith('freebsd'):\n            self._platform = 'freebsd'\n        elif self._platform.startswith('gnukfreebsd'):\n            self._platform = 'freebsd'\n        elif self._platform.startswith('openbsd'):\n            self._platform = 'openbsd'\n        elif self._platform.startswith('solaris') or self._platform == 'sunos5':\n            self._platform = 'solaris'\n        elif self._platform.startswith('mingw'):\n            self._platform = 'mingw'\n        elif self._platform.startswith('win'):\n            self._platform = 'msvc'\n        elif self._platform.startswith('bitrig'):\n            self._platform = 'bitrig'\n        elif self._platform.startswith('netbsd'):\n            self._platform = 'netbsd'\n        elif self._platform.startswith('aix'):\n            self._platform = 'aix'\n\n    @staticmethod\n    def known_platforms():\n      return ['linux', 'darwin', 'freebsd', 'openbsd', 'solaris', 'sunos5',\n              'mingw', 'msvc', 'gnukfreebsd', 'bitrig', 'netbsd', 'aix']\n\n    def platform(self):\n        return self._platform\n\n    def is_linux(self):\n        return self._platform == 'linux'\n\n    def is_mingw(self):\n        return self._platform == 'mingw'\n\n    def is_msvc(self):\n        return self._platform == 'msvc'\n\n    def msvc_needs_fs(self):\n        popen = subprocess.Popen(['cl', '\/nologo', '\/?'],\n                                 stdout=subprocess.PIPE,\n                                 stderr=subprocess.PIPE)\n        out, err = popen.communicate()\n        return b'\/FS' in out\n\n    def is_windows(self):\n        return self.is_mingw() or self.is_msvc()\n\n    def is_solaris(self):\n        return self._platform == 'solaris'\n\n    def is_aix(self):\n        return self._platform == 'aix'\n\n    def uses_usr_local(self):\n        return self._platform in ('freebsd', 'openbsd', 'bitrig')\n\n    def supports_ppoll(self):\n        return self._platform in ('freebsd', 'linux', 'openbsd', 'bitrig')\n\n    def supports_ninja_browse(self):\n        return (not self.is_windows()\n                and not self.is_solaris()\n                and not self.is_aix())\n\n    def can_rebuild_in_place(self):\n        return not (self.is_windows() or self.is_aix())\n\nclass Bootstrap:\n    \"\"\"API shim for ninja_syntax.Writer that instead runs the commands.\n\n    Used to bootstrap Ninja from scratch.  In --bootstrap mode this\n    class is used to execute all the commands to build an executable.\n    It also proxies all calls to an underlying ninja_syntax.Writer, to\n    behave like non-bootstrap mode.\n    \"\"\"\n    def __init__(self, writer, verbose=False):\n        self.writer = writer\n        self.verbose = verbose\n        # Map of variable name => expanded variable value.\n        self.vars = {}\n        # Map of rule name => dict of rule attributes.\n        self.rules = {\n            'phony': {}\n        }\n\n    def comment(self, text):\n        return self.writer.comment(text)\n\n    def newline(self):\n        return self.writer.newline()\n\n    def variable(self, key, val):\n        # In bootstrap mode, we have no ninja process to catch \/showIncludes\n        # output.\n        self.vars[key] = self._expand(val).replace('\/showIncludes', '')\n        return self.writer.variable(key, val)\n\n    def rule(self, name, **kwargs):\n        self.rules[name] = kwargs\n        return self.writer.rule(name, **kwargs)\n\n    def build(self, outputs, rule, inputs=None, **kwargs):\n        ruleattr = self.rules[rule]\n        cmd = ruleattr.get('command')\n        if cmd is None:  # A phony rule, for example.\n            return\n\n        # Implement just enough of Ninja variable expansion etc. to\n        # make the bootstrap build work.\n        local_vars = {\n            'in': self._expand_paths(inputs),\n            'out': self._expand_paths(outputs)\n        }\n        for key, val in kwargs.get('variables', []):\n            local_vars[key] = ' '.join(ninja_syntax.as_list(val))\n\n        self._run_command(self._expand(cmd, local_vars))\n\n        return self.writer.build(outputs, rule, inputs, **kwargs)\n\n    def default(self, paths):\n        return self.writer.default(paths)\n\n    def _expand_paths(self, paths):\n        \"\"\"Expand $vars in an array of paths, e.g. from a 'build' block.\"\"\"\n        paths = ninja_syntax.as_list(paths)\n        return ' '.join(map(self._shell_escape, (map(self._expand, paths))))\n\n    def _expand(self, str, local_vars={}):\n        \"\"\"Expand $vars in a string.\"\"\"\n        return ninja_syntax.expand(str, self.vars, local_vars)\n\n    def _shell_escape(self, path):\n        \"\"\"Quote paths containing spaces.\"\"\"\n        return '\"%s\"' % path if ' ' in path else path\n\n    def _run_command(self, cmdline):\n        \"\"\"Run a subcommand, quietly.  Prints the full command on error.\"\"\"\n        try:\n            if self.verbose:\n                print(cmdline)\n            subprocess.check_call(cmdline, shell=True)\n        except subprocess.CalledProcessError:\n            print('when running: ', cmdline)\n            raise\n\n\nparser = OptionParser()\nprofilers = ['gmon', 'pprof']\nparser.add_option('--bootstrap', action='store_true',\n                  help='bootstrap a ninja binary from nothing')\nparser.add_option('--verbose', action='store_true',\n                  help='enable verbose build')\nparser.add_option('--platform',\n                  help='target platform (' +\n                       '\/'.join(Platform.known_platforms()) + ')',\n                  choices=Platform.known_platforms())\nparser.add_option('--host',\n                  help='host platform (' +\n                       '\/'.join(Platform.known_platforms()) + ')',\n                  choices=Platform.known_platforms())\nparser.add_option('--debug', action='store_true',\n                  help='enable debugging extras',)\nparser.add_option('--profile', metavar='TYPE',\n                  choices=profilers,\n                  help='enable profiling (' + '\/'.join(profilers) + ')',)\nparser.add_option('--with-gtest', metavar='PATH', help='ignored')\nparser.add_option('--with-python', metavar='EXE',\n                  help='use EXE as the Python interpreter',\n                  default=os.path.basename(sys.executable))\nparser.add_option('--force-pselect', action='store_true',\n                  help='ppoll() is used by default where available, '\n                       'but some platforms may need to use pselect instead',)\n(options, args) = parser.parse_args()\nif args:\n    print('ERROR: extra unparsed command-line arguments:', args)\n    sys.exit(1)\n\nplatform = Platform(options.platform)\nif options.host:\n    host = Platform(options.host)\nelse:\n    host = platform\n\nBUILD_FILENAME = 'build.ninja'\nninja_writer = ninja_syntax.Writer(open(BUILD_FILENAME, 'w'))\nn = ninja_writer\n\nif options.bootstrap:\n    # Make the build directory.\n    try:\n        os.mkdir('build')\n    except OSError:\n        pass\n    # Wrap ninja_writer with the Bootstrapper, which also executes the\n    # commands.\n    print('bootstrapping ninja...')\n    n = Bootstrap(n, verbose=options.verbose)\n\nn.comment('This file is used to build ninja itself.')\nn.comment('It is generated by ' + os.path.basename(__file__) + '.')\nn.newline()\n\nn.variable('ninja_required_version', '1.3')\nn.newline()\n\nn.comment('The arguments passed to configure.py, for rerunning it.')\nconfigure_args = sys.argv[1:]\nif '--bootstrap' in configure_args:\n    configure_args.remove('--bootstrap')\nn.variable('configure_args', ' '.join(configure_args))\nenv_keys = set(['CXX', 'AR', 'CFLAGS', 'LDFLAGS'])\nconfigure_env = dict((k, os.environ[k]) for k in os.environ if k in env_keys)\nif configure_env:\n    config_str = ' '.join([k + '=' + pipes.quote(configure_env[k])\n                           for k in configure_env])\n    n.variable('configure_env', config_str + '$ ')\nn.newline()\n\nCXX = configure_env.get('CXX', 'g++')\nobjext = '.o'\nif platform.is_msvc():\n    CXX = 'cl'\n    objext = '.obj'\n\ndef src(filename):\n    return os.path.join('$root', 'src', filename)\ndef built(filename):\n    return os.path.join('$builddir', filename)\ndef doc(filename):\n    return os.path.join('$root', 'doc', filename)\ndef cc(name, **kwargs):\n    return n.build(built(name + objext), 'cxx', src(name + '.c'), **kwargs)\ndef cxx(name, **kwargs):\n    return n.build(built(name + objext), 'cxx', src(name + '.cc'), **kwargs)\ndef binary(name):\n    if platform.is_windows():\n        exe = name + '.exe'\n        n.build(name, 'phony', exe)\n        return exe\n    return name\n\nroot = sourcedir\nif root == os.getcwd():\n    # In the common case where we're building directly in the source\n    # tree, simplify all the paths to just be cwd-relative.\n    root = '.'\nn.variable('root', root)\nn.variable('builddir', 'build')\nn.variable('cxx', CXX)\nif platform.is_msvc():\n    n.variable('ar', 'link')\nelse:\n    n.variable('ar', configure_env.get('AR', 'ar'))\n\nif platform.is_msvc():\n    cflags = ['\/showIncludes',\n              '\/nologo',  # Don't print startup banner.\n              '\/Zi',  # Create pdb with debug info.\n              '\/W4',  # Highest warning level.\n              '\/WX',  # Warnings as errors.\n              '\/wd4530', '\/wd4100', '\/wd4706',\n              '\/wd4512', '\/wd4800', '\/wd4702', '\/wd4819',\n              # Disable warnings about constant conditional expressions.\n              '\/wd4127',\n              # Disable warnings about passing \"this\" during initialization.\n              '\/wd4355',\n              # Disable warnings about ignored typedef in DbgHelp.h\n              '\/wd4091',\n              '\/GR-',  # Disable RTTI.\n              # Disable size_t -> int truncation warning.\n              # We never have strings or arrays larger than 2**31.\n              '\/wd4267',\n              '\/DNOMINMAX', '\/D_CRT_SECURE_NO_WARNINGS',\n              '\/D_HAS_EXCEPTIONS=0',\n              '\/DNINJA_PYTHON=\"%s\"' % options.with_python]\n    if platform.msvc_needs_fs():\n        cflags.append('\/FS')\n    ldflags = ['\/DEBUG', '\/libpath:$builddir']\n    if not options.debug:\n        cflags += ['\/Ox', '\/DNDEBUG', '\/GL']\n        ldflags += ['\/LTCG', '\/OPT:REF', '\/OPT:ICF']\nelse:\n    cflags = ['-g', '-Wall', '-Wextra',\n              '-Wno-deprecated',\n              '-Wno-missing-field-initializers',\n              '-Wno-unused-parameter',\n              '-fno-rtti',\n              '-fno-exceptions',\n              '-fvisibility=hidden', '-pipe',\n              '-DNINJA_PYTHON=\"%s\"' % options.with_python]\n    if options.debug:\n        cflags += ['-D_GLIBCXX_DEBUG', '-D_GLIBCXX_DEBUG_PEDANTIC']\n        cflags.remove('-fno-rtti')  # Needed for above pedanticness.\n    else:\n        cflags += ['-O2', '-DNDEBUG']\n    try:\n        proc = subprocess.Popen(\n            [CXX, '-fdiagnostics-color', '-c', '-x', 'c++', '\/dev\/null',\n             '-o', '\/dev\/null'],\n            stdout=open(os.devnull, 'wb'), stderr=subprocess.STDOUT)\n        if proc.wait() == 0:\n            cflags += ['-fdiagnostics-color']\n    except:\n        pass\n    if platform.is_mingw():\n        cflags += ['-D_WIN32_WINNT=0x0501']\n    ldflags = ['-L$builddir']\n    if platform.uses_usr_local():\n        cflags.append('-I\/usr\/local\/include')\n        ldflags.append('-L\/usr\/local\/lib')\n\nlibs = []\n\nif platform.is_mingw():\n    cflags.remove('-fvisibility=hidden');\n    ldflags.append('-static')\nelif platform.is_solaris():\n    cflags.remove('-fvisibility=hidden')\nelif platform.is_aix():\n    cflags.remove('-fvisibility=hidden')\nelif platform.is_msvc():\n    pass\nelse:\n    if options.profile == 'gmon':\n        cflags.append('-pg')\n        ldflags.append('-pg')\n    elif options.profile == 'pprof':\n        cflags.append('-fno-omit-frame-pointer')\n        libs.extend(['-Wl,--no-as-needed', '-lprofiler'])\n\nif platform.supports_ppoll() and not options.force_pselect:\n    cflags.append('-DUSE_PPOLL')\nif platform.supports_ninja_browse():\n    cflags.append('-DNINJA_HAVE_BROWSE')\n\n# Search for generated headers relative to build dir.\ncflags.append('-I.')\n\ndef shell_escape(str):\n    \"\"\"Escape str such that it's interpreted as a single argument by\n    the shell.\"\"\"\n\n    # This isn't complete, but it's just enough to make NINJA_PYTHON work.\n    if platform.is_windows():\n      return str\n    if '\"' in str:\n        return \"'%s'\" % str.replace(\"'\", \"\\\\'\")\n    return str\n\nif 'CFLAGS' in configure_env:\n    cflags.append(configure_env['CFLAGS'])\nn.variable('cflags', ' '.join(shell_escape(flag) for flag in cflags))\nif 'LDFLAGS' in configure_env:\n    ldflags.append(configure_env['LDFLAGS'])\nn.variable('ldflags', ' '.join(shell_escape(flag) for flag in ldflags))\nn.newline()\n\nif platform.is_msvc():\n    n.rule('cxx',\n        command='$cxx $cflags -c $in \/Fo$out',\n        description='CXX $out',\n        deps='msvc'  # \/showIncludes is included in $cflags.\n    )\nelse:\n    n.rule('cxx',\n        command='$cxx -MMD -MT $out -MF $out.d $cflags -c $in -o $out',\n        depfile='$out.d',\n        deps='gcc',\n        description='CXX $out')\nn.newline()\n\nif host.is_msvc():\n    n.rule('ar',\n           command='lib \/nologo \/ltcg \/out:$out $in',\n           description='LIB $out')\nelif host.is_mingw():\n    n.rule('ar',\n           command='cmd \/c $ar cqs $out.tmp $in && move \/Y $out.tmp $out',\n           description='AR $out')\nelse:\n    n.rule('ar',\n           command='rm -f $out && $ar crs $out $in',\n           description='AR $out')\nn.newline()\n\nif platform.is_msvc():\n    n.rule('link',\n        command='$cxx $in $libs \/nologo \/link $ldflags \/out:$out',\n        description='LINK $out')\nelse:\n    n.rule('link',\n        command='$cxx $ldflags -o $out $in $libs',\n        description='LINK $out')\nn.newline()\n\nobjs = []\n\nif platform.supports_ninja_browse():\n    n.comment('browse_py.h is used to inline browse.py.')\n    n.rule('inline',\n           command='\"%s\"' % src('inline.sh') + ' $varname < $in > $out',\n           description='INLINE $out')\n    n.build(built('browse_py.h'), 'inline', src('browse.py'),\n            implicit=src('inline.sh'),\n            variables=[('varname', 'kBrowsePy')])\n    n.newline()\n\n    objs += cxx('browse', order_only=built('browse_py.h'))\n    n.newline()\n\nn.comment('the depfile parser and ninja lexers are generated using re2c.')\ndef has_re2c():\n    try:\n        proc = subprocess.Popen(['re2c', '-V'], stdout=subprocess.PIPE)\n        return int(proc.communicate()[0], 10) >= 1103\n    except OSError:\n        return False\nif has_re2c():\n    n.rule('re2c',\n           command='re2c -b -i --no-generation-date -o $out $in',\n           description='RE2C $out')\n    # Generate the .cc files in the source directory so we can check them in.\n    n.build(src('depfile_parser.cc'), 're2c', src('depfile_parser.in.cc'))\n    n.build(src('lexer.cc'), 're2c', src('lexer.in.cc'))\nelse:\n    print(\"warning: A compatible version of re2c (>= 0.11.3) was not found; \"\n           \"changes to src\/*.in.cc will not affect your build.\")\nn.newline()\n\nn.comment('Core source files all build into ninja library.')\nfor name in ['build',\n             'build_log',\n             'clean',\n             'clparser',\n             'debug_flags',\n             'depfile_parser',\n             'deps_log',\n             'disk_interface',\n             'edit_distance',\n             'eval_env',\n             'graph',\n             'graphviz',\n             'lexer',\n             'line_printer',\n             'manifest_parser',\n             'metrics',\n             'state',\n             'util',\n             'version']:\n    objs += cxx(name)\nif platform.is_windows():\n    for name in ['subprocess-win32',\n                 'includes_normalize-win32',\n                 'msvc_helper-win32',\n                 'msvc_helper_main-win32']:\n        objs += cxx(name)\n    if platform.is_msvc():\n        objs += cxx('minidump-win32')\n    objs += cc('getopt')\nelse:\n    objs += cxx('subprocess-posix')\nif platform.is_aix():\n    objs += cc('getopt')\nif platform.is_msvc():\n    ninja_lib = n.build(built('ninja.lib'), 'ar', objs)\nelse:\n    ninja_lib = n.build(built('libninja.a'), 'ar', objs)\nn.newline()\n\nif platform.is_msvc():\n    libs.append('ninja.lib')\nelse:\n    libs.append('-lninja')\n\nif platform.is_aix():\n    libs.append('-lperfstat')\n\nall_targets = []\n\nn.comment('Main executable is library plus main() function.')\nobjs = cxx('ninja')\nninja = n.build(binary('ninja'), 'link', objs, implicit=ninja_lib,\n                variables=[('libs', libs)])\nn.newline()\nall_targets += ninja\n\nif options.bootstrap:\n    # We've built the ninja binary.  Don't run any more commands\n    # through the bootstrap executor, but continue writing the\n    # build.ninja file.\n    n = ninja_writer\n\nn.comment('Tests all build into ninja_test executable.')\n\nobjs = []\n\nfor name in ['build_log_test',\n             'build_test',\n             'clean_test',\n             'clparser_test',\n             'depfile_parser_test',\n             'deps_log_test',\n             'disk_interface_test',\n             'edit_distance_test',\n             'graph_test',\n             'lexer_test',\n             'manifest_parser_test',\n             'ninja_test',\n             'state_test',\n             'subprocess_test',\n             'test',\n             'util_test']:\n    objs += cxx(name)\nif platform.is_windows():\n    for name in ['includes_normalize_test', 'msvc_helper_test']:\n        objs += cxx(name)\n\nninja_test = n.build(binary('ninja_test'), 'link', objs, implicit=ninja_lib,\n                     variables=[('libs', libs)])\nn.newline()\nall_targets += ninja_test\n\n\nn.comment('Ancillary executables.')\nobjs = cxx('build_log_perftest')\nall_targets += n.build(binary('build_log_perftest'), 'link', objs,\n                       implicit=ninja_lib, variables=[('libs', libs)])\nobjs = cxx('canon_perftest')\nall_targets += n.build(binary('canon_perftest'), 'link', objs,\n                       implicit=ninja_lib, variables=[('libs', libs)])\nobjs = cxx('depfile_parser_perftest')\nall_targets += n.build(binary('depfile_parser_perftest'), 'link', objs,\n                       implicit=ninja_lib, variables=[('libs', libs)])\nobjs = cxx('hash_collision_bench')\nall_targets += n.build(binary('hash_collision_bench'), 'link', objs,\n                              implicit=ninja_lib, variables=[('libs', libs)])\nobjs = cxx('manifest_parser_perftest')\nall_targets += n.build(binary('manifest_parser_perftest'), 'link', objs,\n                              implicit=ninja_lib, variables=[('libs', libs)])\nn.newline()\n\nn.comment('Generate a graph using the \"graph\" tool.')\nn.rule('gendot',\n       command='.\/ninja -t graph all > $out')\nn.rule('gengraph',\n       command='dot -Tpng $in > $out')\ndot = n.build(built('graph.dot'), 'gendot', ['ninja', 'build.ninja'])\nn.build('graph.png', 'gengraph', dot)\nn.newline()\n\nn.comment('Generate the manual using asciidoc.')\nn.rule('asciidoc',\n       command='asciidoc -b docbook -d book -o $out $in',\n       description='ASCIIDOC $out')\nn.rule('xsltproc',\n       command='xsltproc --nonet doc\/docbook.xsl $in > $out',\n       description='XSLTPROC $out')\ndocbookxml = n.build(built('manual.xml'), 'asciidoc', doc('manual.asciidoc'))\nmanual = n.build(doc('manual.html'), 'xsltproc', docbookxml,\n                 implicit=[doc('style.css'), doc('docbook.xsl')])\nn.build('manual', 'phony',\n        order_only=manual)\nn.newline()\n\nn.rule('dblatex',\n       command='dblatex -q -o $out -p doc\/dblatex.xsl $in',\n       description='DBLATEX $out')\nn.build(doc('manual.pdf'), 'dblatex', docbookxml,\n        implicit=[doc('dblatex.xsl')])\n\nn.comment('Generate Doxygen.')\nn.rule('doxygen',\n       command='doxygen $in',\n       description='DOXYGEN $in')\nn.variable('doxygen_mainpage_generator',\n           src('gen_doxygen_mainpage.sh'))\nn.rule('doxygen_mainpage',\n       command='$doxygen_mainpage_generator $in > $out',\n       description='DOXYGEN_MAINPAGE $out')\nmainpage = n.build(built('doxygen_mainpage'), 'doxygen_mainpage',\n                   ['README', 'COPYING'],\n                   implicit=['$doxygen_mainpage_generator'])\nn.build('doxygen', 'doxygen', doc('doxygen.config'),\n        implicit=mainpage)\nn.newline()\n\nif not host.is_mingw():\n    n.comment('Regenerate build files if build script changes.')\n    n.rule('configure',\n           command='${configure_env}%s $root\/configure.py $configure_args' %\n               options.with_python,\n           generator=True)\n    n.build('build.ninja', 'configure',\n            implicit=['$root\/configure.py',\n                      os.path.normpath('$root\/misc\/ninja_syntax.py')])\n    n.newline()\n\nn.default(ninja)\nn.newline()\n\nif host.is_linux():\n    n.comment('Packaging')\n    n.rule('rpmbuild',\n           command=\"misc\/packaging\/rpmbuild.sh\",\n           description='Building rpms..')\n    n.build('rpm', 'rpmbuild')\n    n.newline()\n\nn.build('all', 'phony', all_targets)\n\nn.close()\nprint('wrote %s.' % BUILD_FILENAME)\n\nif options.bootstrap:\n    print('bootstrap complete.  rebuilding...')\n\n    rebuild_args = []\n\n    if platform.can_rebuild_in_place():\n        rebuild_args.append('.\/ninja')\n    else:\n        if platform.is_windows():\n            bootstrap_exe = 'ninja.bootstrap.exe'\n            final_exe = 'ninja.exe'\n        else:\n            bootstrap_exe = '.\/ninja.bootstrap'\n            final_exe = '.\/ninja'\n\n        if os.path.exists(bootstrap_exe):\n            os.unlink(bootstrap_exe)\n        os.rename(final_exe, bootstrap_exe)\n\n        rebuild_args.append(bootstrap_exe)\n\n    if options.verbose:\n        rebuild_args.append('-v')\n\n    subprocess.check_call(rebuild_args)\n","label":1}
{"content":"\n#\n# This source file is part of appleseed.\n# Visit http:\/\/appleseedhq.net\/ for additional information and resources.\n#\n# This software is released under the MIT license.\n#\n# Copyright (c) 2014-2017 The appleseedhq Organization\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n#\n\nimport bpy\n\n\nclass AppleseedWorldPanelOld(bpy.types.Panel):\n    bl_label = \"Environment\"\n    bl_space_type = \"PROPERTIES\"\n    bl_region_type = \"WINDOW\"\n    COMPAT_ENGINES = {'APPLESEED_RENDER'}\n    bl_context = \"world\"\n\n    @classmethod\n    def poll(cls, context):\n        renderer = context.scene.render\n        if renderer.engine == 'APPLESEED_RENDER':\n            return context.scene.world != None\n        return False\n\n    def draw(self, context):\n        layout = self.layout\n        scene = context.scene\n        asr_sky_props = scene.appleseed_sky\n\n        layout.prop(asr_sky_props, \"env_type\", text=\"\")\n\n        if asr_sky_props.env_type == \"sunsky\":\n            layout.label(\"Sun Lamp:\")\n            layout.prop(asr_sky_props, \"sun_lamp\", text=\"\")\n            layout.prop(asr_sky_props, \"sun_model\", text=\"Sky Model\")\n\n            layout.prop(asr_sky_props, \"luminance_multiplier\")\n            layout.prop(asr_sky_props, \"radiance_multiplier\")\n            layout.prop(asr_sky_props, \"saturation_multiplier\")\n            layout.prop(asr_sky_props, \"sun_theta\")\n            layout.prop(asr_sky_props, \"sun_phi\")\n            layout.prop(asr_sky_props, \"turbidity\")\n            layout.prop(asr_sky_props, \"turbidity_min\")\n            layout.prop(asr_sky_props, \"turbidity_max\")\n            layout.prop(asr_sky_props, \"horiz_shift\")\n            if asr_sky_props.sun_model == \"hosek_environment_edf\":\n                layout.prop(asr_sky_props, \"ground_albedo\")\n\n        elif asr_sky_props.env_type == \"gradient\":\n            layout.prop(scene.world, \"horizon_color\", text=\"\")\n            layout.prop(scene.world, \"zenith_color\", text=\"\")\n\n        elif asr_sky_props.env_type == \"constant\":\n            layout.prop(scene.world, \"horizon_color\", text=\"\")\n\n        elif asr_sky_props.env_type == \"constant_hemisphere\":\n            layout.prop(scene.world, \"horizon_color\", text=\"\")\n            layout.prop(scene.world, \"zenith_color\", text=\"\")\n\n        elif asr_sky_props.env_type == \"mirrorball_map\":\n            layout.prop_search(asr_sky_props, \"env_tex\", scene.world, \"texture_slots\", text=\"\")\n            layout.prop(asr_sky_props, \"env_tex_mult\")\n\n        elif asr_sky_props.env_type == \"latlong_map\":\n            layout.prop_search(asr_sky_props, \"env_tex\", scene.world, \"texture_slots\", text=\"\")\n            layout.prop(asr_sky_props, \"env_tex_mult\")\n\n\ndef register():\n    bpy.types.WORLD_PT_context_world.COMPAT_ENGINES.add('APPLESEED_RENDER')\n    bpy.types.WORLD_PT_custom_props.COMPAT_ENGINES.add('APPLESEED_RENDER')\n\n\ndef unregister():\n    pass\n","label":1}
{"content":"#!\/usr\/bin\/python3\nimport os\nimport requests\nimport json\nfrom brownie import AdvancedCollectible, network\nfrom metadata import sample_metadata\nfrom scripts.helpful_scripts import get_breed\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndog_metadata_dic = {\n    \"PUG\": \"https:\/\/ipfs.io\/ipfs\/QmPfJe3hKzEUSDJgXgJncAbFWRQKtos5qGf4AFSuLvarti?filename=1.json\",\n    \"SHIBA_INU\": \"https:\/\/ipfs.io\/ipfs\/QmPF7rbCnMC5HjUCok2SBAD4cvLjWuZbbH1rmHreBPuRN7?filename=2.json\",\n    \"ST_BERNARD\": \"https:\/\/ipfs.io\/ipfs\/QmRRhLqBWSMJejN1CtXPyDQMPe7o1RetiWHoyuWiWZPgPb?filename=3.json\",\n\t}\n\t\n\ndef main():\n    print(\"Working on \" + network.show_active())\n    advanced_collectible = AdvancedCollectible[len(AdvancedCollectible) - 1]\n    number_of_advanced_collectibles = advanced_collectible.tokenCounter()\n    print(\n        \"The number of tokens you've deployed is: \"\n        + str(number_of_advanced_collectibles)\n    )\n    write_metadata(number_of_advanced_collectibles, advanced_collectible)\n\n\ndef write_metadata(token_ids, nft_contract):\n    for token_id in range(token_ids):\n        collectible_metadata = sample_metadata.metadata_template\n        breed = get_breed(nft_contract.tokenIdToBreed(token_id))\n        metadata_file_name = (\n            \".\/metadata\/{}\/\".format(network.show_active())\n            + str(token_id)\n            + \"-\"\n            + breed\n            + \".json\"\n        )\n        if Path(metadata_file_name).exists():\n            print(\n                \"{} already found, delete it to overwrite!\".format(\n                    metadata_file_name)\n            )\n        else:\n            print(\"Creating Metadata file: \" + metadata_file_name)\n            collectible_metadata[\"name\"] = get_breed(\n                nft_contract.tokenIdToBreed(token_id)\n            )\n            collectible_metadata[\"description\"] = \"A beautiful sunset in Costa Rica version {}!\".format(\n                collectible_metadata[\"name\"]\n            )\n            image_to_upload = None\n            if os.getenv(\"UPLOAD_IPFS\") == \"true\":\n                image_path = \".\/img\/{}.jpg\".format(\n                    breed.lower().replace('_', '-'))\n                image_to_upload = upload_to_ipfs(image_path)\n            image_to_upload = (\n                dog_metadata_dic[breed] if not image_to_upload else image_to_upload\n            )\n            collectible_metadata[\"image\"] = image_to_upload\n            with open(metadata_file_name, \"w\") as file:\n                json.dump(collectible_metadata, file)\n            if os.getenv(\"UPLOAD_IPFS\") == \"true\":\n                upload_to_ipfs(metadata_file_name)\n\n# curl -X POST -F file=@metadata\/rinkeby\/0-SHIBA_INU.json http:\/\/localhost:5001\/api\/v0\/add\n\n\ndef upload_to_ipfs(filepath):\n    with Path(filepath).open(\"rb\") as fp:\n        image_binary = fp.read()\n        ipfs_url = (\n            os.getenv(\"IPFS_URL\")\n            if os.getenv(\"IPFS_URL\")\n            else \"http:\/\/localhost:5001\"\n        )\n        response = requests.post(ipfs_url + \"\/api\/v0\/add\",\n                                 files={\"file\": image_binary})\n        ipfs_hash = response.json()[\"Hash\"]\n        filename = filepath.split(\"\/\")[-1:][0]\n        image_uri = \"https:\/\/ipfs.io\/ipfs\/{}?filename={}\".format(\n            ipfs_hash, filename)\n        print(image_uri)\n    return image_uri\n","label":1}
{"content":"from django import forms\n\n#from pagedown.widget import PagedownWidget\n\nfrom .models import Post\n\nclass PostForm(forms.ModelForm):\n\n    content = forms.CharField()\n    #publish = forms.DateField(widget=forms.SelectDateWidget)\n    class Meta:\n        model = Post\n        fields = [\n            \"title\", \n            \"content\", \n            \"image\", \n            #\"publish\",\n        ]","label":1}
{"content":"# Copyright (C) 2018-2022 Intel Corporation\n# SPDX-License-Identifier: Apache-2.0\n\nimport unittest\n\nfrom openvino.tools.mo.front.onnx.MvnOnnxToMvn import MvnOnnxToMvn\nfrom openvino.tools.mo.front.common.partial_infer.utils import int64_array\nfrom openvino.tools.mo.utils.ir_engine.compare_graphs import compare_graphs\nfrom unit_tests.utils.graph import build_graph, regular_op_with_empty_data, result, const, connect_front\n\nnodes = {\n    **regular_op_with_empty_data('input', {'type': 'Parameter'}),\n    **regular_op_with_empty_data('mvn_onnx', {'op': 'MVNOnnx',\n                                              'axes': int64_array([2, 3]),\n                                              'eps': 1e-9,\n                                              'eps_mode': 'outside_sqrt',\n                                              'normalize_variance': 1}),\n    **result(),\n\n    # nodes after replacement\n    **const('axes', int64_array([2, 3])),\n    **regular_op_with_empty_data('mvn', {'op': 'MVN', 'type': None}),\n}\n\n\nclass MvnOnnxToMvnTest(unittest.TestCase):\n    def test_mvn_normalize(self):\n        graph = build_graph(nodes, [('input', 'mvn_onnx'),\n                                    ('mvn_onnx', 'output')],\n                            nodes_with_edges_only=True)\n        graph.stage = 'front'\n\n        MvnOnnxToMvn().find_and_replace_pattern(graph)\n\n        graph_ref = build_graph(nodes, [('input', 'mvn'),\n                                        *connect_front('axes', '1:mvn'),\n                                        ('mvn', 'output')],\n                                nodes_with_edges_only=True)\n\n        (flag, resp) = compare_graphs(graph, graph_ref, 'output', check_op_attrs=True)\n        self.assertTrue(flag, resp)\n","label":1}
{"content":"import atexit\nimport glob\nimport os\nimport sys\nimport win32api\n\nfrom zipfile import ZipFile\nfrom distutils.dir_util import copy_tree\nfrom pkg_resources import parse_version\n\n_dir = os.path.join(os.path.dirname(sys.executable), 'updates')\n\n\nclass Updater:\n    def __init__(self, path_to_zip):\n        self.zipfile = path_to_zip\n\n    def extract(self):\n        with ZipFile(self.zipfile, 'r') as zipf:\n            print(\"Extracting...\", os.path.join(_dir, zipf.filename.rsplit('.zip')[0]))\n            if not os.path.isdir(_dir):\n                os.mkdir(_dir)\n            if not os.path.isdir(os.path.join(_dir, zipf.filename.rsplit('.zip')[0])):\n                os.mkdir(os.path.join(_dir, zipf.filename.rsplit('.zip')[0]))\n\n            zipf.extractall(os.path.join(_dir, zipf.filename.rsplit('.zip')[0]))\n            print(\"Done.\")\n            return os.path.join(_dir, zipf.filename.rsplit('.zip')[0])\n\n    @staticmethod\n    def install(from_path):\n        try:\n            copy_tree(from_path, os.getcwd(), preserve_mode=1, preserve_times=1)\n        except:\n            pass\n\n    def update(self):\n        zpf = self.extract()\n        current_info = win32api.GetFileVersionInfo('launcher.exe', \"\\\\\")\n        current_ms = current_info['FileVersionMS']\n        current_ls = current_info['FileVersionLS']\n        current_version = \"%d.%d.%d.%d\" % (win32api.HIWORD(current_ms), win32api.LOWORD(current_ms),\n                                           win32api.HIWORD(current_ls), win32api.LOWORD(current_ls))\n        current_ver = parse_version(current_version)\n\n        new_info = win32api.GetFileVersionInfo(os.path.join(zpf, 'launcher.exe'), \"\\\\\")\n        new_ms = new_info['FileVersionMS']\n        new_ls = new_info['FileVersionLS']\n        new_version = \"%d.%d.%d.%d\" % (win32api.HIWORD(new_ms), win32api.LOWORD(new_ms),\n                                       win32api.HIWORD(new_ls), win32api.LOWORD(new_ls))\n        new_ver = parse_version(new_version)\n        \n        if new_ver > current_ver:\n            self.install(zpf)\n            print(f\"Update Finished! {current_ver} -> {new_ver}\")\n        else:\n            print(f\"Update Failed! Package {new_ver} is either older or equivalent to the current installation {current_ver}\")\n\n\nif __name__ == \"__main__\":\n    files = glob.glob(os.path.join(_dir, '*.zip'))\n    latest_update = max(files, key=os.path.getctime)\n    u = Updater(latest_update)\n    u.update()\n    print(\"Starting launcher again..\")\n    atexit.register(os.execl, \"launcher.exe\", \"launcher.exe\")\n","label":1}
{"content":"# coding: utf-8\n\n\nclass TaskPaths:\n    TASK_DIR = '\/sly_task_data'\n    SETTINGS_PATH = '\/sly_task_data\/task_settings.json'  # Deprecated - use TASK_CONFIG_PATH instead\n    TASK_CONFIG_PATH = '\/sly_task_data\/task_config.json'\n    DATA_DIR = '\/sly_task_data\/data'\n    RESULTS_DIR = '\/sly_task_data\/results'\n    DEBUG_DIR = '\/sly_task_data\/tmp'\n    GRAPH_PATH = '\/sly_task_data\/graph.json'\n    MODEL_DIR = '\/sly_task_data\/model'\n    MODEL_CONFIG_PATH = '\/sly_task_data\/model\/config.json'\n    MODEL_CONFIG_NAME = 'config.json'\n\n","label":1}
{"content":"#!\/usr\/bin\/env python\n# -*- coding:utf-8 -*-\n# @Author: Jialiang Shi\nfrom sonarqube.utils.rest_client import RestClient\nfrom sonarqube.utils.config import (\n    API_AUTH_LOGIN_ENDPOINT,\n    API_AUTH_LOGOUT_ENDPOINT,\n    API_AUTH_VALIDATE_ENDPOINT,\n)\nfrom sonarqube.utils.common import GET, POST\n\n\nclass SonarQubeAuth(RestClient):\n    \"\"\"\n    SonarQube authentication Operations\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n\n        :param kwargs:\n        \"\"\"\n        super(SonarQubeAuth, self).__init__(**kwargs)\n\n    @POST(API_AUTH_LOGIN_ENDPOINT)\n    def authenticate_user(self, login, password):\n        \"\"\"\n        Authenticate a user.\n\n        :param login: Login of the user\n        :param password: Password of the user\n        :return:\n        \"\"\"\n\n    @POST(API_AUTH_LOGOUT_ENDPOINT)\n    def logout_user(self):\n        \"\"\"\n        Logout a user.\n\n        :return:\n        \"\"\"\n\n    @GET(API_AUTH_VALIDATE_ENDPOINT)\n    def check_credentials(self):\n        \"\"\"\n        Check credentials.\n\n        :return:\n        \"\"\"\n","label":1}
{"content":"# Ant-FS\n#\n# Copyright (c) 2012, Gustav Tiger <gustav@tiger.name>\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and\/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\n\nimport array\nimport unittest\nimport datetime\nimport sys\n\nfrom ant.fs.file import Directory\nfrom ant.fs.file import File\n\n\nclass DirectoryParse(unittest.TestCase):\n    def test_parse(self):\n\n        self.dir = array.array(\n            \"B\",\n            b\"\\x01\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\"\n            b\"\\x01\\x0c\\x00\\x00\\x00\\x50\\x00\\xe0\\x19\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x01\\x0d\"\n            b\"\\x00\\x00\\x00\\x30\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x80\\x01\\xff\\xff\"\n            b\"\\x00\\x90\\x5c\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x80\\x02\\xff\\xff\\x00\\xd0\"\n            b\"\\x1d\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x80\\x03\\x03\\x00\\x00\\xd0\\xac\\x04\"\n            b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x80\\x03\\x01\\x00\\x00\\xd0\\xac\\x04\\x00\\x00\"\n            b\"\\x00\\x00\\x00\\x00\\x07\\x00\\x80\\x04\\x21\\x00\\x00\\xb0\\x20\\x09\\x00\\x00\\x80\\xfa\"\n            b\"\\xd5\\x29\\x08\\x00\\x80\\x04\\x22\\x00\\x00\\xb0\\xa0\\x31\\x00\\x00\\x82\\xfa\\xd5\\x29\"\n            b\"\\x09\\x00\\x80\\x04\\x23\\x00\\x00\\xb0\\xb8\\x17\\x00\\x00\\x82\\xfa\\xd5\\x29\\x0a\\x00\"\n            b\"\\x80\\x04\\x24\\x00\\x00\\xb0\\xe9\\x02\\x00\\x00\\x82\\xfa\\xd5\\x29\\x0b\\x00\\x80\\x04\"\n            b\"\\x25\\x00\\x00\\xb0\\x8b\\x03\\x00\\x00\\x84\\xfa\\xd5\\x29\\x0c\\x00\\x80\\x04\\x26\\x00\"\n            b\"\\x00\\xb0\\xe9\\x02\\x00\\x00\\x84\\xfa\\xd5\\x29\\x0d\\x00\\x80\\x04\\x27\\x00\\x00\\xb0\"\n            b\"\\x2d\\x04\\x00\\x00\\x86\\xfa\\xd5\\x29\\x0e\\x00\\x80\\x04\\x28\\x00\\x00\\xb0\\x31\\x1d\"\n            b\"\\x00\\x00\\x86\\xfa\\xd5\\x29\\x0f\\x00\\x80\\x04\\x29\\x00\\x00\\xb0\\x59\\x1a\\x00\\x00\"\n            b\"\\x86\\xfa\\xd5\\x29\\x10\\x00\\x80\\x04\\x2a\\x00\\x00\\xb0\\xad\\x3d\\x00\\x00\\x88\\xfa\"\n            b\"\\xd5\\x29\\x11\\x00\\x80\\x04\\x2b\\x00\\x00\\xb0\\x50\\x43\\x00\\x00\\x8a\\xfa\\xd5\\x29\"\n            b\"\\x12\\x00\\x80\\x04\\x2c\\x00\\x00\\xb0\\x6b\\x2e\\x00\\x00\\x8a\\xfa\\xd5\\x29\\x13\\x00\"\n            b\"\\x80\\x04\\x2d\\x00\\x00\\xb0\\x28\\x1a\\x00\\x00\\x8c\\xfa\\xd5\\x29\\x14\\x00\\x80\\x04\"\n            b\"\\x2e\\x00\\x00\\xb0\\xd9\\x17\\x00\\x00\\x8c\\xfa\\xd5\\x29\\x15\\x00\\x80\\x04\\x2f\\x00\"\n            b\"\\x00\\xb0\\x6c\\x03\\x00\\x00\\x90\\xfa\\xd5\\x29\\x16\\x00\\x80\\x04\\x30\\x00\\x00\\xb0\"\n            b\"\\xa6\\x50\\x00\\x00\\x90\\xfa\\xd5\\x29\\x17\\x00\\x80\\x04\\x31\\x00\\x00\\xb0\\x9f\\x3e\"\n            b\"\\x00\\x00\\x92\\xfa\\xd5\\x29\\x18\\x00\\x80\\x04\\x32\\x00\\x00\\xb0\\xfd\\x0f\\x00\\x00\"\n            b\"\\x94\\xfa\\xd5\\x29\\x19\\x00\\x80\\x04\\x33\\x00\\x00\\xb0\\xa3\\x18\\x00\\x00\\x96\\xfa\"\n            b\"\\xd5\\x29\\x1a\\x00\\x80\\x04\\x34\\x00\\x00\\xb0\\x38\\x19\\x00\\x00\\x96\\xfa\\xd5\\x29\"\n            b\"\\x1b\\x00\\x80\\x04\\x35\\x00\\x00\\xb0\\x9e\\x16\\x00\\x00\\x98\\xfa\\xd5\\x29\\x1c\\x00\"\n            b\"\\x80\\x04\\x36\\x00\\x00\\xb0\\x72\\x13\\x00\\x00\\x9a\\xfa\\xd5\\x29\\x1d\\x00\\x80\\x04\"\n            b\"\\x37\\x00\\x00\\xb0\\xef\\x17\\x00\\x00\\x9a\\xfa\\xd5\\x29\\x1e\\x00\\x80\\x04\\x38\\x00\"\n            b\"\\x00\\xb0\\x9b\\x23\\x00\\x00\\x9c\\xfa\\xd5\\x29\\x1f\\x00\\x80\\x04\\x39\\x00\\x00\\xb0\"\n            b\"\\x9c\\x13\\x00\\x00\\x9e\\xfa\\xd5\\x29\",\n        )\n\n        directory = Directory.parse(self.dir)\n        self.assertEqual(directory.get_version(), (0, 1))\n        self.assertEqual(directory.get_time_format(), 0)\n        self.assertEqual(directory.get_current_system_time(), 0)\n        self.assertEqual(directory.get_last_modified(), 0)\n        self.assertEqual(len(directory.get_files()), 31)\n\n\nclass FileParse(unittest.TestCase):\n    def test_parse(self):\n        self.file_binary = array.array(\n            \"B\", b\"\\x07\\x00\\x80\\x04\\x21\\x00\\x00\\xb0\\x20\\x09\\x00\\x00\\x80\\xfa\\xd5\\x29\"\n        )\n\n        file_object = File.parse(self.file_binary)\n        self.assertEqual(file_object.get_index(), 7)\n        self.assertEqual(file_object.get_type(), File.Type.FIT)\n        self.assertEqual(\n            file_object.get_identifier(), array.array(\"B\", b\"\\x04\\x21\\x00\")\n        )\n        self.assertEqual(file_object.get_fit_sub_type(), File.Identifier.ACTIVITY)\n        self.assertEqual(file_object.get_fit_file_number(), 33)\n        self.assertEqual(file_object.get_size(), 2336)\n        self.assertEqual(\n            file_object.get_date().year, datetime.datetime(2012, 3, 28, 17, 12, 32).year\n        )\n        if sys.version_info >= (3, 3):\n            self.assertEqual(\n                file_object.get_date(),\n                datetime.datetime(\n                    2012, 3, 28, 17, 12, 32, tzinfo=datetime.timezone.utc\n                ),\n            )\n        self.assertTrue(file_object.is_readable())\n        self.assertFalse(file_object.is_writable())\n        self.assertTrue(file_object.is_erasable())\n        self.assertTrue(file_object.is_archived())\n        self.assertFalse(file_object.is_append_only())\n        self.assertFalse(file_object.is_encrypted())\n        self.assertEqual(file_object.get_flags_string(), \"r-eA--\")\n","label":1}
{"content":"# coding: utf-8\n\nimport re\nimport six\n\n\nfrom huaweicloudsdkcore.sdk_response import SdkResponse\nfrom huaweicloudsdkcore.utils.http_utils import sanitize_for_serialization\n\n\nclass CreateEdgeApplicationVersionResponse(SdkResponse):\n\n\n    \"\"\"\n    Attributes:\n      openapi_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n\n    sensitive_list = []\n\n    openapi_types = {\n        'edge_app_id': 'str',\n        'name': 'str',\n        'deploy_type': 'str',\n        'version': 'str',\n        'description': 'str',\n        'create_time': 'str',\n        'update_time': 'str',\n        'state': 'str',\n        'liveness_probe': 'ProbeDTO',\n        'readiness_probe': 'ProbeDTO',\n        'arch': 'list[str]',\n        'command': 'list[str]',\n        'args': 'list[str]',\n        'container_settings': 'ContainerSettingsDTO',\n        'outputs': 'list[str]',\n        'inputs': 'list[str]',\n        'services': 'list[str]',\n        'publish_time': 'str',\n        'off_shelf_time': 'str'\n    }\n\n    attribute_map = {\n        'edge_app_id': 'edge_app_id',\n        'name': 'name',\n        'deploy_type': 'deploy_type',\n        'version': 'version',\n        'description': 'description',\n        'create_time': 'create_time',\n        'update_time': 'update_time',\n        'state': 'state',\n        'liveness_probe': 'liveness_probe',\n        'readiness_probe': 'readiness_probe',\n        'arch': 'arch',\n        'command': 'command',\n        'args': 'args',\n        'container_settings': 'container_settings',\n        'outputs': 'outputs',\n        'inputs': 'inputs',\n        'services': 'services',\n        'publish_time': 'publish_time',\n        'off_shelf_time': 'off_shelf_time'\n    }\n\n    def __init__(self, edge_app_id=None, name=None, deploy_type=None, version=None, description=None, create_time=None, update_time=None, state=None, liveness_probe=None, readiness_probe=None, arch=None, command=None, args=None, container_settings=None, outputs=None, inputs=None, services=None, publish_time=None, off_shelf_time=None):\n        \"\"\"CreateEdgeApplicationVersionResponse - a model defined in huaweicloud sdk\"\"\"\n        \n        super(CreateEdgeApplicationVersionResponse, self).__init__()\n\n        self._edge_app_id = None\n        self._name = None\n        self._deploy_type = None\n        self._version = None\n        self._description = None\n        self._create_time = None\n        self._update_time = None\n        self._state = None\n        self._liveness_probe = None\n        self._readiness_probe = None\n        self._arch = None\n        self._command = None\n        self._args = None\n        self._container_settings = None\n        self._outputs = None\n        self._inputs = None\n        self._services = None\n        self._publish_time = None\n        self._off_shelf_time = None\n        self.discriminator = None\n\n        if edge_app_id is not None:\n            self.edge_app_id = edge_app_id\n        if name is not None:\n            self.name = name\n        if deploy_type is not None:\n            self.deploy_type = deploy_type\n        if version is not None:\n            self.version = version\n        if description is not None:\n            self.description = description\n        if create_time is not None:\n            self.create_time = create_time\n        if update_time is not None:\n            self.update_time = update_time\n        if state is not None:\n            self.state = state\n        if liveness_probe is not None:\n            self.liveness_probe = liveness_probe\n        if readiness_probe is not None:\n            self.readiness_probe = readiness_probe\n        if arch is not None:\n            self.arch = arch\n        if command is not None:\n            self.command = command\n        if args is not None:\n            self.args = args\n        if container_settings is not None:\n            self.container_settings = container_settings\n        if outputs is not None:\n            self.outputs = outputs\n        if inputs is not None:\n            self.inputs = inputs\n        if services is not None:\n            self.services = services\n        if publish_time is not None:\n            self.publish_time = publish_time\n        if off_shelf_time is not None:\n            self.off_shelf_time = off_shelf_time\n\n    @property\n    def edge_app_id(self):\n        \"\"\"Gets the edge_app_id of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528ID\n\n        :return: The edge_app_id of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._edge_app_id\n\n    @edge_app_id.setter\n    def edge_app_id(self, edge_app_id):\n        \"\"\"Sets the edge_app_id of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528ID\n\n        :param edge_app_id: The edge_app_id of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._edge_app_id = edge_app_id\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u540d\u79f0\n\n        :return: The name of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u540d\u79f0\n\n        :param name: The name of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._name = name\n\n    @property\n    def deploy_type(self):\n        \"\"\"Gets the deploy_type of this CreateEdgeApplicationVersionResponse.\n\n        \u90e8\u7f72\u7c7b\u578bdocker|process\n\n        :return: The deploy_type of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._deploy_type\n\n    @deploy_type.setter\n    def deploy_type(self, deploy_type):\n        \"\"\"Sets the deploy_type of this CreateEdgeApplicationVersionResponse.\n\n        \u90e8\u7f72\u7c7b\u578bdocker|process\n\n        :param deploy_type: The deploy_type of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._deploy_type = deploy_type\n\n    @property\n    def version(self):\n        \"\"\"Gets the version of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u7248\u672c\n\n        :return: The version of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._version\n\n    @version.setter\n    def version(self, version):\n        \"\"\"Sets the version of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u7248\u672c\n\n        :param version: The version of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._version = version\n\n    @property\n    def description(self):\n        \"\"\"Gets the description of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u63cf\u8ff0\n\n        :return: The description of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._description\n\n    @description.setter\n    def description(self, description):\n        \"\"\"Sets the description of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u63cf\u8ff0\n\n        :param description: The description of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._description = description\n\n    @property\n    def create_time(self):\n        \"\"\"Gets the create_time of this CreateEdgeApplicationVersionResponse.\n\n        \u521b\u5efa\u65f6\u95f4\n\n        :return: The create_time of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._create_time\n\n    @create_time.setter\n    def create_time(self, create_time):\n        \"\"\"Sets the create_time of this CreateEdgeApplicationVersionResponse.\n\n        \u521b\u5efa\u65f6\u95f4\n\n        :param create_time: The create_time of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._create_time = create_time\n\n    @property\n    def update_time(self):\n        \"\"\"Gets the update_time of this CreateEdgeApplicationVersionResponse.\n\n        \u6700\u540e\u4e00\u6b21\u4fee\u6539\u65f6\u95f4\n\n        :return: The update_time of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._update_time\n\n    @update_time.setter\n    def update_time(self, update_time):\n        \"\"\"Sets the update_time of this CreateEdgeApplicationVersionResponse.\n\n        \u6700\u540e\u4e00\u6b21\u4fee\u6539\u65f6\u95f4\n\n        :param update_time: The update_time of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._update_time = update_time\n\n    @property\n    def state(self):\n        \"\"\"Gets the state of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u7248\u672c\u72b6\u6001\n\n        :return: The state of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._state\n\n    @state.setter\n    def state(self, state):\n        \"\"\"Sets the state of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u7248\u672c\u72b6\u6001\n\n        :param state: The state of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._state = state\n\n    @property\n    def liveness_probe(self):\n        \"\"\"Gets the liveness_probe of this CreateEdgeApplicationVersionResponse.\n\n\n        :return: The liveness_probe of this CreateEdgeApplicationVersionResponse.\n        :rtype: ProbeDTO\n        \"\"\"\n        return self._liveness_probe\n\n    @liveness_probe.setter\n    def liveness_probe(self, liveness_probe):\n        \"\"\"Sets the liveness_probe of this CreateEdgeApplicationVersionResponse.\n\n\n        :param liveness_probe: The liveness_probe of this CreateEdgeApplicationVersionResponse.\n        :type: ProbeDTO\n        \"\"\"\n        self._liveness_probe = liveness_probe\n\n    @property\n    def readiness_probe(self):\n        \"\"\"Gets the readiness_probe of this CreateEdgeApplicationVersionResponse.\n\n\n        :return: The readiness_probe of this CreateEdgeApplicationVersionResponse.\n        :rtype: ProbeDTO\n        \"\"\"\n        return self._readiness_probe\n\n    @readiness_probe.setter\n    def readiness_probe(self, readiness_probe):\n        \"\"\"Sets the readiness_probe of this CreateEdgeApplicationVersionResponse.\n\n\n        :param readiness_probe: The readiness_probe of this CreateEdgeApplicationVersionResponse.\n        :type: ProbeDTO\n        \"\"\"\n        self._readiness_probe = readiness_probe\n\n    @property\n    def arch(self):\n        \"\"\"Gets the arch of this CreateEdgeApplicationVersionResponse.\n\n        \u67b6\u6784\n\n        :return: The arch of this CreateEdgeApplicationVersionResponse.\n        :rtype: list[str]\n        \"\"\"\n        return self._arch\n\n    @arch.setter\n    def arch(self, arch):\n        \"\"\"Sets the arch of this CreateEdgeApplicationVersionResponse.\n\n        \u67b6\u6784\n\n        :param arch: The arch of this CreateEdgeApplicationVersionResponse.\n        :type: list[str]\n        \"\"\"\n        self._arch = arch\n\n    @property\n    def command(self):\n        \"\"\"Gets the command of this CreateEdgeApplicationVersionResponse.\n\n        \u542f\u52a8\u547d\u4ee4\n\n        :return: The command of this CreateEdgeApplicationVersionResponse.\n        :rtype: list[str]\n        \"\"\"\n        return self._command\n\n    @command.setter\n    def command(self, command):\n        \"\"\"Sets the command of this CreateEdgeApplicationVersionResponse.\n\n        \u542f\u52a8\u547d\u4ee4\n\n        :param command: The command of this CreateEdgeApplicationVersionResponse.\n        :type: list[str]\n        \"\"\"\n        self._command = command\n\n    @property\n    def args(self):\n        \"\"\"Gets the args of this CreateEdgeApplicationVersionResponse.\n\n        \u542f\u52a8\u53c2\u6570\n\n        :return: The args of this CreateEdgeApplicationVersionResponse.\n        :rtype: list[str]\n        \"\"\"\n        return self._args\n\n    @args.setter\n    def args(self, args):\n        \"\"\"Sets the args of this CreateEdgeApplicationVersionResponse.\n\n        \u542f\u52a8\u53c2\u6570\n\n        :param args: The args of this CreateEdgeApplicationVersionResponse.\n        :type: list[str]\n        \"\"\"\n        self._args = args\n\n    @property\n    def container_settings(self):\n        \"\"\"Gets the container_settings of this CreateEdgeApplicationVersionResponse.\n\n\n        :return: The container_settings of this CreateEdgeApplicationVersionResponse.\n        :rtype: ContainerSettingsDTO\n        \"\"\"\n        return self._container_settings\n\n    @container_settings.setter\n    def container_settings(self, container_settings):\n        \"\"\"Sets the container_settings of this CreateEdgeApplicationVersionResponse.\n\n\n        :param container_settings: The container_settings of this CreateEdgeApplicationVersionResponse.\n        :type: ContainerSettingsDTO\n        \"\"\"\n        self._container_settings = container_settings\n\n    @property\n    def outputs(self):\n        \"\"\"Gets the outputs of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u8f93\u51fa\u8def\u7531\u7aef\u70b9\n\n        :return: The outputs of this CreateEdgeApplicationVersionResponse.\n        :rtype: list[str]\n        \"\"\"\n        return self._outputs\n\n    @outputs.setter\n    def outputs(self, outputs):\n        \"\"\"Sets the outputs of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u8f93\u51fa\u8def\u7531\u7aef\u70b9\n\n        :param outputs: The outputs of this CreateEdgeApplicationVersionResponse.\n        :type: list[str]\n        \"\"\"\n        self._outputs = outputs\n\n    @property\n    def inputs(self):\n        \"\"\"Gets the inputs of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u8f93\u5165\u8def\u7531\n\n        :return: The inputs of this CreateEdgeApplicationVersionResponse.\n        :rtype: list[str]\n        \"\"\"\n        return self._inputs\n\n    @inputs.setter\n    def inputs(self, inputs):\n        \"\"\"Sets the inputs of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u8f93\u5165\u8def\u7531\n\n        :param inputs: The inputs of this CreateEdgeApplicationVersionResponse.\n        :type: list[str]\n        \"\"\"\n        self._inputs = inputs\n\n    @property\n    def services(self):\n        \"\"\"Gets the services of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u5b9e\u73b0\u7684\u670d\u52a1\u5217\u8868\n\n        :return: The services of this CreateEdgeApplicationVersionResponse.\n        :rtype: list[str]\n        \"\"\"\n        return self._services\n\n    @services.setter\n    def services(self, services):\n        \"\"\"Sets the services of this CreateEdgeApplicationVersionResponse.\n\n        \u5e94\u7528\u5b9e\u73b0\u7684\u670d\u52a1\u5217\u8868\n\n        :param services: The services of this CreateEdgeApplicationVersionResponse.\n        :type: list[str]\n        \"\"\"\n        self._services = services\n\n    @property\n    def publish_time(self):\n        \"\"\"Gets the publish_time of this CreateEdgeApplicationVersionResponse.\n\n        \u53d1\u5e03\u65f6\u95f4\n\n        :return: The publish_time of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._publish_time\n\n    @publish_time.setter\n    def publish_time(self, publish_time):\n        \"\"\"Sets the publish_time of this CreateEdgeApplicationVersionResponse.\n\n        \u53d1\u5e03\u65f6\u95f4\n\n        :param publish_time: The publish_time of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._publish_time = publish_time\n\n    @property\n    def off_shelf_time(self):\n        \"\"\"Gets the off_shelf_time of this CreateEdgeApplicationVersionResponse.\n\n        \u4e0b\u7ebf\u65f6\u95f4\n\n        :return: The off_shelf_time of this CreateEdgeApplicationVersionResponse.\n        :rtype: str\n        \"\"\"\n        return self._off_shelf_time\n\n    @off_shelf_time.setter\n    def off_shelf_time(self, off_shelf_time):\n        \"\"\"Sets the off_shelf_time of this CreateEdgeApplicationVersionResponse.\n\n        \u4e0b\u7ebf\u65f6\u95f4\n\n        :param off_shelf_time: The off_shelf_time of this CreateEdgeApplicationVersionResponse.\n        :type: str\n        \"\"\"\n        self._off_shelf_time = off_shelf_time\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.openapi_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                if attr in self.sensitive_list:\n                    result[attr] = \"****\"\n                else:\n                    result[attr] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        import simplejson as json\n        if six.PY2:\n            import sys\n            reload(sys)\n            sys.setdefaultencoding(\"utf-8\")\n        return json.dumps(sanitize_for_serialization(self), ensure_ascii=False)\n\n    def __repr__(self):\n        \"\"\"For `print`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, CreateEdgeApplicationVersionResponse):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        return not self == other\n","label":1}
{"content":"import smtplib  #import SMTP lib\n\nss = smtplib.SMTP('smtp.gmail.com', 587)   #connect with GOOGLE SMTP\n\nss.starttls()                               #startServices\nss.login(\"Enter your Email id\", \"Enter your password\")               #Enter Login Details\n\nmsg = \"this is a testmail\"                           #message you would like to attach      \nss.sendmail(\"Your Email id\", \"id you would like to send\", msg)  \nss.quit()\n","label":1}
{"content":"import tkinter\n\n\nclass SelectFolderFrames(\n        tkinter.Frame):\n    def __init__(\n            self,\n            master,\n            label_text,\n            **kwargs):\n        tkinter.Frame.__init__(\n            self,\n            master,\n            **kwargs)\n\n        self.text_box_label = \\\n            tkinter.Text(\n                height=1, width=100)\n\n        self.text_box_label.pack()\n\n        self.text_box_label.insert(\n            tkinter.END,\n            label_text)\n\n        self.text_box = \\\n            tkinter.Text(\n                height=1, width=100)\n\n        self.text_box.pack()\n\n        # self.text_box.insert(tkinter.END, \"Just a text Widget\\nin two lines\\n\")\n\n\n","label":1}
{"content":"#!\/usr\/bin\/env python\n\nimport os\nimport sys\nimport shutil\n\nVERSION = 0.1\n\nINFO = \"\"\"IBL stuff toolset for Nuke.\n\nCommon commands:\n\n  setup.py install      will copy and register the tools into ~\/.nuke\/\n  setup.py uninstall    will uninstall the tools\n\nInformation display options:\n\n  --help                list all available commands\n  --version             print tool version\"\"\"\n\nREGISTER = \"\"\"# Entries plugin path for ibl_stuff toolset\nnuke.pluginAddPath(\".\/ibl_stuff\")\nnuke.pluginAddPath(\".\/ibl_stuff\/gizmos\")\nnuke.pluginAddPath(\".\/ibl_stuff\/icons\")\"\"\"\n\nNUKE_DIR = os.path.join(os.path.expanduser(\"~\"), \".nuke\")\nINSTALL_DIR = os.path.join(NUKE_DIR, \"ibl_stuff\")\n\n\ndef install():\n    print(\"Copying files...\")\n    if not os.path.exists(NUKE_DIR):\n        os.mkdir(NUKE_DIR)\n    if not os.path.exists(INSTALL_DIR):\n        os.mkdir(INSTALL_DIR)\n    current_dir = os.path.dirname(__file__)\n\n    shutil.copyfile(os.path.join(current_dir, \"menu.py\"),\n                    os.path.join(INSTALL_DIR, \"menu.py\"))\n    for d in (\"gizmos\", \"icons\"):\n        shutil.copytree(os.path.join(current_dir, d),\n                        os.path.join(INSTALL_DIR, d))\n\n    print(\"Registering menu...\")\n    init = \"\"\n    filepath = os.path.join(NUKE_DIR, \"init.py\")\n    if os.path.exists(filepath):\n        with open(filepath) as f:\n            init = f.read()\n    if REGISTER not in init:\n        init += \"\\n\" + REGISTER\n    with open(filepath, \"w\") as f:\n        f.write(init)\n\n    print(\"IBL Stuff has been successfully installed on your system.\")\n\n\ndef uninstall():\n    filepath = os.path.join(NUKE_DIR, \"init.py\")\n    print(\"Unregistering...\")\n    reglines = REGISTER.splitlines()\n    if os.path.exists(filepath):\n        init = list()\n        with open(filepath) as f:\n            for l in f.readlines():\n                if any([r in l for r in reglines]):\n                    continue\n                init.append(l)\n        with open(filepath, \"w\") as f:\n            f.write(\"\".join(init))\n\n    print(\"Removing files...\")\n    if os.path.exists(INSTALL_DIR):\n        shutil.rmtree(INSTALL_DIR)\n\n    print(\"IBL Stuff has been successfully uninstalled.\")\n\n\ndef show_help():\n    print(INFO)\n\n\ndef show_version():\n    print(\"IBL Stuff toolset for Nuke version {}\".format(VERSION))\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        sys.argv.append(\"install\")\n    {\n        \"install\": install,\n        \"uninstall\": uninstall,\n        \"help\": show_help,\n        \"--help\": show_help,\n        \"-h\": show_help,\n        \"version\": show_version,\n        \"--version\": show_version,\n        \"-v\": show_version,\n    }.get(sys.argv[1], show_help)()\n","label":1}
{"content":"from typing import Tuple\nimport aioredis\nfrom discord import Server, Channel, User\n\nfrom .BasePlugin import BasePlugin\nfrom . import Bot\nfrom .Enums import RedisStorageScope\n\n\nclass RedisManager(object):\n\n    def __init__(self, bot: \"Bot.Bot\", address: Tuple[str, int]):\n        self._bot = bot  # type: Bot.Bot\n        self._address = address  # type: Tuple[str, int]\n        self.conn = None  # type: aioredis.Redis\n        self._bot.EventManager.loop.create_task(self.connect())\n\n    async def connect(self):\n        self.conn = await aioredis.create_redis(self._address,\n                                                loop=self._bot.EventManager.loop)\n\n    def get_storage(self, obj=None):\n        if obj is None:\n            return RedisStorage(self.conn)\n        else:\n            if isinstance(obj, BasePlugin):\n                return RedisStorage(self.conn, RedisStorageScope.PLUGIN, obj)\n            elif isinstance(obj, Server):\n                return RedisStorage(self.conn, RedisStorageScope.SERVER, obj)\n            elif isinstance(obj, Channel):\n                return RedisStorage(self.conn, RedisStorageScope.CHANNEL, obj)\n            elif isinstance(obj, User):\n                return RedisStorage(self.conn, RedisStorageScope.USER, obj)\n\n\nclass RedisStorage(object):\n\n    def __init__(self, redis: aioredis.Redis, scope: RedisStorageScope=RedisStorageScope.GLOBAL, scope_obj=None):\n        self.scope = scope\n        self.object = scope_obj  # type: RedisStorageScope\n        self.redis = redis  # type: aioredis.Redis\n\n    def generate_key(self, key: str) -> str:\n        if self.scope == RedisStorageScope.GLOBAL:\n            return \"GLOBAL:{}\".format(key)\n        elif self.scope == RedisStorageScope.PLUGIN:\n            return \"PLUGIN:{}:{}\".format(self.object.PLUGIN_NAME.replace(\" \", \"\"), key)\n        else:\n            return \"{}:{}:{}\".format(self.scope.name, self.object.id, key)\n\n    async def delete(self, key):\n        return await self.redis.delete(self.generate_key(key))\n\n    async def set(self, key, value, expire: int=0):\n        return await self.redis.set(self.generate_key(key),\n                                    value,\n                                    expire=expire)\n\n    async def get(self, key):\n        return await self.redis.get(self.generate_key(key))\n\n    async def smembers(self, key):\n        return await self.redis.smembers(self.generate_key(key))\n\n    async def srem(self, key, value):\n        return await self.redis.srem(self.generate_key(key), value)\n\n    async def sadd(self, key, member, *members):\n        return await self.redis.sadd(self.generate_key(key), member, *members)\n\n    async def ttl(self, key):\n        return await self.redis.ttl(self.generate_key(key))\n\n    async def expire(self, key, timeout):\n        return await self.redis.expire(self.generate_key(key), timeout)\n\n    async def incr(self, key):\n        return await self.redis.incr(self.generate_key(key))\n\n    async def incrby(self, key, amount):\n        return await self.redis.incrby(self.generate_key(key), amount)\n\n    async def setnx(self, key, value):\n        return await self.redis.setnx(self.generate_key(key), value)\n\n    async def lpush(self, key, value, *values):\n        return await self.redis.lpush(self.generate_key(key), value, *values)\n\n    async def lrange(self, key, start, stop):\n        return await self.redis.lrange(self.generate_key(key), start, stop)\n\n    async def lrem(self, key, count, value):\n        return await self.redis.lrem(self.generate_key(key), count, value)\n\n    async def lset(self, key, index, value):\n        return await self.redis.lset(self.generate_key(key), index, value)\n\n    async def exists(self, key):\n        return await self.redis.exists(self.generate_key(key))\n","label":1}
{"content":"from django.shortcuts import render, HttpResponse\nfrom django.views.decorators.cache import cache_page\nfrom django.db.models import Q\n\nfrom news.models import News\nfrom products.models import Product\n\n\n@cache_page(60 * 15)  # \u5355\u4f4d\uff1a\u79d2\u6570\uff0c\u8fd9\u91cc\u6307\u7f13\u5b58 15 \u5206\u949f\ndef home(request):\n    # \u65b0\u95fb\u5c55\u62a5\n    news_list = News.objects.all().filter(~Q(news_type='\u901a\u77e5\u516c\u544a')).order_by('-publish_date')\n    post_list = set()\n    post_num = 0\n    for s in news_list:\n        if s.news_photo:\n            post_list.add(s)\n            post_num += 1\n        if post_num == 3:  # \u53ea\u622a\u53d6\u6700\u8fd1\u76843\u4e2a\u5c55\u62a5\n            break\n\n    # \u65b0\u95fb\u5217\u8868\n    if len(news_list) > 7:\n        news_list = news_list[0:7]\n\n    # \u901a\u77e5\u516c\u544a\n    notice_list = News.objects.all().filter(Q(news_type='\u901a\u77e5\u516c\u544a')).order_by('-publish_date')\n    if len(notice_list) > 4:\n        notice_list = notice_list[0:4]\n\n    # \u4ea7\u54c1\u4e2d\u5fc3\n    product_list = Product.objects.all().order_by('-product_views')\n    if len(product_list) > 4:\n        product_list = product_list[0:4]\n\n    # \u8fd4\u56de\u7ed3\u679c\n    context = {\n        'active_menu': 'home',\n        'post_list': post_list,\n        'news_list': news_list,\n        'notice_list': notice_list,\n        'product_list': product_list,\n    }\n    return render(request, 'home\/home.html', context)\n","label":1}
{"content":"# Licensed under a 3-clause BSD style license - see LICENSE.rst\nfrom __future__ import print_function\nimport os\nimport requests\n\nfrom astropy.tests.helper import pytest\nimport astropy.units as u\nimport astropy.coordinates as coord\nfrom astropy.table import Table\n\nfrom ... import nrao\nfrom ...utils.testing_tools import MockResponse\nfrom ...utils import commons\n\n\ndef data_path(filename):\n    data_dir = os.path.join(os.path.dirname(__file__), 'data')\n    return os.path.join(data_dir, filename)\n\nDATA_FILES = {'votable': 'votable.xml'}\n\n\n@pytest.fixture\ndef patch_parse_coordinates(request):\n    def parse_coordinates_mock_return(c):\n        return c\n    mp = request.getfuncargvalue(\"monkeypatch\")\n    mp.setattr(commons, 'parse_coordinates', parse_coordinates_mock_return)\n    return mp\n\n\n@pytest.fixture\ndef patch_get(request):\n    mp = request.getfuncargvalue(\"monkeypatch\")\n    mp.setattr(requests, 'get', get_mockreturn)\n    return mp\n\n\ndef get_mockreturn(url, params=None, timeout=10, **kwargs):\n    filename = data_path(DATA_FILES['votable'])\n    content = open(filename, 'r').read()\n    return MockResponse(content, **kwargs)\n\n\ndef test_query_region_async(patch_get, patch_parse_coordinates):\n    response = nrao.core.Nrao.query_region_async(coord.ICRS(\"04h33m11.1s 05d21m15.5s\"),\n                                           radius='5d0m0s', equinox='B1950',\n                                           freq_low=1000*u.kHz, freq_up=2000*u.kHz,\n                                           get_query_payload=True)\n\n    assert response['SRAD'].startswith('5') and response['SRAD'].endswith('d')\n    assert response['EQUINOX'] == 'B1950'\n    assert response['OBSFREQ1'] == '1.0-2.0'\n    response = nrao.core.Nrao.query_region_async(coord.ICRS(\"04h33m11.1s 05d21m15.5s\"))\n    assert response is not None\n\n\ndef test_query_region(patch_get, patch_parse_coordinates):\n    result = nrao.core.Nrao.query_region(coord.ICRS(\"04h33m11.1s 05d21m15.5s\"))\n    assert isinstance(result, Table)\n    assert len(result) > 0\n    if 'Start Time' in result.colnames:\n        assert result['Start Time'][0] == b'83-Sep-27 09:19:30'\n    else:\n        assert result['Start_Time'][0] == b'83-Sep-27 09:19:30'\n    assert result['RA'][0] == b'04h33m11.096s'\n\n","label":1}
{"content":"#\n#  BSD LICENSE\n#\n#  Copyright (c) Crane Chu <cranechu@gmail.com>\n#  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions\n#  are met:\n#\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#    * Redistributions in binary form must reproduce the above copyright\n#      notice, this list of conditions and the following disclaimer in\n#      the documentation and\/or other materials provided with the\n#      distribution.\n#    * Neither the name of Intel Corporation nor the names of its\n#      contributors may be used to endorse or promote products derived\n#      from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n#  \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# -*- coding: utf-8 -*-\n\n\n# for pypi package information\nimport setuptools\nimport subprocess\nfrom setuptools.command.install import install\n\nclass CustomInstall(install):\n    def run(self):\n        subprocess.call(\"sudo dnf install -y make\", shell=True)\n        install.run(self)\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\n    \nsetuptools.setup(\n    name=\"pynvme\",\n    version=\"2.3.0\",\n    author=\"Crane Chu\",\n    author_email=\"cranechu@gmail.com\",\n    description=\"builds your own tests.\",\n    long_description=long_description,\n    long_description_content_type=\"text\/markdown\",\n    url=\"https:\/\/github.com\/pynvme\/pynvme\",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        \"Development Status :: 5 - Production\/Stable\",\n        \"Programming Language :: C\",\n        \"Programming Language :: Python :: 3.5\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Operating System :: POSIX :: Linux\",\n    ],\n    python_requires='>=3.5',\n    install_requires=['pytest',\n                      'pytest-excel',\n                      'libpci',\n                      'pylspci',\n                      'quarchpy',\n                      'pytemperature'],\n    data_files=[\n        ('pynvme',\n         ['nvme.so',\n          'Makefile',\n          'conftest.py',\n          'driver_test.py',\n          'pytest.ini']),\n        ('pynvme\/src',\n         ['src\/common.sh',\n          'src\/setup.sh']),\n        ('pynvme\/scripts',\n         ['scripts\/psd.py',\n          'scripts\/tcg.py',\n          'scripts\/test_examples.py',\n          'scripts\/test_utilities.py']),\n        ('pynvme\/scripts\/stress',\n         ['scripts\/stress\/dirty_power_cycle_test.py']),\n        ('pynvme\/scripts\/performance',\n         ['scripts\/performance\/1_fresh_perf_test.py',\n          'scripts\/performance\/2_init_time_test.py',\n          'scripts\/performance\/3_steady_perf_test.py',\n          'scripts\/performance\/4_nvme_cmd_test.py',\n          'scripts\/performance\/5_generate_report_test.py',\n          'scripts\/performance\/report_template.xlsx'\n          ]),\n        ('pynvme\/include\/spdk',\n         ['include\/spdk\/pci_ids.h']),\n        ('pynvme\/scripts\/conformance\/01_admin',\n         ['scripts\/conformance\/01_admin\/abort_test.py',\n          'scripts\/conformance\/01_admin\/identify_test.py',\n          'scripts\/conformance\/01_admin\/queue_test.py',\n          'scripts\/conformance\/01_admin\/firmware_test.py',\n          'scripts\/conformance\/01_admin\/dst_test.py',\n          'scripts\/conformance\/01_admin\/format_test.py',\n          'scripts\/conformance\/01_admin\/aer_test.py',\n          'scripts\/conformance\/01_admin\/sanitize_test.py',\n          'scripts\/conformance\/01_admin\/mi_test.py',\n          'scripts\/conformance\/01_admin\/features_test.py',\n          'scripts\/conformance\/01_admin\/logpage_test.py'\n          ]),\n        ('pynvme\/scripts\/conformance\/02_nvm',\n         ['scripts\/conformance\/02_nvm\/compare_test.py',\n          'scripts\/conformance\/02_nvm\/flush_test.py',\n          'scripts\/conformance\/02_nvm\/read_test.py',\n          'scripts\/conformance\/02_nvm\/write_uncorrectable_test.py',\n          'scripts\/conformance\/02_nvm\/deallocate_test.py',\n          'scripts\/conformance\/02_nvm\/write_test.py',\n          'scripts\/conformance\/02_nvm\/verify_test.py',\n          'scripts\/conformance\/02_nvm\/write_zeroes_test.py'\n          ]),\n        ('pynvme\/scripts\/conformance\/03_features',\n         ['scripts\/conformance\/03_features\/hmb_test.py',\n          'scripts\/conformance\/03_features\/write_protect_test.py',\n          'scripts\/conformance\/03_features\/power_management_test.py',\n          'scripts\/conformance\/03_features\/reset_test.py'\n          ]),\n        ('pynvme\/scripts\/conformance\/04_registers',\n         ['scripts\/conformance\/04_registers\/controller_test.py',\n          'scripts\/conformance\/04_registers\/power_test.py',\n          'scripts\/conformance\/04_registers\/pcie_test.py'\n          ]), \n        ('pynvme\/scripts\/conformance\/05_controller',\n         ['scripts\/conformance\/05_controller\/sq_cq_test.py',\n          'scripts\/conformance\/05_controller\/sqe_cqe_test.py',\n          'scripts\/conformance\/05_controller\/interrupt_test.py',\n          'scripts\/conformance\/05_controller\/prp_test.py',\n          'scripts\/conformance\/05_controller\/arbitration_test.py'\n          ]),\n        ('pynvme\/scripts\/conformance\/06_tcg',\n         ['scripts\/conformance\/06_tcg\/use_case_test.py'\n          ]),\n        ],\n    cmdclass={'install': CustomInstall},\n)\n","label":1}
{"content":"# -*- coding:utf-8 -*-\n\n# ---------------------------------------------\n# @file test_yml.py\n# @description test_yml\n# @author WcJun\n# @date 2021\/07\/15\n# ---------------------------------------------\nimport os\n\nimport pytest\nimport requests\n\nfrom src.main.python.util.yml_reader import YmlReader\n\n# yaml_directory_path = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\nyml_path = os.path.join(os.getcwd() + '{}src{}main{}resources{}test_yml.yml'.format(os.sep, os.sep, os.sep, os.sep))\nreader = YmlReader(yml_path)\ncontent = reader.read_yml()\n\n\nclass TestRefreshAccessToken:\n    \"\"\"\n    refresh access token\n    \"\"\"\n\n    @pytest.mark.parametrize('context', content)\n    def test_refresh_access_token(self, context):\n        \"\"\"\n        do refresh access token\n        \"\"\"\n        url = context['request']['url']\n        parameters = context['request']['parameters']\n        response = requests.get(url=url, params=parameters)\n        print(response.text)\n","label":1}
{"content":"from typing import Dict\n\nimport numpy as np\nfrom jina import Executor, DocumentArray, requests\nfrom jina.types.arrays.memmap import DocumentArrayMemmap\n\n\nclass MyIndexer(Executor):\n    \"\"\"\n    Executor with basic exact search using cosine distance\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._docs = DocumentArrayMemmap(self.workspace + '\/indexer')\n\n    @requests(on='\/index')\n    def index(self, docs: 'DocumentArray', **kwargs):\n        \"\"\"Extend self._docs\n\n        :param docs: DocumentArray containing Documents\n        :param kwargs: other keyword arguments\n        \"\"\"\n        self._docs.extend(docs)\n\n    @requests(on=['\/search', '\/eval'])\n    def search(self, docs: 'DocumentArray', parameters: Dict, **kwargs):\n        \"\"\"Append best matches to each document in docs\n\n        :param docs: documents that are searched\n        :param parameters: dictionary of pairs (parameter,value)\n        :param kwargs: other keyword arguments\n        \"\"\"\n        docs.match(\n            self._docs,\n            metric='cosine',\n            is_distance=False,\n            limit=int(parameters['top_k']),\n        )\n\n\nclass MyEncoder(Executor):\n    \"\"\"\n    Encode data using SVD decomposition\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        np.random.seed(1337)\n        # generate a random orthogonal matrix\n        H = np.random.rand(784, 64)\n        u, s, vh = np.linalg.svd(H, full_matrices=False)\n        self.oth_mat = u @ vh\n\n    @requests\n    def encode(self, docs: 'DocumentArray', **kwargs):\n        \"\"\"Encode the data using an SVD decomposition\n\n        :param docs: input documents to update with an embedding\n        :param kwargs: other keyword arguments\n        \"\"\"\n        # reduce dimension to 50 by random orthogonal projection\n        content = np.stack(docs.get_attributes('content'))\n        # content.shape=(request_size, 28, 28, 3)\n        content = content[:, :, :, 0].reshape(-1, 784)\n        # content.shape=(request_size, 784)\n        embeds = (content.reshape([-1, 784]) \/ 255) @ self.oth_mat\n        for doc, embed in zip(docs, embeds):\n            doc.embedding = embed\n            doc.convert_image_blob_to_uri(width=28, height=28)\n            doc.pop('blob')\n\n\nclass MyConverter(Executor):\n    \"\"\"\n    Convert DocumentArrays removing blob and reshaping blob as image\n    \"\"\"\n\n    @requests\n    def convert(self, docs: 'DocumentArray', **kwargs):\n        \"\"\"\n        Remove blob and reshape documents as squared images\n        :param docs: documents to modify\n        :param kwargs: other keyword arguments\n        \"\"\"\n        for doc in docs:\n            doc.convert_image_blob_to_uri(width=28, height=28)\n            doc.pop('blob')\n\n\nclass MyEvaluator(Executor):\n    \"\"\"\n    Executor that evaluates precision and recall\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.eval_at = 50\n        self.num_docs = 0\n        self.total_precision = 0\n        self.total_recall = 0\n\n    @property\n    def avg_precision(self):\n        \"\"\"\n        Computes precision\n        :return: precision values\n        \"\"\"\n        return self.total_precision \/ self.num_docs\n\n    @property\n    def avg_recall(self):\n        \"\"\"\n        Computes recall\n        :return: np.ndarray with recall values\n        \"\"\"\n        return self.total_recall \/ self.num_docs\n\n    def _precision(self, actual, desired):\n        if self.eval_at == 0:\n            return 0.0\n        actual_at_k = actual[: self.eval_at] if self.eval_at else actual\n        ret = len(set(actual_at_k).intersection(set(desired)))\n        sub = len(actual_at_k)\n        return ret \/ sub if sub != 0 else 0.0\n\n    def _recall(self, actual, desired):\n        if self.eval_at == 0:\n            return 0.0\n        actual_at_k = actual[: self.eval_at] if self.eval_at else actual\n        ret = len(set(actual_at_k).intersection(set(desired)))\n        return ret \/ len(desired)\n\n    @requests(on='\/eval')\n    def evaluate(self, docs: 'DocumentArray', groundtruths: 'DocumentArray', **kwargs):\n        \"\"\"Evaluate documents using the class values from ground truths\n\n        :param docs: documents to evaluate\n        :param groundtruths: ground truth for the documents\n        :param kwargs: other keyword arguments\n        \"\"\"\n        for doc, groundtruth in zip(docs, groundtruths):\n            self.num_docs += 1\n            actual = [match.tags['id'] for match in doc.matches]\n            desired = groundtruth.matches[0].tags['id']  # pseudo_match\n            self.total_precision += self._precision(actual, desired)\n            self.total_recall += self._recall(actual, desired)\n            doc.evaluations['Precision'] = self.avg_precision\n            doc.evaluations['Precision'].op_name = 'Precision'\n            doc.evaluations['Recall'] = self.avg_recall\n            doc.evaluations['Recall'].op_name = 'Recall'\n","label":1}
{"content":"# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http:\/\/www.sphinx-doc.org\/en\/master\/config\n\n# -- Path setup --------------------------------------------------------------\n# Instead of adding things manually to the path, we just ensure we install esc\n# in editable mode in our environment.\n\n\n# -- Project information -----------------------------------------------------\n\nproject = 'tzk'\ncopyright = '2021 Soren Bjornstad'\nauthor = 'Soren Bjornstad'\n\n# The short X.Y version\nversion = \"0.2.0\"\n# The full version, including alpha\/beta\/rc tags\nrelease = \"0.2.0\"\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\nneeds_sphinx = '1.8.5'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosectionlabel',\n    'sphinx.ext.doctest',\n    'sphinx.ext.todo',\n    'sphinx.ext.coverage',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.viewcode',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string.\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'sphinx_rtd_theme'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don't match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',\n# 'searchbox.html']``.\n#\n# html_sidebars = {}\n\n\n\n# -- Extension configuration -------------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n# Prefix section labels with the names of their documents, to avoid ambiguity\n# when the same heading appears on several pages.\nautosectionlabel_prefix_document = False\n","label":1}
{"content":"# -*- coding: utf-8 -*-\r\n#######################################################################\r\n# C\u00f3digo complementar ao Doc2VecFacil para criar traduzir termos simples \r\n# ou compostos do documento VOCAB_TRADUTOR.txt, foi otimizado para textos\r\n# grandes levando em considera\u00e7\u00e3o o case, j\u00e1 que ser\u00e1 usado ap\u00f3s a\r\n# tokeniza\u00e7\u00e3o e limpeza do documento\r\n# Esse c\u00f3digo, dicas de uso e outras informa\u00e7\u00f5es: \r\n#   -> https:\/\/github.com\/luizanisio\/Doc2VecFacil\/\r\n# Luiz An\u00edsio \r\n# 09\/10\/2021 - disponibilizado no GitHub  \r\n#######################################################################\r\n\r\nimport re\r\n\r\nclass TradutorTermosRe(dict):\r\n    \"\"\" Inspirado em: https:\/\/www.oreilly.com\/library\/view\/python-cookbook\/0596001673\/ch03s15.html\r\n        Consultado em: 08\/10\/2021\r\n    \"\"\"\r\n    def __init__(self, termos, ignore_case = True):\r\n        # caso tenha uma lista, \u00e9 uma lista de remo\u00e7\u00e3o de termos\r\n        # caso tenha um dict, \u00e9 um conjunto de tradu\u00e7\u00e3o de termos\r\n        # caso tenha uma tupla, \u00e9 um conjunto de tradu\u00e7\u00e3o de termos\r\n        if type(termos) is dict:\r\n           self.termos = {c.strip():v for c,v in termos.items() if c.strip()}\r\n        else:\r\n           self.termos = {c.strip():v for c,v in termos if c.strip()}\r\n        flags = re.IGNORECASE if ignore_case else 0\r\n        self.ignore_case = ignore_case\r\n        if self.ignore_case:\r\n            self.termos = {c.lower():v for c,v in self.termos.items()}\r\n        self.re_termos = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, self.termos.keys(  )))+r'\\b', flags)\r\n\r\n    def __sub_match__(self, match):\r\n        \"\"\" ser\u00e1 acionado para cada match do regex \"\"\"\r\n        if self.ignore_case:\r\n            return self.termos[match.group(0).lower()]\r\n        else:\r\n            return self.termos[match.group(0)]\r\n\r\n    def sub(self, text):\r\n        \"\"\" Aplica o regex e faz a tradu\u00e7\u00e3o\/remo\u00e7\u00e3o de cada termo \"\"\"\r\n        return self.re_termos.sub(self.__sub_match__, text)\r\n\r\n\r\nclass TradutorTermos(dict):\r\n    \"\"\" Traduz tuplas de texto - case sensitive \r\n        Objetivo fazer tradu\u00e7\u00e3o r\u00e1pida em grande volume de \r\n        textos j\u00e1 tokenizados\r\n    \"\"\"\r\n    def __init__(self, termos):\r\n        # caso tenha uma lista, \u00e9 uma lista de remo\u00e7\u00e3o de termos\r\n        # caso tenha um dict, \u00e9 um conjunto de tradu\u00e7\u00e3o de termos\r\n        # caso tenha uma tupla, \u00e9 um conjunto de tradu\u00e7\u00e3o de termos\r\n        if type(termos) is dict:\r\n           self.termos = [ (c.strip(),v) for c,v in termos.items() if c.strip() ]\r\n        else:\r\n           self.termos = [ (c.strip(),v) for c,v in termos if c.strip() ]\r\n        self.termos.sort(key= lambda k:len(k[0]), reverse = True )\r\n        # inclui espa\u00e7o nos termos\r\n        self.termos = [(f' {c} ',f' {v} ') for c,v in self.termos]\r\n\r\n    def sub(self, texto):\r\n        \"\"\" Aplica a tradu\u00e7\u00e3o\/remo\u00e7\u00e3o de cada termo  \"\"\"\r\n        _texto = f' {texto} '\r\n        for composto, novo in self.termos:\r\n            _texto = _texto.replace(composto, novo)\r\n        return _texto.strip()\r\n\r\ndef teste_tradutores():\r\n    termos = {'c\u00edvel' : '(c\u00edvel)', 'danos':'(danos)', 'ER\u00c1RIO': '(ER\u00c1RIO)','de':''}\r\n    texto    = 'APELA\u00c7\u00c3O C\u00cdVEL A\u00c7\u00c3O QUE OBJETIVA O RESSARCIMENTO DE DANOS CAUSADOS AO ER\u00c1RIO EM FUN\u00c7\u00c3O DA CONCESS\u00c3O ILEGAL E LESIVA DE PERMISS\u00c3O DE USO DE BEM P\u00daBLICO PELO ENT\u00c3O CHEFE DO PODER EXECUTIVO ESTADUAL EM FAVOR DA SEGUNDA R\u00c9'\r\n    texto_ci = 'APELA\u00c7\u00c3O (c\u00edvel) A\u00c7\u00c3O QUE OBJETIVA O RESSARCIMENTO  (danos) CAUSADOS AO (ER\u00c1RIO) EM FUN\u00c7\u00c3O DA CONCESS\u00c3O ILEGAL E LESIVA  PERMISS\u00c3O  USO  BEM P\u00daBLICO PELO ENT\u00c3O CHEFE DO PODER EXECUTIVO ESTADUAL EM FAVOR DA SEGUNDA R\u00c9'\r\n    texto_cs = 'APELA\u00c7\u00c3O C\u00cdVEL A\u00c7\u00c3O QUE OBJETIVA O RESSARCIMENTO DE DANOS CAUSADOS AO (ER\u00c1RIO) EM FUN\u00c7\u00c3O DA CONCESS\u00c3O ILEGAL E LESIVA DE PERMISS\u00c3O DE USO DE BEM P\u00daBLICO PELO ENT\u00c3O CHEFE DO PODER EXECUTIVO ESTADUAL EM FAVOR DA SEGUNDA R\u00c9'\r\n\r\n    for i in range(3):\r\n        if i == 0:\r\n            tipo =  'TradutorTermosRe ic'\r\n            tradutor = TradutorTermosRe(termos, True)\r\n            texto_traduzido = tradutor.sub(texto)\r\n            esperado = texto_ci\r\n        elif i == 1:\r\n            tipo =  'TradutorTermosRe cs'\r\n            tradutor = TradutorTermosRe(termos, False)\r\n            texto_traduzido = tradutor.sub(texto)\r\n            esperado = texto_cs\r\n        elif i == 2:\r\n            tipo =  'TradutorTermos cs'\r\n            tradutor = TradutorTermos(termos)\r\n            texto_traduzido = tradutor.sub(texto)\r\n            esperado = texto_cs\r\n\r\n        # tradutor Re ic\r\n        if texto_traduzido == esperado:\r\n            print(f'{tipo} OK')\r\n        else:\r\n            print(f'{tipo} FALHOU:')\r\n            print(' - Esperado : ', esperado)\r\n            print(' - Retornado: ', texto_traduzido)\r\n\r\nif __name__ == \"__main__\":\r\n    teste_tradutores()\r\n","label":1}
{"content":"import sys\n# sys.path.append(\"...\/\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pdb\nfrom torch.autograd import Variable\nimport math\n\n\nclass RenderNet_G_old(nn.Module):\n    def __init__(self, params):\n        super(RenderNet_G_old, self).__init__()\n        self.params = params\n        self.relu = nn.ReLU(inplace=True)\n        self.tanh = nn.Tanh()\n\n        if (self.params.use_relative_resolution):\n            self.conv_z_1 = nn.Conv3d(int(self.params.z_length_d) + 2 * 3 + 1, self.params.descriptor_length, 1,\n                                      padding=0)\n        else:\n            self.conv_z_1 = nn.Conv3d(int(self.params.z_length_d) + 2 * 3, self.params.descriptor_length, 1, padding=0)\n\n        self.conv_z_2 = nn.Conv3d(self.params.descriptor_length, self.params.descriptor_length, 1, padding=0)\n        self.conv_z_3 = nn.Conv3d(self.params.descriptor_length, self.params.descriptor_length, 1, padding=0)\n        self.conv_z_4 = nn.Conv3d(self.params.descriptor_length, 1, 1, padding=0)\n\n        std = math.sqrt(6.0 \/ self.params.descriptor_length)\n        torch.nn.init.normal_(self.conv_z_1.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_z_1.bias, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_z_2.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_z_2.bias, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_z_3.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_z_3.bias, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_z_4.weight, mean=0.0, std=1e-3)\n        torch.nn.init.normal_(self.conv_z_4.bias, mean=0.0, std=1e-3)\n\n        if (self.params.use_lighting_mlp):\n            self.conv_n = nn.Conv3d(self.params.descriptor_length, 3, 1, padding=0)\n            torch.nn.init.normal_(self.conv_n.weight, mean=0.0, std=1e-3)\n            torch.nn.init.normal_(self.conv_n.bias, mean=0.0, std=1e-3)\n\n    def forward(self, pixel_face_features, coords, view_direction, return_loss=True, rasterize_type=\"test\",\n                relative_resolution=None):\n        z = torch.cat((pixel_face_features, coords \/ self.params.mesh_radius, view_direction), dim=1)\n\n        if (self.params.use_relative_resolution):\n            z = torch.cat((z, relative_resolution), dim=1)\n\n        z = torch.sin(self.conv_z_1(z))\n        z = torch.sin(self.conv_z_2(z))\n        z = torch.sin(self.conv_z_3(z))\n        D = self.params.D_length * torch.sin(self.conv_z_4(z))\n        if (return_loss):\n            self.loss_density = 0.0\n            self.loss_density2 = 0.0\n\n        if (self.params.use_lighting_mlp):\n            self.n = 0.5 * torch.sin(self.conv_n(z))\n        else:\n            self.n = None\n\n        return D\n\n\nclass RenderNet_C(nn.Module):\n    def __init__(self, params):\n        super(RenderNet_C, self).__init__()\n        self.params = params\n\n        self.relu = nn.ReLU(inplace=True)\n        self.tanh = nn.Tanh()\n\n        # self.softmax = nn.Softmax(dim=2)\n\n        if (self.params.use_feature_alpha):\n            self.conv_alpha_1 = nn.Conv3d(int(self.params.descriptor_length_c), 1, 1, padding=0)\n            torch.nn.init.normal_(self.conv_alpha_1.weight, mean=0.0, std=1e-3)\n            torch.nn.init.normal_(self.conv_alpha_1.bias, mean=0.0, std=1e-3)\n\n        if (self.params.use_relative_resolution):\n            self.conv_out_1 = nn.Conv3d(self.params.z_length_c + 1 * 3 + 1, self.params.descriptor_length_c, 1,\n                                        padding=0)\n        else:\n            self.conv_out_1 = nn.Conv3d(self.params.z_length_c + 1 * 3, self.params.descriptor_length_c, 1, padding=0)\n\n        self.conv_out_2 = nn.Conv3d(self.params.descriptor_length_c, self.params.descriptor_length_c, 1, padding=0)\n        self.conv_out_3 = nn.Conv3d(self.params.descriptor_length_c, self.params.descriptor_length_c, 1, padding=0)\n        self.conv_out_3a = nn.Conv3d(self.params.descriptor_length_c, self.params.descriptor_length_c, 1, padding=0)\n\n        # if (self.params.use_2d_network):\n        self.conv_out_4 = nn.Conv3d(self.params.descriptor_length_c, self.params.descriptor_length_c, 1, padding=0)\n        self.conv_out_out = nn.Conv3d(self.params.descriptor_length_c, 3, 1, padding=0)\n        torch.nn.init.normal_(self.conv_out_out.weight, mean=0.0, std=1e-3)\n        torch.nn.init.normal_(self.conv_out_out.bias, mean=0.0, std=1e-3)\n        # else:\n        #     self.conv_out_4 = nn.Conv3d(self.params.descriptor_length_c, 3, 1, padding=0)\n\n        std = math.sqrt(6.0 \/ self.params.descriptor_length_c)\n        torch.nn.init.normal_(self.conv_out_1.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_out_1.bias, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_out_2.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_out_2.bias, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_out_3.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_out_3.bias, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_out_3a.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_out_3a.bias, mean=0.0, std=std)\n\n        torch.nn.init.normal_(self.conv_out_4.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_out_4.bias, mean=0.0, std=std)\n\n        if (self.params.use_lighting_mlp):\n            self.conv_n = nn.Conv3d(self.params.descriptor_length_c, 3, 1, padding=0)\n            torch.nn.init.normal_(self.conv_n.weight, mean=0.0, std=1e-1)\n            torch.nn.init.normal_(self.conv_n.bias, mean=0.0, std=1e-1)\n\n            self.conv_Cad = nn.Conv3d(self.params.descriptor_length_c, 2 * 3, 1, padding=0)\n            torch.nn.init.normal_(self.conv_Cad.weight, mean=0.0, std=1e-3)\n            torch.nn.init.normal_(self.conv_Cad.bias, mean=0.0, std=1e-3)\n            self.conv_Cs = nn.Conv3d(self.params.descriptor_length_c, 3, 1, padding=0)\n            torch.nn.init.normal_(self.conv_Cs.weight, mean=0.0, std=1e-3)\n            torch.nn.init.normal_(self.conv_Cs.bias, mean=0.0, std=1e-3)\n\n    def forward(self, pixel_face_features, coords, n=None, relative_resolution=None):\n\n        z = torch.cat((pixel_face_features, coords \/ self.params.mesh_radius), dim=1)\n\n        if (self.params.use_relative_resolution):\n            z = torch.cat((z, relative_resolution), dim=1)\n\n        z = torch.sin(self.conv_out_1(z))\n        z = torch.sin(self.conv_out_2(z))\n        z = torch.sin(self.conv_out_3(z))\n        z = torch.sin(self.conv_out_3a(z))\n\n        if (self.params.use_lighting_mlp):\n            # n = 0.5 * torch.sin(self.conv_n(z))\n            ad = 0.5 * torch.sin(self.conv_Cad(z))\n            s = 0.5 * torch.sin(self.conv_Cs(z))\n\n        if (self.params.use_feature_alpha):\n            self.alpha = torch.sigmoid(self.conv_alpha_1(z))\n            self.alpha = self.alpha \/ (self.alpha.max(dim=-1)[0][..., None] + 1e-6)\n\n        z = torch.sin(self.conv_out_4(z))\n        z = 0.5 * torch.sin(self.conv_out_out(z))\n        # z = 0.5 * torch.sin(self.conv_out_4(z))\n        # z = self.conv_out_4(z)\n\n        if (self.params.use_lighting_mlp):\n            return (z, n, ad, s)\n        else:\n            return z\n\n\nclass RenderNet_L(nn.Module):\n    def __init__(self, params):\n        super(RenderNet_L, self).__init__()\n        self.params = params\n\n        self.relu = nn.ReLU(inplace=True)\n        self.tanh = nn.Tanh()\n        # self.softmax = nn.Softmax(dim=2)\n\n        # self.conv_ad_1 = nn.Conv3d(3 * 3, self.params.descriptor_light_length, 1, padding=0)\n        # self.conv_ad_2 = nn.Conv3d(self.params.descriptor_light_length, self.params.descriptor_light_length, 1,padding=0)\n        # self.conv_ad_3 = nn.Conv3d(self.params.descriptor_light_length, self.params.descriptor_light_length, 1,padding=0)\n        # self.conv_ad_4 = nn.Conv3d(self.params.descriptor_light_length, 3, 1, padding=0)\n        #\n        std = math.sqrt(6.0 \/ self.params.descriptor_light_length) \/ self.params.siren_omega\n        # torch.nn.init.normal_(self.conv_ad_1.weight, mean=0.0, std=std)\n        # torch.nn.init.normal_(self.conv_ad_1.bias, mean=0.0, std=std)\n        # torch.nn.init.normal_(self.conv_ad_2.weight, mean=0.0, std=std)\n        # torch.nn.init.normal_(self.conv_ad_2.bias, mean=0.0, std=std)\n        # torch.nn.init.normal_(self.conv_ad_3.weight, mean=0.0, std=std)\n        # torch.nn.init.normal_(self.conv_ad_3.bias, mean=0.0, std=std)\n        # torch.nn.init.normal_(self.conv_ad_4.weight, mean=0.0, std=1e-3)\n        # torch.nn.init.normal_(self.conv_ad_4.bias, mean=0.0, std=1e-3)\n        if (self.params.use_lighting_embedding):\n            self.conv_s_1 = nn.Conv3d(4 * 3 + self.params.embedding_light_length, self.params.descriptor_light_length,\n                                      1, padding=0)\n        else:\n            self.conv_s_1 = nn.Conv3d(4 * 3, self.params.descriptor_light_length, 1, padding=0)\n        self.conv_s_2 = nn.Conv3d(self.params.descriptor_light_length, self.params.descriptor_light_length, 1,\n                                  padding=0)\n        self.conv_s_3 = nn.Conv3d(self.params.descriptor_light_length, self.params.descriptor_light_length, 1,\n                                  padding=0)\n        self.conv_s_4 = nn.Conv3d(self.params.descriptor_light_length, 3, 1, padding=0)\n\n        torch.nn.init.normal_(self.conv_s_1.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_s_1.bias, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_s_2.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_s_2.bias, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_s_3.weight, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_s_3.bias, mean=0.0, std=std)\n        torch.nn.init.normal_(self.conv_s_4.weight, mean=0.0, std=1e-4)\n        torch.nn.init.normal_(self.conv_s_4.bias, mean=0.0, std=1e-4)\n\n    def forward(self, z, n, ad, s, pixel_coords, view_direction, lighting_feature=None):\n        # ad = torch.cat((ad, n), dim=1)\n        # ad = torch.sin(self.conv_ad_1(ad))\n        # ad = torch.sin(self.conv_ad_2(ad))\n        # ad = torch.sin(self.conv_ad_3(ad))\n        # # ad = self.tanh(self.conv_ad_4(ad))\n        # ad = 0.5 + 0.5 * torch.sin(self.conv_ad_4(ad))\n\n        s = torch.cat((s, n, pixel_coords, view_direction), dim=1)\n        if (self.params.use_lighting_embedding):\n            s = torch.cat((s, lighting_feature), dim=1)\n        # pdb.set_trace()\n        s = torch.sin(self.params.siren_omega * self.conv_s_1(s))\n        s = torch.sin(self.params.siren_omega * self.conv_s_2(s))\n        # s = torch.sin(self.params.siren_omega * self.conv_s_3(s))\n        s = 0.5 * torch.sin(self.params.siren_omega * self.conv_s_4(s))\n\n        # z = ad * (z + 0.5) - 0.5 + s\n\n        if (self.params.lighting_predict_mode == 'total'):\n            return z + s\n        elif (self.params.lighting_predict_mode == 's'):\n            return s\n        elif (self.params.lighting_predict_mode == 'z'):\n            return z\n        elif (self.params.lighting_predict_mode == 'n'):\n            return n \/ 2\n\n\nclass ReptileModel(nn.Module):\n\n    def __init__(self):\n        nn.Module.__init__(self)\n\n    def point_grad_to(self, target):\n        '''\n        Set .grad attribute of each parameter to be proportional\n        to the difference between self and target\n        '''\n        for p, target_p in zip(self.parameters(), target.parameters()):\n            if p.grad is None:\n                if self.is_cuda():\n                    p.grad = Variable(torch.zeros(p.size())).cuda()\n                else:\n                    p.grad = Variable(torch.zeros(p.size()))\n            p.grad.data.zero_()  # not sure this is required\n            p.grad.data.add_(p.data - target_p.data)\n\n    def is_cuda(self):\n        return next(self.parameters()).is_cuda\n\n\nclass ReptileEmbedding(nn.ModuleList):\n\n    def __init__(self):\n        nn.ModuleList.__init__(self)\n\n    def point_grad_to(self, target):\n        '''\n        Set .grad attribute of each parameter to be proportional\n        to the difference between self and target\n        '''\n        for p, target_p in zip(self.parameters(), target.parameters()):\n            if p.grad is None:\n                if self.is_cuda():\n                    p.grad = Variable(torch.zeros(p.size())).cuda()\n                else:\n                    p.grad = Variable(torch.zeros(p.size()))\n            p.grad.data.zero_()  # not sure this is required\n            p.grad.data.add_(p.data - target_p.data)\n\n    def is_cuda(self):\n        return next(self.parameters()).is_cuda","label":1}
{"content":"def apply_discount(product, discount):\n\tprice = int(product['price'] * (1.0 - discount))\n\t\n\t# This asserts that the discount can't be lower than zero and the new price can't be higher than the original\n\t# The part in single quotes will be printed when assertion fails and AssertionError arises\n\tassert 0 <= price <= product['price'], 'Must be less than original price'\n\treturn price\n\n\nshoes = {'name': 'Fancy Shoes', 'price': 14900}\n\nprint(apply_discount(shoes, 2))\n\n# Don't use assert for data validation.\n# it can be globally disabled with the -0 and -00 command line switches as well as PYTHONOPTIMIZE environment variable\n# in cpython. This turns the assert statement into a null operation that won't be evaluated\n\n# Never try to assert a tuple, it will always return True\n# don't do this assert (x==10, 'x should be equal to 10)\n","label":1}
{"content":"import os, shutil\nimport torch\nfrom argparse import ArgumentParser\nfrom pytorch_lightning import Trainer\nfrom cifar10_module import CIFAR10_Module\n\ndef main(hparams):\n    # If only train on 1 GPU. Must set_device otherwise PyTorch always store model on GPU 0 first\n    if type(hparams.gpus) == str:\n        if len(hparams.gpus) == 2: # GPU number and comma e.g. '0,' or '1,'\n            torch.cuda.set_device(int(hparams.gpus[0]))\n\n    save_to_path = os.path.join(kThisPath, 'test_temp')\n    os.makedirs(save_to_path, exist_ok=True)\n\n    model = CIFAR10_Module(hparams, pretrained=True)\n    trainer = Trainer(gpus=hparams.gpus, default_save_path=save_to_path)\n    trainer.test(model)\n    shutil.rmtree(save_to_path)\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument('--classifier', type=str, default='resnet18')\n    parser.add_argument('--data_dir', type=str, default='\/data\/huy\/cifar10\/')\n    parser.add_argument('--gpus', default='0,')\n    parser.add_argument('--max_epochs', type=int, default=100)\n    parser.add_argument('--batch_size', type=int, default=256)\n    parser.add_argument('--learning_rate', type=float, default=1e-2)\n    parser.add_argument('--weight_decay', type=float, default=1e-2)\n    args = parser.parse_args()\n    main(args)\n","label":1}
{"content":"# -*- coding: utf-8 -*-\n\"\"\"\nProfile: http:\/\/hl7.org\/fhir\/StructureDefinition\/PaymentReconciliation\nRelease: STU3\nVersion: 3.0.2\nRevision: 11917\nLast updated: 2019-10-24T11:53:00+11:00\n\"\"\"\nimport typing\n\nfrom pydantic import Field\n\nfrom . import backboneelement, domainresource, fhirtypes\n\n\nclass PaymentReconciliation(domainresource.DomainResource):\n    \"\"\"Disclaimer: Any field name ends with ``__ext`` doesn't part of\n    Resource StructureDefinition, instead used to enable Extensibility feature\n    for FHIR Primitive Data Types.\n\n    PaymentReconciliation resource.\n    This resource provides payment details and claim references supporting a\n    bulk payment.\n    \"\"\"\n\n    resource_type = Field(\"PaymentReconciliation\", const=True)\n\n    created: fhirtypes.DateTime = Field(\n        None,\n        alias=\"created\",\n        title=\"Creation date\",\n        description=(\n            \"The date when the enclosed suite of services were performed or \"\n            \"completed.\"\n        ),\n        # if property is element of this resource.\n        element_property=True,\n    )\n    created__ext: fhirtypes.FHIRPrimitiveExtensionType = Field(\n        None, alias=\"_created\", title=\"Extension field for ``created``.\"\n    )\n\n    detail: typing.List[fhirtypes.PaymentReconciliationDetailType] = Field(\n        None,\n        alias=\"detail\",\n        title=\"List of settlements\",\n        description=(\n            \"List of individual settlement amounts and the corresponding \"\n            \"transaction.\"\n        ),\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    disposition: fhirtypes.String = Field(\n        None,\n        alias=\"disposition\",\n        title=\"Disposition Message\",\n        description=\"A description of the status of the adjudication.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n    disposition__ext: fhirtypes.FHIRPrimitiveExtensionType = Field(\n        None, alias=\"_disposition\", title=\"Extension field for ``disposition``.\"\n    )\n\n    form: fhirtypes.CodeableConceptType = Field(\n        None,\n        alias=\"form\",\n        title=\"Printed Form Identifier\",\n        description=\"The form to be used for printing the content.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    identifier: typing.List[fhirtypes.IdentifierType] = Field(\n        None,\n        alias=\"identifier\",\n        title=\"Business Identifier\",\n        description=\"The Response business identifier.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    organization: fhirtypes.ReferenceType = Field(\n        None,\n        alias=\"organization\",\n        title=\"Insurer\",\n        description=\"The Insurer who produced this adjudicated response.\",\n        # if property is element of this resource.\n        element_property=True,\n        # note: Listed Resource Type(s) should be allowed as Reference.\n        enum_reference_types=[\"Organization\"],\n    )\n\n    outcome: fhirtypes.CodeableConceptType = Field(\n        None,\n        alias=\"outcome\",\n        title=\"complete | error | partial\",\n        description=\"Transaction status: error, complete.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    period: fhirtypes.PeriodType = Field(\n        None,\n        alias=\"period\",\n        title=\"Period covered\",\n        description=(\n            \"The period of time for which payments have been gathered into this \"\n            \"bulk payment for settlement.\"\n        ),\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    processNote: typing.List[fhirtypes.PaymentReconciliationProcessNoteType] = Field(\n        None,\n        alias=\"processNote\",\n        title=\"Processing comments\",\n        description=\"Suite of notes.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    request: fhirtypes.ReferenceType = Field(\n        None,\n        alias=\"request\",\n        title=\"Claim reference\",\n        description=\"Original request resource reference.\",\n        # if property is element of this resource.\n        element_property=True,\n        # note: Listed Resource Type(s) should be allowed as Reference.\n        enum_reference_types=[\"ProcessRequest\"],\n    )\n\n    requestOrganization: fhirtypes.ReferenceType = Field(\n        None,\n        alias=\"requestOrganization\",\n        title=\"Responsible organization\",\n        description=(\n            \"The organization which is responsible for the services rendered to the\"\n            \" patient.\"\n        ),\n        # if property is element of this resource.\n        element_property=True,\n        # note: Listed Resource Type(s) should be allowed as Reference.\n        enum_reference_types=[\"Organization\"],\n    )\n\n    requestProvider: fhirtypes.ReferenceType = Field(\n        None,\n        alias=\"requestProvider\",\n        title=\"Responsible practitioner\",\n        description=(\n            \"The practitioner who is responsible for the services rendered to the \"\n            \"patient.\"\n        ),\n        # if property is element of this resource.\n        element_property=True,\n        # note: Listed Resource Type(s) should be allowed as Reference.\n        enum_reference_types=[\"Practitioner\"],\n    )\n\n    status: fhirtypes.Code = Field(\n        None,\n        alias=\"status\",\n        title=\"active | cancelled | draft | entered-in-error\",\n        description=\"The status of the resource instance.\",\n        # if property is element of this resource.\n        element_property=True,\n        # note: Enum values can be used in validation,\n        # but use in your own responsibilities, read official FHIR documentation.\n        enum_values=[\"active\", \"cancelled\", \"draft\", \"entered-in-error\"],\n    )\n    status__ext: fhirtypes.FHIRPrimitiveExtensionType = Field(\n        None, alias=\"_status\", title=\"Extension field for ``status``.\"\n    )\n\n    total: fhirtypes.MoneyType = Field(\n        None,\n        alias=\"total\",\n        title=\"Total amount of Payment\",\n        description=\"Total payment amount.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    @classmethod\n    def elements_sequence(cls):\n        \"\"\"returning all elements names from\n        ``PaymentReconciliation`` according specification,\n        with preserving original sequence order.\n        \"\"\"\n        return [\n            \"id\",\n            \"meta\",\n            \"implicitRules\",\n            \"language\",\n            \"text\",\n            \"contained\",\n            \"extension\",\n            \"modifierExtension\",\n            \"identifier\",\n            \"status\",\n            \"period\",\n            \"created\",\n            \"organization\",\n            \"request\",\n            \"outcome\",\n            \"disposition\",\n            \"requestProvider\",\n            \"requestOrganization\",\n            \"detail\",\n            \"form\",\n            \"total\",\n            \"processNote\",\n        ]\n\n\nclass PaymentReconciliationDetail(backboneelement.BackboneElement):\n    \"\"\"Disclaimer: Any field name ends with ``__ext`` doesn't part of\n    Resource StructureDefinition, instead used to enable Extensibility feature\n    for FHIR Primitive Data Types.\n\n    List of settlements.\n    List of individual settlement amounts and the corresponding transaction.\n    \"\"\"\n\n    resource_type = Field(\"PaymentReconciliationDetail\", const=True)\n\n    amount: fhirtypes.MoneyType = Field(\n        None,\n        alias=\"amount\",\n        title=\"Amount being paid\",\n        description=\"Amount paid for this detail.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    date: fhirtypes.Date = Field(\n        None,\n        alias=\"date\",\n        title=\"Invoice date\",\n        description=\"The date of the invoice or financial resource.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n    date__ext: fhirtypes.FHIRPrimitiveExtensionType = Field(\n        None, alias=\"_date\", title=\"Extension field for ``date``.\"\n    )\n\n    payee: fhirtypes.ReferenceType = Field(\n        None,\n        alias=\"payee\",\n        title=\"Organization which is receiving the payment\",\n        description=\"The organization which is receiving the payment.\",\n        # if property is element of this resource.\n        element_property=True,\n        # note: Listed Resource Type(s) should be allowed as Reference.\n        enum_reference_types=[\"Organization\"],\n    )\n\n    request: fhirtypes.ReferenceType = Field(\n        None,\n        alias=\"request\",\n        title=\"Claim\",\n        description=\"The claim or financial resource.\",\n        # if property is element of this resource.\n        element_property=True,\n        # note: Listed Resource Type(s) should be allowed as Reference.\n        enum_reference_types=[\"Resource\"],\n    )\n\n    response: fhirtypes.ReferenceType = Field(\n        None,\n        alias=\"response\",\n        title=\"Claim Response\",\n        description=\"The claim response resource.\",\n        # if property is element of this resource.\n        element_property=True,\n        # note: Listed Resource Type(s) should be allowed as Reference.\n        enum_reference_types=[\"Resource\"],\n    )\n\n    submitter: fhirtypes.ReferenceType = Field(\n        None,\n        alias=\"submitter\",\n        title=\"Organization which submitted the claim\",\n        description=\"The Organization which submitted the claim or financial transaction.\",\n        # if property is element of this resource.\n        element_property=True,\n        # note: Listed Resource Type(s) should be allowed as Reference.\n        enum_reference_types=[\"Organization\"],\n    )\n\n    type: fhirtypes.CodeableConceptType = Field(\n        ...,\n        alias=\"type\",\n        title=\"Type code\",\n        description=(\n            \"Code to indicate the nature of the payment, adjustment, funds advance,\"\n            \" etc.\"\n        ),\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    @classmethod\n    def elements_sequence(cls):\n        \"\"\"returning all elements names from\n        ``PaymentReconciliationDetail`` according specification,\n        with preserving original sequence order.\n        \"\"\"\n        return [\n            \"id\",\n            \"extension\",\n            \"modifierExtension\",\n            \"type\",\n            \"request\",\n            \"response\",\n            \"submitter\",\n            \"payee\",\n            \"date\",\n            \"amount\",\n        ]\n\n\nclass PaymentReconciliationProcessNote(backboneelement.BackboneElement):\n    \"\"\"Disclaimer: Any field name ends with ``__ext`` doesn't part of\n    Resource StructureDefinition, instead used to enable Extensibility feature\n    for FHIR Primitive Data Types.\n\n    Processing comments.\n    Suite of notes.\n    \"\"\"\n\n    resource_type = Field(\"PaymentReconciliationProcessNote\", const=True)\n\n    text: fhirtypes.String = Field(\n        None,\n        alias=\"text\",\n        title=\"Comment on the processing\",\n        description=\"The note text.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n    text__ext: fhirtypes.FHIRPrimitiveExtensionType = Field(\n        None, alias=\"_text\", title=\"Extension field for ``text``.\"\n    )\n\n    type: fhirtypes.CodeableConceptType = Field(\n        None,\n        alias=\"type\",\n        title=\"display | print | printoper\",\n        description=\"The note purpose: Print\/Display.\",\n        # if property is element of this resource.\n        element_property=True,\n    )\n\n    @classmethod\n    def elements_sequence(cls):\n        \"\"\"returning all elements names from\n        ``PaymentReconciliationProcessNote`` according specification,\n        with preserving original sequence order.\n        \"\"\"\n        return [\"id\", \"extension\", \"modifierExtension\", \"type\", \"text\"]\n","label":1}
{"content":"# Copyright 2015 The Bazel Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndef _groovy_jar_impl(ctx):\n  \"\"\"Creates a .jar file from Groovy sources. Users should rely on\n  groovy_library instead of using this rule directly.\n  \"\"\"\n  class_jar = ctx.outputs.class_jar\n  build_output = class_jar.path + \".build_output\"\n\n  # Extract all transitive dependencies\n  # TODO(bazel-team): get transitive dependencies from other groovy libraries\n  all_deps = depset(ctx.files.deps)\n  for this_dep in ctx.attr.deps:\n    if hasattr(this_dep, \"java\"):\n      all_deps += this_dep.java.transitive_runtime_deps\n\n  # Set up the output directory and set JAVA_HOME\n  cmd = \"rm -rf %s\\n\" % build_output\n  cmd += \"mkdir -p %s\\n\" % build_output\n  cmd += \"export JAVA_HOME=%s\\n\" % ctx.attr._jdk[java_common.JavaRuntimeInfo].java_home\n\n  # Set GROOVY_HOME by scanning through the groovy SDK to find the license file,\n  # which should be at the root of the SDK.\n  for file in ctx.files._groovysdk:\n    if file.basename == \"CLI-LICENSE.txt\":\n      cmd += \"export GROOVY_HOME=%s\\n\" % file.dirname\n      break\n\n  # Compile all files in srcs with groovyc\n  cmd += \"$GROOVY_HOME\/bin\/groovyc %s -d %s %s\\n\" % (\n      \"-cp \" + \":\".join([dep.path for dep in all_deps]) if len(all_deps) != 0 else \"\",\n      build_output,\n      \" \".join([src.path for src in ctx.files.srcs]),\n  )\n\n  # Discover all of the generated class files and write their paths to a file.\n  # Run the paths through sed to trim out everything before the package root so\n  # that the paths match how they should look in the jar file.\n  cmd += \"find %s -name '*.class' | sed 's:^%s\/::' > %s\/class_list\\n\" % (\n      build_output,\n      build_output,\n      build_output,\n  )\n\n  # Create a jar file using the discovered paths\n  cmd += \"root=`pwd`\\n\"\n  cmd += \"cd %s; $root\/%s Cc ..\/%s @class_list\\n\" % (\n      build_output,\n      ctx.executable._zipper.path,\n      class_jar.basename,\n  )\n  cmd += \"cd $root\\n\"\n\n  # Clean up temporary output\n  cmd += \"rm -rf %s\" % build_output\n\n  # Execute the command\n  ctx.action(\n      inputs = (\n          ctx.files.srcs\n          + list(all_deps)\n          + ctx.files._groovysdk\n          + ctx.files._jdk\n          + ctx.files._zipper),\n      outputs = [class_jar],\n      mnemonic = \"Groovyc\",\n      command = \"set -e;\" + cmd,\n      use_default_shell_env = True,\n  )\n\n_groovy_jar = rule(\n    attrs = {\n        \"srcs\": attr.label_list(\n            non_empty = True,\n            allow_files = FileType([\".groovy\"]),\n        ),\n        \"deps\": attr.label_list(\n            mandatory = False,\n            allow_files = FileType([\".jar\"]),\n        ),\n        \"_groovysdk\": attr.label(\n            default = Label(\"\/\/external:groovy-sdk\"),\n        ),\n        \"_jdk\": attr.label(\n            default = Label(\"@bazel_tools\/\/tools\/jdk:current_java_runtime\"),\n        ),\n        \"_zipper\": attr.label(\n            default = Label(\"@bazel_tools\/\/tools\/zip:zipper\"),\n            executable = True,\n            single_file = True,\n            cfg = \"host\",\n        ),\n    },\n    outputs = {\n        \"class_jar\": \"lib%{name}.jar\",\n    },\n    implementation = _groovy_jar_impl,\n)\n\ndef groovy_library(name, srcs=[], testonly=0, deps=[], **kwargs):\n  \"\"\"Rule analagous to java_library that accepts .groovy sources instead of\n  .java sources. The result is wrapped in a java_import so that java rules may\n  depend on it.\n  \"\"\"\n  _groovy_jar(\n      name = name + \"-impl\",\n      srcs = srcs,\n      testonly = testonly,\n      deps = deps,\n  )\n  native.java_import(\n      name = name,\n      jars = [name + \"-impl\"],\n      testonly = testonly,\n      deps = deps,\n      **kwargs\n  )\n\ndef groovy_and_java_library(name, srcs=[], testonly=0, deps=[], **kwargs):\n  \"\"\"Accepts .groovy and .java srcs to create a groovy_library and a\n  java_library. The groovy_library will depend on the java_library, so the\n  Groovy code may reference the Java code but not vice-versa.\n  \"\"\"\n  groovy_deps = deps\n  jars = []\n\n  # Put all .java sources in a java_library\n  java_srcs = [src for src in srcs if src.endswith(\".java\")]\n  if java_srcs:\n    native.java_library(\n        name = name + \"-java\",\n        srcs = java_srcs,\n        testonly = testonly,\n        deps = deps,\n    )\n    groovy_deps += [name + \"-java\"]\n    jars += [\"lib\"  + name + \"-java.jar\"]\n\n  # Put all .groovy sources in a groovy_library depending on the java_library\n  groovy_srcs = [src for src in srcs if src.endswith(\".groovy\")]\n  if groovy_srcs:\n    _groovy_jar(\n        name = name + \"-groovy\",\n        srcs = groovy_srcs,\n        testonly = testonly,\n        deps = groovy_deps,\n    )\n    jars += [\"lib\" + name + \"-groovy.jar\"]\n\n  # Output a java_import combining both libraries\n  native.java_import(\n      name = name,\n      jars = jars,\n      testonly = testonly,\n      deps = deps,\n      **kwargs\n  )\n\ndef groovy_binary(name, main_class, srcs=[], testonly=0, deps=[], **kwargs):\n  \"\"\"Rule analagous to java_binary that accepts .groovy sources instead of .java\n  sources.\n  \"\"\"\n  all_deps = deps + [\"\/\/external:groovy\"]\n  if srcs:\n    groovy_library(\n        name = name + \"-lib\",\n        srcs = srcs,\n        testonly = testonly,\n        deps = deps,\n    )\n    all_deps += [name + \"-lib\"]\n\n  native.java_binary(\n      name = name,\n      main_class = main_class,\n      runtime_deps = all_deps,\n      testonly = testonly,\n      **kwargs\n  )\n\ndef path_to_class(path):\n  if path.startswith(\"src\/test\/groovy\/\"):\n    return path[len(\"src\/test\/groovy\/\") : path.index(\".groovy\")].replace('\/', '.')\n  elif path.startswith(\"src\/test\/java\/\"):\n    return path[len(\"src\/test\/java\/\") : path.index(\".groovy\")].replace('\/', '.')\n  else:\n    fail(\"groovy_test sources must be under src\/test\/java or src\/test\/groovy\")\n\ndef _groovy_test_impl(ctx):\n  # Collect jars from the Groovy sdk\n  groovy_sdk_jars = [file\n      for file in ctx.files._groovysdk\n      if file.basename.endswith(\".jar\")\n  ]\n\n  # Extract all transitive dependencies\n  all_deps = depset(ctx.files.deps + ctx.files._implicit_deps + groovy_sdk_jars)\n  for this_dep in ctx.attr.deps:\n    if hasattr(this_dep, 'java'):\n      all_deps += this_dep.java.transitive_runtime_deps\n\n  # Infer a class name from each src file\n  classes = [path_to_class(src.path) for src in ctx.files.srcs]\n\n  # Write a file that executes JUnit on the inferred classes\n  cmd = \"external\/local_jdk\/bin\/java %s -cp %s org.junit.runner.JUnitCore %s\\n\" % (\n    \" \".join(ctx.attr.jvm_flags),\n    \":\".join([dep.short_path for dep in all_deps]),\n    \" \".join(classes),\n  )\n  ctx.file_action(\n    output = ctx.outputs.executable,\n    content = cmd\n  )\n\n  # Return all dependencies needed to run the tests\n  return struct(\n    runfiles=ctx.runfiles(files=list(all_deps) + ctx.files.data + ctx.files._jdk),\n  )\n\n_groovy_test = rule(\n    attrs = {\n        \"srcs\": attr.label_list(\n            mandatory = True,\n            allow_files = FileType([\".groovy\"]),\n        ),\n        \"deps\": attr.label_list(allow_files = FileType([\".jar\"])),\n        \"data\": attr.label_list(allow_files = True),\n        \"jvm_flags\": attr.string_list(),\n        \"_groovysdk\": attr.label(\n            default = Label(\"\/\/external:groovy-sdk\"),\n        ),\n        \"_jdk\": attr.label(\n            default = Label(\"@bazel_tools\/\/tools\/jdk:current_java_runtime\"),\n        ),\n        \"_implicit_deps\": attr.label_list(default = [\n            Label(\"\/\/external:junit\"),\n        ]),\n    },\n    test = True,\n    implementation = _groovy_test_impl,\n)\n\ndef groovy_test(\n    name,\n    deps=[],\n    srcs=[],\n    data=[],\n    resources=[],\n    jvm_flags=[],\n    size=\"medium\",\n    tags=[]):\n  # Create an extra jar to hold the resource files if any were specified\n  all_deps = deps\n  if resources:\n    native.java_library(\n      name = name + \"-resources\",\n      resources = resources,\n      testonly = 1,\n    )\n    all_deps += [name + \"-resources\"]\n\n  _groovy_test(\n    name = name,\n    size = size,\n    tags = tags,\n    srcs = srcs,\n    deps = all_deps,\n    data = data,\n    jvm_flags = jvm_flags,\n  )\n\ndef groovy_junit_test(\n    name,\n    tests,\n    deps=[],\n    groovy_srcs=[],\n    java_srcs=[],\n    data=[],\n    resources=[],\n    jvm_flags=[],\n    size=\"small\",\n    tags=[]):\n  groovy_lib_deps = deps + [\"\/\/external:junit\"]\n  test_deps = deps + [\"\/\/external:junit\"]\n\n  if len(tests) == 0:\n    fail(\"Must provide at least one file in tests\")\n\n  # Put all Java sources into a Java library\n  if java_srcs:\n    native.java_library(\n      name = name + \"-javalib\",\n      srcs = java_srcs,\n      testonly = 1,\n      deps = deps + [\"\/\/external:junit\"],\n    )\n    groovy_lib_deps += [name + \"-javalib\"]\n    test_deps += [name + \"-javalib\"]\n\n  # Put all tests and Groovy sources into a Groovy library\n  groovy_library(\n    name = name + \"-groovylib\",\n    srcs = tests + groovy_srcs,\n    testonly = 1,\n    deps = groovy_lib_deps,\n  )\n  test_deps += [name + \"-groovylib\"]\n\n  # Create a groovy test\n  groovy_test(\n    name = name,\n    deps = test_deps,\n    srcs = tests,\n    data = data,\n    resources = resources,\n    jvm_flags = jvm_flags,\n    size = size,\n    tags = tags,\n  )\n\ndef spock_test(\n    name,\n    specs,\n    deps=[],\n    groovy_srcs=[],\n    java_srcs=[],\n    data=[],\n    resources=[],\n    jvm_flags=[],\n    size=\"small\",\n    tags=[]):\n  groovy_lib_deps = deps + [\n    \"\/\/external:junit\",\n    \"\/\/external:spock\",\n  ]\n  test_deps = deps + [\n    \"\/\/external:junit\",\n    \"\/\/external:spock\",\n  ]\n\n  if len(specs) == 0:\n    fail(\"Must provide at least one file in specs\")\n\n  # Put all Java sources into a Java library\n  if java_srcs:\n    native.java_library(\n      name = name + \"-javalib\",\n      srcs = java_srcs,\n      testonly = 1,\n      deps = deps + [\n        \"\/\/external:junit\",\n        \"\/\/external:spock\",\n      ],\n    )\n    groovy_lib_deps += [name + \"-javalib\"]\n    test_deps += [name + \"-javalib\"]\n\n  # Put all specs and Groovy sources into a Groovy library\n  groovy_library(\n    name = name + \"-groovylib\",\n    srcs = specs + groovy_srcs,\n    testonly = 1,\n    deps = groovy_lib_deps,\n  )\n  test_deps += [name + \"-groovylib\"]\n\n  # Create a groovy test\n  groovy_test(\n    name = name,\n    deps = test_deps,\n    srcs = specs,\n    data = data,\n    resources = resources,\n    jvm_flags = jvm_flags,\n    size = size,\n    tags = tags,\n  )\n\ndef groovy_repositories():\n  native.new_http_archive(\n    name = \"groovy_sdk_artifact\",\n    url = \"http:\/\/bazel-mirror.storage.googleapis.com\/dl.bintray.com\/groovy\/maven\/apache-groovy-binary-2.4.4.zip\",\n    sha256 = \"a7cc1e5315a14ea38db1b2b9ce0792e35174161141a6a3e2ef49b7b2788c258c\",\n    build_file_content = \"\"\"\nfilegroup(\n    name = \"sdk\",\n    srcs = glob([\"groovy-2.4.4\/**\"]),\n    visibility = [\"\/\/visibility:public\"],\n)\njava_import(\n    name = \"groovy\",\n    jars = [\"groovy-2.4.4\/lib\/groovy-2.4.4.jar\"],\n    visibility = [\"\/\/visibility:public\"],\n)\n\"\"\",\n  )\n  native.bind(\n    name = \"groovy-sdk\",\n    actual = \"@groovy_sdk_artifact\/\/:sdk\",\n  )\n  native.bind(\n    name = \"groovy\",\n    actual = \"@groovy_sdk_artifact\/\/:groovy\",\n  )\n\n  native.maven_server(\n    name = \"groovy_maven_server\",\n    url = \"http:\/\/bazel-mirror.storage.googleapis.com\/repo1.maven.org\/maven2\/\",\n  )\n\n  native.maven_jar(\n    name = \"junit_artifact\",\n    artifact = \"junit:junit:4.12\",\n    server = \"groovy_maven_server\",\n  )\n  native.bind(\n    name = \"junit\",\n    actual = \"@junit_artifact\/\/jar\",\n  )\n\n  native.maven_jar(\n    name = \"spock_artifact\",\n    artifact = \"org.spockframework:spock-core:0.7-groovy-2.0\",\n    server = \"groovy_maven_server\",\n  )\n  native.bind(\n    name = \"spock\",\n    actual = \"@spock_artifact\/\/jar\",\n  )\n","label":1}
{"content":"import logging\nimport os\n\nimport json\nfrom unittest import mock\nimport numpy as np\nimport pandas as pd\nimport pyspark\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.pipeline import Pipeline\nfrom pyspark.ml.wrapper import JavaModel\nimport pytest\nfrom sklearn import datasets\nimport shutil\nfrom collections import namedtuple\nimport yaml\n\nimport mlflow\nimport mlflow.pyfunc.scoring_server as pyfunc_scoring_server\nimport mlflow.tracking\nfrom mlflow import pyfunc\nfrom mlflow import spark as sparkm\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.models import Model, infer_signature\nfrom mlflow.models.utils import _read_example\nfrom mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\nfrom mlflow.tracking.artifact_utils import _download_artifact_from_uri\nfrom mlflow.utils.environment import _mlflow_conda_env\nfrom mlflow.utils.file_utils import TempDir\nfrom mlflow.utils.model_utils import _get_flavor_configuration\nfrom mlflow.tracking._model_registry import DEFAULT_AWAIT_MAX_SLEEP_SECONDS\n\nfrom tests.helper_functions import (\n    score_model_in_sagemaker_docker_container,\n    _compare_conda_env_requirements,\n    _assert_pip_requirements,\n)\nfrom tests.pyfunc.test_spark import score_model_as_udf, get_spark_session\nfrom tests.helper_functions import set_boto_credentials  # pylint: disable=unused-import\nfrom tests.helper_functions import mock_s3_bucket  # pylint: disable=unused-import\n\n_logger = logging.getLogger(__name__)\n\n\n@pytest.fixture\ndef spark_custom_env(tmpdir):\n    conda_env = os.path.join(str(tmpdir), \"conda_env.yml\")\n    _mlflow_conda_env(conda_env, additional_pip_deps=[\"pyspark\", \"pytest\"])\n    return conda_env\n\n\nSparkModelWithData = namedtuple(\n    \"SparkModelWithData\", [\"model\", \"spark_df\", \"pandas_df\", \"predictions\"]\n)\n\n\n# Specify `autouse=True` to ensure that a context is created\n# before any tests are executed. This ensures that the Hadoop filesystem\n# does not create its own SparkContext without the MLeap libraries required by\n# other tests.\n@pytest.fixture(scope=\"session\", autouse=True)\ndef spark_context():\n    conf = pyspark.SparkConf()\n    max_tries = 3\n    for num_tries in range(max_tries):\n        try:\n            spark = get_spark_session(conf)\n            return spark.sparkContext\n        except Exception as e:\n            if num_tries >= max_tries - 1:\n                raise\n            _logger.exception(\n                e, \"Attempt %s to create a SparkSession failed, retrying...\" % num_tries\n            )\n\n\n@pytest.fixture(scope=\"session\")\ndef iris_df(spark_context):\n    iris = datasets.load_iris()\n    X = iris.data  # we only take the first two features.\n    y = iris.target\n    feature_names = [\"0\", \"1\", \"2\", \"3\"]\n    iris_pandas_df = pd.DataFrame(X, columns=feature_names)  # to make spark_udf work\n    iris_pandas_df[\"label\"] = pd.Series(y)\n    spark_session = pyspark.sql.SparkSession(spark_context)\n    iris_spark_df = spark_session.createDataFrame(iris_pandas_df)\n    return feature_names, iris_pandas_df, iris_spark_df\n\n\n@pytest.fixture(scope=\"session\")\ndef spark_model_iris(iris_df):\n    feature_names, iris_pandas_df, iris_spark_df = iris_df\n    assembler = VectorAssembler(inputCols=feature_names, outputCol=\"features\")\n    lr = LogisticRegression(maxIter=50, regParam=0.1, elasticNetParam=0.8)\n    pipeline = Pipeline(stages=[assembler, lr])\n    # Fit the model\n    model = pipeline.fit(iris_spark_df)\n    preds_df = model.transform(iris_spark_df)\n    preds = [x.prediction for x in preds_df.select(\"prediction\").collect()]\n    return SparkModelWithData(\n        model=model, spark_df=iris_spark_df, pandas_df=iris_pandas_df, predictions=preds\n    )\n\n\n@pytest.fixture(scope=\"session\")\ndef spark_model_transformer(iris_df):\n    feature_names, iris_pandas_df, iris_spark_df = iris_df\n    assembler = VectorAssembler(inputCols=feature_names, outputCol=\"features\")\n    # Fit the model\n    preds_df = assembler.transform(iris_spark_df)\n    preds = [x.features for x in preds_df.select(\"features\").collect()]\n    return SparkModelWithData(\n        model=assembler, spark_df=iris_spark_df, pandas_df=iris_pandas_df, predictions=preds\n    )\n\n\n@pytest.fixture(scope=\"session\")\ndef spark_model_estimator(iris_df, spark_context):\n    # pylint: disable=unused-argument\n    feature_names, iris_pandas_df, iris_spark_df = iris_df\n    assembler = VectorAssembler(inputCols=feature_names, outputCol=\"features\")\n    features_df = assembler.transform(iris_spark_df)\n    lr = LogisticRegression(maxIter=50, regParam=0.1, elasticNetParam=0.8)\n    # Fit the model\n    model = lr.fit(features_df)\n    preds_df = model.transform(features_df)\n    preds = [x.prediction for x in preds_df.select(\"prediction\").collect()]\n    return SparkModelWithData(\n        model=model, spark_df=features_df, pandas_df=iris_pandas_df, predictions=preds\n    )\n\n\n@pytest.fixture\ndef model_path(tmpdir):\n    return str(tmpdir.mkdir(\"model\"))\n\n\n@pytest.mark.large\ndef test_hadoop_filesystem(tmpdir):\n    # copy local dir to and back from HadoopFS and make sure the results match\n    from mlflow.spark import _HadoopFileSystem as FS\n\n    test_dir_0 = os.path.join(str(tmpdir), \"expected\")\n    test_file_0 = os.path.join(test_dir_0, \"root\", \"file_0\")\n    test_dir_1 = os.path.join(test_dir_0, \"root\", \"subdir\")\n    test_file_1 = os.path.join(test_dir_1, \"file_1\")\n    os.makedirs(os.path.dirname(test_file_0))\n    with open(test_file_0, \"w\") as f:\n        f.write(\"test0\")\n    os.makedirs(os.path.dirname(test_file_1))\n    with open(test_file_1, \"w\") as f:\n        f.write(\"test1\")\n    remote = \"\/tmp\/mlflow\/test0\"\n    # File should not be copied in this case\n    assert os.path.abspath(test_dir_0) == FS.maybe_copy_from_local_file(test_dir_0, remote)\n    FS.copy_from_local_file(test_dir_0, remote, remove_src=False)\n    local = os.path.join(str(tmpdir), \"actual\")\n    FS.copy_to_local_file(remote, local, remove_src=True)\n    assert sorted(os.listdir(os.path.join(local, \"root\"))) == sorted(\n        [\"subdir\", \"file_0\", \".file_0.crc\"]\n    )\n    assert sorted(os.listdir(os.path.join(local, \"root\", \"subdir\"))) == sorted(\n        [\"file_1\", \".file_1.crc\"]\n    )\n    # compare the files\n    with open(os.path.join(test_dir_0, \"root\", \"file_0\")) as expected_f:\n        with open(os.path.join(local, \"root\", \"file_0\")) as actual_f:\n            assert expected_f.read() == actual_f.read()\n    with open(os.path.join(test_dir_0, \"root\", \"subdir\", \"file_1\")) as expected_f:\n        with open(os.path.join(local, \"root\", \"subdir\", \"file_1\")) as actual_f:\n            assert expected_f.read() == actual_f.read()\n\n    # make sure we cleanup\n    assert not os.path.exists(FS._remote_path(remote).toString())  # skip file: prefix\n    FS.copy_from_local_file(test_dir_0, remote, remove_src=False)\n    assert os.path.exists(FS._remote_path(remote).toString())  # skip file: prefix\n    FS.delete(remote)\n    assert not os.path.exists(FS._remote_path(remote).toString())  # skip file: prefix\n\n\n@pytest.mark.large\ndef test_model_export(spark_model_iris, model_path, spark_custom_env):\n    sparkm.save_model(spark_model_iris.model, path=model_path, conda_env=spark_custom_env)\n    # 1. score and compare reloaded sparkml model\n    reloaded_model = sparkm.load_model(model_uri=model_path)\n    preds_df = reloaded_model.transform(spark_model_iris.spark_df)\n    preds1 = [x.prediction for x in preds_df.select(\"prediction\").collect()]\n    assert spark_model_iris.predictions == preds1\n    m = pyfunc.load_pyfunc(model_path)\n    # 2. score and compare reloaded pyfunc\n    preds2 = m.predict(spark_model_iris.pandas_df)\n    assert spark_model_iris.predictions == preds2\n    # 3. score and compare reloaded pyfunc Spark udf\n    preds3 = score_model_as_udf(model_uri=model_path, pandas_df=spark_model_iris.pandas_df)\n    assert spark_model_iris.predictions == preds3\n    assert os.path.exists(sparkm.DFS_TMP)\n\n\n@pytest.mark.large\ndef test_model_export_with_signature_and_examples(iris_df, spark_model_iris):\n    _, _, iris_spark_df = iris_df\n    signature_ = infer_signature(iris_spark_df)\n    example_ = iris_spark_df.toPandas().head(3)\n    for signature in (None, signature_):\n        for example in (None, example_):\n            with TempDir() as tmp:\n                path = tmp.path(\"model\")\n                sparkm.save_model(\n                    spark_model_iris.model, path=path, signature=signature, input_example=example\n                )\n                mlflow_model = Model.load(path)\n                assert signature == mlflow_model.signature\n                if example is None:\n                    assert mlflow_model.saved_input_example_info is None\n                else:\n                    assert all((_read_example(mlflow_model, path) == example).all())\n\n\n@pytest.mark.large\ndef test_log_model_with_signature_and_examples(iris_df, spark_model_iris):\n    _, _, iris_spark_df = iris_df\n    signature_ = infer_signature(iris_spark_df)\n    example_ = iris_spark_df.toPandas().head(3)\n    artifact_path = \"model\"\n    for signature in (None, signature_):\n        for example in (None, example_):\n            with mlflow.start_run():\n                sparkm.log_model(\n                    spark_model_iris.model,\n                    artifact_path=artifact_path,\n                    signature=signature,\n                    input_example=example,\n                )\n                artifact_uri = mlflow.get_artifact_uri()\n                model_path = os.path.join(artifact_uri, artifact_path)\n                mlflow_model = Model.load(model_path)\n                assert signature == mlflow_model.signature\n                if example is None:\n                    assert mlflow_model.saved_input_example_info is None\n                else:\n                    assert all((_read_example(mlflow_model, model_path) == example).all())\n\n\n@pytest.mark.large\ndef test_estimator_model_export(spark_model_estimator, model_path, spark_custom_env):\n    sparkm.save_model(spark_model_estimator.model, path=model_path, conda_env=spark_custom_env)\n    # score and compare the reloaded sparkml model\n    reloaded_model = sparkm.load_model(model_uri=model_path)\n    preds_df = reloaded_model.transform(spark_model_estimator.spark_df)\n    preds = [x.prediction for x in preds_df.select(\"prediction\").collect()]\n    assert spark_model_estimator.predictions == preds\n    # 2. score and compare reloaded pyfunc\n    m = pyfunc.load_pyfunc(model_path)\n    preds2 = m.predict(spark_model_estimator.spark_df.toPandas())\n    assert spark_model_estimator.predictions == preds2\n\n\n@pytest.mark.large\ndef test_transformer_model_export(spark_model_transformer, model_path, spark_custom_env):\n    with pytest.raises(MlflowException) as e:\n        sparkm.save_model(\n            spark_model_transformer.model, path=model_path, conda_env=spark_custom_env\n        )\n    assert \"Cannot serialize this model\" in e.value.message\n\n\n@pytest.mark.large\ndef test_model_deployment(spark_model_iris, model_path, spark_custom_env):\n    sparkm.save_model(\n        spark_model_iris.model, path=model_path, conda_env=spark_custom_env,\n    )\n    scoring_response = score_model_in_sagemaker_docker_container(\n        model_uri=model_path,\n        data=spark_model_iris.pandas_df,\n        content_type=pyfunc_scoring_server.CONTENT_TYPE_JSON_SPLIT_ORIENTED,\n        flavor=mlflow.pyfunc.FLAVOR_NAME,\n    )\n    np.testing.assert_array_almost_equal(\n        spark_model_iris.predictions, np.array(json.loads(scoring_response.content)), decimal=4\n    )\n\n\n@pytest.mark.large\n@pytest.mark.skipif(\n    \"dev\" in pyspark.__version__,\n    reason=\"The dev version of pyspark built from the source doesn't exist on PyPI or Anaconda\",\n)\ndef test_sagemaker_docker_model_scoring_with_default_conda_env(spark_model_iris, model_path):\n    sparkm.save_model(spark_model_iris.model, path=model_path, conda_env=None)\n\n    scoring_response = score_model_in_sagemaker_docker_container(\n        model_uri=model_path,\n        data=spark_model_iris.pandas_df,\n        content_type=pyfunc_scoring_server.CONTENT_TYPE_JSON,\n        flavor=mlflow.pyfunc.FLAVOR_NAME,\n    )\n    deployed_model_preds = np.array(json.loads(scoring_response.content))\n\n    np.testing.assert_array_almost_equal(\n        deployed_model_preds, spark_model_iris.predictions, decimal=4\n    )\n\n\n@pytest.mark.large\ndef test_sparkml_model_log(tmpdir, spark_model_iris):\n    # Print the coefficients and intercept for multinomial logistic regression\n    old_tracking_uri = mlflow.get_tracking_uri()\n    cnt = 0\n    # should_start_run tests whether or not calling log_model() automatically starts a run.\n    for should_start_run in [False, True]:\n        for dfs_tmp_dir in [None, os.path.join(str(tmpdir), \"test\")]:\n            print(\"should_start_run =\", should_start_run, \"dfs_tmp_dir =\", dfs_tmp_dir)\n            try:\n                tracking_dir = os.path.abspath(str(tmpdir.join(\"mlruns\")))\n                mlflow.set_tracking_uri(\"file:\/\/%s\" % tracking_dir)\n                if should_start_run:\n                    mlflow.start_run()\n                artifact_path = \"model%d\" % cnt\n                cnt += 1\n                sparkm.log_model(\n                    artifact_path=artifact_path,\n                    spark_model=spark_model_iris.model,\n                    dfs_tmpdir=dfs_tmp_dir,\n                )\n                model_uri = \"runs:\/{run_id}\/{artifact_path}\".format(\n                    run_id=mlflow.active_run().info.run_id, artifact_path=artifact_path\n                )\n\n                # test reloaded model\n                reloaded_model = sparkm.load_model(model_uri=model_uri, dfs_tmpdir=dfs_tmp_dir)\n                preds_df = reloaded_model.transform(spark_model_iris.spark_df)\n                preds = [x.prediction for x in preds_df.select(\"prediction\").collect()]\n                assert spark_model_iris.predictions == preds\n            finally:\n                mlflow.end_run()\n                mlflow.set_tracking_uri(old_tracking_uri)\n                x = dfs_tmp_dir or sparkm.DFS_TMP\n                shutil.rmtree(x)\n                shutil.rmtree(tracking_dir)\n\n\n@pytest.mark.large\ndef test_sparkml_estimator_model_log(tmpdir, spark_model_estimator):\n    # Print the coefficients and intercept for multinomial logistic regression\n    old_tracking_uri = mlflow.get_tracking_uri()\n    cnt = 0\n    # should_start_run tests whether or not calling log_model() automatically starts a run.\n    for should_start_run in [False, True]:\n        for dfs_tmp_dir in [None, os.path.join(str(tmpdir), \"test\")]:\n            print(\"should_start_run =\", should_start_run, \"dfs_tmp_dir =\", dfs_tmp_dir)\n            try:\n                tracking_dir = os.path.abspath(str(tmpdir.join(\"mlruns\")))\n                mlflow.set_tracking_uri(\"file:\/\/%s\" % tracking_dir)\n                if should_start_run:\n                    mlflow.start_run()\n                artifact_path = \"model%d\" % cnt\n                cnt += 1\n                sparkm.log_model(\n                    artifact_path=artifact_path,\n                    spark_model=spark_model_estimator.model,\n                    dfs_tmpdir=dfs_tmp_dir,\n                )\n                model_uri = \"runs:\/{run_id}\/{artifact_path}\".format(\n                    run_id=mlflow.active_run().info.run_id, artifact_path=artifact_path\n                )\n\n                # test reloaded model\n                reloaded_model = sparkm.load_model(model_uri=model_uri, dfs_tmpdir=dfs_tmp_dir)\n                preds_df = reloaded_model.transform(spark_model_estimator.spark_df)\n                preds = [x.prediction for x in preds_df.select(\"prediction\").collect()]\n                assert spark_model_estimator.predictions == preds\n            finally:\n                mlflow.end_run()\n                mlflow.set_tracking_uri(old_tracking_uri)\n                x = dfs_tmp_dir or sparkm.DFS_TMP\n                shutil.rmtree(x)\n                shutil.rmtree(tracking_dir)\n\n\n@pytest.mark.large\ndef test_sparkml_model_log_invalid_args(spark_model_transformer, model_path):\n    # pylint: disable=unused-argument\n    with pytest.raises(MlflowException) as e:\n        sparkm.log_model(spark_model=spark_model_transformer.model, artifact_path=\"model0\")\n    assert \"Cannot serialize this model\" in e.value.message\n\n\n@pytest.mark.large\ndef test_log_model_calls_register_model(tmpdir, spark_model_iris):\n    artifact_path = \"model\"\n    dfs_tmp_dir = os.path.join(str(tmpdir), \"test\")\n    try:\n        register_model_patch = mock.patch(\"mlflow.register_model\")\n        with mlflow.start_run(), register_model_patch:\n            sparkm.log_model(\n                artifact_path=artifact_path,\n                spark_model=spark_model_iris.model,\n                dfs_tmpdir=dfs_tmp_dir,\n                registered_model_name=\"AdsModel1\",\n            )\n            model_uri = \"runs:\/{run_id}\/{artifact_path}\".format(\n                run_id=mlflow.active_run().info.run_id, artifact_path=artifact_path\n            )\n            mlflow.register_model.assert_called_once_with(\n                model_uri, \"AdsModel1\", await_registration_for=DEFAULT_AWAIT_MAX_SLEEP_SECONDS\n            )\n    finally:\n        x = dfs_tmp_dir or sparkm.DFS_TMP\n        shutil.rmtree(x)\n\n\n@pytest.mark.large\ndef test_log_model_no_registered_model_name(tmpdir, spark_model_iris):\n    artifact_path = \"model\"\n    dfs_tmp_dir = os.path.join(str(tmpdir), \"test\")\n    try:\n        register_model_patch = mock.patch(\"mlflow.register_model\")\n        with mlflow.start_run(), register_model_patch:\n            sparkm.log_model(\n                artifact_path=artifact_path,\n                spark_model=spark_model_iris.model,\n                dfs_tmpdir=dfs_tmp_dir,\n            )\n            mlflow.register_model.assert_not_called()\n    finally:\n        x = dfs_tmp_dir or sparkm.DFS_TMP\n        shutil.rmtree(x)\n\n\n@pytest.mark.large\ndef test_sparkml_model_load_from_remote_uri_succeeds(spark_model_iris, model_path, mock_s3_bucket):\n    sparkm.save_model(spark_model=spark_model_iris.model, path=model_path)\n\n    artifact_root = \"s3:\/\/{bucket_name}\".format(bucket_name=mock_s3_bucket)\n    artifact_path = \"model\"\n    artifact_repo = S3ArtifactRepository(artifact_root)\n    artifact_repo.log_artifacts(model_path, artifact_path=artifact_path)\n\n    model_uri = artifact_root + \"\/\" + artifact_path\n    reloaded_model = sparkm.load_model(model_uri=model_uri)\n    preds_df = reloaded_model.transform(spark_model_iris.spark_df)\n    preds = [x.prediction for x in preds_df.select(\"prediction\").collect()]\n    assert spark_model_iris.predictions == preds\n\n\n@pytest.mark.large\ndef test_sparkml_model_save_persists_specified_conda_env_in_mlflow_model_directory(\n    spark_model_iris, model_path, spark_custom_env\n):\n    sparkm.save_model(\n        spark_model=spark_model_iris.model, path=model_path, conda_env=spark_custom_env\n    )\n\n    pyfunc_conf = _get_flavor_configuration(model_path=model_path, flavor_name=pyfunc.FLAVOR_NAME)\n    saved_conda_env_path = os.path.join(model_path, pyfunc_conf[pyfunc.ENV])\n    assert os.path.exists(saved_conda_env_path)\n    assert saved_conda_env_path != spark_custom_env\n\n    with open(spark_custom_env, \"r\") as f:\n        spark_custom_env_parsed = yaml.safe_load(f)\n    with open(saved_conda_env_path, \"r\") as f:\n        saved_conda_env_parsed = yaml.safe_load(f)\n    assert saved_conda_env_parsed == spark_custom_env_parsed\n\n\n@pytest.mark.large\ndef test_sparkml_model_save_persists_requirements_in_mlflow_model_directory(\n    spark_model_iris, model_path, spark_custom_env\n):\n    sparkm.save_model(\n        spark_model=spark_model_iris.model, path=model_path, conda_env=spark_custom_env\n    )\n\n    saved_pip_req_path = os.path.join(model_path, \"requirements.txt\")\n    _compare_conda_env_requirements(spark_custom_env, saved_pip_req_path)\n\n\n@pytest.mark.large\ndef test_log_model_with_pip_requirements(spark_model_iris, tmpdir):\n    # Path to a requirements file\n    req_file = tmpdir.join(\"requirements.txt\")\n    req_file.write(\"a\")\n    with mlflow.start_run():\n        mlflow.spark.log_model(spark_model_iris.model, \"model\", pip_requirements=req_file.strpath)\n        _assert_pip_requirements(mlflow.get_artifact_uri(\"model\"), [\"mlflow\", \"a\"])\n\n    # List of requirements\n    with mlflow.start_run():\n        mlflow.spark.log_model(\n            spark_model_iris.model, \"model\", pip_requirements=[f\"-r {req_file.strpath}\", \"b\"]\n        )\n        _assert_pip_requirements(mlflow.get_artifact_uri(\"model\"), [\"mlflow\", \"a\", \"b\"])\n\n    # Constraints file\n    with mlflow.start_run():\n        mlflow.spark.log_model(\n            spark_model_iris.model, \"model\", pip_requirements=[f\"-c {req_file.strpath}\", \"b\"]\n        )\n        _assert_pip_requirements(\n            mlflow.get_artifact_uri(\"model\"), [\"mlflow\", \"b\", \"-c constraints.txt\"], [\"a\"]\n        )\n\n\n@pytest.mark.large\ndef test_log_model_with_extra_pip_requirements(spark_model_iris, tmpdir):\n    default_reqs = mlflow.spark.get_default_pip_requirements()\n\n    # Path to a requirements file\n    req_file = tmpdir.join(\"requirements.txt\")\n    req_file.write(\"a\")\n    with mlflow.start_run():\n        mlflow.spark.log_model(\n            spark_model_iris.model, \"model\", extra_pip_requirements=req_file.strpath\n        )\n        _assert_pip_requirements(mlflow.get_artifact_uri(\"model\"), [\"mlflow\", *default_reqs, \"a\"])\n\n    # List of requirements\n    with mlflow.start_run():\n        mlflow.spark.log_model(\n            spark_model_iris.model, \"model\", extra_pip_requirements=[f\"-r {req_file.strpath}\", \"b\"]\n        )\n        _assert_pip_requirements(\n            mlflow.get_artifact_uri(\"model\"), [\"mlflow\", *default_reqs, \"a\", \"b\"]\n        )\n\n    # Constraints file\n    with mlflow.start_run():\n        mlflow.spark.log_model(\n            spark_model_iris.model, \"model\", extra_pip_requirements=[f\"-c {req_file.strpath}\", \"b\"]\n        )\n        _assert_pip_requirements(\n            mlflow.get_artifact_uri(\"model\"),\n            [\"mlflow\", *default_reqs, \"b\", \"-c constraints.txt\"],\n            [\"a\"],\n        )\n\n\n@pytest.mark.large\ndef test_sparkml_model_save_accepts_conda_env_as_dict(spark_model_iris, model_path):\n    conda_env = dict(mlflow.spark.get_default_conda_env())\n    conda_env[\"dependencies\"].append(\"pytest\")\n    sparkm.save_model(spark_model=spark_model_iris.model, path=model_path, conda_env=conda_env)\n\n    pyfunc_conf = _get_flavor_configuration(model_path=model_path, flavor_name=pyfunc.FLAVOR_NAME)\n    saved_conda_env_path = os.path.join(model_path, pyfunc_conf[pyfunc.ENV])\n    assert os.path.exists(saved_conda_env_path)\n\n    with open(saved_conda_env_path, \"r\") as f:\n        saved_conda_env_parsed = yaml.safe_load(f)\n    assert saved_conda_env_parsed == conda_env\n\n\n@pytest.mark.large\ndef test_sparkml_model_log_persists_specified_conda_env_in_mlflow_model_directory(\n    spark_model_iris, model_path, spark_custom_env\n):\n    artifact_path = \"model\"\n    with mlflow.start_run():\n        sparkm.log_model(\n            spark_model=spark_model_iris.model,\n            artifact_path=artifact_path,\n            conda_env=spark_custom_env,\n        )\n        model_uri = \"runs:\/{run_id}\/{artifact_path}\".format(\n            run_id=mlflow.active_run().info.run_id, artifact_path=artifact_path\n        )\n\n    model_path = _download_artifact_from_uri(artifact_uri=model_uri)\n    pyfunc_conf = _get_flavor_configuration(model_path=model_path, flavor_name=pyfunc.FLAVOR_NAME)\n    saved_conda_env_path = os.path.join(model_path, pyfunc_conf[pyfunc.ENV])\n    assert os.path.exists(saved_conda_env_path)\n    assert saved_conda_env_path != spark_custom_env\n\n    with open(spark_custom_env, \"r\") as f:\n        spark_custom_env_parsed = yaml.safe_load(f)\n    with open(saved_conda_env_path, \"r\") as f:\n        saved_conda_env_parsed = yaml.safe_load(f)\n    assert saved_conda_env_parsed == spark_custom_env_parsed\n\n\n@pytest.mark.large\ndef test_sparkml_model_log_persists_requirements_in_mlflow_model_directory(\n    spark_model_iris, model_path, spark_custom_env\n):\n    artifact_path = \"model\"\n    with mlflow.start_run():\n        sparkm.log_model(\n            spark_model=spark_model_iris.model,\n            artifact_path=artifact_path,\n            conda_env=spark_custom_env,\n        )\n        model_uri = \"runs:\/{run_id}\/{artifact_path}\".format(\n            run_id=mlflow.active_run().info.run_id, artifact_path=artifact_path\n        )\n\n    model_path = _download_artifact_from_uri(artifact_uri=model_uri)\n    saved_pip_req_path = os.path.join(model_path, \"requirements.txt\")\n    _compare_conda_env_requirements(spark_custom_env, saved_pip_req_path)\n\n\n@pytest.mark.large\ndef test_sparkml_model_save_without_specified_conda_env_uses_default_env_with_expected_dependencies(\n    spark_model_iris, model_path\n):\n    sparkm.save_model(spark_model=spark_model_iris.model, path=model_path, conda_env=None)\n\n    pyfunc_conf = _get_flavor_configuration(model_path=model_path, flavor_name=pyfunc.FLAVOR_NAME)\n    conda_env_path = os.path.join(model_path, pyfunc_conf[pyfunc.ENV])\n    with open(conda_env_path, \"r\") as f:\n        conda_env = yaml.safe_load(f)\n\n    assert conda_env == sparkm.get_default_conda_env()\n\n\n@pytest.mark.large\ndef test_sparkml_model_log_without_specified_conda_env_uses_default_env_with_expected_dependencies(\n    spark_model_iris,\n):\n    artifact_path = \"model\"\n    with mlflow.start_run():\n        sparkm.log_model(\n            spark_model=spark_model_iris.model, artifact_path=artifact_path, conda_env=None\n        )\n        model_uri = \"runs:\/{run_id}\/{artifact_path}\".format(\n            run_id=mlflow.active_run().info.run_id, artifact_path=artifact_path\n        )\n\n    model_path = _download_artifact_from_uri(artifact_uri=model_uri)\n    pyfunc_conf = _get_flavor_configuration(model_path=model_path, flavor_name=pyfunc.FLAVOR_NAME)\n    conda_env_path = os.path.join(model_path, pyfunc_conf[pyfunc.ENV])\n    with open(conda_env_path, \"r\") as f:\n        conda_env = yaml.safe_load(f)\n\n    assert conda_env == sparkm.get_default_conda_env()\n\n\n@pytest.mark.large\ndef test_default_conda_env_strips_dev_suffix_from_pyspark_version(spark_model_iris, model_path):\n    mock_version_standard = mock.PropertyMock(return_value=\"2.4.0\")\n    with mock.patch(\"pyspark.__version__\", new_callable=mock_version_standard):\n        default_conda_env_standard = sparkm.get_default_conda_env()\n\n    for dev_version in [\"2.4.0.dev0\", \"2.4.0.dev\", \"2.4.0.dev1\", \"2.4.0dev.a\", \"2.4.0.devb\"]:\n        mock_version_dev = mock.PropertyMock(return_value=dev_version)\n        with mock.patch(\"pyspark.__version__\", new_callable=mock_version_dev):\n            default_conda_env_dev = sparkm.get_default_conda_env()\n            assert default_conda_env_dev == default_conda_env_standard\n\n            with mlflow.start_run():\n                sparkm.log_model(\n                    spark_model=spark_model_iris.model, artifact_path=\"model\", conda_env=None\n                )\n                model_uri = \"runs:\/{run_id}\/{artifact_path}\".format(\n                    run_id=mlflow.active_run().info.run_id, artifact_path=\"model\"\n                )\n\n            model_path = _download_artifact_from_uri(artifact_uri=model_uri)\n            pyfunc_conf = _get_flavor_configuration(\n                model_path=model_path, flavor_name=pyfunc.FLAVOR_NAME\n            )\n            conda_env_path = os.path.join(model_path, pyfunc_conf[pyfunc.ENV])\n            with open(conda_env_path, \"r\") as f:\n                persisted_conda_env_dev = yaml.safe_load(f)\n            assert persisted_conda_env_dev == default_conda_env_standard\n\n    for unaffected_version in [\"2.0\", \"2.3.4\", \"2\"]:\n        mock_version = mock.PropertyMock(return_value=unaffected_version)\n        with mock.patch(\"pyspark.__version__\", new_callable=mock_version):\n            assert unaffected_version in yaml.safe_dump(sparkm.get_default_conda_env())\n\n\n@pytest.mark.large\ndef test_spark_module_model_save_with_mleap_and_unsupported_transformer_raises_exception(\n    spark_model_iris, model_path\n):\n    class CustomTransformer(JavaModel):\n        def _transform(self, dataset):\n            return dataset\n\n    unsupported_pipeline = Pipeline(stages=[CustomTransformer()])\n    unsupported_model = unsupported_pipeline.fit(spark_model_iris.spark_df)\n\n    with pytest.raises(ValueError):\n        sparkm.save_model(\n            spark_model=unsupported_model, path=model_path, sample_input=spark_model_iris.spark_df\n        )\n\n\n@pytest.mark.large\ndef test_mleap_module_model_save_with_invalid_sample_input_type_raises_exception(\n    spark_model_iris, model_path\n):\n    with pytest.raises(Exception):\n        invalid_input = pd.DataFrame()\n        sparkm.save_model(\n            spark_model=spark_model_iris.model, path=model_path, sample_input=invalid_input\n        )\n\n\ndef test_shutil_copytree_without_file_permissions(tmpdir):\n    src_dir = tmpdir.mkdir(\"src-dir\")\n    dst_dir = tmpdir.mkdir(\"dst-dir\")\n    # Test copying empty directory\n    mlflow.spark._shutil_copytree_without_file_permissions(src_dir.strpath, dst_dir.strpath)\n    assert len(os.listdir(dst_dir.strpath)) == 0\n    # Test copying directory with contents\n    src_dir.mkdir(\"subdir\").join(\"subdir-file.txt\").write(\"testing 123\")\n    src_dir.join(\"top-level-file.txt\").write(\"hi\")\n    mlflow.spark._shutil_copytree_without_file_permissions(src_dir.strpath, dst_dir.strpath)\n    assert set(os.listdir(dst_dir.strpath)) == {\"top-level-file.txt\", \"subdir\"}\n    assert set(os.listdir(dst_dir.join(\"subdir\").strpath)) == {\"subdir-file.txt\"}\n    assert dst_dir.join(\"subdir\").join(\"subdir-file.txt\").read() == \"testing 123\"\n    assert dst_dir.join(\"top-level-file.txt\").read() == \"hi\"\n","label":1}
{"content":"# --------------\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n# Code starts here\ndf=pd.read_csv(path)\n\ndf.head()\n\nX=df.drop(columns=[\"attr1089\"],axis=1)\ny=df[\"attr1089\"]\n#train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=4)\n\n#Scaling\nscaler=MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n# Code ends here\n\n\n# --------------\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\n#Initiate a Logistic Regression model\nlr=LogisticRegression()\n\n#Fitting model on train data\nlr.fit(X_train,y_train)\n\ny_pred=lr.predict(X_test)\n\n#ROC AUC score of prediction\nroc_score=roc_auc_score(y_test,y_pred)\nprint(\"roc_score:%.2f\"%roc_score)\n\n\n# --------------\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Initiate DecisionTreeClassifier\ndt=DecisionTreeClassifier(random_state=4)\n\n#Fitting model on train data\ndt.fit(X_train,y_train)\n\ny_pred=dt.predict(X_test)\n\n#ROC AUC score of prediction\nroc_score=roc_auc_score(y_test,y_pred)\nprint(\"roc_score:%.2f\"%roc_score)\n\n\n\n# --------------\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Code strats here\n\n#Initiate a Random Forest\nrfc=RandomForestClassifier(random_state=4)\n\n#Fitting model on train data\nrfc.fit(X_train,y_train)\n\ny_pred=rfc.predict(X_test)\n\n#ROC AUC score of prediction\nroc_score=roc_auc_score(y_test,y_pred)\nprint(\"roc_score:%.2f\"%roc_score)\n\n\n\n# Code ends here\n\n\n# --------------\n# Import Bagging Classifier\nfrom sklearn.ensemble import BaggingClassifier\n\n\n# Code starts here\n\n#Initiate BaggingClassifier\nbagging_clf=BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100,max_samples=100,random_state=0)\n\n#Fitting model on train data\nbagging_clf.fit(X_train,y_train)\n\n\n#accuracy score of test data\nscore_bagging=bagging_clf.score(X_test,y_test)\nprint(\"score_bagging:%.2f\"%score_bagging)\n\n\n\n# Code ends here\n\n\n# --------------\n# Import libraries\nfrom sklearn.ensemble import VotingClassifier\n\n# Various models\nclf_1 = LogisticRegression()\nclf_2 = DecisionTreeClassifier(random_state=4)\nclf_3 = RandomForestClassifier(random_state=4)\n\nmodel_list = [('lr',clf_1),('DT',clf_2),('RF',clf_3)]\n\n\n# Code starts here\nvoting_clf_hard=VotingClassifier(estimators=model_list,voting=\"hard\")\n\n#Fitting model on train data\nvoting_clf_hard.fit(X_train,y_train)\n\n\n#accuracy score of test data\nhard_voting_score=voting_clf_hard.score(X_test,y_test)\nprint(\"hard_voting_score:%.2f\"%hard_voting_score)\n\n\n\n# Code ends here\n\n\n","label":1}
{"content":"from __future__ import absolute_import, division, print_function, with_statement\n\nimport sys\n\nfrom tornado.options import OptionParser, Error\nfrom tornado.test.util import unittest\n\ntry:\n    from cStringIO import StringIO  # python 2\nexcept ImportError:\n    from io import StringIO  # python 3\n\ntry:\n    from unittest import mock  # python 3.3\nexcept ImportError:\n    try:\n        import mock  # third-party mock package\n    except ImportError:\n        mock = None\n\n\nclass OptionsTest(unittest.TestCase):\n    def test_parse_command_line(self):\n        options = OptionParser()\n        options.define(\"port\", default=80)\n        options.parse_command_line([\"main.py\", \"--port=443\"])\n        self.assertEqual(options.port, 443)\n\n    def test_parse_callbacks(self):\n        options = OptionParser()\n        self.called = False\n\n        def callback():\n            self.called = True\n        options.add_parse_callback(callback)\n\n        # non-final parse doesn't run callbacks\n        options.parse_command_line([\"main.py\"], final=False)\n        self.assertFalse(self.called)\n\n        # final parse does\n        options.parse_command_line([\"main.py\"])\n        self.assertTrue(self.called)\n\n        # callbacks can be run more than once on the same options\n        # object if there are multiple final parses\n        self.called = False\n        options.parse_command_line([\"main.py\"])\n        self.assertTrue(self.called)\n\n    def test_help(self):\n        options = OptionParser()\n        try:\n            orig_stderr = sys.stderr\n            sys.stderr = StringIO()\n            with self.assertRaises(SystemExit):\n                options.parse_command_line([\"main.py\", \"--help\"])\n            usage = sys.stderr.getvalue()\n        finally:\n            sys.stderr = orig_stderr\n        self.assertIn(\"Usage:\", usage)\n\n    def test_subcommand(self):\n        base_options = OptionParser()\n        base_options.define(\"verbose\", default=False)\n        sub_options = OptionParser()\n        sub_options.define(\"foo\", type=str)\n        rest = base_options.parse_command_line(\n            [\"main.py\", \"--verbose\", \"subcommand\", \"--foo=bar\"])\n        self.assertEqual(rest, [\"subcommand\", \"--foo=bar\"])\n        self.assertTrue(base_options.verbose)\n        rest2 = sub_options.parse_command_line(rest)\n        self.assertEqual(rest2, [])\n        self.assertEqual(sub_options.foo, \"bar\")\n\n        # the two option sets are distinct\n        try:\n            orig_stderr = sys.stderr\n            sys.stderr = StringIO()\n            with self.assertRaises(Error):\n                sub_options.parse_command_line([\"subcommand\", \"--verbose\"])\n        finally:\n            sys.stderr = orig_stderr\n\n    def test_setattr(self):\n        options = OptionParser()\n        options.define('foo', default=1, type=int)\n        options.foo = 2\n        self.assertEqual(options.foo, 2)\n\n    def test_setattr_type_check(self):\n        # setattr requires that options be the right type and doesn't\n        # parse from string formats.\n        options = OptionParser()\n        options.define('foo', default=1, type=int)\n        with self.assertRaises(Error):\n            options.foo = '2'\n\n    def test_setattr_with_callback(self):\n        values = []\n        options = OptionParser()\n        options.define('foo', default=1, type=int, callback=values.append)\n        options.foo = 2\n        self.assertEqual(values, [2])\n\n    @unittest.skipIf(mock is None, 'mock package not present')\n    def test_mock_patch(self):\n        # ensure that our setattr hooks don't interfere with mock.patch\n        options = OptionParser()\n        options.define('foo', default=1)\n        options.parse_command_line(['main.py', '--foo=2'])\n        self.assertEqual(options.foo, 2)\n\n        with mock.patch.object(options.mockable(), 'foo', 3):\n            self.assertEqual(options.foo, 3)\n        self.assertEqual(options.foo, 2)\n\n        # Try nested patches mixed with explicit sets\n        with mock.patch.object(options.mockable(), 'foo', 4):\n            self.assertEqual(options.foo, 4)\n            options.foo = 5\n            self.assertEqual(options.foo, 5)\n            with mock.patch.object(options.mockable(), 'foo', 6):\n                self.assertEqual(options.foo, 6)\n            self.assertEqual(options.foo, 5)\n        self.assertEqual(options.foo, 2)\n","label":1}
{"content":"import utils\nfrom bili_global import API_LIVE\n\n\nclass BuyLatiaoReq:\n    # \u5176\u5b9e\u4e0eutils\u90e8\u5206\u5dee\u4e0d\u591a\uff0c\u6000\u7591\u53ef\u80fd\u662f\u65b0\u65e7api\n    @staticmethod\n    async def fetch_livebili_userinfo_pc(user):\n        url = f'{API_LIVE}\/xlive\/web-ucenter\/user\/get_user_info'\n        json_rsp = await user.bililive_session.request_json('GET', url, headers=user.dict_bili['pcheaders'])\n        return json_rsp\n        \n        \nclass BuyMedalReq:\n    @staticmethod\n    async def buy_medal(user, uid, coin_type):\n        url = f'https:\/\/api.vc.bilibili.com\/link_group\/v1\/member\/buy_medal?coin_type={coin_type}&master_uid={uid}&platform=android'\n        json_rsp = await user.other_session.request_json('GET', url, headers=user.dict_bili['pcheaders'])\n        return json_rsp\n","label":1}
{"content":"import ctypes\nimport os\nimport sys\nimport warnings\nfrom functools import reduce\n\nimport numpy as np\n\nimport theano\nimport theano.pathparse\nfrom theano import Apply, Op, Variable, config, tensor\nfrom theano.compile.ops import shape_i, shape_i_op\nfrom theano.configdefaults import SUPPORTED_DNN_CONV_ALGO_RUNTIME\nfrom theano.gof import COp, EnumList, ParamsType\nfrom theano.gof.cmodule import GCC_compiler\nfrom theano.gof.type import CDataType, Generic\nfrom theano.gpuarray import cudnn_defs, pygpu\nfrom theano.gpuarray.basic_ops import (\n    GpuAllocEmpty,\n    GpuArrayType,\n    HostFromGpu,\n    as_gpuarray_variable,\n    empty_like,\n    gpu_contiguous,\n    gpuarray_helper_inc_dir,\n    infer_context_name,\n)\nfrom theano.gpuarray.type import GpuArraySharedVariable, get_context, gpu_context_type\nfrom theano.gradient import DisconnectedType, grad_not_implemented\nfrom theano.scalar import as_scalar\nfrom theano.scalar import bool as bool_t\nfrom theano.scalar import constant, get_scalar_type\nfrom theano.scalar import int32 as int_t\nfrom theano.scalar import uint32 as uint32_t\nfrom theano.tensor.basic import as_tensor_variable\nfrom theano.tensor.extra_ops import cpu_contiguous\nfrom theano.tensor.nnet.abstract_conv import (\n    AbstractConv2d,\n    AbstractConv2d_gradInputs,\n    AbstractConv2d_gradWeights,\n    AbstractConv3d,\n    AbstractConv3d_gradInputs,\n    AbstractConv3d_gradWeights,\n    assert_conv_shape,\n    get_conv_output_shape,\n)\nfrom theano.tensor.opt import Assert\n\n\nDNN_CONV_ALGO_CHOOSE_ONCE = [\"guess_once\", \"time_once\"]\nDNN_CONV_ALGO_CHOOSE_TIME = [\"time_once\", \"time_on_shape_change\"]\n\ntry:\n    from pygpu import gpuarray\nexcept ImportError:\n    pass\n\n# Update these names when new versions of cudnn are supported.\nWIN32_CUDNN_NAMES = [\"cudnn64_7.dll\", \"cudnn64_6.dll\", \"cudnn64_5.dll\"]\n\nif sys.platform == \"win32\":\n    theano.pathparse.PathParser(theano.config.dnn.bin_path)\n\n\ndef _load_lib(name):\n    try:\n        return ctypes.cdll.LoadLibrary(name)\n    except OSError:\n        return None\n\n\ndef _dnn_lib():\n    if _dnn_lib.handle is None:\n        import ctypes.util\n\n        if config.dnn.bin_path != \"\":\n            if sys.platform == \"darwin\":\n                dnn_handle = _load_lib(\n                    os.path.join(config.dnn.bin_path, \"libcudnn.dylib\")\n                )\n            elif sys.platform == \"win32\":\n                for name in WIN32_CUDNN_NAMES:\n                    dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, name))\n                    if dnn_handle is not None:\n                        break\n            else:\n                dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, \"libcudnn.so\"))\n        else:\n            lib_name = ctypes.util.find_library(\"cudnn\")\n            if lib_name is None and sys.platform == \"win32\":\n                for name in WIN32_CUDNN_NAMES:\n                    lib_name = ctypes.util.find_library(name)\n                    if lib_name:\n                        break\n            if lib_name is None:\n                raise RuntimeError(\n                    \"Could not find cudnn library (looked for v5* to v7*).\"\n                    \" Check your cudnn installation. Maybe using the Theano\"\n                    ' flag dnn.base_path can help you. Current value \"%s\"'\n                    % config.dnn.base_path\n                )\n            else:\n                dnn_handle = ctypes.cdll.LoadLibrary(lib_name)\n        if dnn_handle is None:\n            raise RuntimeError(\n                \"Could not load cudnn library. Check your cudnn\"\n                \" installation. Maybe using the Theano\"\n                ' flag dnn.base_path can help you. Current value \"%s\"'\n                % config.dnn.base_path\n            )\n        _dnn_lib.handle = dnn_handle\n        cudnn = _dnn_lib.handle\n        cudnn.cudnnCreate.argtypes = [ctypes.POINTER(ctypes.c_void_p)]\n        cudnn.cudnnCreate.restype = ctypes.c_int\n        cudnn.cudnnDestroy.argtypes = [ctypes.c_void_p]\n        cudnn.cudnnDestroy.restype = ctypes.c_int\n    return _dnn_lib.handle\n\n\n_dnn_lib.handle = None\n\n\ndef _make_handle(ctx):\n    cudnn = _dnn_lib()\n    handle = ctypes.c_void_p()\n    with ctx:\n        err = cudnn.cudnnCreate(ctypes.byref(handle))\n    if err != 0:\n        raise RuntimeError(\n            \"Error creating cudnn handle. \" \"This can be a sign of a too old driver.\",\n            err,\n        )\n    return handle\n\n\ndef _dnn_check_compile():\n    preambule = \"\"\"\n#include <stdio.h>\n#include <cudnn.h>\n#include <cudnn_helper.h>\n\"\"\"\n\n    # No need for the context in here since we won't execute that code\n    body = \"\"\"\ncudnnHandle_t _handle = NULL;\ncudnnStatus_t err;\nif ((err = cudnnCreate(&_handle)) != CUDNN_STATUS_SUCCESS) {\n  fprintf(stderr, \"could not create cuDNN handle: %s\",\n          cudnnGetErrorString(err));\n  return 1;\n}\n\"\"\"\n\n    path_wrapper = '\"' if os.name == \"nt\" else \"\"\n    params = [\"-l\", \"cudnn\"]\n    params.extend(\n        [\"-I{}{}{}\".format(path_wrapper, gpuarray_helper_inc_dir(), path_wrapper)]\n    )\n    if config.dnn.include_path:\n        params.extend(\n            [\"-I{}{}{}\".format(path_wrapper, config.dnn.include_path, path_wrapper)]\n        )\n    if config.cuda.include_path:\n        params.extend(\n            [\"-I{}{}{}\".format(path_wrapper, config.cuda.include_path, path_wrapper)]\n        )\n    if config.dnn.library_path:\n        params.extend(\n            [\"-L{}{}{}\".format(path_wrapper, config.dnn.library_path, path_wrapper)]\n        )\n    # Do not run here the test program. It would run on the\n    # default gpu, not the one selected by the user. If mixed\n    # GPU are installed or if the GPUs are configured in\n    # exclusive mode, this cause bad detection.\n\n    # NB: GCC_compiler.try_flags() may return just a boolean instead of a tuple (avail, out, here).\n    compiler_res = GCC_compiler.try_flags(\n        params, preambule=preambule, body=body, try_run=False, output=True\n    )\n\n    avail, out, err = (\n        compiler_res if isinstance(compiler_res, tuple) else (compiler_res, None, None)\n    )\n\n    if not avail:\n        return False, (\"cannot compile with cuDNN. \" \"We got this error:\\n\" + str(err))\n    return True, None\n\n\ndef _dnn_check_version():\n    v = version()\n    if v < 5000:\n        return False, \"cuDNN version is too old. Update to v5* or higher, was %d.\" % v\n    if v >= 7200:\n        warnings.warn(\n            \"Your cuDNN version is more recent than \"\n            \"Theano. If you encounter problems, try \"\n            \"updating Theano or downgrading cuDNN to \"\n            \"a version >= v5 and <= v7.\"\n        )\n    return True, None\n\n\ndef dnn_present():\n    if dnn_present.avail is not None:\n        return dnn_present.avail\n    if config.dnn.enabled == \"False\":\n        dnn_present.msg = \"Disabled by dnn.enabled flag\"\n        dnn_present.avail = False\n        return False\n\n    if pygpu is None:\n        dnn_present.msg = \"PyGPU not available\"\n        dnn_present.avail = False\n        return False\n\n    if config.dnn.enabled == \"no_check\":\n        dnn_present.avail, dnn_present.msg = (\n            True,\n            \"presence check disabled by dnn.enabled flag\",\n        )\n    else:\n        dnn_present.avail, dnn_present.msg = _dnn_check_compile()\n    if dnn_present.avail:\n        dnn_present.avail, dnn_present.msg = _dnn_check_version()\n        if not dnn_present.avail:\n            return False\n\n    return dnn_present.avail\n\n\ndnn_present.avail = None\ndnn_present.msg = None\n\n\ndef dnn_available(context_name):\n    if not dnn_present():\n        dnn_available.msg = dnn_present.msg\n        return False\n\n    ctx = get_context(context_name)\n\n    if not ctx.kind == b\"cuda\":\n        dnn_available.msg = \"Not on a CUDA device.\"\n        return False\n\n    # This is a hack because bin_id is in the from of\n    # \"<something>_<major><minor>\" for cuda devices.\n    if int(ctx.bin_id[-2:]) < 30:\n        dnn_available.msg = \"Device not supported\"\n        return False\n\n    # On V100, cuDNN lower then 7002 don't raise error but\n    # takes hours to load or execute! So raise a good user error.\n    if version() < 7002:\n        if int(ctx.bin_id[-2:]) >= 70:\n            dnn_available.msg = \"Use cuDNN 7.0.2 or higher for Volta.\"\n            return False\n    return True\n\n\ndnn_available.msg = None\n\n\ndef CUDNNDataType(name, freefunc=None):\n    cargs = []\n    if config.dnn.bin_path and sys.platform != \"win32\":\n        cargs.append(\"-Wl,-rpath,\" + config.dnn.bin_path)\n\n    return CDataType(\n        name,\n        freefunc,\n        headers=[\"cudnn.h\"],\n        header_dirs=[config.dnn.include_path, config.cuda.include_path],\n        libraries=[\"cudnn\"],\n        lib_dirs=[config.dnn.library_path],\n        compile_args=cargs,\n        version=version(raises=False),\n    )\n\n\nclass DnnVersion(Op):\n    __props__ = ()\n\n    def c_headers(self):\n        return [\"cudnn.h\"]\n\n    def c_header_dirs(self):\n        return [config.dnn.include_path, config.cuda.include_path]\n\n    def c_libraries(self):\n        return [\"cudnn\"]\n\n    def c_lib_dirs(self):\n        return [config.dnn.library_path]\n\n    def c_compile_args(self):\n        if config.dnn.bin_path and sys.platform != \"win32\":\n            return [\"-Wl,-rpath,\" + config.dnn.bin_path]\n        return []\n\n    def c_support_code(self):\n        return \"\"\"\n#if PY_MAJOR_VERSION >= 3\n#define PyInt_FromLong PyLong_FromLong\n#endif\n\"\"\"\n\n    def make_node(self):\n        return Apply(self, [], [Generic()()])\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        o = outputs[0]\n        return (\n            \"\"\"\n        %(o)s = PyTuple_Pack(2, PyInt_FromLong(CUDNN_VERSION), PyInt_FromLong(cudnnGetVersion()));\n        \"\"\"\n            % locals()\n        )\n\n    def do_constant_folding(self, node):\n        # Needed as we do not want to cache this information.\n        return False\n\n    def c_code_cache_version(self):\n        # Not needed, but make it clear that we do not want to cache this.\n        return None\n\n\ndef version(raises=True):\n    \"\"\"Return the current cuDNN version we link with.\n\n    This also does a check that the header version matches the runtime version.\n\n    :raises: If True, raise an exception if cuDNN is not present.\n        Otherwise, return -1.\n\n    It always raise an RuntimeError if the header and library version\n    are not the same.\n\n    \"\"\"\n    if not dnn_present():\n        if raises:\n            raise RuntimeError(\n                \"We can't determine the cudnn version as it is not available\",\n                dnn_available.msg,\n            )\n        else:\n            return -1\n\n    if version.v is None:\n        f = theano.function(\n            [], DnnVersion()(), theano.Mode(optimizer=None), profile=False\n        )\n        v = f()\n        if v[0] != v[1]:\n            raise RuntimeError(\n                \"Mixed dnn version. The header is version %s \"\n                \"while the library is version %s.\" % v\n            )\n        version.v = v[1]\n    return version.v\n\n\nversion.v = None\n\nhandle_type = CUDNNDataType(\"cudnnHandle_t\", \"cudnnDestroy\")\n\n# Get cuDNN definitions to be used.\ncudnn = cudnn_defs.get_definitions(version(raises=False))\n\n\ndef get_precision(precision, inputs, for_grad=False):\n    common_dtype = theano.scalar.upcast(*[i.dtype for i in inputs])\n    if not common_dtype.startswith(\"float\"):\n        raise TypeError(\"cuDNN convolution only works on real numbers\")\n\n    if precision is None:\n        precision = theano.config.dnn.conv.precision\n    if precision == \"as_input\" or precision == \"as_input_f32\":\n        if common_dtype == \"float16\" and precision == \"as_input_f32\":\n            precision = \"float32\"\n        else:\n            precision = common_dtype\n    if for_grad and precision == \"float16\":\n        raise TypeError(\n            \"Float16 precision is disabled for cuDNN backward convolutions due to computation errors.\"\n        )\n    return precision, common_dtype\n\n\nclass DnnBase(COp):\n\n    \"\"\"\n    Creates a handle for cudnn and pulls in the cudnn libraries and headers.\n\n    \"\"\"\n\n    # dnn does not know about broadcasting, so we do not need to assert\n    # the input broadcasting pattern.\n    check_broadcast = False\n    params_type = handle_type\n\n    def dnn_context(self, node):\n        return node.outputs[0].type.context_name\n\n    def get_params(self, node):\n        ctx_name = self.dnn_context(node)\n        ctx = get_context(ctx_name)\n        if not hasattr(ctx, \"cudnn_handle_param\"):\n            ptr = ctx.cudnn_handle.value\n            res = handle_type.make_value(ptr)\n            ctx.cudnn_handle_param = res\n        if isinstance(self.params_type, ParamsType):\n            if not self.params_type.has_type(handle_type):\n                raise TypeError(\n                    \"DnnBase: params_type must take into account the cuDNN handle type.\"\n                )\n            handle_field = self.params_type.get_field(handle_type)\n            return self.params_type.get_params(\n                self, **{handle_field: ctx.cudnn_handle_param}\n            )\n        return ctx.cudnn_handle_param\n\n    def __init__(self, files=None, c_func=None):\n        if files is None:\n            files = []\n        COp.__init__(self, [\"c_code\/dnn_base.c\"] + files, c_func)\n\n    def c_headers(self):\n        return [\n            \"gpuarray\/types.h\",\n            \"gpuarray\/array.h\",\n            \"gpuarray\/kernel.h\",\n            \"gpuarray\/util.h\",\n            \"gpuarray\/ext_cuda.h\",\n            \"gpuarray_api.h\",\n            \"numpy_compat.h\",\n            \"cudnn.h\",\n            \"cudnn_helper.h\",\n            \"gpuarray_helper.h\",\n        ]\n\n    def c_header_dirs(self):\n        return [\n            gpuarray_helper_inc_dir(),\n            pygpu.get_include(),\n            config.dnn.include_path,\n            config.cuda.include_path,\n        ]\n\n    def c_libraries(self):\n        return [\"cudnn\", \"gpuarray\"]\n\n    def c_lib_dirs(self):\n        return [config.dnn.library_path]\n\n    def c_compile_args(self):\n        if config.dnn.bin_path and sys.platform != \"win32\":\n            return [\"-Wl,-rpath,\" + config.dnn.bin_path]\n        return []\n\n    def c_code_cache_version(self):\n        return (super().c_code_cache_version(), version(), 4)\n\n\nclass GpuDnnConvDesc(COp):\n\n    \"\"\"\n    This Op builds a convolution descriptor for use in the other convolution\n    operations.\n\n    See the doc of :func:`dnn_conv` for a description of the parameters\n\n    \"\"\"\n\n    __props__ = (\n        \"border_mode\",\n        \"subsample\",\n        \"dilation\",\n        \"conv_mode\",\n        \"precision\",\n        \"num_groups\",\n    )\n    params_type = ParamsType(\n        pad0=int_t,\n        pad1=int_t,\n        pad2=int_t,\n        sub0=int_t,\n        sub1=int_t,\n        sub2=int_t,\n        dil0=int_t,\n        dil1=int_t,\n        dil2=int_t,\n        nb_dims=int_t,\n        bmode=EnumList(\n            (\"BORDER_MODE_FULL\", \"full\"),\n            (\"BORDER_MODE_VALID\", \"valid\"),\n            (\"BORDER_MODE_HALF\", \"half\"),\n        ),\n        conv_mode=cudnn.cudnnConvolutionMode_t,\n        precision=cudnn.cudnnDataType_t,\n        num_groups=int_t,\n    )\n\n    def c_headers(self):\n        return [\"cudnn.h\", \"cudnn_helper.h\"]\n\n    def c_header_dirs(self):\n        return [\n            gpuarray_helper_inc_dir(),\n            config.dnn.include_path,\n            config.cuda.include_path,\n        ]\n\n    def c_libraries(self):\n        return [\"cudnn\"]\n\n    def c_lib_dirs(self):\n        return [config.dnn.library_path]\n\n    def c_compile_args(self):\n        if config.dnn.bin_path and sys.platform != \"win32\":\n            return [\"-Wl,-rpath,\" + config.dnn.bin_path]\n        return []\n\n    def do_constant_folding(self, node):\n        return False\n\n    def __init__(\n        self,\n        border_mode,\n        subsample=(1, 1),\n        dilation=(1, 1),\n        conv_mode=\"conv\",\n        precision=\"float32\",\n        num_groups=1,\n    ):\n        COp.__init__(self, [\"c_code\/conv_desc.c\"], \"APPLY_SPECIFIC(conv_desc)\")\n\n        if version() < 6000 and any([d != 1 for d in dilation]):\n            raise RuntimeError(\"Dilation > 1 not supported for cuDNN version < 6.\")\n\n        if isinstance(border_mode, int):\n            border_mode = (border_mode,) * len(subsample)\n        if isinstance(border_mode, tuple):\n            assert len(border_mode) == len(subsample)\n            border_mode = tuple(map(int, border_mode))\n        if not (\n            (isinstance(border_mode, tuple) and min(border_mode) >= 0)\n            or border_mode in (\"valid\", \"full\", \"half\")\n        ):\n            raise ValueError(\n                \"invalid border_mode {}, which must be either \"\n                '\"valid\", \"full\", \"half\", an integer or a pair of'\n                \" integers\".format(border_mode)\n            )\n        self.border_mode = border_mode\n        assert len(subsample) in (2, 3)\n        self.subsample = subsample\n        assert cudnn.cudnnConvolutionMode_t.has_alias(conv_mode)\n        self.conv_mode = conv_mode\n        self.num_groups = num_groups\n\n        assert len(dilation) == len(subsample)\n        self.dilation = dilation\n\n        assert cudnn.cudnnDataType_t.has_alias(precision)\n        self.precision = precision\n\n    def make_node(self, kern_shape):\n        kern_shape = as_tensor_variable(kern_shape)\n        if (\n            kern_shape.type.ndim != 1\n            or kern_shape.dtype not in theano.tensor.basic.int_dtypes\n        ):\n            raise TypeError(\"kern must be an int64 1D shape tensor\")\n        kern_shape = theano.tensor.basic.cast(kern_shape, \"int64\")\n\n        node = Apply(\n            self,\n            [kern_shape],\n            [\n                CUDNNDataType(\n                    \"cudnnConvolutionDescriptor_t\",\n                    freefunc=\"cudnnDestroyConvolutionDescriptor\",\n                )()\n            ],\n        )\n        # DebugMode cannot compare the values of CDataType variables, so by\n        # default it returns False all the time. To prevent DebugMode from\n        # complaining because of the MergeOptimizer, we make this variable\n        # always compare to True.\n        out = node.outputs[0]\n        out.tag.values_eq_approx = tensor.type.values_eq_approx_always_true\n        return node\n\n    bmode = property(\n        lambda self: \"valid\"\n        if isinstance(self.border_mode, tuple)\n        else self.border_mode\n    )\n    pad0 = property(\n        lambda self: self.border_mode[0] if isinstance(self.border_mode, tuple) else 0\n    )\n    pad1 = property(\n        lambda self: self.border_mode[1] if isinstance(self.border_mode, tuple) else 0\n    )\n    pad2 = property(\n        lambda self: self.border_mode[2]\n        if (isinstance(self.border_mode, tuple) and len(self.border_mode) > 2)\n        else 0\n    )\n    sub0 = property(lambda self: self.subsample[0])\n    sub1 = property(lambda self: self.subsample[1])\n    sub2 = property(lambda self: self.subsample[2] if len(self.subsample) > 2 else 0)\n    dil0 = property(lambda self: self.dilation[0])\n    dil1 = property(lambda self: self.dilation[1])\n    dil2 = property(lambda self: self.dilation[2] if len(self.dilation) > 2 else 0)\n    nb_dims = property(lambda self: len(self.subsample))\n\n    def c_code_cache_version(self):\n        return (super().c_code_cache_version(), version())\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        if not hasattr(self, \"dilation\"):\n            self.dilation = (1,) * len(self.subsample)\n        if not hasattr(self, \"num_groups\"):\n            self.num_groups = 1\n\n\n# scalar constants\n_zero = constant(np.asarray(0.0, dtype=\"float64\"))\n_one = constant(np.asarray(1.0, dtype=\"float64\"))\n\n\ndef ensure_dt(val, default, name, dtype):\n    if dtype == \"float16\":\n        dtype = \"float32\"\n    if val is None:\n        val = default.clone()\n    if not isinstance(val, Variable):\n        val = constant(val)\n    if hasattr(val, \"ndim\") and val.ndim == 0:\n        val = as_scalar(val)\n    if not isinstance(val.type, theano.scalar.Scalar):\n        raise TypeError(\"{}: expected a scalar value\".format(name))\n    if not val.type.dtype == dtype:\n        val = val.astype(dtype)\n    return val\n\n\nclass GpuDnnConv(DnnBase):\n\n    \"\"\"\n    The forward convolution.\n\n    Parameters\n    ----------\n    image\n    kernel\n    descr :\n        The convolution descriptor.\n    algo : {'small', 'none', 'large', 'fft', 'fft_tiling', 'winograd', 'guess_once',\n            'guess_on_shape_change', 'time_once', 'time_on_shape_change'}\n        Default is the value of :attr:`config.dnn.conv.algo_fwd`.\n    num_groups :\n        Divides the image, kernel and output tensors into num_groups\n        separate groups. Each which carry out convolutions separately\n\n    \"\"\"\n\n    _f16_ok = True\n    __props__ = (\"algo\", \"inplace\", \"num_groups\")\n\n    check_input = False\n    params_type = ParamsType(\n        conv_algo=cudnn.cudnnConvolutionFwdAlgo_t,\n        choose_algo=bool_t,\n        choose_once=bool_t,\n        choose_time=bool_t,\n        inplace=bool_t,\n        handle=handle_type,\n        num_groups=int_t,\n    )\n\n    def __init__(self, algo=None, inplace=False, num_groups=1):\n        DnnBase.__init__(\n            self,\n            [\"c_code\/dnn_conv_base.c\", \"c_code\/dnn_fwd.c\"],\n            \"APPLY_SPECIFIC(conv_fwd)\",\n        )\n\n        if algo is None:\n            algo = config.dnn.conv.algo_fwd\n        self.algo = algo\n\n        self.inplace = bool(inplace)\n        if self.inplace:\n            self.destroy_map = {0: [2]}\n\n        assert (\n            cudnn.cudnnConvolutionFwdAlgo_t.has_alias(self.algo)\n            or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME\n        )\n\n        self.conv_algo = (\n            cudnn.cudnnConvolutionFwdAlgo_t.CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM\n        )\n        if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:\n            self.conv_algo = self.algo\n        self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME\n        self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE\n        self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME\n        self.num_groups = num_groups\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        if not hasattr(self, \"algo\"):\n            if hasattr(self, \"workmem\"):\n                self.algo = self.workmem\n            else:\n                self.algo = config.dnn.conv.algo_fwd\n        if not hasattr(self, \"inplace\"):\n            self.inplace = False\n        if not hasattr(self, \"num_groups\"):\n            self.num_groups = 1\n\n    def make_node(self, img, kern, output, desc, alpha=None, beta=None):\n        ctx_name = infer_context_name(img, kern, output)\n        img = as_gpuarray_variable(img, ctx_name)\n        kern = as_gpuarray_variable(kern, ctx_name)\n        output = as_gpuarray_variable(output, ctx_name)\n\n        if img.type.ndim not in (4, 5):\n            raise TypeError(\"img must be 4D or 5D tensor\")\n        if kern.type.ndim not in (4, 5):\n            raise TypeError(\"kern must be 4D or 5D tensor\")\n        if output.type.ndim not in (4, 5):\n            raise TypeError(\"output must be a 4D or 5D tensor\")\n\n        if img.type.ndim != kern.type.ndim or img.type.ndim != output.type.ndim:\n            raise TypeError(\n                \"The number of dimensions of \" \"img, kern and output must match\"\n            )\n\n        if img.type.ndim == 5 and self.algo not in (\n            cudnn.conv3d_fwd_algorithms + SUPPORTED_DNN_CONV_ALGO_RUNTIME\n        ):\n            raise ValueError(\n                \"convolution algo %s can't be used for \" \"3d convolutions\", (self.algo,)\n            )\n\n        if (\n            not isinstance(desc.type, CDataType)\n            or desc.type.ctype != \"cudnnConvolutionDescriptor_t\"\n        ):\n            raise TypeError(\"desc must be cudnnConvolutionDescriptor_t\")\n\n        alpha = ensure_dt(alpha, _one, \"alpha\", img.dtype)\n        beta = ensure_dt(beta, _zero, \"beta\", img.dtype)\n\n        return Apply(self, [img, kern, output, desc, alpha, beta], [output.type()])\n\n    def grad(self, inp, grads):\n        img, kerns, output, desc, alpha, beta = inp\n        (top,) = grads\n\n        top = gpu_contiguous(top)\n\n        d_img = GpuDnnConvGradI(num_groups=self.num_groups)(\n            kerns, top, empty_like(img), desc\n        )\n        d_kerns = GpuDnnConvGradW(num_groups=self.num_groups)(\n            img, top, empty_like(kerns), desc\n        )\n        d_alpha = grad_not_implemented(self, 4, alpha)\n        d_beta = grad_not_implemented(self, 5, beta)\n\n        return [\n            d_img * alpha,\n            d_kerns * alpha,\n            top * beta,\n            DisconnectedType()(),\n            d_alpha,\n            d_beta,\n        ]\n\n    def connection_pattern(self, node):\n        # not connected to desc\n        return [[1], [1], [1], [0], [1], [1]]\n\n    @staticmethod\n    def get_out_shape(ishape, kshape, border_mode, subsample, dilation):\n        \"\"\"\n        This function computes the output shape for a convolution with\n        the specified parameters. `ishape` and `kshape` can be symbolic\n        or scalar.\n\n        \"\"\"\n\n        # if ishape and\/or kshape are not tuples or list, but rather symbolic\n        # vectors, turn them into lists of symbolic scalars.\n        if not isinstance(ishape, (list, tuple)):\n            ishape = [ishape[i] for i in range(len(subsample) + 2)]\n        if not isinstance(kshape, (list, tuple)):\n            kshape = [kshape[i] for i in range(len(subsample) + 2)]\n\n        return get_conv_output_shape(ishape, kshape, border_mode, subsample, dilation)\n\n    def infer_shape(self, node, shape):\n        return [shape[2]]\n\n\nclass GpuDnnConvGradW(DnnBase):\n\n    \"\"\"\n    The convolution gradient with respect to the weights.\n\n    Parameters\n    ----------\n    image\n    kernel\n    descr :\n        The convolution descriptor.\n    algo : {'none', 'deterministic', 'fft', 'small', 'guess_once',\n            'guess_on_shape_change', 'time_once', 'time_on_shape_change'}\n        Default is the value of :attr:`config.dnn.conv.algo_bwd_filter`.\n    num_groups :\n        Divides the image, kernel and output tensors into num_groups\n        separate groups. Each which carry out convolutions separately\n\n    \"\"\"\n\n    _f16_ok = True\n    __props__ = (\"algo\", \"inplace\", \"num_groups\")\n\n    check_input = False\n    params_type = ParamsType(\n        conv_algo=cudnn.cudnnConvolutionBwdFilterAlgo_t,\n        choose_algo=bool_t,\n        choose_once=bool_t,\n        choose_time=bool_t,\n        inplace=bool_t,\n        handle=handle_type,\n        num_groups=int_t,\n    )\n\n    def __init__(self, inplace=False, algo=None, num_groups=1):\n        DnnBase.__init__(\n            self,\n            [\"c_code\/dnn_conv_base.c\", \"c_code\/dnn_gw.c\"],\n            \"APPLY_SPECIFIC(conv_gw)\",\n        )\n        self.inplace = bool(inplace)\n        if self.inplace:\n            self.destroy_map = {0: [2]}\n        if algo is None:\n            algo = config.dnn.conv.algo_bwd_filter\n        self.algo = algo\n\n        assert (\n            cudnn.cudnnConvolutionBwdFilterAlgo_t.has_alias(self.algo)\n            or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME\n        )\n\n        self.conv_algo = (\n            cudnn.cudnnConvolutionBwdFilterAlgo_t.CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0\n        )\n        if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:\n            self.conv_algo = self.algo\n        self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME\n        self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE\n        self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME\n        self.num_groups = num_groups\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        if not hasattr(self, \"inplace\"):\n            self.inplace = False\n        if not hasattr(self, \"algo\"):\n            self.algo = config.dnn.conv.algo_bwd_filter\n        if not hasattr(self, \"num_groups\"):\n            self.num_groups = 1\n\n    def grad(self, inp, grads):\n        img, top, output, desc, alpha, beta = inp\n        (kerns,) = grads\n\n        kerns = gpu_contiguous(kerns)\n\n        d_img = GpuDnnConvGradI(num_groups=self.num_groups)(\n            kerns, top, empty_like(img), desc\n        )\n        d_top = GpuDnnConv(num_groups=self.num_groups)(\n            img, kerns, empty_like(top), desc\n        )\n        d_alpha = grad_not_implemented(self, 4, alpha)\n        d_beta = grad_not_implemented(self, 5, beta)\n\n        return (\n            d_img * alpha,\n            d_top * alpha,\n            kerns * beta,\n            DisconnectedType()(),\n            d_alpha,\n            d_beta,\n        )\n\n    def connection_pattern(self, node):\n        # not connected to desc\n        return [[1], [1], [1], [0], [1], [1]]\n\n    def op_may_fail_with_subsample(self, img, desc):\n        return (\n            version() < 6000\n            and img.type.dtype == \"float32\"\n            and img.type.ndim == 5\n            and self.algo != \"none\"\n            and desc.owner.op.subsample != (1, 1, 1)\n        )\n\n    def op_may_fail_with_beta(self, img, beta):\n        return (\n            version() < 6000\n            and img.type.dtype == \"float32\"\n            and self.algo not in (\"none\", \"deterministic\", \"fft\", \"small\")\n            and beta is not None\n            and theano.tensor.extract_constant(beta) != 1\n        )\n\n    def make_node(self, img, topgrad, output, desc, alpha=None, beta=None):\n        if self.op_may_fail_with_subsample(img, desc):\n            warnings.warn(\n                \"cuDNN backward filter operation for 3D convolutions may produce bad results \"\n                \"with certain cuDNN algorithms depending on the compute capability of your GPU \"\n                \"if subsample is not (1, 1, 1). If you encounter problems, consider \"\n                'setting the theano flag \"dnn.conv.algo_bwd_filter\" to \"none\".'\n            )\n        if self.op_may_fail_with_beta(img, beta):\n            warnings.warn(\n                \"cuDNN backward filter operation for convolutions may produce bad results \"\n                \"with certain cuDNN algorithms depending on the compute capability of your GPU \"\n                \"if beta != 1. If you encounter problems, consider \"\n                'setting the theano flag \"dnn.conv.algo_bwd_filter\" to '\n                '\"none\", \"deterministic\", \"fft\", or \"small\".'\n            )\n        ctx_name = infer_context_name(img, topgrad, output)\n        img = as_gpuarray_variable(img, ctx_name)\n        topgrad = as_gpuarray_variable(topgrad, ctx_name)\n        output = as_gpuarray_variable(output, ctx_name)\n        if img.type.ndim not in (4, 5):\n            raise TypeError(\"img must be 4D or 5D tensor\")\n        if topgrad.type.ndim not in (4, 5):\n            raise TypeError(\"topgrad must be 4D or 5D tensor\")\n        if output.type.ndim not in (4, 5):\n            raise TypeError(\"output must be 4D or 5D tensor\")\n\n        if img.type.ndim != topgrad.type.ndim or img.type.ndim != output.type.ndim:\n            raise TypeError(\n                \"The number of dimensions of \" \"img, topgrad and output must match\"\n            )\n\n        if img.type.ndim == 5 and self.algo not in (\n            cudnn.conv3d_bwd_filter_algorithms + SUPPORTED_DNN_CONV_ALGO_RUNTIME\n        ):\n            raise ValueError(\n                \"convolution algo %s can't be used for \" \"3d convolutions\", (self.algo,)\n            )\n\n        if (\n            not isinstance(desc.type, CDataType)\n            or desc.type.ctype != \"cudnnConvolutionDescriptor_t\"\n        ):\n            raise TypeError(\"desc must be cudnnConvolutionDescriptor_t\")\n\n        alpha = ensure_dt(alpha, _one, \"alpha\", img.dtype)\n        beta = ensure_dt(beta, _zero, \"beta\", img.dtype)\n\n        return Apply(self, [img, topgrad, output, desc, alpha, beta], [output.type()])\n\n    def infer_shape(self, node, shape):\n        return [shape[2]]\n\n\nclass GpuDnnConvGradI(DnnBase):\n    \"\"\"\n    The convolution gradient with respect to the inputs.\n\n    Parameters\n    ----------\n    image\n    kernel\n    descr\n        The convolution descriptor.\n    algo : {'none', 'deterministic', 'fft', 'fft_tiling', 'winograd', 'guess_once',\n            'guess_on_shape_change', 'time_once', 'time_on_shape_change'}\n        Default is the value of :attr:`config.dnn.conv.algo_bwd_data`.\n    num_groups :\n        Divides the image, kernel and output tensors into num_groups\n        separate groups. Each which carry out convolutions separately\n\n    \"\"\"\n\n    _f16_ok = True\n    __props__ = (\"algo\", \"inplace\", \"num_groups\")\n\n    check_input = False\n    params_type = ParamsType(\n        conv_algo=cudnn.cudnnConvolutionBwdDataAlgo_t,\n        choose_algo=bool_t,\n        choose_once=bool_t,\n        choose_time=bool_t,\n        inplace=bool_t,\n        handle=handle_type,\n        num_groups=int_t,\n    )\n\n    def __init__(self, inplace=False, algo=None, num_groups=1):\n        DnnBase.__init__(\n            self,\n            [\"c_code\/dnn_conv_base.c\", \"c_code\/dnn_gi.c\"],\n            \"APPLY_SPECIFIC(conv_gi)\",\n        )\n        self.inplace = bool(inplace)\n        if self.inplace:\n            self.destroy_map = {0: [2]}\n        if algo is None:\n            algo = config.dnn.conv.algo_bwd_data\n        self.algo = algo\n        assert (\n            cudnn.cudnnConvolutionBwdDataAlgo_t.has_alias(self.algo)\n            or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME\n        )\n\n        self.conv_algo = (\n            cudnn.cudnnConvolutionBwdDataAlgo_t.CUDNN_CONVOLUTION_BWD_DATA_ALGO_0\n        )\n        if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:\n            self.conv_algo = self.algo\n        self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME\n        self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE\n        self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME\n        self.num_groups = num_groups\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        if not hasattr(self, \"algo\"):\n            self.algo = config.dnn.conv.algo_bwd_data\n        if not hasattr(self, \"inplace\"):\n            self.inplace = False\n        if not hasattr(self, \"num_groups\"):\n            self.num_groups = 1\n\n    def grad(self, inp, grads):\n        kerns, top, output, desc, alpha, beta = inp\n        (img,) = grads\n\n        img = gpu_contiguous(img)\n\n        d_kerns = GpuDnnConvGradW(num_groups=self.num_groups)(\n            img, top, empty_like(kerns), desc\n        )\n        d_top = GpuDnnConv(num_groups=self.num_groups)(\n            img, kerns, empty_like(top), desc\n        )\n        d_alpha = grad_not_implemented(self, 4, alpha)\n        d_beta = grad_not_implemented(self, 5, beta)\n\n        return (\n            d_kerns * alpha,\n            d_top * alpha,\n            img * beta,\n            DisconnectedType()(),\n            d_alpha,\n            d_beta,\n        )\n\n    def connection_pattern(self, node):\n        # not connected to desc\n        return [[1], [1], [1], [0], [1], [1]]\n\n    def make_node(self, kern, topgrad, output, desc, alpha=None, beta=None):\n        ctx_name = infer_context_name(kern, topgrad, output)\n        kern = as_gpuarray_variable(kern, ctx_name)\n        topgrad = as_gpuarray_variable(topgrad, ctx_name)\n        output = as_gpuarray_variable(output, ctx_name)\n        if kern.type.ndim not in (4, 5):\n            raise TypeError(\"kern must be 4D or 5D tensor\")\n        if topgrad.type.ndim not in (4, 5):\n            raise TypeError(\"topgrad must be 4D or 5D tensor\")\n        if output.type.ndim not in (4, 5):\n            raise TypeError(\"output must be 4D or 5D tensor\")\n\n        if kern.type.ndim != topgrad.type.ndim or kern.type.ndim != output.type.ndim:\n            raise TypeError(\n                \"The number of dimensions of \" \"kern, topgrad and output must match\"\n            )\n\n        if kern.type.ndim == 5 and self.algo not in (\n            cudnn.conv3d_bwd_data_algorithms + SUPPORTED_DNN_CONV_ALGO_RUNTIME\n        ):\n            raise ValueError(\n                \"convolution algo %s can't be used for \" \"3d convolutions\", (self.algo,)\n            )\n\n        if (\n            not isinstance(desc.type, CDataType)\n            or desc.type.ctype != \"cudnnConvolutionDescriptor_t\"\n        ):\n            raise TypeError(\"desc must be cudnnConvolutionDescriptor_t\")\n\n        alpha = ensure_dt(alpha, _one, \"alpha\", kern.dtype)\n        beta = ensure_dt(beta, _zero, \"beta\", kern.dtype)\n\n        return Apply(self, [kern, topgrad, output, desc, alpha, beta], [output.type()])\n\n    def infer_shape(self, node, shape):\n        return [shape[2]]\n\n\n# These internal implementations for dnn_conv, dnn_gradweight and dnn_gradinput\n# support alpha, beta and out as parameters. Public interfaces follow without\n# underscore prefix.\n\n\ndef _dnn_conv(\n    img,\n    kerns,\n    alpha=1,\n    beta=0,\n    out=None,\n    border_mode=\"valid\",\n    subsample=(1, 1),\n    dilation=(1, 1),\n    conv_mode=\"conv\",\n    algo=None,\n    precision=None,\n    num_groups=1,\n):\n    ctx_name = infer_context_name(img, kerns)\n\n    img = as_gpuarray_variable(img, ctx_name)\n    kerns = as_gpuarray_variable(kerns, ctx_name)\n\n    precision, dt = get_precision(precision, [img, kerns])\n\n    img = gpu_contiguous(img.astype(dt))\n    kerns = gpu_contiguous(kerns.astype(dt))\n\n    desc = GpuDnnConvDesc(\n        border_mode=border_mode,\n        subsample=subsample,\n        dilation=dilation,\n        conv_mode=conv_mode,\n        precision=precision,\n        num_groups=num_groups,\n    )(kerns.shape)\n    desc_op = desc.owner.op\n    # We can use Shape_i and bypass the infer_shape here as this is on\n    # the input of node and it will always be present.\n    ishape = [shape_i_op(i)(img) for i in range(img.ndim)]\n    kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]\n    out_shp = get_conv_output_shape(\n        ishape, kshape, desc_op.border_mode, desc_op.subsample, filter_dilation=dilation\n    )\n    out_shp = assert_conv_shape(out_shp)\n    if beta == 0:\n        real_out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)\n    else:\n        assert out is not None\n        out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))\n        check = Assert(\n            \"GpuDnnConv: given output (for beta not null) does not have expected shape\"\n        )\n        real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, out_shp)))\n    return GpuDnnConv(algo=algo, num_groups=num_groups)(\n        img, kerns, real_out, desc, alpha, beta\n    )\n\n\ndef _dnn_gradweight(\n    img,\n    topgrad,\n    kerns_shp,\n    alpha=1,\n    beta=0,\n    out=None,\n    border_mode=\"valid\",\n    subsample=(1, 1),\n    dilation=(1, 1),\n    conv_mode=\"conv\",\n    algo=None,\n    precision=None,\n    num_groups=1,\n):\n    ctx_name = infer_context_name(img, topgrad)\n\n    img = as_gpuarray_variable(img, ctx_name)\n    topgrad = as_gpuarray_variable(topgrad, ctx_name)\n    kerns_shp = theano.tensor.as_tensor_variable(kerns_shp)\n\n    precision, dt = get_precision(precision, [img, topgrad], for_grad=True)\n\n    img = gpu_contiguous(img.astype(dt))\n    topgrad = gpu_contiguous(topgrad.astype(dt))\n\n    desc = GpuDnnConvDesc(\n        border_mode=border_mode,\n        subsample=subsample,\n        dilation=dilation,\n        conv_mode=conv_mode,\n        precision=precision,\n        num_groups=num_groups,\n    )(kerns_shp)\n    if beta == 0:\n        real_out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*kerns_shp)\n    else:\n        assert out is not None\n        out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))\n        check = Assert(\n            \"GpuDnnConvGradW: given output (for beta not null) does not have expected shape\"\n        )\n        real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, kerns_shp)))\n    return GpuDnnConvGradW(algo=algo, num_groups=num_groups)(\n        img, topgrad, real_out, desc, alpha, beta\n    )\n\n\ndef _dnn_gradinput(\n    kerns,\n    topgrad,\n    img_shp,\n    alpha=1,\n    beta=0,\n    out=None,\n    border_mode=\"valid\",\n    subsample=(1, 1),\n    dilation=(1, 1),\n    conv_mode=\"conv\",\n    algo=None,\n    precision=None,\n    num_groups=1,\n):\n    ctx_name = infer_context_name(kerns, topgrad)\n\n    kerns = as_gpuarray_variable(kerns, ctx_name)\n    topgrad = as_gpuarray_variable(topgrad, ctx_name)\n    img_shp = theano.tensor.as_tensor_variable(img_shp)\n\n    precision, dt = get_precision(precision, [kerns, topgrad], for_grad=True)\n\n    kerns = gpu_contiguous(kerns.astype(dt))\n    topgrad = gpu_contiguous(topgrad.astype(dt))\n\n    desc = GpuDnnConvDesc(\n        border_mode=border_mode,\n        subsample=subsample,\n        dilation=dilation,\n        conv_mode=conv_mode,\n        precision=precision,\n        num_groups=num_groups,\n    )(kerns.shape)\n    if beta == 0:\n        real_out = GpuAllocEmpty(dtype=kerns.dtype, context_name=ctx_name)(*img_shp)\n    else:\n        assert out is not None\n        out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))\n        check = Assert(\n            \"GpuDnnConvGradI: given output (for beta not null) does not have expected shape\"\n        )\n        real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, img_shp)))\n    return GpuDnnConvGradI(algo=algo, num_groups=num_groups)(\n        kerns, topgrad, real_out, desc, alpha, beta\n    )\n\n\ndef dnn_conv(\n    img,\n    kerns,\n    border_mode=\"valid\",\n    subsample=(1, 1),\n    dilation=(1, 1),\n    conv_mode=\"conv\",\n    direction_hint=None,\n    workmem=None,\n    algo=None,\n    precision=None,\n    num_groups=1,\n):\n    \"\"\"\n    GPU convolution using cuDNN from NVIDIA.\n\n    The memory layout to use is 'bc01', that is 'batch', 'channel',\n    'first dim', 'second dim' in that order.\n\n    Parameters\n    ----------\n    img\n        Images to do the convolution over.\n    kerns\n        Convolution filters.\n    border_mode\n        One of 'valid', 'full', 'half'; additionally, the padding size\n        could be directly specified by an integer or a pair of integers.\n    subsample\n        Perform subsampling of the output (default: (1, 1)).\n    dilation\n        Filter dilation factor. A dilation factor of d is equivalent to a\n        convolution with d - 1 zeros inserted between neighboring filter\n        values.\n    conv_mode\n        Perform convolution (kernels flipped) or cross-correlation.\n        One of 'conv', 'cross' (default: 'conv').\n    direction_hint\n        Used by graph optimizers to change algorithm choice.\n        By default, GpuDnnConv will be used to carry out the convolution.\n        If border_mode is 'valid', subsample is (1, 1), dilation is (1, 1), and\n        direction_hint is 'bprop weights', it will use GpuDnnConvGradW.\n        If border_mode is 'full', subsample is (1, 1), dilation is (1, 1), and\n        direction_hint is *not* 'forward!', it will use GpuDnnConvGradI.\n        This parameter is used internally by graph optimizers and may be\n        removed at any time without a deprecation period. You have been warned.\n    algo : {'none', 'small', 'large', 'fft', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change'}\n        Convolution implementation to use. Some of its values may\n        require certain versions of cuDNN to be installed. Default is\n        the value of :attr:`config.dnn.conv.algo_fwd`.\n    precision : {'as_input_f32', 'as_input', 'float16', 'float32', 'float64'}\n        Description of the dtype in which the computation of the convolution\n        should be done. Possible values are 'as_input', 'float16', 'float32'\n        and 'float64'. Default is the value of\n        :attr:`config.dnn.conv.precision`.\n    num_groups :\n        Divides the image, kernel and output tensors into num_groups\n        separate groups. Each which carry out convolutions separately\n\n\n    .. warning:: The cuDNN library only works with GPUs that have a compute\n        capability of 3.0 or higher. This means that older GPUs will not\n        work with this Op.\n\n    \"\"\"\n\n    if workmem is not None:\n        if algo is not None:\n            raise ValueError(\"You can't use both algo and workmem\")\n        warnings.warn(\"workmem is deprecated, use algo instead\", stacklevel=2)\n        algo = workmem\n    fgraph = getattr(img, \"fgraph\", None) or getattr(kerns, \"fgraph\", None)\n    ctx_name = infer_context_name(img, kerns)\n    if (\n        border_mode == \"valid\"\n        and subsample == (1, 1)\n        and dilation == (1, 1)\n        and direction_hint == \"bprop weights\"\n        and num_groups == 1\n    ):\n        # Special case: We are asked to use GpuDnnConvGradW. We need to set\n        # up a suitable 'fake' convolution to compute the gradient for.\n        img = gpu_contiguous(img.dimshuffle(1, 0, 2, 3))\n        if conv_mode == \"conv\":\n            # We need to flip manually. These 'kerns' are not the kernels\n            # that would be flipped by conv_mode='conv' in GpuDnnConvGradW.\n            kerns = kerns[:, :, ::-1, ::-1]\n        kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3))\n        out_shp = (\n            shape_i(kerns, 1, fgraph),\n            shape_i(img, 1, fgraph),\n            shape_i(img, 2, fgraph) - shape_i(kerns, 2, fgraph) + 1,\n            shape_i(img, 3, fgraph) - shape_i(kerns, 3, fgraph) + 1,\n        )\n        out_shp = assert_conv_shape(out_shp)\n        out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)\n        precision, _ = get_precision(precision, [img, kerns], for_grad=True)\n        desc = GpuDnnConvDesc(\n            border_mode=\"valid\",\n            subsample=(1, 1),\n            dilation=(1, 1),\n            num_groups=num_groups,\n            conv_mode=\"cross\",\n            precision=precision,\n        )(out.shape)\n        conv = GpuDnnConvGradW(num_groups=num_groups)(img, kerns, out, desc)\n        return as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3), ctx_name)\n\n    elif (\n        border_mode == \"full\"\n        and subsample == (1, 1)\n        and direction_hint != \"forward!\"\n        and num_groups == 1\n    ):\n        # Special case: We can be faster by using GpuDnnConvGradI to compute\n        # the full convolution as the backward pass of a valid convolution.\n        # We just need to set up a suitable 'fake' valid convolution.\n        img = gpu_contiguous(img)  # cudnn v2 rc3 need contiguous data\n        kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3))\n        conv_mode = \"cross\" if conv_mode == \"conv\" else \"conv\"\n        out_shp = (\n            shape_i(img, 0, fgraph),\n            shape_i(kerns, 1, fgraph),\n            shape_i(img, 2, fgraph) + (shape_i(kerns, 2, fgraph) - 1) * dilation[0],\n            shape_i(img, 3, fgraph) + (shape_i(kerns, 3, fgraph) - 1) * dilation[1],\n        )\n        out_shp = assert_conv_shape(out_shp)\n        out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)\n        precision, _ = get_precision(precision, [img, kerns], for_grad=True)\n        desc = GpuDnnConvDesc(\n            border_mode=\"valid\",\n            subsample=(1, 1),\n            dilation=dilation,\n            num_groups=num_groups,\n            conv_mode=conv_mode,\n            precision=precision,\n        )(kerns.shape)\n        return GpuDnnConvGradI(num_groups=num_groups)(kerns, img, out, desc)\n\n    # Standard case: We use GpuDnnConv with suitable padding.\n    return _dnn_conv(\n        img,\n        kerns,\n        algo=algo,\n        border_mode=border_mode,\n        subsample=subsample,\n        dilation=dilation,\n        conv_mode=conv_mode,\n        precision=precision,\n        num_groups=num_groups,\n    )\n\n\ndef dnn_conv3d(\n    img,\n    kerns,\n    border_mode=\"valid\",\n    subsample=(1, 1, 1),\n    dilation=(1, 1, 1),\n    conv_mode=\"conv\",\n    direction_hint=None,\n    algo=None,\n    precision=None,\n    num_groups=1,\n):\n    \"\"\"\n    GPU convolution using cuDNN from NVIDIA.\n\n    The memory layout to use is 'bc012', that is 'batch', 'channel',\n    'first dim', 'second dim', 'third dim' in that order.\n\n    Parameters\n    ----------\n    img\n        Images to do the convolution over.\n    kerns\n        Convolution filters.\n    border_mode\n        One of 'valid', 'full', 'half'; additionally, the padding size\n        could be directly specified by an integer or a pair of integers.\n    subsample\n        Perform subsampling of the output (default: (1, 1, 1)).\n    dilation\n        Filter dilation factor. A dilation factor of d is equivalent to a\n        convolution with d - 1 zeros inserted between neighboring filter\n        values.\n    conv_mode\n        Perform convolution (kernels flipped) or cross-correlation.\n        One of 'conv', 'cross' (default: 'conv').\n    direction_hint\n        Used by graph optimizers to change algorithm choice.\n        By default, GpuDnnConv will be used to carry out the convolution.\n        If border_mode is 'valid', subsample is (1, 1, 1), dilation is\n        (1, 1, 1), and direction_hint is 'bprop weights', it will use\n        GpuDnnConvGradW.\n        If border_mode is 'full', subsample is (1, 1, 1), dilation is\n        (1, 1, 1), and direction_hint is *not* 'forward!', it will use\n        GpuDnnConvGradI.\n        This parameter is used internally by graph optimizers and may be\n        removed at any time without a deprecation period. You have been warned.\n    algo : convolution implementation to use. Only 'none' is implemented\n        for the conv3d. Default is the value of :attr:`config.dnn.conv.algo_fwd`.\n    precision : {'as_input_f32', 'as_input', 'float16', 'float32', 'float64'}\n        Description of the dtype in which the computation of the convolution\n        should be done. Possible values are 'as_input', 'float16', 'float32'\n        and 'float64'. Default is the value of\n        :attr:`config.dnn.conv.precision`.\n    num_groups :\n        Divides the image, kernel and output tensors into num_groups\n        separate groups. Each which carry out convolutions separately\n\n\n    .. warning:: The cuDNN library only works with GPUs that have a compute\n        capability of 3.0 or higher. This means that older GPUs will not\n        work with this Op.\n\n    \"\"\"\n\n    fgraph = getattr(img, \"fgraph\", None) or getattr(kerns, \"fgraph\", None)\n    ctx_name = infer_context_name(img, kerns)\n    if (\n        border_mode == \"valid\"\n        and subsample == (1, 1, 1)\n        and dilation == (1, 1, 1)\n        and direction_hint == \"bprop weights\"\n        and num_groups == 1\n    ):\n        # Special case: We are asked to use GpuDnnConvGradW. We need to set\n        # up a suitable 'fake' convolution to compute the gradient for.\n        img = gpu_contiguous(img.dimshuffle(1, 0, 2, 3, 4))\n        if conv_mode == \"conv\":\n            # We need to flip manually. These 'kerns' are not the kernels\n            # that would be flipped by conv_mode='conv' in GpuDnnConvGradW.\n            kerns = kerns[:, :, ::-1, ::-1, ::-1]\n        kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3, 4))\n        out_shp = (\n            shape_i(kerns, 1, fgraph),\n            shape_i(img, 1, fgraph),\n            shape_i(img, 2, fgraph) - shape_i(kerns, 2, fgraph) + 1,\n            shape_i(img, 3, fgraph) - shape_i(kerns, 3, fgraph) + 1,\n            shape_i(img, 4, fgraph) - shape_i(kerns, 4, fgraph) + 1,\n        )\n        out_shp = assert_conv_shape(out_shp)\n        out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)\n        precision, _ = get_precision(precision, [img, kerns], for_grad=True)\n        desc = GpuDnnConvDesc(\n            border_mode=\"valid\",\n            subsample=(1, 1, 1),\n            dilation=(1, 1, 1),\n            num_groups=num_groups,\n            conv_mode=\"cross\",\n            precision=precision,\n        )(out.shape)\n        conv = GpuDnnConvGradW(num_groups=num_groups)(img, kerns, out, desc)\n        return as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3, 4), ctx_name)\n\n    elif (\n        border_mode == \"full\"\n        and subsample == (1, 1, 1)\n        and direction_hint != \"forward!\"\n        and num_groups == 1\n    ):\n        # Special case: We can be faster by using GpuDnnConvGradI to compute\n        # the full convolution as the backward pass of a valid convolution.\n        # We just need to set up a suitable 'fake' valid convolution.\n        img = gpu_contiguous(img)  # cudnn v2 rc3 need contiguous data\n        kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3, 4))\n        conv_mode = \"cross\" if conv_mode == \"conv\" else \"conv\"\n        out_shp = (\n            shape_i(img, 0, fgraph),\n            shape_i(kerns, 1, fgraph),\n            shape_i(img, 2, fgraph) + (shape_i(kerns, 2, fgraph) - 1) * dilation[0],\n            shape_i(img, 3, fgraph) + (shape_i(kerns, 3, fgraph) - 1) * dilation[1],\n            shape_i(img, 4, fgraph) + (shape_i(kerns, 4, fgraph) - 1) * dilation[2],\n        )\n        out_shp = assert_conv_shape(out_shp)\n        out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)\n        precision, _ = get_precision(precision, [img, kerns], for_grad=True)\n        desc = GpuDnnConvDesc(\n            border_mode=\"valid\",\n            subsample=(1, 1, 1),\n            dilation=dilation,\n            num_groups=num_groups,\n            conv_mode=conv_mode,\n            precision=precision,\n        )(kerns.shape)\n        return GpuDnnConvGradI(num_groups=num_groups)(kerns, img, out, desc)\n\n    # Standard case: We use GpuDnnConv with suitable padding.\n    return _dnn_conv(\n        img,\n        kerns,\n        algo=algo,\n        border_mode=border_mode,\n        subsample=subsample,\n        dilation=dilation,\n        conv_mode=conv_mode,\n        precision=precision,\n        num_groups=num_groups,\n    )\n\n\ndef dnn_gradweight(\n    img,\n    topgrad,\n    kerns_shp,\n    border_mode=\"valid\",\n    subsample=(1, 1),\n    dilation=(1, 1),\n    conv_mode=\"conv\",\n    precision=None,\n    algo=None,\n    num_groups=1,\n):\n    \"\"\"\n    TODO: document this\n    \"\"\"\n    return _dnn_gradweight(\n        img,\n        topgrad,\n        kerns_shp,\n        border_mode=border_mode,\n        subsample=subsample,\n        dilation=dilation,\n        conv_mode=conv_mode,\n        algo=algo,\n        precision=precision,\n        num_groups=num_groups,\n    )\n\n\ndef dnn_gradweight3d(\n    img,\n    topgrad,\n    kerns_shp,\n    border_mode=\"valid\",\n    subsample=(1, 1, 1),\n    dilation=(1, 1, 1),\n    conv_mode=\"conv\",\n    precision=None,\n    algo=None,\n    num_groups=1,\n):\n    \"\"\"\n    3d version of dnn_gradweight\n    \"\"\"\n    return dnn_gradweight(\n        img,\n        topgrad,\n        kerns_shp,\n        border_mode,\n        subsample,\n        dilation,\n        conv_mode,\n        precision,\n        algo,\n        num_groups,\n    )\n\n\ndef dnn_gradinput(\n    kerns,\n    topgrad,\n    img_shp,\n    border_mode=\"valid\",\n    subsample=(1, 1),\n    dilation=(1, 1),\n    conv_mode=\"conv\",\n    precision=None,\n    algo=None,\n    num_groups=1,\n):\n    \"\"\"\n    TODO: document this\n    \"\"\"\n    return _dnn_gradinput(\n        kerns,\n        topgrad,\n        img_shp,\n        border_mode=border_mode,\n        subsample=subsample,\n        dilation=dilation,\n        conv_mode=conv_mode,\n        algo=algo,\n        precision=precision,\n        num_groups=num_groups,\n    )\n\n\ndef dnn_gradinput3d(\n    kerns,\n    topgrad,\n    img_shp,\n    border_mode=\"valid\",\n    subsample=(1, 1, 1),\n    dilation=(1, 1, 1),\n    conv_mode=\"conv\",\n    precision=None,\n    algo=None,\n    num_groups=1,\n):\n    \"\"\"\n    3d version of `dnn_gradinput`.\n    \"\"\"\n    return dnn_gradinput(\n        kerns,\n        topgrad,\n        img_shp,\n        border_mode,\n        subsample,\n        dilation,\n        conv_mode,\n        precision,\n        algo,\n        num_groups,\n    )\n\n\nclass GpuDnnPoolDesc(Op):\n\n    \"\"\"\n    This Op builds a pooling descriptor for use in the other\n    pooling operations.\n\n    `ws`, `stride` and `pad` must have the same length.\n\n    Parameters\n    ----------\n    ws : tuple\n        Window size.\n    stride : tuple\n        (dx, dy) or (dx, dy, dz).\n    mode : {'max', 'average_inc_pad', 'average_exc_pad'}\n        The old deprecated name 'average' corresponds to 'average_inc_pad'.\n    pad : tuple\n        (padX, padY) or (padX, padY, padZ)\n\n    Notes\n    -----\n    Not used anymore. Only needed to reload old pickled files.\n    \"\"\"\n\n    __props__ = (\"ws\", \"stride\", \"mode\", \"pad\")\n\n    def c_headers(self):\n        return [\"cudnn.h\", \"cudnn_helper.h\"]\n\n    def c_header_dirs(self):\n        return [gpuarray_helper_inc_dir(), config.dnn.include_path]\n\n    def c_libraries(self):\n        return [\"cudnn\"]\n\n    def c_lib_dirs(self):\n        return [config.dnn.library_path]\n\n    def do_constant_folding(self, node):\n        return False\n\n    def __init__(self, ws=(1, 1), stride=(1, 1), mode=\"max\", pad=(0, 0)):\n        if mode == \"average\":\n            mode = \"average_inc_pad\"\n        assert mode in (\"max\", \"average_inc_pad\", \"average_exc_pad\")\n        self.mode = mode\n\n        assert len(ws) == len(stride) and len(stride) == len(pad)\n        assert len(ws) in (2, 3)\n        self.ws = ws\n        self.stride = stride\n        self.pad = pad\n\n    def get_ndim(self):\n        return len(self.ws)\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        if not hasattr(self, \"pad\"):\n            self.pad = (0, 0)\n\n    def make_node(self):\n        node = Apply(\n            self,\n            [],\n            [\n                CUDNNDataType(\n                    \"cudnnPoolingDescriptor_t\", freefunc=\"cudnnDestroyPoolingDescriptor\"\n                )()\n            ],\n        )\n        # DebugMode cannot compare the values of CDataType variables, so by\n        # default it returns False all the time. To prevent DebugMode from\n        # complaining because of the MergeOptimizer, we make this variable\n        # always compare to True.\n        out = node.outputs[0]\n        out.tag.values_eq_approx = tensor.type.values_eq_approx_always_true\n        return node\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        (desc,) = outputs\n\n        if self.mode == \"max\":\n            mode_flag = \"CUDNN_POOLING_MAX\"\n        elif self.mode == \"average_inc_pad\":\n            mode_flag = \"CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING\"\n        elif self.mode == \"average_exc_pad\":\n            mode_flag = \"CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING\"\n        else:\n            raise NotImplementedError(\"Unsupported pooling model.\")\n\n        return \"\"\"\n{\n  cudnnStatus_t err;\n\n  if ((err = cudnnCreatePoolingDescriptor(&%(desc)s)) != CUDNN_STATUS_SUCCESS) {\n    PyErr_Format(PyExc_MemoryError, \"could not allocate pooling \"\n                 \"descriptor: %%s\", cudnnGetErrorString(err));\n    %(fail)s\n  }\n\n  static const int win[%(nd)d] = {%(win)s};\n  static const int pad[%(nd)d] = {%(pad)s};\n  static const int str[%(nd)d] = {%(str)s};\n\n    err = cudnnSetPoolingNdDescriptor(%(desc)s, %(mode_flag)s, CUDNN_PROPAGATE_NAN, %(nd)d, win, pad, str);\n\n  if (err != CUDNN_STATUS_SUCCESS) {\n    PyErr_Format(PyExc_RuntimeError, \"could not set op descriptor: %%s\",\n                 cudnnGetErrorString(err));\n    %(fail)s\n  }\n}\n\"\"\" % dict(\n            name=name,\n            desc=desc,\n            mode_flag=mode_flag,\n            fail=sub[\"fail\"],\n            nd=self.get_ndim(),\n            win=\", \".join(map(str, self.ws)),\n            pad=\", \".join(map(str, self.pad)),\n            str=\", \".join(map(str, self.stride)),\n        )\n\n    def c_code_cache_version(self):\n        return (4, version())\n\n\nclass GpuDnnPoolBase(DnnBase):\n\n    \"\"\"\n    Abstract base class for GpuDnnPool and GpuDnnPoolGrad.\n\n    \"\"\"\n\n    # c_file and c_function must be defined in sub-classes.\n    c_file = None\n    c_function = None\n\n    _f16_ok = True\n    __props__ = (\"mode\",)\n    check_input = False\n    params_type = ParamsType(mode=cudnn.cudnnPoolingMode_t, handle=handle_type)\n\n    def __init__(self, mode=\"max\"):\n        DnnBase.__init__(self, [self.c_file], self.c_function)\n        if mode == \"average\":\n            mode = \"average_inc_pad\"\n        # Supported modes depend on runtime cuDNN version.\n        assert cudnn.cudnnPoolingMode_t.has_alias(mode)\n        self.mode = mode\n\n\nclass GpuDnnPool(GpuDnnPoolBase):\n\n    \"\"\"\n    Parameters\n    ----------\n    img : tensor\n        The image 4d or 5d tensor.\n    ws : tensor\n        Window size.\n    stride : tensor\n        (dx, dy) or (dx, dy, dz).\n    mode : {'max', 'average_inc_pad', 'average_exc_pad'}\n        The old deprecated name 'average' corresponds to 'average_inc_pad'.\n    pad : tensor\n        (padX, padY) or (padX, padY, padZ)\n\n    \"\"\"\n\n    c_file = \"c_code\/dnn_pool.c\"\n    c_function = \"APPLY_SPECIFIC(dnn_pool)\"\n\n    def make_node(self, img, ws, stride, pad):\n        ctx_name = infer_context_name(img)\n        img = as_gpuarray_variable(img, ctx_name)\n\n        ws = tensor.as_tensor_variable(ws)\n        stride = tensor.as_tensor_variable(stride)\n        pad = tensor.as_tensor_variable(pad)\n        assert ws.type.ndim == stride.type.ndim and ws.type.ndim == pad.type.ndim\n        assert ws.type.ndim == 1\n\n        return Apply(self, [img, ws, stride, pad], [img.type()])\n\n    def infer_shape(self, node, shape):\n        w = node.inputs[1]\n        s = node.inputs[2]\n        p = node.inputs[3]\n\n        res = [\n            shape[0][0],\n            shape[0][1],\n            (shape[0][2] + 2 * p[0] - w[0]) \/\/ s[0] + 1,\n            (shape[0][3] + 2 * p[1] - w[1]) \/\/ s[1] + 1,\n        ]\n        if node.inputs[0].ndim == 5:\n            res.append((shape[0][4] + 2 * p[2] - w[2]) \/\/ s[2] + 1)\n        return [res]\n\n    def L_op(self, inp, outputs, grads):\n        img, ws, stride, pad = inp\n        (grad,) = grads\n\n        grad = gpu_contiguous(grad)\n\n        (out,) = outputs\n\n        g_out = GpuDnnPoolGrad(mode=self.mode)(img, out, grad, ws, stride, pad)\n\n        return (\n            g_out,\n            theano.gradient.DisconnectedType()(),\n            theano.gradient.DisconnectedType()(),\n            theano.gradient.DisconnectedType()(),\n        )\n\n    def connection_pattern(self, node):\n        # not connected to parameters\n        return [[1], [0], [0], [0]]\n\n\nclass GpuDnnPoolGrad(GpuDnnPoolBase):\n\n    \"\"\"\n    The pooling gradient.\n\n    Parameters\n    ----------\n    inp\n        The input of the pooling.\n    out\n        The output of the pooling in the forward.\n    out_grad\n        Same size as out, but is the corresponding gradient information.\n    ws : tensor variable\n        Window size.\n    stride : tensor variable\n        (dx, dy) or (dx, dy, dz).\n    mode : {'max', 'average_inc_pad', 'average_exc_pad'}\n        The old deprecated name 'average' corresponds to 'average_inc_pad'.\n    pad : tensor\n        (padX, padY) or (padX, padY, padZ)\n\n    \"\"\"\n\n    c_file = \"c_code\/dnn_pool_grad.c\"\n    c_function = \"APPLY_SPECIFIC(dnn_pool_grad)\"\n\n    def make_node(self, inp, out, out_grad, ws, stride, pad):\n        ctx_name = infer_context_name(inp, out, out_grad)\n        inp = as_gpuarray_variable(inp, ctx_name)\n        assert inp.ndim in [4, 5]\n        out_grad = as_gpuarray_variable(out_grad, ctx_name)\n        assert out_grad.ndim in [4, 5]\n        out = as_gpuarray_variable(out, ctx_name)\n        assert out.ndim in [4, 5]\n\n        assert out_grad.ndim == inp.ndim\n        assert inp.ndim == out.ndim\n\n        ws = tensor.as_tensor_variable(ws)\n        stride = tensor.as_tensor_variable(stride)\n        pad = tensor.as_tensor_variable(pad)\n        assert ws.type.ndim == stride.type.ndim and ws.type.ndim == pad.type.ndim\n        assert ws.type.ndim == 1\n\n        return Apply(self, [inp, out, out_grad, ws, stride, pad], [inp.type()])\n\n    def infer_shape(self, node, shape):\n        return [shape[0]]\n\n\ndef dnn_pool(img, ws, stride=None, mode=\"max\", pad=None):\n    \"\"\"\n    GPU pooling using cuDNN from NVIDIA.\n\n    The memory layout to use is 'bc01', that is 'batch', 'channel',\n    'first dim', 'second dim' in that order.\n\n    `ws`, `stride` and `pad` must have the same length.\n\n    Parameters\n    ----------\n    img\n        Images to do the pooling over.\n    ws : tuple\n        Subsampling window size.  Should have 2 or 3 elements.\n    stride : tuple\n        Subsampling stride (default: (1, 1) or (1, 1, 1)).\n    mode : {'max', 'average_inc_pad', 'average_exc_pad', 'sum', 'max_deterministic'}\n        **NB**: 'max_deterministic' is supported since cuDNN v6.\n    pad : tuple\n        (padX, padY) or (padX, padY, padZ)\n        default: (0, 0) or (0, 0, 0)\n\n\n    .. warning:: The cuDNN library only works with GPU that have a compute\n        capability of 3.0 or higher.  This means that older GPU will not\n        work with this Op.\n\n    Notes\n    -----\n    This Op implements the ignore_border=True of max_pool_2d.\n\n    \"\"\"\n    img = gpu_contiguous(img)\n    if stride is None:\n        stride = (1,) * len(ws)\n    if pad is None:\n        pad = (0,) * len(ws)\n    if mode == \"sum\":\n        ret = GpuDnnPool(mode=\"average_inc_pad\")(img, ws, stride, pad)\n        context_name = ret.type.context_name\n        window_elem = theano.tensor.prod(ws).astype(ret.dtype)\n        return as_gpuarray_variable(ret * window_elem, context_name)\n    return GpuDnnPool(mode=mode)(img, ws, stride, pad)\n\n\nclass GpuDnnSoftmaxBase(DnnBase):\n\n    \"\"\"\n    Op for the cuDNN Softmax.\n\n    Parameters\n    ----------\n    algo : {'fast', 'accurate', 'log'}\n        Indicating whether, respectively, computations should be optimized for\n        speed, for accuracy, or if cuDNN should rather compute the log-softmax instead.\n    mode : {'instance', 'channel'}\n        Indicating whether the softmax should be computed per image across 'c01'\n        or per spatial location '01' per image across 'c'.\n\n    \"\"\"\n\n    __props__ = (\"mode\", \"algo\")\n    # Neither inputs nor output types properties are used\n    # neither in dnn_base.c nor in dnn_softmax*.c,\n    # so we can disable input checking.\n    check_input = False\n    params_type = ParamsType(\n        algo=cudnn.cudnnSoftmaxAlgorithm_t,\n        mode=cudnn.cudnnSoftmaxMode_t,\n        handle=handle_type,\n    )\n\n    def __init__(self, algo, mode):\n        DnnBase.__init__(self, [self.file], self.c_func)\n\n        assert cudnn.cudnnSoftmaxAlgorithm_t.has_alias(algo)\n        self.algo = algo\n\n        assert cudnn.cudnnSoftmaxMode_t.has_alias(mode)\n        self.mode = mode\n\n    def infer_shape(self, node, shape):\n        if self.direction == \"forward\":\n            return [shape[0]]\n        else:\n            return [shape[1]]\n\n\nclass GpuDnnSoftmax(GpuDnnSoftmaxBase):\n\n    \"\"\"\n    Op for the cuDNN Softmax.\n\n    algo : {'fast', 'accurate', 'log'}\n        Indicating whether, respectively, computations should be optimized for\n        speed, for accuracy, or if cuDNN should rather compute the log-softmax instead.\n    mode : {'instance', 'channel'}\n        Indicating whether the softmax should be computed per image across 'c01'\n        or per spatial location '01' per image across 'c'.\n\n    \"\"\"\n\n    _f16_ok = True\n    direction = \"forward\"\n    file = \"c_code\/dnn_softmax.c\"\n    c_func = \"APPLY_SPECIFIC(softmax)\"\n\n    def make_node(self, x):\n        x = as_gpuarray_variable(x, infer_context_name(x))\n        assert x.ndim == 4\n        return Apply(self, [x], [x.type()])\n\n    def L_op(self, inp, outputs, grads):\n        (x,) = inp\n        (g_sm,) = grads\n        (sm,) = outputs\n        return [GpuDnnSoftmaxGrad(self.algo, self.mode)(g_sm, sm)]\n\n\nclass GpuDnnSoftmaxGrad(GpuDnnSoftmaxBase):\n\n    \"\"\"\n    Op for the cuDNN SoftmaxGrad.\n\n    Parameters\n    ----------\n    algo\n        'fast', 'accurate' or 'log' indicating whether, respectively,\n        computations should be optimized for speed, for accuracy, or if cuDNN\n        should rather compute the gradient of the log-softmax instead.\n    mode\n        'instance' or 'channel' indicating whether the softmax should\n        be computed per image across 'c01' or per spatial location '01' per\n        image across 'c'.\n\n    \"\"\"\n\n    _f16_ok = True\n    direction = \"backward\"\n    file = \"c_code\/dnn_softmax_grad.c\"\n    c_func = \"APPLY_SPECIFIC(softmax_grad)\"\n\n    def make_node(self, dy, sm):\n        ctx_name = infer_context_name(dy, sm)\n        dy = as_gpuarray_variable(dy, ctx_name)\n        sm = as_gpuarray_variable(sm, ctx_name)\n        assert dy.ndim == 4\n        assert sm.ndim == 4\n        return Apply(self, [dy, sm], [sm.type()])\n\n\nclass GpuDnnReduction(DnnBase):\n    check_input = False\n    _f16_ok = True\n    _cop_num_outputs = 2\n\n    __props__ = (\"red_op\", \"axis\", \"acc_dtype\", \"dtype\", \"return_indices\")\n\n    params_type = ParamsType(\n        red_op=cudnn.cudnnReduceTensorOp_t,\n        acc_dtype=cudnn.cudnnDataType_t,\n        c_axis=uint32_t,\n        handle=handle_type,\n    )\n\n    def __init__(self, red_op, axis, acc_dtype, dtype, return_indices):\n        DnnBase.__init__(self, [\"c_code\/dnn_redux.c\"], \"APPLY_SPECIFIC(dnn_redux)\")\n        assert cudnn.cudnnReduceTensorOp_t.has_alias(red_op)\n        self.red_op = red_op\n        assert acc_dtype in [\"float16\", \"float32\", \"float64\"]\n        self.acc_dtype = acc_dtype\n        assert dtype in [\"float16\", \"float32\", \"float64\"]\n        self.dtype = dtype\n        # 8 is the current limit for cudnn\n        if axis is not None:\n            if len(axis) > 8:\n                raise ValueError(\"Too many axes to reduce on\")\n            if any(a >= 8 for a in axis):\n                raise ValueError(\"Axes larger than 8 not supported\")\n            axis = tuple(axis)\n        # c_axis is a bitfield (1 to reduce)\n        self.c_axis = self._convert_axis(axis)\n        # axis is a list of axes to reduce on\n        self.axis = axis\n        if return_indices and (red_op != \"maximum\" and red_op != \"minimum\"):\n            raise ValueError(\n                \"Can't request indices for something other than\" \" minimum or maximum\"\n            )\n        self.return_indices = return_indices\n\n    def _convert_axis(self, axis):\n        if axis is None:\n            return np.uint32(-1)\n        else:\n            return reduce(lambda a, b: a | b, map(lambda a: 1 << a, axis), 0)\n\n    def make_node(self, inp):\n        ctx_name = infer_context_name(inp)\n        inp = as_gpuarray_variable(inp, ctx_name)\n        inp = gpu_contiguous(inp)\n        if inp.ndim > 8:\n            raise ValueError(\"cuDNN reduction doesn't support nd > 8\")\n        assert inp.dtype in [\"float16\", \"float32\", \"float64\"]\n\n        # These restrictions where guessed from vague clues since\n        # there is no actual documentation on this\n        if inp.dtype == \"float64\":\n            assert self.acc_dtype == \"float64\"\n        if inp.dtype == \"float32\":\n            assert self.acc_dtype == \"float32\"\n        if inp.dtype == \"float16\":\n            assert self.acc_dtype != \"float64\"\n\n        bcast = []\n        for i in range(inp.ndim):\n            if not (self.c_axis & (1 << i)):\n                bcast.append(inp.broadcastable[i])\n        outs = [inp.type.clone(dtype=self.dtype, broadcastable=bcast)()]\n        if self.return_indices:\n            outs.append(\n                GpuArrayType(\n                    dtype=\"uint32\", broadcastable=bcast, context_name=ctx_name\n                )()\n            )\n\n        return Apply(self, [inp], outs)\n\n\nclass GpuDnnBatchNorm(DnnBase):\n    \"\"\"\n    Base Op for cuDNN Batch Normalization.\n\n    Parameters\n    ----------\n    mode : {'per-activation', 'spatial'}\n        Whether to normalize per activation (in this mode, bias and scale\n        tensor dimensions are 1xCxHxW) or share normalization factors across\n        spatial dimensions (in this mode, bias and scale tensor dimensions\n        are 1xCx1x1).\n    epsilon\n        Epsilon value used in the batch normalization formula. Minimum allowed\n        value is 1e-5 (imposed by cuDNN).\n    running_average_factor : float\n        Factor for updating the values or `running_mean` and `running_var`.\n        If the factor is close to one, the running averages will update quickly,\n        if the factor is close to zero it will update slowly.\n    running_mean : tensor or None\n        Previous value of the running mean. If this is given, the new value\n        ``running_mean * (1 - r_a_factor) + batch mean * r_a_factor``\n        will be returned as one of the outputs of this function.\n        `running_mean` and `running_var` should either both be given or\n        both be None.\n    running_var : tensor or None\n        Previous value of the running variance. If this is given, the new value\n        ``running_var * (1 - r_a_factor) + (m \/ (m - 1)) * batch var * r_a_factor``\n        will be returned as one of the outputs of this function,\n        where `m` is the product of lengths of the averaged-over dimensions.\n        `running_mean` and `running_var` should either both be given or\n        both be None.\n    \"\"\"\n\n    __props__ = (\n        \"mode\",\n        \"running_averages\",\n        \"inplace_running_mean\",\n        \"inplace_running_var\",\n        \"inplace_output\",\n    )\n    _cop_num_inputs = 7\n    _cop_num_outputs = 5\n    check_input = False\n    params_type = ParamsType(\n        mode=cudnn.cudnnBatchNormMode_t,\n        inplace_output=bool_t,\n        inplace_running_mean=bool_t,\n        inplace_running_var=bool_t,\n        handle=handle_type,\n    )\n\n    def __init__(\n        self,\n        mode=\"per-activation\",\n        running_averages=False,\n        inplace_running_mean=False,\n        inplace_running_var=False,\n        inplace_output=False,\n    ):\n        DnnBase.__init__(\n            self,\n            [\"c_code\/dnn_batchnorm_base.c\", \"c_code\/dnn_batchnorm.c\"],\n            \"dnn_batchnorm_op\",\n        )\n\n        assert cudnn.cudnnBatchNormMode_t.has_alias(mode)\n        self.mode = mode\n        self.running_averages = running_averages\n        self.inplace_output = inplace_output\n        self.inplace_running_mean = inplace_running_mean\n        self.inplace_running_var = inplace_running_var\n        self.destroy_map = {}\n        if self.inplace_output:\n            self.destroy_map[0] = [0]\n        if self.running_averages and self.inplace_running_mean:\n            self.destroy_map[3] = [5]\n        if self.running_averages and self.inplace_running_var:\n            self.destroy_map[4] = [6]\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        if not hasattr(self, \"running_average_factor\"):\n            self.running_average_factor = 0\n        if not hasattr(self, \"running_averages\"):\n            self.running_averages = False\n        if not (\n            hasattr(self, \"inplace_running_mean\")\n            and hasattr(self, \"inplace_running_var\")\n            and hasattr(self, \"inplace_output\")\n        ):\n            self.inplace_running_mean = False\n            self.inplace_running_var = False\n            self.inplace_output = False\n            self.destroy_map = {}\n\n    def infer_shape(self, node, shape):\n        return [shape[0]] + [shape[1]] * (len(node.outputs) - 1)\n\n    def make_node(\n        self,\n        x,\n        scale,\n        bias,\n        epsilon=1e-4,\n        running_average_factor=0.1,\n        running_mean=None,\n        running_var=None,\n    ):\n        assert x.ndim == scale.ndim == bias.ndim\n        assert x.ndim in (4, 5)\n        assert (\n            self.running_averages\n            == (running_mean is not None)\n            == (running_var is not None)\n        )\n        assert running_mean is None or running_mean.ndim == x.ndim\n        assert running_var is None or running_var.ndim == x.ndim\n        ctx_name = infer_context_name(x, scale, bias)\n        x = as_gpuarray_variable(x, ctx_name)\n        scale = as_gpuarray_variable(scale, ctx_name)\n        bias = as_gpuarray_variable(bias, ctx_name)\n        epsilon = as_scalar(epsilon).astype(\"float64\")\n        running_average_factor = as_scalar(running_average_factor).astype(\"float64\")\n        inputs = [x, scale, bias, epsilon, running_average_factor]\n        output_types = [x.type(), scale.type(), scale.type()]\n        if running_mean is not None and running_var is not None:\n            inputs.append(as_gpuarray_variable(running_mean, ctx_name))\n            inputs.append(as_gpuarray_variable(running_var, ctx_name))\n            output_types.append(scale.type())\n            output_types.append(scale.type())\n        return Apply(self, inputs, output_types)\n\n    def L_op(self, inputs, outputs, grads):\n        x, scale, bias, epsilon, running_average_factor = inputs[:5]\n        dy = grads[0]\n        _, x_mean, x_invstd = outputs[:3]\n        disconnected_outputs = [\n            DisconnectedType()(),  # epsilon\n            DisconnectedType()(),\n        ]  # running_average_factor\n        # Optional running_mean and running_var.\n        for i in range(5, len(inputs)):\n            disconnected_outputs.append(DisconnectedType()())\n        return (\n            GpuDnnBatchNormGrad(self.mode)(x, dy, scale, x_mean, x_invstd, epsilon)\n            + disconnected_outputs\n        )\n\n    def connection_pattern(self, node):\n        # Specificy that epsilon and running_average_factor are not connected to outputs.\n        patterns = [\n            [True, True, True],  # x\n            [True, True, True],  # scale\n            [True, True, True],  # bias\n            [False, False, False],  # epsilon\n            [False, False, False],\n        ]  # running_average_factor\n        # Optional running_mean and running_var are only\n        # connected to their new values.\n        for i in range(5, len(node.inputs)):\n            patterns[0].append(True)\n            for pattern in patterns[1:]:\n                pattern.append(False)\n            patterns.append([False] * (3 + i - 5) + [True])\n        return patterns\n\n\nclass GpuDnnBatchNormInference(DnnBase):\n    \"\"\"\n    Base Op for cuDNN Batch Normalization.\n\n    Parameters\n    ----------\n    mode : {'per-activation', 'spatial'}\n        Whether to normalize per activation (in this mode, bias and scale\n        tensor dimensions are 1xCxHxW) or share normalization factors across\n        spatial dimensions (in this mode, bias and scale tensor dimensions\n        are 1xCx1x1).\n    epsilon\n        Epsilon value used in the batch normalization formula. Minimum allowed\n        value is 1e-5 (imposed by cuDNN).\n    \"\"\"\n\n    __props__ = (\"mode\", \"inplace\")\n\n    check_input = False\n    params_type = ParamsType(\n        mode=cudnn.cudnnBatchNormMode_t, inplace=bool_t, handle=handle_type\n    )\n\n    def __init__(self, mode=\"per-activation\", inplace=False):\n        DnnBase.__init__(\n            self,\n            [\"c_code\/dnn_batchnorm_base.c\", \"c_code\/dnn_batchnorm_inf.c\"],\n            \"dnn_batchnorm_op\",\n        )\n\n        assert cudnn.cudnnBatchNormMode_t.has_alias(mode)\n        self.mode = mode\n        self.inplace = bool(inplace)\n        if self.inplace:\n            self.destroy_map = {0: [0]}\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        if not hasattr(self, \"inplace\"):\n            self.inplace = False\n\n    def infer_shape(self, node, shape):\n        return [shape[0]]\n\n    def make_node(\n        self, x, scale, bias, estimated_mean, estimated_variance, epsilon=1e-4\n    ):\n        ctx_name = infer_context_name(\n            x, scale, bias, estimated_mean, estimated_variance\n        )\n        x = as_gpuarray_variable(x, ctx_name)\n        scale = as_gpuarray_variable(scale, ctx_name)\n        bias = as_gpuarray_variable(bias, ctx_name)\n        estimated_mean = as_gpuarray_variable(estimated_mean, ctx_name)\n        estimated_variance = as_gpuarray_variable(estimated_variance, ctx_name)\n        epsilon = as_scalar(epsilon).astype(\"float64\")\n        assert (\n            x.ndim\n            == scale.ndim\n            == bias.ndim\n            == estimated_mean.ndim\n            == estimated_variance.ndim\n        )\n        assert x.ndim in (4, 5)\n        return Apply(\n            self,\n            [x, scale, bias, estimated_mean, estimated_variance, epsilon],\n            [x.type()],\n        )\n\n    def grad(self, inputs, grads):\n        x, scale, bias, est_mean, est_var, epsilon = inputs\n        dy = grads[0]\n\n        if self.mode == \"per-activation\":\n            axes = (0,)\n        elif self.mode == \"spatial\":\n            axes = (0,) + tuple(range(2, x.ndim))\n        scale, bias, est_mean, est_var = (\n            theano.tensor.addbroadcast(t, *axes)\n            for t in (scale, bias, est_mean, est_var)\n        )\n\n        # define helper expressions\n        est_var_eps = est_var + epsilon\n        est_std = theano.tensor.sqrt(est_var_eps)\n        two = theano.tensor.constant(2.0)\n\n        # define and return gradients\n        dx = dy * (scale \/ est_std)\n        dscale = (dy * (x - est_mean)).sum(axes, keepdims=True) \/ est_std\n        dbias = dy.sum(axes, keepdims=True)\n        dmean = -dy.sum(axes, keepdims=True) * (scale \/ est_std)\n        dvar = -(dy * (x - est_mean)).sum(axes, keepdims=True) * (\n            scale \/ (two * est_var_eps * est_std)\n        )\n        return [dx, dscale, dbias, dmean, dvar, DisconnectedType()()]\n\n    def connection_pattern(self, node):\n        # Specificy that epsilon is not connected to outputs.\n        return [[True], [True], [True], [True], [True], [False]]\n\n\nclass GpuDnnBatchNormGrad(DnnBase):\n    __props__ = (\"mode\",)\n\n    check_input = False\n    params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t, handle=handle_type)\n\n    def __init__(self, mode=\"per-activation\"):\n        DnnBase.__init__(\n            self,\n            [\"c_code\/dnn_batchnorm_base.c\", \"c_code\/dnn_batchnorm_grad.c\"],\n            \"dnn_batchnorm_grad\",\n        )\n\n        assert cudnn.cudnnBatchNormMode_t.has_alias(mode)\n        self.mode = mode\n\n    def make_node(self, x, dy, scale, x_mean, x_invstd, epsilon=1e-4):\n        ctx_name = infer_context_name(x, dy, scale, x_mean, x_invstd)\n        x = as_gpuarray_variable(x, ctx_name)\n        dy = as_gpuarray_variable(dy, ctx_name)\n        scale = as_gpuarray_variable(scale, ctx_name)\n        x_mean = as_gpuarray_variable(x_mean, ctx_name)\n        x_invstd = as_gpuarray_variable(x_invstd, ctx_name)\n        epsilon = as_scalar(epsilon).astype(\"float64\")\n        assert x.ndim == dy.ndim == scale.ndim == x_mean.ndim == x_invstd.ndim\n        assert x.ndim in (4, 5)\n        return Apply(\n            self,\n            [x, dy, scale, x_mean, x_invstd, epsilon],\n            [x.type(), scale.type(), scale.type()],\n        )\n\n    def infer_shape(self, node, shape):\n        return [shape[0], shape[2], shape[2]]\n\n\ngpudata_type = CDataType(\"gpudata *\", \"gpudata_release\")\ndropoutdesc_type = CUDNNDataType(\n    \"cudnnDropoutDescriptor_t\", \"cudnnDestroyDropoutDescriptor\"\n)\n\n\nclass GpuDnnDropoutOp(DnnBase):\n    __props__ = (\"inplace\",)\n\n    def __init__(self, inplace=False):\n        DnnBase.__init__(self, [\"c_code\/dnn_dropout_fwd.c\"], \"dnn_dropout_fwd\")\n        self.inplace = inplace\n        if self.inplace:\n            self.destroy_map = {1: [2]}\n\n    def make_node(self, inp, descriptor, state):\n        ctx_name = infer_context_name(inp)\n        inp = as_gpuarray_variable(inp, ctx_name)\n        return Apply(\n            self, [inp, descriptor, state], [inp.type(), state.type(), gpudata_type()]\n        )\n\n    def prepare_node(self, node, storage_map, compute_map, impl):\n        assert self.inplace, \"GpuDnnDropoutOp not inplace\"\n\n\nclass _DropoutDescriptor(DnnBase):\n    __props__ = (\"context_name\",)\n\n    def __init__(self, context_name):\n        DnnBase.__init__(self, [\"c_code\/dnn_dropout_desc.c\"], \"dnn_dropout_desc\")\n        self.context_name = context_name\n\n    def dnn_context(self, node):\n        return self.context_name\n\n    def do_constant_folding(self, node):\n        return False\n\n    def make_node(self, dropout, seed, context_name):\n        dropout = as_scalar(dropout).astype(\"float32\")\n        seed = as_scalar(seed).astype(\"uint64\")\n\n        assert context_name == self.context_name\n        # This is a dirty hack to pass the context because params is\n        # occupied by the cudnn handle\n        context = gpu_context_type.make_constant(get_context(context_name))\n\n        return Apply(\n            self,\n            [dropout, seed, context],\n            [\n                dropoutdesc_type(),\n                GpuArrayType(\"uint8\", (False,), context_name=context_name)(),\n            ],\n        )\n\n    def c_code_cache_version_apply(self, node):\n        # disable the cache since we can't pickle contexts\n        return None\n\n\ndef _make_dropout_desc(dropout, seed, context_name):\n    desc, states = theano.function(\n        [],\n        _DropoutDescriptor(context_name)(dropout, seed, context_name),\n        theano.Mode(optimizer=None),\n        profile=False,\n    )()\n    return desc, states\n\n\ndef dropout(x, dropout=0.0, seed=4242):\n    desc, states = _make_dropout_desc(dropout, seed, x.type.context_name)\n    y, odesc = GpuDnnDropoutOp()(x, desc)\n    return y, desc, odesc, states\n\n\nrnndesc_type = CUDNNDataType(\"cudnnRNNDescriptor_t\", \"cudnnDestroyRNNDescriptor\")\n\n\ndef as_i32(v):\n    return as_scalar(v).astype(\"int32\")\n\n\nclass _RNNDescriptor(DnnBase):\n    __props__ = (\"context_name\",)\n\n    def __init__(self, context_name):\n        if version() < 5005:\n            raise RuntimeError(\"cudnn RNN require cudnn v5 final or higher.\")\n        DnnBase.__init__(self, [\"c_code\/dnn_rnn_desc.c\"], \"dnn_rnn_desc\")\n        self.context_name = context_name\n\n    def dnn_context(self, node):\n        return self.context_name\n\n    def do_constant_folding(self, node):\n        return False\n\n    def make_node(\n        self,\n        hidden_size,\n        num_layers,\n        ddesc,\n        input_mode,\n        direction_mode,\n        rnn_mode,\n        dtype,\n    ):\n\n        hidden_size = as_i32(hidden_size)\n        num_layers = as_i32(num_layers)\n\n        if version() < 5005:\n            raise RuntimeError(\"cudnn RNN require cudnn v5 final or higher.\")\n\n        if input_mode == \"linear\":\n            input_mode = as_i32(0)\n        elif input_mode == \"skip\":\n            input_mode = as_i32(1)\n        else:\n            raise ValueError(\"input_mode\")\n\n        if direction_mode == \"unidirectional\":\n            direction_mode = as_i32(0)\n        elif direction_mode == \"bidirectional\":\n            direction_mode = as_i32(1)\n        else:\n            raise ValueError(\"direction_mode\")\n\n        if rnn_mode == \"rnn_relu\":\n            rnn_mode = as_i32(0)\n        elif rnn_mode == \"rnn_tanh\":\n            rnn_mode = as_i32(1)\n        elif rnn_mode == \"lstm\":\n            rnn_mode = as_i32(2)\n        elif rnn_mode == \"gru\":\n            rnn_mode = as_i32(3)\n        else:\n            raise ValueError(\"rnn_mode\")\n\n        dtype = as_i32(gpuarray.dtype_to_typecode(dtype))\n\n        return Apply(\n            self,\n            [\n                hidden_size,\n                num_layers,\n                dropoutdesc_type.make_constant(ddesc),\n                input_mode,\n                direction_mode,\n                rnn_mode,\n                dtype,\n            ],\n            [rnndesc_type()],\n        )\n\n\ndef _make_rnn_desc(\n    hidden_size,\n    num_layers,\n    ddesc,\n    rnn_mode,\n    input_mode,\n    direction_mode,\n    dtype,\n    context_name,\n):\n    desc = theano.function(\n        [],\n        _RNNDescriptor(context_name)(\n            hidden_size, num_layers, ddesc, input_mode, direction_mode, rnn_mode, dtype\n        ),\n        theano.Mode(optimizer=None),\n        profile=False,\n    )()\n    return desc\n\n\nclass _RNNParamSize(DnnBase):\n    __props__ = (\"context_name\",)\n\n    def __init__(self, context_name):\n        DnnBase.__init__(self, [\"c_code\/dnn_rnn_paramsize.c\"], \"dnn_rnn_paramsize\")\n        self.context_name = context_name\n\n    def dnn_context(self, node):\n        return self.context_name\n\n    def do_constant_folding(self, node):\n        return False\n\n    def make_node(self, desc, input_size, typecode):\n        input_size = as_tensor_variable(input_size).astype(\"uint64\")\n        typecode = as_i32(typecode)\n        return Apply(\n            self,\n            [rnndesc_type.make_constant(desc), input_size, typecode],\n            [get_scalar_type(\"uint64\")()],\n        )\n\n\ndef _get_param_size(desc, input_size, dtype, context_name):\n    typecode = gpuarray.dtype_to_typecode(dtype)\n    return theano.function(\n        [],\n        _RNNParamSize(context_name)(desc, input_size, typecode),\n        theano.Mode(optimizer=None),\n        profile=False,\n    )()\n\n\nclass _RNNSplitParams(DnnBase):\n    __props__ = (\"rnn_mode\",)\n\n    def __init__(self, rnn_mode):\n        DnnBase.__init__(self)\n        self.rnn_mode = rnn_mode\n\n    def make_node(self, w, desc, layer, isize, typecode):\n        w = as_gpuarray_variable(w, infer_context_name(w))\n        assert w.ndim == 1\n        layer = as_scalar(layer).astype(\"int32\")\n        isize = as_tensor_variable(isize).astype(\"uint64\")\n        assert isize.ndim == 1\n        typecode = as_scalar(typecode).astype(\"int32\")\n        _1d = GpuArrayType(w.type.dtype, [False], context_name=w.type.context_name)\n        _2d = GpuArrayType(\n            w.type.dtype, [False, False], context_name=w.type.context_name\n        )\n        outputs = []\n        if self.rnn_mode == \"rnn_relu\" or self.rnn_mode == \"rnn_tanh\":\n            outputs.extend([_2d(), _1d()])  # input\n            outputs.extend([_2d(), _1d()])  # recurrent\n        elif self.rnn_mode == \"lstm\":\n            outputs.extend([_2d(), _1d()])  # input input\n            outputs.extend([_2d(), _1d()])  # input forget\n            outputs.extend([_2d(), _1d()])  # input newmem\n            outputs.extend([_2d(), _1d()])  # input output\n            outputs.extend([_2d(), _1d()])  # recur input\n            outputs.extend([_2d(), _1d()])  # recur forget\n            outputs.extend([_2d(), _1d()])  # recur newmem\n            outputs.extend([_2d(), _1d()])  # recur output\n        elif self.rnn_mode == \"gru\":\n            outputs.extend([_2d(), _1d()])  # input reset\n            outputs.extend([_2d(), _1d()])  # input update\n            outputs.extend([_2d(), _1d()])  # input newmem\n            outputs.extend([_2d(), _1d()])  # recur reset\n            outputs.extend([_2d(), _1d()])  # recur update\n            outputs.extend([_2d(), _1d()])  # recur newmem\n\n        return Apply(\n            self, [w, layer, rnndesc_type.make_constant(desc), isize, typecode], outputs\n        )\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        kw = dict(\n            fail=sub[\"fail\"],\n            w=inputs[0],\n            layer=inputs[1],\n            desc=inputs[2],\n            isize=inputs[3],\n            typecode=inputs[4],\n            handle=sub[\"params\"],\n        )\n        code = (\n            \"\"\"\n  cudnnTensorDescriptor_t xdesc;\n  cudnnFilterDescriptor_t wdesc;\n  cudnnFilterDescriptor_t odesc;\n  size_t nshp[2];\n  void *w;\n  void *o;\n  ptrdiff_t off;\n#if CUDNN_VERSION < 7100\n  size_t bshp;\n#endif\n  cudnnStatus_t err;\n  cudnnDataType_t dt;\n  cudnnTensorFormat_t tf;\n  int nd;\n  int dims[3];\n  int strs[3];\n\n  if (PyArray_DIM(%(isize)s, 0) != 2) {\n    PyErr_SetString(PyExc_ValueError, \"input_size should be of length two\");\n    %(fail)s;\n  }\n\n  switch (%(typecode)s) {\n  case GA_FLOAT:\n    dt = CUDNN_DATA_FLOAT;\n    break;\n  case GA_DOUBLE:\n    dt = CUDNN_DATA_DOUBLE;\n    break;\n  case GA_HALF:\n    dt = CUDNN_DATA_HALF;\n    break;\n  default:\n    PyErr_SetString(PyExc_ValueError, \"Unsupported data type\");\n    %(fail)s;\n  }\n\n  err = cudnnCreateTensorDescriptor(&xdesc);\n  if (err != CUDNN_STATUS_SUCCESS) {\n    PyErr_SetString(PyExc_RuntimeError, \"Could not create xdesc\");\n    %(fail)s;\n  }\n\n  dims[0] = *(npy_uint64 *)PyArray_GETPTR1(%(isize)s, 0);\n  dims[1] = *(npy_uint64 *)PyArray_GETPTR1(%(isize)s, 1);\n  dims[2] = 1;\n  strs[0] = dims[2] * dims[1];\n  strs[1] = dims[2];\n  strs[2] = 1;\n\n  err = cudnnSetTensorNdDescriptor(xdesc, dt, 3, dims, strs);\n  if (err != CUDNN_STATUS_SUCCESS) {\n    cudnnDestroyTensorDescriptor(xdesc);\n    PyErr_Format(PyExc_RuntimeError, \"Could not set xdesc: %%s\",\n                 cudnnGetErrorString(err));\n    %(fail)s;\n  }\n\n  if (c_make_filter(%(w)s, &wdesc)) {\n    cudnnDestroyTensorDescriptor(xdesc);\n    %(fail)s\n  }\n\n  err = cudnnCreateFilterDescriptor(&odesc);\n  if (err != CUDNN_STATUS_SUCCESS) {\n    PyErr_SetString(PyExc_RuntimeError, \"could not create odesc\");\n    cudnnDestroyTensorDescriptor(xdesc);\n    cudnnDestroyFilterDescriptor(wdesc);\n    %(fail)s\n  }\n\n  w = PyGpuArray_DEV_DATA(%(w)s);\n  nshp[0] = PyGpuArray_DIM(%(w)s, 0);\n  nshp[1] = 1;\n        \"\"\"\n            % kw\n        )\n\n        def get_params(id, m, b):\n            kw2 = kw.copy()\n            kw2[\"id\"] = id\n            kw2[\"m\"] = m\n            kw2[\"b\"] = b\n            return (\n                \"\"\"\n  err = cudnnGetRNNLinLayerBiasParams(%(handle)s, %(desc)s, %(layer)s, xdesc, wdesc, w, %(id)s, odesc, &o);\n  if (err != CUDNN_STATUS_SUCCESS) {\n    cudnnDestroyTensorDescriptor(xdesc);\n    cudnnDestroyFilterDescriptor(wdesc);\n    cudnnDestroyFilterDescriptor(odesc);\n    PyErr_SetString(PyExc_RuntimeError, \"can't fetch bias for id %(id)s\");\n    %(fail)s\n  }\n  off = (intptr_t)o - (intptr_t)w;\n  assert(off >= 0 && \"bias\");\n\n  err = cudnnGetFilterNdDescriptor(odesc, 3, &dt, &tf, &nd, dims);\n  if (err != CUDNN_STATUS_SUCCESS) {\n    cudnnDestroyTensorDescriptor(xdesc);\n    cudnnDestroyFilterDescriptor(wdesc);\n    cudnnDestroyFilterDescriptor(odesc);\n    PyErr_SetString(PyExc_RuntimeError, \"could not get bias shape for id %(id)s\");\n    %(fail)s;\n  }\n  \/\/ We assume that the typecode matches\n#if CUDNN_VERSION < 7100\n  assert(dims[2] == 1 && \"bias\");\n  assert(dims[1] == 1 && \"bias\");\n  %(b)s = pygpu_view(%(w)s, Py_None);\n  %(b)s->ga.offset += off;\n  %(b)s->ga.dimensions[0] = dims[0];\n  bshp = dims[0];\n#else\n  assert(dims[0] == 1 && \"bias\");\n  assert(dims[2] == 1 && \"bias\");\n  %(b)s = pygpu_view(%(w)s, Py_None);\n  %(b)s->ga.offset += off;\n  %(b)s->ga.dimensions[0] = dims[1];\n#endif\n  GpuArray_fix_flags(&%(b)s->ga);\n\n  err = cudnnGetRNNLinLayerMatrixParams(%(handle)s, %(desc)s, %(layer)s, xdesc, wdesc, w, %(id)s, odesc, &o);\n  if (err != CUDNN_STATUS_SUCCESS) {\n    cudnnDestroyTensorDescriptor(xdesc);\n    cudnnDestroyFilterDescriptor(wdesc);\n    cudnnDestroyFilterDescriptor(odesc);\n    PyErr_SetString(PyExc_RuntimeError, \"can't fetch matrix for id %(id)s\");\n    %(fail)s\n  }\n  off = (intptr_t)o - (intptr_t)w;\n  assert(off >= 0 && \"matrix\");\n\n  \/\/ This is 3d because of cudnn limitations.\n  err = cudnnGetFilterNdDescriptor(odesc, 3, &dt, &tf, &nd, dims);\n  if (err != CUDNN_STATUS_SUCCESS) {\n    cudnnDestroyTensorDescriptor(xdesc);\n    cudnnDestroyFilterDescriptor(wdesc);\n    cudnnDestroyFilterDescriptor(odesc);\n    PyErr_SetString(PyExc_RuntimeError, \"could not get matrix shape for id %(id)s\");\n    %(fail)s;\n  }\n\n#if CUDNN_VERSION < 7100\n  assert(dims[1] == 1 && \"matrix\");\n  assert(dims[2] == 1 && \"matrix\");\n  \/\/ We assume that the typecode matches\n  %(m)s = pygpu_reshape(%(w)s, 2, nshp, GA_F_ORDER, 1, -1);\n  %(m)s->ga.offset += off;\n  assert(dims[0] %% bshp == 0);\n  %(m)s->ga.dimensions[0] = dims[0] \/ bshp;\n  %(m)s->ga.dimensions[1] = bshp;\n#else\n  assert(dims[0] == 1 && \"matrix\");\n  \/\/ We assume that the typecode matches\n  %(m)s = pygpu_reshape(%(w)s, 2, nshp, GA_F_ORDER, 1, -1);\n  %(m)s->ga.offset += off;\n  %(m)s->ga.dimensions[1] = dims[1];\n  %(m)s->ga.dimensions[0] = dims[2];\n#endif\n  %(m)s->ga.strides[1] = %(m)s->ga.dimensions[0] * gpuarray_get_elsize(%(m)s->ga.typecode);\n  GpuArray_fix_flags(&%(m)s->ga);\n            \"\"\"\n                % kw2\n            )\n\n        for i in range(len(outputs) \/\/ 2):\n            code += get_params(i, outputs[2 * i], outputs[(2 * i) + 1])\n\n        code += \"\"\"\n  cudnnDestroyTensorDescriptor(xdesc);\n  cudnnDestroyFilterDescriptor(wdesc);\n  cudnnDestroyFilterDescriptor(odesc);\n        \"\"\"\n        return code\n\n    def c_code_cache_version(self):\n        return (5, version())\n\n\ndef _split_rnn_params(w, desc, layer, input_size, dtype, rnn_mode):\n    typecode = gpuarray.dtype_to_typecode(dtype)\n    outs = _RNNSplitParams(rnn_mode)(w, desc, layer, input_size, typecode)\n    outs = [theano.Out(o, borrow=True) for o in outs]\n    return theano.function([], outs, theano.Mode(optimizer=None), profile=False)()\n\n\nclass GpuDnnRNNOp(DnnBase):\n    __props__ = ()\n    _cop_num_inputs = 6\n    _cop_num_outputs = 4\n\n    def __init__(self, rnn_mode, direction_mode):\n        DnnBase.__init__(self, [\"c_code\/dnn_rnn_fwd.c\"], \"dnn_rnn_fwd\")\n        self.rnn_mode = rnn_mode\n        if direction_mode == \"bidirectional\":\n            self.num_dirs = 2\n        elif direction_mode == \"unidirectional\":\n            self.num_dirs = 1\n        else:\n            raise ValueError(\n                \"direction_mode is invalid (got {})\".format(direction_mode)\n            )\n\n    def dnn_context(self, node):\n        return node.outputs[1].type.context_name\n\n    def make_node(self, desc, w, x, hx, cx=None):\n        if cx is None:\n            context_name = infer_context_name(w, x, hx)\n        else:\n            context_name = infer_context_name(w, x, hx, cx)\n\n        w = as_gpuarray_variable(w, context_name)\n        x = as_gpuarray_variable(x, context_name)\n        hx = as_gpuarray_variable(hx, context_name)\n        inputs = [desc, as_i32(self.num_dirs), w, x, hx]\n        assert w.ndim == 1\n        assert x.ndim == 3  # seqLength, minibatch, inputSize\n        assert hx.ndim == 3  # numLayers, minibatch, hiddenSize * bidi\n\n        if self.rnn_mode == \"lstm\":\n            cx = as_gpuarray_variable(cx, context_name)\n            assert cx.ndim == 3  # numLayers, minibatch, hiddenSize * bidi\n            inputs.append(cx)\n\n        _3d = GpuArrayType(\n            dtype=x.dtype,\n            broadcastable=(False, False, False),\n            context_name=context_name,\n        )\n        reserve = gpudata_type()\n        y = _3d()  # seqLength, minibatch, hiddenSize * bidi\n        hy = _3d()  # numLayers, miniBatch, hiddenSize * bidi\n        outputs = [reserve, y, hy]\n\n        if self.rnn_mode == \"lstm\":\n            cy = _3d()  # numLayers, miniBatch, hiddenSize * bidi\n            outputs.append(cy)\n\n        return Apply(self, inputs, outputs)\n\n    def L_op(self, inputs, outputs, output_grads):\n        desc, numDirs, w, x, hx = inputs[:5]\n        cx = inputs[5] if len(inputs) == 6 else None\n        reserve, y, hy = outputs[:3]\n        _, dy, dhy = output_grads[:3]\n        dcy = output_grads[3] if len(output_grads) == 4 else None\n        # Since the op return two outputs which contain essentially\n        # the same information, the user will most likely only use one\n        # of them. This leads to the situation that the other is\n        # considered \"disconnected\" by theano in the gradient.\n        # However we know that this isn't really the case so we fix it\n        # here.\n\n        # If all the ys are disconnected, then you get a boring\n        # gradient instead of an error.  But in that case you\n        # shouldn't call this method anyway.\n        if isinstance(dy.type, DisconnectedType):\n            dy = as_gpuarray_variable(y.zeros_like(), context_name=y.type.context_name)\n        if isinstance(dhy.type, DisconnectedType):\n            dhy = None\n        if dcy and isinstance(dcy.type, DisconnectedType):\n            dcy = None\n        dinputs = GpuDnnRNNGradInputs(\n            rnn_mode=self.rnn_mode, grad_h=(dhy is not None), grad_c=(dcy is not None)\n        )(desc, x, y, dy, dhy, dcy, w, hx, cx, reserve, return_list=True)\n        reserve2, dx, dhx = dinputs[:3]\n        dw = GpuDnnRNNGradWeights()(desc, x, hx, y, reserve2, w)\n        res = [DisconnectedType()(), DisconnectedType()(), dw, dx, dhx]\n        if cx is not None:\n            res.append(dinputs[3])  # dcx\n        return res\n\n    def connection_pattern(self, node):\n        deconn = [[False] * len(node.outputs)] * 2\n        conn = [[True] * len(node.outputs)] * (len(node.inputs) - 2)\n        return deconn + conn\n\n\nclass GpuDnnRNNGradInputs(DnnBase):\n    __props__ = (\"rnn_mode\", \"grad_c\", \"grad_h\")\n    _cop_num_inputs = 10\n    _cop_num_outputs = 4\n\n    def __init__(self, rnn_mode, grad_h, grad_c):\n        DnnBase.__init__(self, [\"c_code\/dnn_rnn_gi.c\"], \"dnn_rnn_gi\")\n        self.rnn_mode = rnn_mode\n        self.grad_h = grad_h\n        self.grad_c = grad_c\n        if self.grad_c:\n            assert self.rnn_mode == \"lstm\"\n\n    def dnn_context(self, node):\n        return node.outputs[1].type.context_name\n\n    def make_node(self, desc, x, y, dy, dhy, dcy, w, hx, cx, reserve):\n        # We trust the callers here\n        xshp = as_scalar(x.shape[2]).astype(\"uint64\")\n        inputs = [desc, xshp, y, dy, w, hx, reserve]\n        outputs = [reserve.type(), x.type(), hx.type()]\n        if self.rnn_mode == \"lstm\":\n            inputs.append(cx)\n            outputs.append(cx.type())\n        if self.grad_h:\n            inputs.append(dhy)\n        if self.grad_c:\n            inputs.append(dcy)\n\n        return Apply(self, inputs, outputs)\n\n    # We have special requirements so this is hooking into COp\n    def format_c_function_args(self, inp, out):\n        rinp = inp[:7]\n        others = inp[7:]\n        if self.rnn_mode == \"lstm\":\n            rinp.append(others.pop(0))\n        else:\n            rinp.append(\"NULL\")\n        if self.grad_h:\n            rinp.append(others.pop(0))\n        else:\n            rinp.append(\"NULL\")\n        if self.grad_c:\n            rinp.append(others.pop(0))\n        else:\n            rinp.append(\"NULL\")\n        assert len(others) == 0\n        return COp.format_c_function_args(self, rinp, out)\n\n\nclass GpuDnnRNNGradWeights(DnnBase):\n    __props__ = ()\n\n    def __init__(self):\n        DnnBase.__init__(self, [\"c_code\/dnn_rnn_gw.c\"], \"dnn_rnn_gw\")\n\n    def make_node(self, desc, x, hx, y, reserve, w):\n        # We trust the callers here\n        wsize = as_scalar(w.shape[0]).astype(\"uint64\")\n        inputs = [desc, wsize, x, hx, y, reserve]\n        outputs = [w.type()]\n        return Apply(self, inputs, outputs)\n\n\nclass RNNBlock:\n    \"\"\"\n    An object that allow us to use CuDNN RNN implementation.\n    TODO: make an example how to use. You can check Theano tests\n    test_dnn_rnn_gru() and test_dnn_rnn_lstm() in the file\n    theano\/gpuarray\/tests\/test_dnn.py for now.\n\n\n    Parameters\n    ----------\n    dtype : data type of computation\n    hidden_size : int\n        hidden layer dimension.\n    num_layers : int\n        number of the recurrent layer you want to set.\n    rnn_mode : {'rnn_relu', 'rnn_tanh', 'lstm', 'gru'}\n        rnn_relu: A single-gate recurrent neural network with a ReLU activation function.\n\n        .. math::\n\n        h_t=ReLU(W_ix_t+U_ih_{t-1}+b_{wi}+b_{Ri})\n        rnn_tanh: A single-gate recurrent neural network with a tanh activation function.\n\n        .. math::\n\n        h_t=tanh(W_ix_t+U_ih_{t-1}+b_{wi}+b_{Ri})\n\n        lstm: A four-gate Long Short-Term Memory network with no peephole connections.\n        gru: A three-gate network consisting of Gated Recurrent Units.\n    input_mode : {'linear', 'skip'}\n        linear: input will be multiplied by a biased matrix\n        skip: No operation is performed on the input.  The size must match the hidden size.\n    direction_mode : {'unidirectional', 'bidirectional'}\n        unidirectional: The network operates recurrently from the first input to the last.\n        bidirectional: The network operates from first to last then from last to first and concatenates the results at each layer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dtype,\n        hidden_size,\n        num_layers,\n        rnn_mode,\n        input_mode=\"linear\",\n        direction_mode=\"unidirectional\",\n        context_name=None,\n    ):\n        # This is not supported for any value other than 0, so don't change it\n        ddesc, states = _make_dropout_desc(0, 4242, context_name)\n        self.ddesc = ddesc\n        self.dstates = states\n        self.desc = _make_rnn_desc(\n            hidden_size,\n            num_layers,\n            ddesc,\n            rnn_mode,\n            input_mode,\n            direction_mode,\n            dtype,\n            context_name,\n        )\n        self.rnn_mode = rnn_mode\n        self.direction_mode = direction_mode\n        self.context_name = context_name\n        self.dtype = dtype\n\n    def get_param_size(self, input_size):\n        \"\"\"\n        Get the size of the shared variable for the parameters of the RNN.\n\n        This will return a size (in items) necessary to store all the\n        parameters for the RNN.  You should allocate a variable of\n        that size to store those parameters.  The order and layout of\n        the parameters is opaque.\n\n        Parameters\n        ----------\n        input_size: (int, int)\n            Size of the input blocks\n\n        \"\"\"\n        bytesize = _get_param_size(self.desc, input_size, self.dtype, self.context_name)\n        bytesize = int(bytesize)\n        assert bytesize % np.dtype(self.dtype).itemsize == 0\n        return bytesize \/\/ np.dtype(self.dtype).itemsize\n\n    def split_params(self, w, layer, input_size):\n        \"\"\"\n        Split the opaque parameter block into components.\n\n        Parameters\n        ----------\n        w: GpuArraySharedVariable\n            opaque parameter block\n        layer: int\n            ID of the layer\n        input_size: (int, int)\n            Size of the input blocks\n\n        \"\"\"\n        if not isinstance(w, GpuArraySharedVariable):\n            raise TypeError(\"split_params only works on gpuarray shared variables\")\n        return _split_rnn_params(\n            w, self.desc, layer, input_size, self.dtype, self.rnn_mode\n        )\n\n    def apply(self, w, x, hx, cx=None):\n        \"\"\"\n        Apply the RNN to some data\n\n        Parameters\n        ----------\n        w:\n            opaque parameter block\n        x:\n            input\n        hx:\n            initial hidden state\n        cx:\n            initial cell state (for LSTM)\n        \"\"\"\n        # Don't return the reserve as an output\n        return GpuDnnRNNOp(self.rnn_mode, self.direction_mode)(\n            rnndesc_type.make_constant(self.desc), w, x, hx, cx, return_list=True\n        )[1:]\n\n\ndef dnn_batch_normalization_train(\n    inputs,\n    gamma,\n    beta,\n    mode=\"per-activation\",\n    epsilon=1e-4,\n    running_average_factor=0.1,\n    running_mean=None,\n    running_var=None,\n):\n    \"\"\"\n    Performs batch normalization of the given inputs, using the mean and\n    variance of the inputs.\n\n    Parameters\n    ----------\n    mode : {'per-activation', 'spatial'}\n        Whether to normalize per activation or share normalization factors\n        across spatial dimensions (i.e., all dimensions past the second).\n    gamma : tensor\n        Learnable scale factors. Must match the dimensionality of `inputs`,\n        but have sizes of `1` for all axes normalized over (i.e., in the first\n        dimension for ``mode='per-activation'`, and additionally in all\n        dimensions past the second for ``mode='spatial'``).\n    beta : tensor\n        Learnable biases. Must match the tensor layout of `gamma`.\n    epsilon : float\n        Epsilon value used in the batch normalization formula. Minimum allowed\n        value is 1e-5 (imposed by cuDNN).\n    running_average_factor : float\n        Factor for updating the values or `running_mean` and `running_var`.\n        If the factor is close to one, the running averages will update quickly,\n        if the factor is close to zero it will update slowly.\n    running_mean : tensor or None\n        Previous value of the running mean. If this is given, the new value\n        ``running_mean * (1 - r_a_factor) + batch mean * r_a_factor``\n        will be returned as one of the outputs of this function.\n        `running_mean` and `running_var` should either both be given or\n        both be None.\n    running_var : tensor or None\n        Previous value of the running variance. If this is given, the new value\n        ``running_var * (1 - r_a_factor) + (m \/ (m - 1)) * batch var * r_a_factor``\n        will be returned as one of the outputs of this function,\n        where `m` is the product of lengths of the averaged-over dimensions.\n        `running_mean` and `running_var` should either both be given or\n        both be None.\n\n    Returns\n    -------\n    out : tensor\n        Batch-normalized inputs.\n    mean : tensor\n        Means of `inputs` across the normalization axes.\n    invstd : tensor\n        Inverse standard deviations of `inputs` across the normalization axes.\n    new_running_mean : tensor\n        New value of the running mean (only if both `running_mean` and\n        `running_var` were given).\n    new_running_var : tensor\n        New value of the running variance (only if both `running_var` and\n        `running_mean` were given).\n\n    Notes\n    -----\n    Requires cuDNN 5 and Theano 0.9dev2 or more recent.\n\n    For 4d tensors, returned values are equivalent to:\n\n    .. code-block:: python\n\n        axes = 0 if mode == 'per-activation' else (0, 2, 3)\n        mean = inputs.mean(axes, keepdims=True)\n        var = inputs.var(axes, keepdims=True)\n        invstd = T.inv(T.sqrt(var + epsilon))\n        out = (inputs - mean) * gamma * invstd + beta\n\n        m = T.cast(T.prod(inputs.shape) \/ T.prod(mean.shape), 'float32')\n        running_mean = running_mean * (1 - running_average_factor) + \\\\\n                       mean * running_average_factor\n        running_var = running_var * (1 - running_average_factor) + \\\\\n                      (m \/ (m - 1)) * var * running_average_factor\n\n    For 5d tensors, the axes are (0, 2, 3, 4).\n    \"\"\"\n    ndim = inputs.ndim\n    if gamma.ndim != ndim or beta.ndim != ndim:\n        raise ValueError(\n            \"gamma and beta must be of the same dimensionality \"\n            \"as inputs; got %d and %d instead of %d\" % (gamma.ndim, beta.ndim, ndim)\n        )\n    if (running_mean is None) != (running_var is None):\n        raise ValueError(\n            \"running_mean and running_var must either both be \" \"given or both be None\"\n        )\n    if running_mean is not None and running_mean.ndim != ndim:\n        raise ValueError(\n            \"running_mean must be of the same dimensionality \"\n            \"as inputs; got %d instead of %d\" % (running_mean.ndim, ndim)\n        )\n    if running_var is not None and running_var.ndim != ndim:\n        raise ValueError(\n            \"running_var must be of the same dimensionality \"\n            \"as inputs; got %d instead of %d\" % (running_var.ndim, ndim)\n        )\n    if epsilon < 1e-5:\n        raise ValueError(\"epsilon must be at least 1e-5, got %f\" % epsilon)\n\n    running_averages = running_mean is not None and running_var is not None\n\n    if ndim < 4:\n        inputs = theano.tensor.shape_padright(inputs, 4 - ndim)\n        gamma = theano.tensor.shape_padright(gamma, 4 - ndim)\n        beta = theano.tensor.shape_padright(beta, 4 - ndim)\n        if running_averages:\n            running_mean = theano.tensor.shape_padright(running_mean, 4 - ndim)\n            running_var = theano.tensor.shape_padright(running_var, 4 - ndim)\n    elif ndim > 5:\n        inputs_shape = inputs.shape\n        params_shape = gamma.shape\n        inputs = theano.tensor.flatten(inputs, 5)\n        gamma = theano.tensor.flatten(gamma, 5)\n        beta = theano.tensor.flatten(beta, 5)\n        if running_averages:\n            running_mean = theano.tensor.flatten(running_mean, 5)\n            running_var = theano.tensor.flatten(running_var, 5)\n\n    batchnorm_op = GpuDnnBatchNorm(mode=mode, running_averages=running_averages)\n    if running_averages:\n        out, mean, invstd, new_running_mean, new_running_var = batchnorm_op(\n            gpu_contiguous(inputs),\n            gpu_contiguous(gamma),\n            gpu_contiguous(beta),\n            epsilon=epsilon,\n            running_average_factor=running_average_factor,\n            running_mean=gpu_contiguous(running_mean),\n            running_var=gpu_contiguous(running_var),\n        )\n        if new_running_mean.broadcastable != running_mean.broadcastable:\n            new_running_mean = tensor.patternbroadcast(\n                new_running_mean, running_mean.broadcastable\n            )\n        if new_running_var.broadcastable != running_var.broadcastable:\n            new_running_var = tensor.patternbroadcast(\n                new_running_var, running_var.broadcastable\n            )\n        result = (out, mean, invstd, new_running_mean, new_running_var)\n    else:\n        result = batchnorm_op(\n            gpu_contiguous(inputs),\n            gpu_contiguous(gamma),\n            gpu_contiguous(beta),\n            epsilon=epsilon,\n        )\n    if ndim < 4:\n        result = tuple(theano.tensor.flatten(r, ndim) for r in result)\n    elif ndim > 5:\n        result = (theano.tensor.reshape(result[0], inputs_shape),) + tuple(\n            theano.tensor.reshape(r, params_shape) for r in result[1:]\n        )\n    return result\n\n\ndef dnn_batch_normalization_test(\n    inputs, gamma, beta, mean, var, mode=\"per-activation\", epsilon=1e-4\n):\n    \"\"\"\n    Performs batch normalization of the given inputs, using the given mean and\n    variance.\n\n    Parameters\n    ----------\n    mode : {'per-activation', 'spatial'}\n        Whether to normalize per activation or share normalization factors\n        across spatial dimensions (i.e., all dimensions past the second).\n    gamma : tensor\n        Scale factors. Must match the dimensionality of `inputs`, but have\n        sizes of `1` for all axes normalized over (i.e., in the first dimension\n        for ``mode='per-activation'`, and additionally in all dimensions past\n        the second for ``mode='spatial'``).\n    beta : tensor\n        Biases. Must match the tensor layout of `gamma`.\n    mean : tensor\n        Means. Usually these are running averages computed during training.\n        Must match the tensor layout of `gamma`.\n    var : tensor\n        Variances. Usually these are running averages computed during training.\n        Must match the tensor layout of `gamma`.\n    epsilon : float\n        Epsilon value used in the batch normalization formula. Minimum allowed\n        value is 1e-5 (imposed by cuDNN).\n\n    Returns\n    -------\n    out : tensor\n        Batch-normalized inputs.\n\n    Notes\n    -----\n    Requires cuDNN 5 and Theano 0.9dev2 or more recent.\n\n    For 4d tensors, the returned value is equivalent to:\n\n    .. code-block:: python\n\n        axes = (0,) if mode == 'per-activation' else (0, 2, 3)\n        gamma, beta, mean, var = (T.addbroadcast(t, *axes)\n                                  for t in (gamma, beta, mean, var))\n        out = (inputs - mean) * gamma \/ T.sqrt(var + epsilon) + beta\n\n    For 5d tensors, the axes would be (0, 2, 3, 4).\n    \"\"\"\n    ndim = inputs.ndim\n    if gamma.ndim != ndim or beta.ndim != ndim:\n        raise ValueError(\n            \"gamma and beta must be of the same dimensionality \"\n            \"as inputs; got %d and %d instead of %d\" % (gamma.ndim, beta.ndim, ndim)\n        )\n    if mean.ndim != ndim or var.ndim != ndim:\n        raise ValueError(\n            \"mean and var must be of the same dimensionality \"\n            \"as inputs; got %d and %d instead of %d\" % (mean.ndim, var.ndim, ndim)\n        )\n    if epsilon < 1e-5:\n        raise ValueError(\"epsilon must be at least 1e-5, got %f\" % epsilon)\n\n    if ndim < 4:\n        inputs = theano.tensor.shape_padright(inputs, 4 - ndim)\n        gamma = theano.tensor.shape_padright(gamma, 4 - ndim)\n        beta = theano.tensor.shape_padright(beta, 4 - ndim)\n        mean = theano.tensor.shape_padright(mean, 4 - ndim)\n        var = theano.tensor.shape_padright(var, 4 - ndim)\n    elif ndim > 5:\n        inputs_shape = inputs.shape\n        inputs = theano.tensor.flatten(inputs, 5)\n        gamma = theano.tensor.flatten(gamma, 5)\n        beta = theano.tensor.flatten(beta, 5)\n        mean = theano.tensor.flatten(mean, 5)\n        var = theano.tensor.flatten(var, 5)\n    batchnorm_op = GpuDnnBatchNormInference(mode=mode)\n    result = batchnorm_op(\n        gpu_contiguous(inputs),\n        gpu_contiguous(gamma),\n        gpu_contiguous(beta),\n        gpu_contiguous(mean),\n        gpu_contiguous(var),\n        epsilon=epsilon,\n    )\n    if ndim < 4:\n        result = theano.tensor.flatten(result, ndim)\n    elif ndim > 5:\n        result = theano.tensor.reshape(result, inputs_shape)\n    return result\n\n\nclass GpuDnnTransformerGrid(DnnBase):\n    \"\"\"\n    Grid generator Op for cuDNN Spatial Transformer.\n    \"\"\"\n\n    __props__ = ()\n    _cop_num_inputs = 2\n    _cop_num_outputs = 1\n    _f16_ok = True\n    check_input = False\n\n    def __init__(self):\n        DnnBase.__init__(\n            self, [\"c_code\/dnn_sptf_grid.c\"], \"APPLY_SPECIFIC(dnn_sptf_grid)\"\n        )\n\n    def make_node(self, theta, out_dims):\n        \"\"\"\n        Create a grid generator node for a cuDNN Spatial Transformer\n\n        Parameters\n        ----------\n        theta : tensor\n            Affine transformation tensor containing one affine transformation\n            matrix per image. ``theta`` is usually generated by the localization\n            network.\n\n        out_dims : tuple\n            Dimensions of the transformed inputs, containing four elements, and is given\n            by (N, C, H, W), where N is the number of inputs, C the number of channels,\n            H and W are the height and width of each input.\n        \"\"\"\n        context_name = infer_context_name(theta)\n\n        theta = gpu_contiguous(as_gpuarray_variable(theta, context_name))\n        assert theta.dtype in (\"float16\", \"float32\", \"float64\")\n        assert theta.ndim == 3\n\n        out_dims = cpu_contiguous(as_tensor_variable(out_dims))\n        assert out_dims.dtype in theano.tensor.basic.integer_dtypes\n        assert out_dims.ndim == 1\n        # Ensure 64-bit ints are passed to the C code\n        out_dims = theano.tensor.basic.cast(out_dims, \"int64\")\n        grid = GpuArrayType(\n            dtype=theta.dtype,\n            broadcastable=(theta.type.ndim + 1) * (False,),\n            context_name=context_name,\n        )()\n\n        inputs = [theta, out_dims]\n        outputs = [grid]\n        return Apply(self, inputs, outputs)\n\n    def grad(self, inputs, grads):\n        theta, out_dims = inputs\n        dgrid = grads[0]\n\n        dtheta = GpuDnnTransformerGradT()(dgrid)\n        return [dtheta, grad_not_implemented(self, 1, out_dims)]\n\n\nclass GpuDnnTransformerSampler(DnnBase):\n    \"\"\"\n    Grid sampler Op for cuDNN Spatial Transformer.\n    \"\"\"\n\n    __props__ = ()\n    _cop_num_inputs = 2\n    _cop_num_outputs = 1\n    _f16_ok = True\n    check_input = False\n\n    def __init__(self):\n        DnnBase.__init__(\n            self, [\"c_code\/dnn_sptf_sampler.c\"], \"APPLY_SPECIFIC(dnn_sptf_sampler)\"\n        )\n\n    def make_node(self, img, grid):\n        \"\"\"\n        Create a grid sampler node for a cuDNN Spatial Transformer\n\n        Parameters\n        ----------\n        img : tensor\n            Images from which the pixels will be sampled. The implementation\n            assumes the tensor is in NCHW format, where N is the number of images,\n            C is the number of color channels, H is the height of the inputs, and\n            W is width of the inputs.\n\n        grid : GpuDnnTransformerGrid\n            Grid that contains the coordinates of the pixels to be sampled from\n            the inputs images.\n        \"\"\"\n        context_name = infer_context_name(img, grid)\n\n        img = gpu_contiguous(as_gpuarray_variable(img, context_name))\n        if img.type.ndim != 4:\n            raise TypeError(\"img must be a 4D tensor\")\n        elif img.dtype not in (\"float16\", \"float32\", \"float64\"):\n            raise TypeError(\"img type must be floating-point\")\n\n        grid = gpu_contiguous(as_gpuarray_variable(grid, context_name))\n        if grid.type.ndim != 4:\n            raise TypeError(\"grid must be a 4D tensor\")\n        elif grid.dtype not in (\"float16\", \"float32\", \"float64\"):\n            raise TypeError(\"grid type must be floating-point\")\n\n        out = GpuArrayType(\n            dtype=img.dtype,\n            broadcastable=img.type.ndim * (False,),\n            context_name=context_name,\n        )()\n\n        inputs = [img, grid]\n        outputs = [out]\n        return Apply(self, inputs, outputs)\n\n    def grad(self, inputs, grads):\n        img, grid = inputs\n        dy = grads[0]\n\n        dimg, dgrid = GpuDnnTransformerGradI()(img, grid, dy)\n        return [dimg, dgrid]\n\n\nclass GpuDnnTransformerGradI(DnnBase):\n    \"\"\"\n    Gradient of inputs Op for cuDNN Spatial Transformer.\n    \"\"\"\n\n    __props__ = ()\n    _cop_num_inputs = 3\n    _cop_num_outputs = 2\n    _f16_ok = True\n    check_input = False\n\n    def __init__(self):\n        DnnBase.__init__(self, [\"c_code\/dnn_sptf_gi.c\"], \"APPLY_SPECIFIC(dnn_sptf_gi)\")\n\n    def make_node(self, img, grid, dy):\n        context_name = infer_context_name(img, grid, dy)\n\n        img = as_gpuarray_variable(gpu_contiguous(img), context_name)\n        if img.ndim != 4:\n            raise TypeError(\"img must have 4 dimensions.\")\n\n        grid = as_gpuarray_variable(gpu_contiguous(grid), context_name)\n        if img.ndim != grid.ndim:\n            raise TypeError(\"grid should have the same number of dimensions as img\")\n\n        dy = as_gpuarray_variable(dy, context_name)\n        if dy.ndim != 4:\n            raise TypeError(\"dy must have 4 dimensions.\")\n\n        dimg = img.type()\n        dgrid = grid.type()\n\n        inputs = [img, grid, dy]\n        outputs = [dimg, dgrid]\n\n        return Apply(self, inputs, outputs)\n\n\nclass GpuDnnTransformerGradT(DnnBase):\n    \"\"\"\n    Gradient of affine transformations Op for cuDNN Spatial Transformer.\n    \"\"\"\n\n    __props__ = ()\n    _cop_num_inputs = 1\n    _cop_num_outputs = 1\n    _f16_ok = True\n    check_input = False\n\n    def __init__(self):\n        DnnBase.__init__(self, [\"c_code\/dnn_sptf_gt.c\"], \"APPLY_SPECIFIC(dnn_sptf_gt)\")\n\n    def make_node(self, dgrid):\n        context_name = infer_context_name(dgrid)\n\n        dgrid = as_gpuarray_variable(dgrid, context_name)\n        assert dgrid.dtype in (\"float16\", \"float32\", \"float64\")\n        assert dgrid.ndim == 4\n\n        dtheta = GpuArrayType(\n            dtype=dgrid.dtype,\n            broadcastable=(dgrid.type.ndim - 1) * (False,),\n            context_name=context_name,\n        )()\n        inputs = [dgrid]\n        outputs = [dtheta]\n\n        return Apply(self, inputs, outputs)\n\n\ndef dnn_spatialtf(img, theta, scale_width=1, scale_height=1):\n    \"\"\"\n    GPU spatial transformer using cuDNN from NVIDIA.\n\n    Parameters\n    ----------\n    img : tensor\n        Images to which the transformations will be applied. The implementation\n        assumes the tensor is in NCHW format, where N is the number of images,\n        C is the number of color channels, H is the height of the inputs, and\n        W is width of the inputs.\n    theta : tensor\n        Affine transformation tensor containing one affine transformation\n        matrix per image. ``theta`` is usually generated by the localization\n        network.\n    scale_height: float\n        A float specifying the scaling factor for the height of the output\n        image. A value of 1 will keep the original height of the input. Values\n        larger than 1 will upsample the input. Values below 1 will downsample\n        the input.\n    scale_width: float\n        A float specifying the scaling factor for the width of the output\n        image. A value of 1 will keep the original width of the input. Values\n        larger than 1 will upsample the input. Values below 1 will downsample\n        the input.\n\n    Returns\n    -------\n    out : tensor\n        Transformed images with width and height properly scaled.\n\n    Notes\n    -----\n    Currently, cuDNN only supports 2D transformations with 2x3 affine\n    transformation matrices.\n\n    Bilinear interpolation is the only grid sampler method available.\n    \"\"\"\n    out_dims = (\n        img.shape[0],\n        img.shape[1],\n        theano.tensor.ceil(img.shape[2] * scale_height),\n        theano.tensor.ceil(img.shape[3] * scale_width),\n    )\n    out_dims = tuple([as_scalar(v).astype(\"int64\") for v in out_dims])\n    # Setup spatial transformer\n    grid = GpuDnnTransformerGrid()(theta, out_dims)\n    sampler = GpuDnnTransformerSampler()(img, grid)\n    return sampler\n\n\ndef local_abstractconv_cudnn_graph(op, context_name, inputs, outputs):\n    if not isinstance(\n        op, (AbstractConv2d, AbstractConv2d_gradWeights, AbstractConv2d_gradInputs)\n    ):\n        return\n\n    if version(raises=False) < 6000 and op.filter_dilation != (1, 1):\n        return None\n\n    if op.unshared:\n        return None\n\n    if isinstance(op.border_mode, tuple) and any(\n        isinstance(p, tuple) for p in op.border_mode\n    ):\n        # Asymmetric padding not yet supported\n        return None\n\n    inp1 = inputs[0]\n    inp2 = inputs[1]\n\n    if not dnn_available(inp1.type.context_name):\n        return\n\n    if op.filter_flip:\n        conv_mode = \"conv\"\n    else:\n        conv_mode = \"cross\"\n\n    if isinstance(op, AbstractConv2d):\n        rval = dnn_conv(\n            inp1,\n            inp2,\n            border_mode=op.border_mode,\n            subsample=op.subsample,\n            dilation=op.filter_dilation,\n            direction_hint=\"forward!\",\n            conv_mode=conv_mode,\n            num_groups=op.num_groups,\n        )\n    elif isinstance(op, AbstractConv2d_gradWeights):\n        shape = (\n            inp2.shape[1],\n            inp1.shape[1] \/\/ op.num_groups,\n            inputs[2][0],\n            inputs[2][1],\n        )\n        rval = dnn_gradweight(\n            inp1,\n            inp2,\n            shape,\n            border_mode=op.border_mode,\n            subsample=op.subsample,\n            dilation=op.filter_dilation,\n            conv_mode=conv_mode,\n            num_groups=op.num_groups,\n        )\n    elif isinstance(op, AbstractConv2d_gradInputs):\n        shape = (\n            inp2.shape[0],\n            inp1.shape[1] * op.num_groups,\n            inputs[2][0],\n            inputs[2][1],\n        )\n        rval = dnn_gradinput(\n            inp1,\n            inp2,\n            shape,\n            border_mode=op.border_mode,\n            subsample=op.subsample,\n            dilation=op.filter_dilation,\n            conv_mode=conv_mode,\n            num_groups=op.num_groups,\n        )\n    return [rval]\n\n\ndef local_abstractconv3d_cudnn_graph(op, context_name, inputs, outputs):\n    if not isinstance(\n        op, (AbstractConv3d, AbstractConv3d_gradWeights, AbstractConv3d_gradInputs)\n    ):\n        return\n\n    if version(raises=False) < 6000 and op.filter_dilation != (1, 1, 1):\n        return None\n\n    inp1 = inputs[0]\n    inp2 = inputs[1]\n\n    if not dnn_available(inp1.type.context_name):\n        return\n\n    if op.filter_flip:\n        conv_mode = \"conv\"\n    else:\n        conv_mode = \"cross\"\n\n    if isinstance(op, AbstractConv3d):\n        rval = dnn_conv3d(\n            inp1,\n            inp2,\n            border_mode=op.border_mode,\n            subsample=op.subsample,\n            dilation=op.filter_dilation,\n            direction_hint=\"forward!\",\n            conv_mode=conv_mode,\n            num_groups=op.num_groups,\n        )\n    elif isinstance(op, AbstractConv3d_gradWeights):\n        shape = (\n            inp2.shape[1],\n            inp1.shape[1] \/\/ op.num_groups,\n            inputs[2][0],\n            inputs[2][1],\n            inputs[2][2],\n        )\n        rval = dnn_gradweight3d(\n            inp1,\n            inp2,\n            shape,\n            border_mode=op.border_mode,\n            subsample=op.subsample,\n            dilation=op.filter_dilation,\n            conv_mode=conv_mode,\n            num_groups=op.num_groups,\n        )\n    elif isinstance(op, AbstractConv3d_gradInputs):\n        shape = (\n            inp2.shape[0],\n            inp1.shape[1] * op.num_groups,\n            inputs[2][0],\n            inputs[2][1],\n            inputs[2][2],\n        )\n        rval = dnn_gradinput3d(\n            inp1,\n            inp2,\n            shape,\n            border_mode=op.border_mode,\n            subsample=op.subsample,\n            dilation=op.filter_dilation,\n            conv_mode=conv_mode,\n            num_groups=op.num_groups,\n        )\n    return [rval]\n\n\ndef local_abstract_batch_norm_train_cudnn(op, ctx_name, inputs, outputs):\n    x, scale, bias, epsilon, running_average_factor = inputs[:5]\n    running_mean = inputs[5] if len(inputs) > 5 else None\n    running_var = inputs[6] if len(inputs) > 6 else None\n\n    # convert axes to cuDNN mode\n    axes = tuple(op.axes)\n    if axes == (0,):\n        mode = \"per-activation\"\n    elif axes == (0,) + tuple(range(2, x.ndim)):\n        mode = \"spatial\"\n    else:\n        return None\n\n    try:\n        eps = theano.tensor.get_scalar_constant_value(epsilon)\n    except theano.tensor.NotScalarConstantError:\n        return None\n    if eps < 1e-5:\n        return None\n    try:\n        running_average_factor = theano.tensor.get_scalar_constant_value(\n            running_average_factor\n        )\n    except theano.tensor.NotScalarConstantError:\n        return None\n\n    ctx = infer_context_name(*inputs)\n    if not dnn_available(ctx):\n        return\n    x = as_gpuarray_variable(x, context_name=ctx)\n    scale = as_gpuarray_variable(scale, context_name=ctx)\n    bias = as_gpuarray_variable(bias, context_name=ctx)\n\n    inputs = [x, scale, bias, mode, eps, running_average_factor]\n    if running_mean is not None and running_var is not None:\n        inputs.append(running_mean)\n        inputs.append(running_var)\n\n    results = list(dnn_batch_normalization_train(*inputs))\n\n    return results\n\n\ndef local_abstract_batch_norm_train_grad_cudnn(op, ctx_name, inputs, outputs):\n    x, dy, scale, x_mean, x_invstd, epsilon = inputs\n\n    # input on gpu?  TODO what about the output?\n    x_on_gpu = isinstance(x.type, GpuArrayType) or (\n        x.owner and isinstance(x.owner.op, HostFromGpu)\n    )\n    dy_on_gpu = isinstance(dy.type, GpuArrayType) or (\n        dy.owner and isinstance(dy.owner.op, HostFromGpu)\n    )\n    if not (x_on_gpu or dy_on_gpu):\n        return None\n\n    # convert axes to cuDNN mode\n    axes = tuple(op.axes)\n    if axes == (0,):\n        mode = \"per-activation\"\n    elif axes == (0,) + tuple(range(2, x.ndim)):\n        mode = \"spatial\"\n    else:\n        return None\n\n    ndim = x.ndim\n    if ndim < 4:\n        x = theano.tensor.shape_padright(x, 4 - ndim)\n        dy = theano.tensor.shape_padright(dy, 4 - ndim)\n        scale = theano.tensor.shape_padright(scale, 4 - ndim)\n        x_mean = theano.tensor.shape_padright(x_mean, 4 - ndim)\n        x_invstd = theano.tensor.shape_padright(x_invstd, 4 - ndim)\n    elif ndim > 5:\n        x_shape = x.shape\n        params_shape = scale.shape\n        x = theano.tensor.flatten(x, 5)\n        dy = theano.tensor.flatten(dy, 5)\n        scale = theano.tensor.flatten(scale, 5)\n        x_mean = theano.tensor.flatten(x_mean, 5)\n        x_invstd = theano.tensor.flatten(x_invstd, 5)\n\n    try:\n        eps = theano.tensor.get_scalar_constant_value(epsilon)\n    except theano.tensor.NotScalarConstantError:\n        return None\n    if eps < 1e-5:\n        return None\n\n    ctx = infer_context_name(*inputs)\n    if not dnn_available(ctx):\n        return\n    x = as_gpuarray_variable(x, context_name=ctx)\n    dy = as_gpuarray_variable(dy, context_name=ctx)\n    scale = as_gpuarray_variable(scale, context_name=ctx)\n    x_mean = as_gpuarray_variable(x_mean, context_name=ctx)\n    x_invstd = as_gpuarray_variable(x_invstd, context_name=ctx)\n\n    g_wrt_inputs, g_wrt_scale, g_wrt_bias = GpuDnnBatchNormGrad(mode)(\n        x, dy, scale, x_mean, x_invstd, eps\n    )\n\n    if ndim < 4:\n        g_wrt_inputs = theano.tensor.flatten(g_wrt_inputs, ndim)\n        g_wrt_scale = theano.tensor.flatten(g_wrt_scale, ndim)\n        g_wrt_bias = theano.tensor.flatten(g_wrt_bias, ndim)\n    elif ndim > 5:\n        g_wrt_inputs = theano.tensor.reshape(g_wrt_inputs, x_shape)\n        g_wrt_scale = theano.tensor.reshape(g_wrt_scale, params_shape)\n        g_wrt_bias = theano.tensor.reshape(g_wrt_bias, params_shape)\n\n    return [g_wrt_inputs, g_wrt_scale, g_wrt_bias]\n\n\ndef local_abstract_batch_norm_inference_cudnn(op, ctx_name, inputs, outputs):\n    x, scale, bias, estimated_mean, estimated_variance, epsilon = inputs\n\n    axes = tuple(op.axes)\n    if axes == (0,):\n        mode = \"per-activation\"\n    elif axes == (0,) + tuple(range(2, x.ndim)):\n        mode = \"spatial\"\n    else:\n        return None\n\n    try:\n        eps = theano.tensor.get_scalar_constant_value(epsilon)\n    except theano.tensor.NotScalarConstantError:\n        return None\n    if eps < 1e-5:\n        return None\n\n    ctx = infer_context_name(*inputs)\n    if not dnn_available(ctx):\n        return\n    x = as_gpuarray_variable(x, context_name=ctx)\n    scale = as_gpuarray_variable(scale, context_name=ctx)\n    bias = as_gpuarray_variable(bias, context_name=ctx)\n    estimated_mean = as_gpuarray_variable(estimated_mean, context_name=ctx)\n    estimated_variance = as_gpuarray_variable(estimated_variance, context_name=ctx)\n\n    out = dnn_batch_normalization_test(\n        x, scale, bias, estimated_mean, estimated_variance, mode, eps\n    )\n\n    return [out]\n","label":1}
{"content":"from __future__ import unicode_literals\n\nimport httplib\n\nimport wac\n\n\nclass BalancedError(Exception):\n\n    def __str__(self):\n        attrs = ', '.join([\n            '{0}={1}'.format(k, repr(v))\n            for k, v in self.__dict__.iteritems()\n        ])\n        return '{0}({1})'.format(self.__class__.__name__, attrs)\n\n\nclass ResourceError(BalancedError):\n    pass\n\n\nclass NoResultFound(BalancedError):\n    pass\n\n\nclass MultipleResultsFound(BalancedError):\n    pass\n\n\nclass FundingSourceNotCreditable(Exception):\n    pass\n\n\ndef convert_error(ex):\n    if not hasattr(ex.response, 'data'):\n        return ex\n    return HTTPError.from_response(**ex.response.data)(ex)\n\n\nclass HTTPError(BalancedError, wac.Error):\n\n    class __metaclass__(type):\n\n        def __new__(meta_cls, name, bases, dikt):\n            cls = type.__new__(meta_cls, name, bases, dikt)\n            cls.types = [\n                getattr(cls, k)\n                for k in dir(cls)\n                if k.isupper() and isinstance(getattr(cls, k), basestring)\n            ]\n            cls.type_to_error.update(zip(cls.types, [cls] * len(cls.types)))\n            return cls\n\n    def __init__(self, requests_ex):\n        super(wac.Error, self).__init__(requests_ex)\n        self.status_code = requests_ex.response.status_code\n        data = getattr(requests_ex.response, 'data', {})\n        for k, v in data.get('errors', [{}])[0].iteritems():\n            setattr(self, k, v)\n\n    @classmethod\n    def format_message(cls, requests_ex):\n        data = getattr(requests_ex.response, 'data', {})\n        status = httplib.responses[requests_ex.response.status_code]\n        error = data['errors'][0]\n        status = error.pop('status', status)\n        status_code = error.pop('status_code',\n                                requests_ex.response.status_code)\n        desc = error.pop('description', None)\n        message = ': '.join(str(v) for v in [status, status_code, desc] if v)\n        return message\n\n    @classmethod\n    def from_response(cls, **data):\n        try:\n            err = data['errors'][0]\n            exc = cls.type_to_error.get(err['category_code'], HTTPError)\n        except:\n            exc = HTTPError\n        return exc\n\n    type_to_error = {}\n\n\nclass FundingInstrumentVerificationFailure(HTTPError):\n    pass\n\n\nclass BankAccountVerificationFailure(FundingInstrumentVerificationFailure):\n    AUTH_NOT_PENDING = 'bank-account-authentication-not-pending'\n    AUTH_FAILED = 'bank-account-authentication-failed'\n    AUTH_DUPLICATED = 'bank-account-authentication-already-exists'\n","label":1}
{"content":"# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#!\/usr\/bin\/python3\n\"\"\"A utility script that generates libc function stubs.\n\nIt automates the process of writing a wrapper for libc functions that are\nrelevant for PathAuditor.\n\nrun with: python3 template_generator.py path_auditor_libc.cc\n\nIMPORTANT: the script depends on a file from a third-party library that's not\nimported into Google's third_party: https:\/\/github.com\/zachriggle\/functions\n\"\"\"\n\nimport argparse\nfrom functions import functions\n\n\ndef argname(arg):\n  \"\"\"Format illegal argument names (like 'new').\"\"\"\n  name = arg.name\n  if arg.name == 'new' or arg.name == 'old':\n    name += 'path'\n\n  return name\n\n\ndef args(func):\n  \"\"\"Return a list of function's arguments with types.\"\"\"\n  arg_strings = []\n  for arg in func.args:\n    name = argname(arg)\n\n    if name == 'vararg':\n      arg_strings.append('...')\n    elif arg.type == 'char' and arg.derefcnt == 1:\n      arg_strings.append(f'const char *{name}')\n    elif arg.type == 'char' and arg.derefcnt == 2:\n      arg_strings.append(f'char *const {name}[]')\n    else:\n      arg_strings.append(f'{arg.type} {\"*\"*arg.derefcnt}{name}')\n\n  return arg_strings\n\n\ndef typedef(func):\n  \"\"\"Produce a typedef string for a function.\"\"\"\n  return (f'typedef {func.type}{\"*\"*func.derefcnt} (*orig_{func.name}_type)'\n          f'({\", \".join(args(func))});')\n\n\ndef dlsym_call(func):\n  \"\"\"Produce a string with a dlsym call and the original function call.\"\"\"\n  fun_args = ', '.join(argname(arg) for arg in func.args)\n  return (f'  orig_{func.name}_type orig_{func.name};\\n'\n          f'  orig_{func.name} = reinterpret_cast<orig_{func.name}_type>'\n          f'(dlsym(RTLD_NEXT, \\\"{func.name}\\\"));\\n'\n          f'  return orig_{func.name}({fun_args});')\n\n\ndef signature(func):\n  \"\"\"Produce a string with the function's signature and body.\"\"\"\n  return (f'{func.type}{\"*\"*func.derefcnt} {func.name}({\", \".join(args(func))})'\n          ' {\\n'\n          f'{dlsym_call(func)}\\n'\n          '}')\n\n\ndef main():\n  parser = argparse.ArgumentParser(description='Generate libc function stubs')\n  parser.add_argument('file', type=str, help='Output file')\n\n  cmdline_args = parser.parse_args()\n\n  func_names = [\n      'open',\n      'open64',\n      'openat',\n      'openat64',\n      'creat',\n      'creat64',\n      'chdir',\n      'chmod',\n      'chown',\n      'execl',\n      'execve',\n      'execle',\n      'execvp',\n      'execlp',\n      'fopen',\n      'fopen64',\n      'freopen',\n      'freopen64',\n      'truncate',\n      'truncate64',\n      'mkdir',\n      'link',\n      'linkat',\n      'unlink',\n      'unlinkat',\n      'chroot',\n      'lchown',\n      'remove',\n      'rmdir',\n      'mount',\n      'umount',\n      'umount2',\n      'rename',\n      'renameat',\n      'symlink',\n      'symlinkat',\n      'mkdirat',\n      'fchmodat',\n      'fchownat',\n  ]\n\n  func_info = [functions[name] for name in func_names]\n  assert len(func_names) == len(func_info)\n\n  with open(cmdline_args.file, 'w') as f:\n    f.write('#define _GNU_SOURCE\\n'\n            '#include <sys\/types.h>\\n'\n            '#include <stdio.h>\\n'\n            '#include <dlfcn.h>\\n\\n')\n\n    for func in func_info:\n      f.write(typedef(func) + '\\n')\n\n    f.write('\\nextern \\\"C\\\" {\\n')\n    for func in func_info:\n      f.write(signature(func) + '\\n\\n')\n    f.write('}')\n\n\nif __name__ == '__main__':\n  main()\n","label":1}
{"content":"import re\nimport numpy as np\nimport logging\n\ntry:\n    import pycuda.driver as cuda\n    import pycuda.autoinit\n    from pycuda.compiler import SourceModule, DEFAULT_NVCC_FLAGS\n    from pycuda import gpuarray\n\n    cuda_available = True\n\nexcept ImportError:\n    cuda_available = False\n\n    class gpuarray:\n        pass\n\ndef compile_cuda_kernel(cuda_kernel_code):\n    \"\"\"\n    compiles a cuda kernel and return compiled module\n    \"\"\"\n    try:\n        cuda_code = cuda_kernel_code if 1 else replace_local_floats_with_double(cuda_kernel_code)\n        logging.debug(\"Compiling cuda code:\\n\" + cuda_code)\n        mod = SourceModule(cuda_code, options=DEFAULT_NVCC_FLAGS + ['--use_fast_math'])\n    except cuda.CompileError as e:\n        logging.error(cuda_code)\n        logging.error(\"CUDA compilation error:\")\n        logging.error(e.stderr)\n        raise e\n    return mod\n\ndef cuda_function(mod, function_name, datadim, additional_arguments = ()):\n    \"\"\"\n    Returns a callable for function <function_name> from the compile cuda kernel <mod>\n    setting up block and grid sizes for a 1D data block with datadim elements.\n\n    The callable's signature matches the cuda kernel signature (additional_arguments are appended).\n    \"\"\"\n    cuda_func = mod.get_function(function_name)\n    block = (min(datadim, cuda_func.MAX_THREADS_PER_BLOCK), 1, 1)\n    grid = (datadim\/\/block[0],1,1)\n    logger = lambda x: logging.debug(\"Cuda function %s execution time: %.2f ms\", function_name, x*1000) or x\n    result = lambda *args: logger(cuda_func(*(args+additional_arguments), grid=grid, block=block, time_kernel=True))\n    return result\n\nclass NumpyAdapter:\n    \"\"\"\n    To keep algorithms free of cuda clutter, we use these adapter classes to map\n    calls to either numpy or gpuarray.\n    \"\"\"\n    def __init__(self, floattype = np.float64):\n        self.floattype = floattype\n\n    def zeros(self, *args, **kw):\n        kw['dtype'] = self.floattype\n        return np.zeros(*args, **kw)\n\n    def to_np(self, matrix):\n        return matrix\n\n    def from_np(self, matrix):\n        return matrix.astype(self.floattype)\n\n    def scalar(self, s):\n        return self.floattype(s)\n\n    def reshape(self, *args):\n        return np.reshape(*args)\n\n    def flatten(self, matrix):\n        return matrix.flatten()\n\n    def copyto(self, tgt, src):\n        np.copyto(tgt, src)\n\n    def implem(self):\n        return 'numpy'\n\nclass PyCudaAdapter:\n    \"\"\"\n    To keep algorithms free of cuda clutter, we use these adapter classes to map\n    calls to either numpy or gpuarray.\n    \"\"\"\n    def __init__(self, floattype = np.float32):\n        self.floattype = floattype\n\n    def zeros(self, *args, **kw):\n        kw['dtype'] = self.floattype\n        return gpuarray.zeros(*args, **kw)\n\n    def to_np(self, matrix):\n        return matrix.get()\n\n    def from_np(self, matrix):\n        return gpuarray.to_gpu(matrix.astype(self.floattype))\n\n    def scalar(self, s):\n        return self.floattype(s)\n\n    def reshape(self, *args):\n        return gpuarray.reshape(*args)\n\n    def flatten(self, matrix):\n        return gpuarray.reshape(matrix, int(np.prod(matrix.shape)))\n\n    def copyto(self, tgt, src):\n        tgt[:] = src\n\n    def implem(self):\n        return 'pycuda'\n\ndef float_constant(v):\n    \"\"\"\n    return a floating point constant for usage in cuda kernels (a string).\n    \"\"\"\n    # TODO maybe use hexadecimal floating point constants here\n    return \"%.10ef\" % v\n\ndef indent(code, level):\n    \"\"\"\n    indent code to the given level\n    \"\"\"\n    return code.replace(\"\\n\", \"\\n\" + (\" \"*level))\n\ndef sub2ind(idx, shape):\n    \"\"\"\n    Calculates a linear index from multuple sub indices (like the matlab function).\n    Intended to be used in cuda kernel generators.\n    \"\"\"\n    assert(len(idx) == len(shape))\n    if all([not isinstance(i, str) for i in idx]):\n        res = 0\n        for d,i in enumerate(idx):\n            res += i*np.prod(shape[(d+1):])\n        return res\n    else:\n\n        res = \"(\" + (\"+\".join([\"(%s)*%d\" % (i, np.prod(shape[(d+1):])) for d,i in enumerate(idx)])) + \")\"\n    return res\n\ndef ind2sub(ind, shape):\n    \"\"\"\n    Calculates a sub indices from a linear index (like the matlab function).\n    Intended to be used in cuda kernel generators.\n    \"\"\"\n    if not isinstance(ind, str):\n        res = []\n        for d in range(len(shape)):\n            res.append( (ind % int(np.prod(shape[d:])) \/\/ int(np.prod(shape[d+1:]))) )\n    else:\n        res = []\n        for d in range(len(shape)):\n            exp1 = \"\"\n            exp2 = \"\"\n            if d > 0:\n                exp1 = \" %% %d\" % int(np.prod(shape[d:]))\n            if d < len(shape)-1:\n                exp2 = \"\/ %d\" % int(np.prod(shape[(d+1):]))\n            res.append( \"((%(ind)s)%(exp1)s)%(exp2)s\" % locals() )\n    return res\n\ndef ind2subCode(ind, shape, target_var_names):\n    \"\"\"\n    Same as ind2sub, but it will be more efficient by reusing results.\n    Pass the variable names to be defined and initialized in target_var_names.\n    \"\"\"\n    code = \"\"\n    av = target_var_names[-1]\n    code += \"int %(av)s = 0;\\n\" % locals()\n    for d in range(len(shape)):\n        v = target_var_names[d]\n        divisor = int(np.prod(shape[(d+1):]))\n        if d != len(shape) - 1:\n            code += \"int \"\n        if divisor != 1:\n            code += \"%(v)s = ( (%(ind)s) + %(av)s ) \/ %(divisor)d;\\n\" % locals()\n        else:\n            code += \"%(v)s = ( (%(ind)s) + %(av)s );\\n\" % locals()\n        if d != len(shape) - 1:\n            code += \"%(av)s -= %(v)s*%(divisor)d;\\n\" % locals()\n    return code\n\nclass NodeReverseInOut(object):\n    \"\"\"\n    When generating cuda kernels, the graphs are traversed implicitely.\n    Sometimes it is necessary to \"reverse\" the node such that the forward\n    operation gets the adjoint and vice versa. This class simulates a single\n    reversed node.\n    \"\"\"\n    def __init__(self, n, parent):\n        self.n = n\n        self.parent = parent\n\n    def adjoint_cuda_kernel(self, *args, **kw):\n        args = (a if not a is self.parent else self.parent.o for a in args)\n        return self.n.forward_cuda_kernel(*args, **kw)\n\n    def forward_cuda_kernel(self, *args,**kw):\n        args = (a if not a is self.parent else self.parent.o for a in args)\n        return self.n.adjoint_cuda_kernel(*args, **kw)\n\n    def cuda_kernel_available(self):\n        return self.n.cuda_kernel_available()\n\n    @property\n    def size(self):\n        return self.n.size\n\nclass ReverseInOut(object):\n    \"\"\"\n    When generating cuda kernels, the graphs are traversed implicitely.\n    Sometimes it is necessary to \"reverse\" the node such that the forward\n    operation gets the adjoint and vice versa. This is the helper class for\n    the operation.\n    \"\"\"\n    def __init__(self, o, reverseNodes = True):\n        self.o = o\n        self.reverseNodes = reverseNodes\n        self.in_nodes = {}\n        self.out_nodes = {}\n\n    def input_nodes(self, n):\n        if self.reverseNodes:\n            if not n in self.in_nodes:\n                self.in_nodes[n] = list([NodeReverseInOut(x, self) for x in self.o.output_nodes(n)])\n            return self.in_nodes[n]\n        else:\n            return self.o.output_nodes(n)\n\n    def output_nodes(self, n):\n        if self.reverseNodes:\n            if not n in self.out_nodes:\n                self.out_nodes[n] = list([NodeReverseInOut(x, self) for x in self.o.input_nodes(n)])\n            return self.out_nodes[n]\n        else:\n            return self.o.input_nodes(n)\n\ndef replace_local_floats_with_double(src):\n    \"\"\"\n    This function replaces all internal float variables and constants with doubles,\n    but keeps the pointer types.\n    \"\"\"\n    Rd = re.compile(r\"\\bfloat\\b(?! *[*])\")\n    Rc = re.compile(r\"\\b(-?(0(\\.\\d*)?|([1-9]\\d*\\.?\\d*)|(\\.\\d+))([Ee][+-]?\\d+)?)f\\b\")\n\n    return Rc.subn(r\"\\1\", Rd.subn(\"double\",src)[0])[0]\n\nclass ProxyNode:\n    \"\"\"\n    This class is used as a dummy node when the computation graph is split up\n    into multiple parts.\n    \"\"\"\n    def __init__(self, n, argname, shape):\n        self.n = n\n        self.argname = argname\n        self.shape = shape\n\n    def adjoint_cuda_kernel(self, cg, num_tmp_vars, idx, parent):\n        var = \"var_%d\" % num_tmp_vars\n        num_tmp_vars += 1\n        argname = self.argname\n        linidx = sub2ind(idx, self.shape)\n        code = \"float %(var)s = %(argname)s[%(linidx)s];\\n\" % locals()\n        return code, var, num_tmp_vars\n\n    def forward_cuda_kernel(self, *args):\n        return self.adjoint_cuda_kernel(*args)\n\n    @property\n    def size(self):\n        return self.n.size\n\nclass CudaSubGraph:\n    \"\"\"\n    This class splits a linear operation graph into multiple computable subgraphs.\n    The constructor is intended to be called with the end node of a comp graph.\n    The comp graph is split in pieces with single kernels, and the pieces\n    are recursively stored in the \"dependent_subgraphs\" member.\n    \"\"\"\n    instance_cnt = 0\n\n    def __init__(self, get_input_nodes, get_output_nodes, endnode):\n        assert endnode.cuda_kernel_available()\n        self.subgraph_id = CudaSubGraph.instance_cnt\n        CudaSubGraph.instance_cnt += 1\n        self.end = endnode\n        self._input_nodes = {}\n        self._output_nodes = {}\n        self.kernel_nodes = []\n        self.nokernel_nodes = [] # where the graph must be splitted\n        self.nokernel_results = {}\n        self.nokernel_inputs = {}\n        self.nokernel_proxynodes = {}\n        active_nodes = [self.end]\n        visited_nodes = {}\n        nokernel_innodes = []\n        from ..lin_ops.sum import copy\n        from ..lin_ops.vstack import vstack\n        while len(active_nodes) > 0:\n            n = active_nodes.pop(0)\n            if n in visited_nodes:\n                continue\n            visited_nodes[n] = True\n            try:\n                if not n in self._output_nodes:\n                    self._output_nodes[n] = get_output_nodes(n)\n            except KeyError:\n                pass\n            if n.cuda_kernel_available():\n                self.kernel_nodes.append(n)\n                try:\n                    innodes = get_input_nodes(n)\n                    # avoid situations where the same node instance is referenced\n                    # multiple times in innodes. This situation is solved by\n                    # inserting copy nodes\n                    new_innodes = []\n                    replacements = {}\n                    for innidx, inn in enumerate(innodes):\n                        if not isinstance(n, vstack) and inn in innodes[innidx+1:]:\n                            # insert a copy node between inn and n\n                            nn = copy(n.shape)\n                            if not inn in replacements: replacements[inn] = []\n                            replacements[inn].append(nn)\n                            new_innodes.append(nn)\n                            self._output_nodes[nn] = [n]\n                            self._input_nodes[nn] = [inn]\n                            active_nodes.append(inn)\n                        else:\n                            new_innodes.append(inn)\n                            active_nodes.append(inn)\n                    for inn in replacements:\n                        output_nodes = get_output_nodes(inn)\n                        new_output_nodes = []\n                        idx = 0\n                        for onn in output_nodes:\n                            if onn is n and idx < len(replacements[inn]):\n                                new_output_nodes.append(replacements[inn][idx])\n                                idx += 1\n                            else:\n                                new_output_nodes.append(onn)\n                        self._output_nodes[inn] = new_output_nodes\n                    self._input_nodes[n] = new_innodes\n                except KeyError:\n                    pass\n            else:\n                logging.info(\"%s: no cuda kernel available\", str(n))\n                cn = [n]\n                while 1:\n                    innodes = get_input_nodes(cn[0])\n                    assert (len(innodes) == 1)\n                    assert (len(get_output_nodes(cn[0])) == 1)\n                    if innodes[0].cuda_kernel_available():\n                        nokernel_innodes += innodes\n                        break\n                    cn = [innodes[0]] + cn\n                self.nokernel_nodes.append(cn)\n        self.dependent_subgraphs = []\n        for n in nokernel_innodes:\n            dsg = CudaSubGraph(get_input_nodes, get_output_nodes, n)\n            self.dependent_subgraphs.append(dsg)\n\n        self.orig_input_nodes = get_input_nodes\n        self.orig_output_nodes = get_output_nodes\n\n    def __str__(self):\n        res = \"CudaSubGraph(\\n\"\n        for n in self.kernel_nodes:\n            res += \"  %s <- \" % repr(n)\n            res += \", \".join([\"%s\" % repr(x) for x in self._input_nodes.get(n, [])])\n            res += \"\\n\"\n        res += \")\"\n        return res\n\n    def visualize(self, dot = None):\n        import graphviz\n        from IPython.display import display\n        root = False\n        if not dot:\n            root = True\n            dot = graphviz.Digraph()\n        for csg in self.dependent_subgraphs:\n            csg.visualize(dot)\n        nodes = {}\n        visited = {}\n        active = [self.end]\n        while len(active) > 0:\n            n = active.pop(0)\n            if not n in nodes:\n                nodes[n] = 'N%d' % len(nodes)\n                dot.node(nodes[n], str(type(n)))\n                try:\n                    innodes = self.input_nodes(n)\n                    for inn in innodes:\n                        active.append(inn)\n                except KeyError:\n                    pass\n        active = [self.end]\n        while len(active) > 0:\n            n = active.pop(0)\n            if not n in visited:\n                visited[n] = True\n                try:\n                    innodes = self.input_nodes(n)\n                    for inn in self.input_nodes(n):\n                        active.append(inn)\n                        dot.edge(nodes[inn], nodes[n])\n                except KeyError:\n                    pass\n        if root:\n            display(dot)\n\n    def input_nodes(self, n):\n        \"\"\"\n        returns the input nodes of node n using proxy nodes wherever necessary\n        \"\"\"\n        innodes = self._input_nodes[n]\n        res = []\n        for inn in innodes:\n            noKernelNode = False\n            for cn in self.nokernel_nodes:\n                if inn in cn:\n                    noKernelNode = True\n            if noKernelNode:\n                res.append(self.nokernel_proxynodes[inn])\n            else:\n                res.append(inn)\n        #print(\"innodes(%s) -> %s\" % (repr(n), res))\n        return res\n\n    def output_nodes(self, n):\n        \"\"\"\n        returns the output nodes of node n (this is called seldomly)\n        \"\"\"\n        return self._output_nodes[n]\n\n    def gen_code(self, fcn, parent = None, shape = None):\n        \"\"\"\n        generates the cuda kernel code. fcn should either be \"forward_cuda_kernel\" or \"adjoint_cuda_kernel\"\n        \"\"\"\n        self.cuda_args = []\n        self.fcn = fcn\n        # do we need additional arguments for the kernel except x (=input) and y (=output)\n        for n in self.kernel_nodes:\n            try:\n                buffers = n.cuda_additional_buffers()\n            except AttributeError:\n                buffers = []\n            for aname, aval in buffers:\n                if type(aval) == np.ndarray and aval.dtype == np.int32:\n                    aval = gpuarray.to_gpu(aval)\n                    self.cuda_args.append( (aname, aval, \"int\") )\n                else:\n                    aval = gpuarray.to_gpu(aval.astype(np.float32))\n                    self.cuda_args.append( (aname, aval, \"float\") )\n\n        for cn in self.nokernel_nodes:\n            for n in cn:\n                o = self.orig_output_nodes(n)\n                assert(len(o) == 1)\n                o = o[0]\n                rshape = n.shape if fcn == \"forward_cuda_kernel\" else o.shape\n                o = gpuarray.zeros(rshape, dtype=np.float32)\n                self.nokernel_results[n] = o\n            n = cn[-1]\n            self.cuda_args.append( (\"linop_proxy_output_%d\" % n.linop_id, o, \"float\") )\n            self.nokernel_proxynodes[n] = ProxyNode(n, self.cuda_args[-1][0], rshape)\n\n        add_args = \"\".join((\", %s *%s\" % (x[2], x[0]) for x in self.cuda_args))\n\n        cg = self if fcn == \"forward_cuda_kernel\" else ReverseInOut(self, reverseNodes=False)\n        self.shape = shape if not shape is None else self.end.shape\n        # generate the cuda kernel for this subgraph\n        cucode, var, num_tmp_vars = getattr(self.end, fcn)(cg, 0, ind2sub(\"yidx\", self.shape), parent)\n        cucode = indent(cucode, 8)\n        dimy = int(np.prod(self.shape))\n        subgraph_id = self.subgraph_id\n        code  = \"\"\"\\\n__global__ void %(fcn)s_%(subgraph_id)d(const float *x, float *y%(add_args)s)\n{\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    for( int yidx = index; yidx < %(dimy)d; yidx += stride )\n    {\n        %(cucode)s\n        y[yidx] = %(var)s;\n    }\n}\n\n\"\"\" % locals()\n\n        # generate the cuda kernels for the dependent subgraphs\n        for i,dsg in enumerate(self.dependent_subgraphs):\n            cn = self.nokernel_nodes[i]\n            if fcn == \"forward_cuda_kernel\":\n                parent = self.orig_input_nodes(cn[0])\n                assert(len(parent) == 1)\n                parent = parent[0]\n            else:\n                parent = cn[0]\n            dsg.gen_code(fcn, cn[0], parent.shape)\n            for ni, n in enumerate(cn):\n                self.nokernel_inputs[n] = gpuarray.zeros(dsg.shape, dtype=np.float32) if ni == 0 else self.nokernel_results[cn[ni-1]]\n\n        self._cuda_code = code\n        self.cuda_mod = compile_cuda_kernel(code)\n        arg_vals = tuple(x[1] for x in self.cuda_args)\n        self.cuda_kernel_func = cuda_function(self.cuda_mod, \"%(fcn)s_%(subgraph_id)d\" % locals(), dimy, arg_vals)\n\n    def apply(self, x, y):\n        \"\"\"\n        apply the compiled cuda kernels and all dependent subgraphs\n        \"\"\"\n        t = 0.0\n        for i,dsg in enumerate(self.dependent_subgraphs):\n            cn = self.nokernel_nodes[i]\n            n = cn[0]\n            t += dsg.apply(x, self.nokernel_inputs[n])\n        for i,cn in enumerate(self.nokernel_nodes):\n            if self.fcn == \"forward_cuda_kernel\":\n                for n in cn:\n                    n.forward_cuda([self.nokernel_inputs[n]], [self.nokernel_results[n]])\n            else:\n                for n in cn:\n                    n.adjoint_cuda([self.nokernel_inputs[n]], [self.nokernel_results[n]])\n        t += self.cuda_kernel_func(x, y)\n        self.output = gpuarray.reshape(y, self.shape)\n        return t\n\n    @property\n    def cuda_code(self):\n        \"\"\"\n        return the cuda code for this and the dependent kernels\n        \"\"\"\n        return self._cuda_code + \"\\n\".join([x.cuda_code for x in self.dependent_subgraphs])\n","label":1}
{"content":"# sqlalchemy\/log.py\n# Copyright (C) 2006-2021 the SQLAlchemy authors and contributors\n# <see AUTHORS file>\n# Includes alterations by Vinay Sajip vinay_sajip@yahoo.co.uk\n#\n# This module is part of SQLAlchemy and is released under\n# the MIT License: https:\/\/www.opensource.org\/licenses\/mit-license.php\n\n\"\"\"Logging control and utilities.\n\nControl of logging for SA can be performed from the regular python logging\nmodule.  The regular dotted module namespace is used, starting at\n'sqlalchemy'.  For class-level logging, the class name is appended.\n\nThe \"echo\" keyword parameter, available on SQLA :class:`_engine.Engine`\nand :class:`_pool.Pool` objects, corresponds to a logger specific to that\ninstance only.\n\n\"\"\"\n\nimport logging\nimport sys\n\n\n# set initial level to WARN.  This so that\n# log statements don't occur in the absence of explicit\n# logging being enabled for 'sqlalchemy'.\nrootlogger = logging.getLogger(\"sqlalchemy\")\nif rootlogger.level == logging.NOTSET:\n    rootlogger.setLevel(logging.WARN)\n\n\ndef _add_default_handler(logger):\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(\n        logging.Formatter(\"%(asctime)s %(levelname)s %(name)s %(message)s\")\n    )\n    logger.addHandler(handler)\n\n\n_logged_classes = set()\n\n\ndef _qual_logger_name_for_cls(cls):\n    return (\n        getattr(cls, \"_sqla_logger_namespace\", None)\n        or cls.__module__ + \".\" + cls.__name__\n    )\n\n\ndef class_logger(cls):\n    logger = logging.getLogger(_qual_logger_name_for_cls(cls))\n    cls._should_log_debug = lambda self: logger.isEnabledFor(logging.DEBUG)\n    cls._should_log_info = lambda self: logger.isEnabledFor(logging.INFO)\n    cls.logger = logger\n    _logged_classes.add(cls)\n    return cls\n\n\nclass Identified(object):\n    logging_name = None\n\n    def _should_log_debug(self):\n        return self.logger.isEnabledFor(logging.DEBUG)\n\n    def _should_log_info(self):\n        return self.logger.isEnabledFor(logging.INFO)\n\n\nclass InstanceLogger(object):\n    \"\"\"A logger adapter (wrapper) for :class:`.Identified` subclasses.\n\n    This allows multiple instances (e.g. Engine or Pool instances)\n    to share a logger, but have its verbosity controlled on a\n    per-instance basis.\n\n    The basic functionality is to return a logging level\n    which is based on an instance's echo setting.\n\n    Default implementation is:\n\n    'debug' -> logging.DEBUG\n    True    -> logging.INFO\n    False   -> Effective level of underlying logger (\n    logging.WARNING by default)\n    None    -> same as False\n    \"\"\"\n\n    # Map echo settings to logger levels\n    _echo_map = {\n        None: logging.NOTSET,\n        False: logging.NOTSET,\n        True: logging.INFO,\n        \"debug\": logging.DEBUG,\n    }\n\n    def __init__(self, echo, name):\n        self.echo = echo\n        self.logger = logging.getLogger(name)\n\n        # if echo flag is enabled and no handlers,\n        # add a handler to the list\n        if self._echo_map[echo] <= logging.INFO and not self.logger.handlers:\n            _add_default_handler(self.logger)\n\n    #\n    # Boilerplate convenience methods\n    #\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"Delegate a debug call to the underlying logger.\"\"\"\n\n        self.log(logging.DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"Delegate an info call to the underlying logger.\"\"\"\n\n        self.log(logging.INFO, msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"Delegate a warning call to the underlying logger.\"\"\"\n\n        self.log(logging.WARNING, msg, *args, **kwargs)\n\n    warn = warning\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger.\n        \"\"\"\n        self.log(logging.ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"Delegate an exception call to the underlying logger.\"\"\"\n\n        kwargs[\"exc_info\"] = 1\n        self.log(logging.ERROR, msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"Delegate a critical call to the underlying logger.\"\"\"\n\n        self.log(logging.CRITICAL, msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"Delegate a log call to the underlying logger.\n\n        The level here is determined by the echo\n        flag as well as that of the underlying logger, and\n        logger._log() is called directly.\n\n        \"\"\"\n\n        # inline the logic from isEnabledFor(),\n        # getEffectiveLevel(), to avoid overhead.\n\n        if self.logger.manager.disable >= level:\n            return\n\n        selected_level = self._echo_map[self.echo]\n        if selected_level == logging.NOTSET:\n            selected_level = self.logger.getEffectiveLevel()\n\n        if level >= selected_level:\n            self.logger._log(level, msg, args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"Is this logger enabled for level 'level'?\"\"\"\n\n        if self.logger.manager.disable >= level:\n            return False\n        return level >= self.getEffectiveLevel()\n\n    def getEffectiveLevel(self):\n        \"\"\"What's the effective level for this logger?\"\"\"\n\n        level = self._echo_map[self.echo]\n        if level == logging.NOTSET:\n            level = self.logger.getEffectiveLevel()\n        return level\n\n\ndef instance_logger(instance, echoflag=None):\n    \"\"\"create a logger for an instance that implements :class:`.Identified`.\"\"\"\n\n    if instance.logging_name:\n        name = \"%s.%s\" % (\n            _qual_logger_name_for_cls(instance.__class__),\n            instance.logging_name,\n        )\n    else:\n        name = _qual_logger_name_for_cls(instance.__class__)\n\n    instance._echo = echoflag\n\n    if echoflag in (False, None):\n        # if no echo setting or False, return a Logger directly,\n        # avoiding overhead of filtering\n        logger = logging.getLogger(name)\n    else:\n        # if a specified echo flag, return an EchoLogger,\n        # which checks the flag, overrides normal log\n        # levels by calling logger._log()\n        logger = InstanceLogger(echoflag, name)\n\n    instance.logger = logger\n\n\nclass echo_property(object):\n    __doc__ = \"\"\"\\\n    When ``True``, enable log output for this element.\n\n    This has the effect of setting the Python logging level for the namespace\n    of this element's class and object reference.  A value of boolean ``True``\n    indicates that the loglevel ``logging.INFO`` will be set for the logger,\n    whereas the string value ``debug`` will set the loglevel to\n    ``logging.DEBUG``.\n    \"\"\"\n\n    def __get__(self, instance, owner):\n        if instance is None:\n            return self\n        else:\n            return instance._echo\n\n    def __set__(self, instance, value):\n        instance_logger(instance, echoflag=value)\n","label":1}
{"content":"# PyZX - Python library for quantum circuit rewriting\n#        and optimisation using the ZX-calculus\n# Copyright (C) 2021 - Aleks Kissinger and John van de Wetering\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\"\"\"\nThis module contains objective functions to guide local search over ZX-diagrams. The wgc method defines a measure of circuit complexity -- a weighted gate count where 2-qubit counts incur a higher cost. The g_wgc takes a ZX-diagram as input and optionally applies various optimizations before measuring the complexity of the circuit obtained via extraction.\n\"\"\"\n\nimport sys\nif __name__ == '__main__':\n    sys.path.append('..')\nfrom pyzx.extract import extract_circuit\nfrom pyzx.simplify import full_reduce\nfrom pyzx.optimize import basic_optimization\n\n# Weighted gate count\ndef wgc(c, two_qb_weight=10):\n    \"\"\"A measure of the complexity of a given circuit. By default, 2-qubit\n    gates are treated as 10X more costly than single-qubit gates\"\"\"\n\n    c_tmp = c.to_basic_gates()\n    total = len(c_tmp.gates)\n    n2 = c_tmp.twoqubitcount()\n    single_qubit_count = total - n2\n    return two_qb_weight * n2 + single_qubit_count\n\n# Weighted gate count of a ZX-diagram\ndef g_wgc(g, two_qb_weight=10, g_simplify=True, c_simplify=True):\n    \"\"\"A measure of the complexity of the circuit obtained from a a ZX-diagram\n    upon extraction using the above measure of circuit complexity\"\"\"\n\n    g_tmp = g.copy()\n    if g_simplify:\n        full_reduce(g_tmp)\n\n    c = extract_circuit(g_tmp.copy()).to_basic_gates()\n\n    if c_simplify:\n        c = basic_optimization(c)\n\n    return wgc(c, two_qb_weight=two_qb_weight)\n","label":1}
{"content":"# -*- coding: utf-8 -*-\n# This software is distributed under the two-clause BSD license.\n# Copyright (c) The django-ldapdb project\n\nfrom __future__ import unicode_literals\n\nimport time\n\nimport factory\nimport factory.django\nimport factory.fuzzy\nimport ldap\nimport volatildap\nfrom django.conf import settings\nfrom django.contrib.auth import hashers as auth_hashers\nfrom django.contrib.auth import models as auth_models\nfrom django.core import management\nfrom django.db import connections\nfrom django.db.models import Count, Q\nfrom django.test import TestCase\nfrom django.utils import timezone\n\nfrom examples.models import ConcreteGroup, LdapGroup, LdapMultiPKRoom, LdapUser, RCLdapUser, RCLdapGroup\nfrom ldapdb.backends.ldap.compiler import SQLCompiler, query_as_ldap\n\nucbgroup = ('testucb,ou=UCB,ou=Groups,dc=rc,dc=int,dc=colorado,dc=edu', {\n    'objectClass': ['top', 'organizationalUnit'], 'ou': ['groups']})\nucb_user = ('uid=dup_user,ou=users,ou=UCB,dc=rc,dc=int,dc=colorado,dc=edu', {\n    'objectClass': ['posixAccount', 'shadowAccount', 'inetOrgPerson'],\n    'cn': ['Bar Test'], 'givenName': ['ucb_user'], 'sn': ['Bar'],\n    'uid': ['dup_user'], 'uidNumber': ['2001'], 'gidNumber': ['1000'],\n    'homeDirectory': ['\/home\/dup_user'], 'loginShell': ['\/bin\/bash'],\n    'jpegPhoto': []})\n\n\ngroups = ('ou=groups,dc=example,dc=org', {\n    'objectClass': ['top', 'organizationalUnit'], 'ou': ['groups']})\npeople = ('ou=people,dc=example,dc=org', {\n    'objectClass': ['top', 'organizationalUnit'], 'ou': ['groups']})\ncontacts = ('ou=contacts,ou=groups,dc=example,dc=org', {\n    'objectClass': ['top', 'organizationalUnit'], 'ou': ['groups']})\nusers = ('ou=users,ou=people,dc=example,dc=org', {\n    'objectClass': ['top', 'organizationalUnit'], 'ou': ['users']})\nrooms = ('ou=rooms,dc=example,dc=org', {\n    'objectClass': ['top', 'organizationalUnit'], 'ou': ['rooms']})\nfoogroup = ('cn=foogroup,ou=groups,dc=example,dc=org', {\n    'objectClass': ['posixGroup'], 'memberUid': ['foouser', 'baruser'],\n    'gidNumber': ['1000'], 'cn': ['foogroup']})\nbargroup = ('cn=bargroup,ou=groups,dc=example,dc=org', {\n    'objectClass': ['posixGroup'], 'memberUid': ['zoouser', 'baruser'],\n    'gidNumber': ['1001'], 'cn': ['bargroup']})\nwizgroup = ('cn=wizgroup,ou=groups,dc=example,dc=org', {\n    'objectClass': ['posixGroup'], 'memberUid': ['wizuser', 'baruser'],\n    'gidNumber': ['1002'], 'cn': ['wizgroup']})\nfoouser = ('uid=foouser,ou=people,dc=example,dc=org', {\n    'cn': [b'F\\xc3\\xb4o Us\\xc3\\xa9r'],\n    'objectClass': ['posixAccount', 'shadowAccount', 'inetOrgPerson'],\n    'loginShell': ['\/bin\/bash'],\n    'jpegPhoto': [\n        b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff'\n        b'\\xfe\\x00\\x1cCreated with GIMP on a Mac\\xff\\xdb\\x00C\\x00\\x05\\x03\\x04'\n        b'\\x04\\x04\\x03\\x05\\x04\\x04\\x04\\x05\\x05\\x05\\x06\\x07\\x0c\\x08\\x07\\x07\\x07'\n        b'\\x07\\x0f\\x0b\\x0b\\t\\x0c\\x11\\x0f\\x12\\x12\\x11\\x0f\\x11\\x11\\x13\\x16\\x1c'\n        b'\\x17\\x13\\x14\\x1a\\x15\\x11\\x11\\x18!\\x18\\x1a\\x1d\\x1d\\x1f\\x1f\\x1f\\x13'\n        b'\\x17\"$\"\\x1e$\\x1c\\x1e\\x1f\\x1e\\xff\\xdb\\x00C\\x01\\x05\\x05\\x05\\x07\\x06\\x07'\n        b'\\x0e\\x08\\x08\\x0e\\x1e\\x14\\x11\\x14\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e'\n        b'\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e'\n        b'\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e'\n        b'\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\xff\\xc0\\x00\\x11\\x08\\x00\\x08\\x00\\x08\\x03'\n        b'\\x01\"\\x00\\x02\\x11\\x01\\x03\\x11\\x01\\xff\\xc4\\x00\\x15\\x00\\x01\\x01\\x00\\x00'\n        b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\xff\\xc4\\x00'\n        b'\\x19\\x10\\x00\\x03\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n        b'\\x00\\x00\\x01\\x02\\x06\\x11A\\xff\\xc4\\x00\\x14\\x01\\x01\\x00\\x00\\x00\\x00\\x00'\n        b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xc4\\x00\\x14\\x11\\x01'\n        b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff'\n        b'\\xda\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00\\x9d\\xf29wU5Q\\xd6'\n        b'\\xfd\\x00\\x01\\xff\\xd9'],\n    'uidNumber': ['2000'], 'gidNumber': ['1000'], 'sn': [b'Us\\xc3\\xa9r'],\n    'homeDirectory': ['\/home\/foouser'], 'givenName': [b'F\\xc3\\xb4o'],\n    'uid': ['foouser']})\nbaruser = ('uid=baruser,ou=users,ou=people,dc=example,dc=org', {\n    'objectClass': ['posixAccount', 'shadowAccount', 'inetOrgPerson'],\n    'cn': ['Bar Test'], 'givenName': ['Test'], 'sn': ['Bar'],\n    'uid': ['baruser'], 'uidNumber': ['2001'], 'gidNumber': ['1000'],\n    'homeDirectory': ['\/home\/baruser'], 'loginShell': ['\/bin\/bash'],\n    'jpegPhoto': []})\n\n\nclass UserFactory(factory.django.DjangoModelFactory):\n    username = factory.Faker('username')\n    email = factory.Faker('email')\n    is_active = True\n    password = factory.LazyAttribute(lambda o: auth_hashers.make_password(o.cleartext_password))\n\n    class Meta:\n        model = auth_models.User\n\n    class Params:\n        cleartext_password = factory.fuzzy.FuzzyText(30)\n        superuser = factory.Trait(\n            is_staff=True,\n            is_superuser=True,\n        )\n\n\nclass BaseTestCase(TestCase):\n    directory = {}\n\n    databases = ['default', 'ldap']\n\n    @classmethod\n    def setUpClass(cls):\n        super(BaseTestCase, cls).setUpClass()\n        cls.ldap_server = volatildap.LdapServer(\n            initial_data=cls.directory,\n            schemas=['core.schema', 'cosine.schema', 'inetorgperson.schema', 'nis.schema'],\n        )\n        settings.DATABASES['ldap']['USER'] = cls.ldap_server.rootdn\n        settings.DATABASES['ldap']['PASSWORD'] = cls.ldap_server.rootpw\n        settings.DATABASES['ldap']['NAME'] = cls.ldap_server.uri\n\n    @classmethod\n    def tearDownClass(cls):\n        cls.ldap_server.stop()\n        super(BaseTestCase, cls).tearDownClass()\n\n    def setUp(self):\n        super(BaseTestCase, self).setUp()\n        self.ldap_server.start()\n\n\nclass ConnectionTestCase(BaseTestCase):\n    directory = dict([people, foouser])\n\n    def test_system_checks(self):\n        management.call_command('check')\n\n    def test_make_migrations(self):\n        management.call_command('makemigrations', dry_run=True)\n\n    def test_connection_options(self):\n        LdapUser.objects.get(username='foouser')\n        # self.assertEqual(self.ldapobj.get_option(ldap.OPT_X_TLS_DEMAND), True)\n\n    def test_start_tls(self):\n        # self.assertFalse(self.ldapobj.tls_enabled)\n        LdapUser.objects.get(username='foouser')\n\n    def test_bound_as_admin(self):\n        LdapUser.objects.get(username='foouser')\n        # self.assertEqual(self.ldapobj.bound_as, admin[0])\n\n    def test_reconect(self):\n        LdapUser.objects.get(username='foouser')\n        self.ldap_server.stop()\n        self.ldap_server.start()\n        LdapUser.objects.get(username='foouser')\n\n\nclass GroupTestCase(BaseTestCase):\n    directory = dict([groups, foogroup, bargroup, wizgroup, people, foouser])\n\n    def test_count_none(self):\n        qs = LdapGroup.objects.none()\n        self.assertEqual(qs.count(), 0)\n\n    def test_count_all(self):\n        qs = LdapGroup.objects.all()\n        self.assertEqual(qs.count(), 3)\n\n    def test_aggregate_count(self):\n        qs = LdapGroup.objects.all()\n        result = qs.aggregate(num_groups=Count('name'))\n        self.assertEqual(result['num_groups'], 3)\n\n    def test_annotate_count(self):\n        groups = LdapGroup.objects.order_by('name').annotate(num_usernames=Count('usernames'))\n        self.assertEqual(len(groups), 3)\n        self.assertEqual(groups[0].name, 'bargroup')\n        self.assertEqual(groups[0].num_usernames, 2)\n        self.assertEqual(groups[1].name, 'foogroup')\n        self.assertEqual(groups[1].num_usernames, 2)\n        self.assertEqual(groups[2].name, 'wizgroup')\n        self.assertEqual(groups[2].num_usernames, 2)\n        groups = LdapGroup.objects.filter(name='foogroup').annotate(num_usernames=Count('usernames'))\n        self.assertEqual(len(groups), 1)\n        self.assertEqual(groups[0].name, 'foogroup')\n        self.assertEqual(groups[0].num_usernames, 2)\n        groups = LdapGroup.objects.annotate(num_usernames=Count('usernames')).filter(name='foogroup')\n        self.assertEqual(len(groups), 1)\n        self.assertEqual(groups[0].name, 'foogroup')\n        self.assertEqual(groups[0].num_usernames, 2)\n\n    def test_length_all(self):\n        qs = LdapGroup.objects.all()\n        self.assertEqual(len(qs), 3)\n\n    def test_length_none(self):\n        qs = LdapGroup.objects.none()\n        self.assertEqual(len(qs), 0)\n\n    def test_ldap_filter(self):\n        def get_filterstr(qs):\n            connection = connections['ldap']\n            compiler = SQLCompiler(\n                query=qs.query,\n                connection=connection,\n                using=None,\n            )\n            return query_as_ldap(qs.query, compiler, connection).filterstr\n\n        # single filter\n        qs = LdapGroup.objects.filter(name='foogroup')\n        self.assertEqual(get_filterstr(qs), '(&(objectClass=posixGroup)(cn=foogroup))')\n\n        qs = LdapGroup.objects.filter(Q(name='foogroup'))\n        self.assertEqual(get_filterstr(qs), '(&(objectClass=posixGroup)(cn=foogroup))')\n\n        # AND filter\n        qs = LdapGroup.objects.filter(gid=1000, name='foogroup')\n        self.assertIn(get_filterstr(qs), [\n            '(&(objectClass=posixGroup)(&(gidNumber=1000)(cn=foogroup)))',\n            '(&(objectClass=posixGroup)(&(cn=foogroup)(gidNumber=1000)))',\n        ])\n\n        qs = LdapGroup.objects.filter(Q(gid=1000) & Q(name='foogroup'))\n        self.assertIn(get_filterstr(qs), [\n            '(&(objectClass=posixGroup)(&(gidNumber=1000)(cn=foogroup)))',\n            '(&(objectClass=posixGroup)(&(cn=foogroup)(gidNumber=1000)))',\n        ])\n\n        # OR filter\n        qs = LdapGroup.objects.filter(Q(gid=1000) | Q(name='foogroup'))\n        self.assertIn(get_filterstr(qs), [\n            '(&(objectClass=posixGroup)(|(gidNumber=1000)(cn=foogroup)))',\n            '(&(objectClass=posixGroup)(|(cn=foogroup)(gidNumber=1000)))',\n        ])\n\n        # single exclusion\n        qs = LdapGroup.objects.exclude(name='foogroup')\n        self.assertEqual(get_filterstr(qs), '(&(objectClass=posixGroup)(!(cn=foogroup)))')\n\n        qs = LdapGroup.objects.filter(~Q(name='foogroup'))\n        self.assertEqual(get_filterstr(qs), '(&(objectClass=posixGroup)(!(cn=foogroup)))')\n\n        # multiple exclusion\n        qs = LdapGroup.objects.exclude(name='foogroup', gid=1000)\n        self.assertIn(get_filterstr(qs), [\n            '(&(objectClass=posixGroup)(!(&(gidNumber=1000)(cn=foogroup))))',\n            '(&(objectClass=posixGroup)(!(&(cn=foogroup)(gidNumber=1000))))',\n        ])\n\n        qs = LdapGroup.objects.filter(name='foogroup').exclude(gid=1000)\n        self.assertIn(get_filterstr(qs), [\n            '(&(objectClass=posixGroup)(&(cn=foogroup)(!(gidNumber=1000))))',\n            '(&(objectClass=posixGroup)(&(!(gidNumber=1000))(cn=foogroup)))',\n        ])\n\n    def test_filter(self):\n        qs = LdapGroup.objects.filter(name='foogroup')\n        self.assertEqual(qs.count(), 1)\n\n        qs = LdapGroup.objects.filter(name='foogroup')\n        self.assertEqual(len(qs), 1)\n\n        g = qs[0]\n        self.assertEqual(g.dn, 'cn=foogroup,%s' % LdapGroup.base_dn)\n        self.assertEqual(g.name, 'foogroup')\n        self.assertEqual(g.gid, 1000)\n        self.assertCountEqual(g.usernames, ['foouser', 'baruser'])\n\n        # try to filter non-existent entries\n        qs = LdapGroup.objects.filter(name='does_not_exist')\n        self.assertEqual(qs.count(), 0)\n\n        qs = LdapGroup.objects.filter(name='does_not_exist')\n        self.assertEqual(len(qs), 0)\n\n    def test_get(self):\n        g = LdapGroup.objects.get(name='foogroup')\n        self.assertEqual(g.dn, 'cn=foogroup,%s' % LdapGroup.base_dn)\n        self.assertEqual(g.name, 'foogroup')\n        self.assertEqual(g.gid, 1000)\n        self.assertCountEqual(g.usernames, ['foouser', 'baruser'])\n\n        # try to get a non-existent entry\n        self.assertRaises(LdapGroup.DoesNotExist, LdapGroup.objects.get,\n                          name='does_not_exist')\n\n    def test_exists(self):\n        qs = LdapGroup.objects.filter(name='foogroup')\n        self.assertTrue(qs.exists())\n\n        qs2 = LdapGroup.objects.filter(name='missing')\n        self.assertFalse(qs2.exists())\n\n    def test_get_by_dn(self):\n        g = LdapGroup.objects.get(dn='cn=foogroup,%s' % LdapGroup.base_dn)\n        self.assertEqual(g.dn, 'cn=foogroup,%s' % LdapGroup.base_dn)\n        self.assertEqual(g.name, 'foogroup')\n        self.assertEqual(g.gid, 1000)\n        self.assertCountEqual(g.usernames, ['foouser', 'baruser'])\n\n    def test_gid_lookup(self):\n        g = LdapGroup.objects.get(gid__in=[1000, 2000, 3000])\n        self.assertEqual(g.dn, 'cn=foogroup,%s' % LdapGroup.base_dn)\n        self.assertEqual(g.name, 'foogroup')\n        self.assertEqual(g.gid, 1000)\n        self.assertCountEqual(g.usernames, ['foouser', 'baruser'])\n\n    def test_insert(self):\n        g = LdapGroup()\n        g.name = 'newgroup'\n        g.gid = 1010\n        g.usernames = ['someuser', 'foouser']\n        g.save()\n\n        # check group was created\n        new = LdapGroup.objects.get(name='newgroup')\n        self.assertEqual(new.name, 'newgroup')\n        self.assertEqual(new.gid, 1010)\n        self.assertCountEqual(new.usernames, ['someuser', 'foouser'])\n\n    def test_create(self):\n        LdapGroup.objects.create(\n            name='newgroup',\n            gid=1010,\n            usernames=['someuser', 'foouser'],\n        )\n\n        # check group was created\n        new = LdapGroup.objects.get(name='newgroup')\n        self.assertEqual(new.name, 'newgroup')\n        self.assertEqual(new.gid, 1010)\n        self.assertCountEqual(new.usernames, ['someuser', 'foouser'])\n\n    def test_order_by(self):\n        # ascending name\n        qs = LdapGroup.objects.order_by('name')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].name, 'bargroup')\n        self.assertEqual(qs[1].name, 'foogroup')\n        self.assertEqual(qs[2].name, 'wizgroup')\n\n        # descending name\n        qs = LdapGroup.objects.order_by('-name')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].name, 'wizgroup')\n        self.assertEqual(qs[1].name, 'foogroup')\n        self.assertEqual(qs[2].name, 'bargroup')\n\n        # ascending gid\n        qs = LdapGroup.objects.order_by('gid')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].gid, 1000)\n        self.assertEqual(qs[1].gid, 1001)\n        self.assertEqual(qs[2].gid, 1002)\n\n        # descending gid\n        qs = LdapGroup.objects.order_by('-gid')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].gid, 1002)\n        self.assertEqual(qs[1].gid, 1001)\n        self.assertEqual(qs[2].gid, 1000)\n\n        # ascending pk\n        qs = LdapGroup.objects.order_by('pk')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].name, 'bargroup')\n        self.assertEqual(qs[1].name, 'foogroup')\n        self.assertEqual(qs[2].name, 'wizgroup')\n\n        # descending pk\n        qs = LdapGroup.objects.order_by('-pk')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].name, 'wizgroup')\n        self.assertEqual(qs[1].name, 'foogroup')\n        self.assertEqual(qs[2].name, 'bargroup')\n\n        # ascending dn\n        qs = LdapGroup.objects.order_by('dn')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].name, 'bargroup')\n        self.assertEqual(qs[1].name, 'foogroup')\n        self.assertEqual(qs[2].name, 'wizgroup')\n\n        # descending dn\n        qs = LdapGroup.objects.order_by('-dn')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].name, 'wizgroup')\n        self.assertEqual(qs[1].name, 'foogroup')\n        self.assertEqual(qs[2].name, 'bargroup')\n\n    def test_bulk_delete(self):\n        LdapGroup.objects.all().delete()\n\n        qs = LdapGroup.objects.all()\n        self.assertEqual(len(qs), 0)\n\n    def test_bulk_delete_none(self):\n        LdapGroup.objects.none().delete()\n\n        qs = LdapGroup.objects.all()\n        self.assertEqual(len(qs), 3)\n\n    def test_slice(self):\n        qs = LdapGroup.objects.order_by('gid')\n        objs = list(qs)\n        self.assertEqual(len(objs), 3)\n        self.assertEqual(objs[0].gid, 1000)\n        self.assertEqual(objs[1].gid, 1001)\n        self.assertEqual(objs[2].gid, 1002)\n\n        # limit only\n        qs = LdapGroup.objects.order_by('gid')\n        objs = qs[:2]\n        self.assertEqual(objs.count(), 2)\n\n        objs = qs[:2]\n        self.assertEqual(len(objs), 2)\n        self.assertEqual(objs[0].gid, 1000)\n        self.assertEqual(objs[1].gid, 1001)\n\n        # offset only\n        qs = LdapGroup.objects.order_by('gid')\n        objs = qs[1:]\n        self.assertEqual(objs.count(), 2)\n\n        objs = qs[1:]\n        self.assertEqual(len(objs), 2)\n        self.assertEqual(objs[0].gid, 1001)\n        self.assertEqual(objs[1].gid, 1002)\n\n        # offset and limit\n        qs = LdapGroup.objects.order_by('gid')\n        objs = qs[1:2]\n        self.assertEqual(objs.count(), 1)\n\n        objs = qs[1:2]\n        self.assertEqual(len(objs), 1)\n        self.assertEqual(objs[0].gid, 1001)\n\n    def test_update(self):\n        g = LdapGroup.objects.get(name='foogroup')\n        g.gid = 1002\n        g.usernames = ['foouser2', u'baruseeer2']\n        g.save()\n\n        # check group was updated\n        new = LdapGroup.objects.get(name='foogroup')\n        self.assertEqual(new.name, 'foogroup')\n        self.assertEqual(new.gid, 1002)\n        self.assertCountEqual(new.usernames, ['foouser2', u'baruseeer2'])\n\n    def test_update_change_dn(self):\n        g = LdapGroup.objects.get(name='foogroup')\n        g.name = 'foogroup2'\n        g.save()\n        self.assertEqual(g.dn, 'cn=foogroup2,%s' % LdapGroup.base_dn)\n\n        # check group was updated\n        new = LdapGroup.objects.get(name='foogroup2')\n        self.assertEqual(new.name, 'foogroup2')\n        self.assertEqual(new.gid, 1000)\n        self.assertCountEqual(new.usernames, ['foouser', 'baruser'])\n\n    def test_values(self):\n        qs = sorted(LdapGroup.objects.values_list('name', flat=True))\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0], 'bargroup')\n        self.assertEqual(qs[1], 'foogroup')\n        self.assertEqual(qs[2], 'wizgroup')\n\n    def test_search(self):\n        qs = sorted(LdapGroup.objects.filter(name__contains='foo'))\n        self.assertEqual(len(qs), 1)\n        self.assertEqual(qs[0].name, 'foogroup')\n\n    def test_values_list(self):\n        qs = sorted(LdapGroup.objects.values_list('name'))\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(list(qs[0]), ['bargroup'])\n        self.assertEqual(list(qs[1]), ['foogroup'])\n        self.assertEqual(list(qs[2]), ['wizgroup'])\n\n    def test_delete(self):\n        g = LdapGroup.objects.get(name='foogroup')\n        g.delete()\n\n        qs = LdapGroup.objects.all()\n        self.assertEqual(len(qs), 2)\n\n    def test_paginated_search(self):\n        # This test will change the settings, assert we don't break things\n        self.assertIsNone(settings.DATABASES['ldap'].get('CONNECTION_OPTIONS', {}).get('page_size'))\n\n        # Test without BATCH_SIZE\n        qs = LdapGroup.objects.filter(name__contains='group').order_by('name')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].name, 'bargroup')\n        self.assertEqual(qs[1].name, 'foogroup')\n        self.assertEqual(qs[2].name, 'wizgroup')\n\n        # Set new page size\n        settings.DATABASES['ldap']['CONNECTION_OPTIONS'] = settings.DATABASES['ldap'].get('CONNECTION_OPTIONS', {})\n        settings.DATABASES['ldap']['CONNECTION_OPTIONS']['page_size'] = 1\n        connections['ldap'].close()  # Force connection reload\n\n        qs = LdapGroup.objects.filter(name__contains='group').order_by('name')\n        self.assertEqual(len(qs), 3)\n        self.assertEqual(qs[0].name, 'bargroup')\n        self.assertEqual(qs[1].name, 'foogroup')\n        self.assertEqual(qs[2].name, 'wizgroup')\n\n        # Restore previous configuration\n        del settings.DATABASES['ldap']['CONNECTION_OPTIONS']['page_size']\n\n    def test_listfield(self):\n        g = LdapGroup.objects.get(name='foogroup')\n        self.assertCountEqual(['foouser', 'baruser'], g.usernames)\n\n        # Try to filter on the list field, with and without contains\n        qs = LdapGroup.objects.filter(usernames='foouser')\n        self.assertEqual(qs.count(), 1)\n        qs = LdapGroup.objects.filter(usernames__contains='foouser')\n        self.assertEqual(qs.count(), 1)\n\n        # Try to filter negatively on the list field, with and without contains\n        qs = LdapGroup.objects.filter(~Q(usernames='foouser'))\n        self.assertEqual(qs.count(), 2)\n        qs = LdapGroup.objects.filter(~Q(usernames__contains='foouser'))\n        self.assertEqual(qs.count(), 2)\n\n        # Try to exclude on the list field, with and without contains\n        qs = LdapGroup.objects.exclude(usernames='foouser')\n        self.assertEqual(qs.count(), 2)\n        qs = LdapGroup.objects.exclude(usernames__contains='foouser')\n        self.assertEqual(qs.count(), 2)\n\n    def test_listfield_manipulation(self):\n        g = LdapGroup.objects.get(name='foogroup')\n        self.assertCountEqual(['foouser', 'baruser'], g.usernames)\n\n        # Replace values, with duplicated.\n        g.usernames = ['john', 'jane', 'john']\n        g.save()\n        g = LdapGroup.objects.get(name='foogroup')\n        self.assertCountEqual(['john', 'jane'], g.usernames)\n\n        # Clear values\n        g.usernames = []\n        g.save()\n        g = LdapGroup.objects.get(name='foogroup')\n        self.assertEqual([], g.usernames)\n\n\nclass GroupSubclassingTestCase(BaseTestCase):\n    directory = dict([groups, foogroup, bargroup, wizgroup, people, foouser])\n\n    def test_concrete_group(self):\n        g = ConcreteGroup.objects.get(name='foogroup')\n        self.assertCountEqual(['foouser', 'baruser'], g.usernames)\n\n        g.name = 'modified'\n        g.save()\n\n        g = ConcreteGroup.objects.get(name='modified')\n        self.assertCountEqual(['foouser', 'baruser'], g.usernames)\n\n\nclass UserTestCase(BaseTestCase):\n    directory = dict([groups, people, users, foouser, baruser])\n\n    def test_verbose_name(self):\n        self.assertEqual(\"Prime name\", LdapUser._meta.get_field('first_name').verbose_name)\n        self.assertEqual(\"Final name\", LdapUser._meta.get_field('last_name').verbose_name)\n\n    def test_get(self):\n        u = LdapUser.objects.get(username='foouser')\n        self.assertEqual(u.first_name, u'F\u00f4o')\n        self.assertEqual(u.last_name, u'Us\u00e9r')\n        self.assertEqual(u.full_name, u'F\u00f4o Us\u00e9r')\n\n        self.assertEqual(u.group, 1000)\n        self.assertEqual(u.home_directory, '\/home\/foouser')\n        self.assertEqual(u.uid, 2000)\n        self.assertEqual(u.username, 'foouser')\n        self.assertEqual(\n            u.photo,\n            b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01'\n            b'\\x01\\x00H\\x00H\\x00\\x00\\xff\\xfe\\x00\\x1cCreated with '\n            b'GIMP on a Mac\\xff\\xdb\\x00C\\x00\\x05\\x03\\x04\\x04\\x04'\n            b'\\x03\\x05\\x04\\x04\\x04\\x05\\x05\\x05\\x06\\x07\\x0c\\x08'\n            b'\\x07\\x07\\x07\\x07\\x0f\\x0b\\x0b\\t\\x0c\\x11\\x0f\\x12\\x12'\n            b'\\x11\\x0f\\x11\\x11\\x13\\x16\\x1c\\x17\\x13\\x14\\x1a\\x15'\n            b'\\x11\\x11\\x18!\\x18\\x1a\\x1d\\x1d\\x1f\\x1f\\x1f\\x13\\x17'\n            b'\"$\"\\x1e$\\x1c\\x1e\\x1f\\x1e\\xff\\xdb\\x00C\\x01\\x05\\x05'\n            b'\\x05\\x07\\x06\\x07\\x0e\\x08\\x08\\x0e\\x1e\\x14\\x11\\x14'\n            b'\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e'\n            b'\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e'\n            b'\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e'\n            b'\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e'\n            b'\\x1e\\x1e\\xff\\xc0\\x00\\x11\\x08\\x00\\x08\\x00\\x08\\x03'\n            b'\\x01\"\\x00\\x02\\x11\\x01\\x03\\x11\\x01\\xff\\xc4\\x00\\x15'\n            b'\\x00\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n            b'\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\xff\\xc4\\x00\\x19\\x10'\n            b'\\x00\\x03\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n            b'\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x06\\x11A\\xff\\xc4\\x00'\n            b'\\x14\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n            b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xc4\\x00\\x14\\x11'\n            b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n            b'\\x00\\x00\\x00\\x00\\x00\\xff\\xda\\x00\\x0c\\x03\\x01\\x00'\n            b'\\x02\\x11\\x03\\x11\\x00?\\x00\\x9d\\xf29wU5Q\\xd6\\xfd\\x00'\n            b'\\x01\\xff\\xd9',\n        )\n\n        self.assertRaises(LdapUser.DoesNotExist, LdapUser.objects.get,\n                          username='does_not_exist')\n\n    def test_update(self):\n        # slapd removes microsecond details.\n        before = timezone.now().replace(microsecond=0)\n\n        u = LdapUser.objects.get(username='foouser')\n        u.first_name = u'F\u00f4o2'\n        u.save()\n\n        after = timezone.now().replace(microsecond=0)\n\n        self.assertLessEqual(before, u.last_modified)\n        self.assertLessEqual(u.last_modified, after)\n\n        # make sure DN gets updated if we change the pk\n        u.username = 'foouser2'\n        u.save()\n        self.assertEqual(u.dn, 'uid=foouser2,%s' % LdapUser.base_dn)\n\n    def test_charfield_empty_values(self):\n        \"\"\"CharField should accept empty values.\"\"\"\n        u = LdapUser.objects.get(username='foouser')\n        # Both '' and None are accepted\n        u.phone = ''\n        u.mobile_phone = None\n        u.save()\n\n        # '' and None are normalized to ''.\n        u2 = LdapUser.objects.get(dn=u.dn)\n        self.assertEqual('', u2.phone)\n        self.assertEqual('', u2.mobile_phone)\n\n    def test_intfield_empty_value(self):\n        u = LdapUser.objects.get(username='foouser')\n        # Set to 0\n        u.uid = 0\n        u.save()\n\n        # Ensure we still fetch a '0', not an empty ID.\n        u2 = LdapUser.objects.get(dn=u.dn)\n        self.assertEqual(0, u2.uid)\n\n    def test_datetime_lookup(self):\n\n        # Due to slapd ignoring microsecond in last_modified,\n        # wait for one second to ensure that all timestamps are on the proper\n        # side of the 'before' boundary.\n        time.sleep(1)\n        before = timezone.now().replace(microsecond=0)\n\n        qs = LdapUser.objects.filter(last_modified__gte=before)\n        self.assertEqual([], list(qs))\n\n        u = LdapUser.objects.get(username='foouser')\n        u.first_name = u\"Foo2\"\n        u.save()\n\n        u = LdapUser.objects.get(username='foouser')\n        self.assertLessEqual(before, u.last_modified)\n\n        qs = LdapUser.objects.filter(last_modified__gte=before)\n        self.assertEqual([u], list(qs))\n\n        # Test __in lookup\n        lm = u.last_modified\n        u = LdapUser.objects.get(last_modified__in=[before, lm])\n        self.assertEqual(u.username, 'foouser')\n\n    def test_dn_consistency(self):\n        u = LdapUser.objects.get(username='baruser')\n        u.first_name = u\"Barr\"\n        try:\n            u.save()\n        except ldap.LDAPError:\n            self.fail(\"Cannot save object\")\n        # DN shouldn't be changed\n        self.assertEqual(\n            LdapUser.objects.get(username='baruser').dn,\n            'uid=baruser,ou=users,ou=people,dc=example,dc=org')\n\n\nclass RCTestCases(TestCase):\n    # These were mostly used to learn how ldap, slapd, and volatildap work\n\n    groups = ('ou=groups,dc=example,dc=org', {\n        'objectClass': ['top', 'organizationalUnit'], 'ou': ['groups']})\n\n    users = ('ou=users,dc=example,dc=org', {\n        'objectClass': ['top', 'organizationalUnit'], 'ou': ['users']})\n\n    ucbgroup = ('cn=ucbgroup,ou=groups,dc=example,dc=org', {\n        'objectClass': ['posixGroup'], 'memberUid': ['ucb_user'],\n        'gidNumber': ['10001'], 'cn': ['ucbgroup']})\n\n    ucb_user = ('uid=ucb_user,ou=users,dc=example,dc=org', {\n        'objectClass': ['posixAccount', 'shadowAccount', 'inetOrgPerson'],\n        'cn': ['UCB_User Test'], 'givenName': ['ucb_user'], 'sn': ['Bar'],\n        'uid': ['ucb_user'], 'uidNumber': ['2001'], 'gidNumber': ['1000'],\n        'homeDirectory': ['\/home\/ucb_user'], 'loginShell': ['\/bin\/bash']})\n\n    directory = dict([groups, ucbgroup, users, ucb_user])\n\n    databases = ['default', 'ldap']\n\n    @classmethod\n    def setUpClass(cls):\n        super(RCTestCases, cls).setUpClass()\n        cls._slapd = volatildap.LdapServer(\n            suffix='dc=example,dc=org',\n            schemas=['core.schema', 'cosine.schema', 'inetorgperson.schema', 'nis.schema'],\n            initial_data=cls.directory)\n\n        settings.DATABASES['ldap']['USER'] = cls._slapd.rootdn\n        settings.DATABASES['ldap']['PASSWORD'] = cls._slapd.rootpw\n        settings.DATABASES['ldap']['NAME'] = cls._slapd.uri\n\n    def setUp(self):\n        # Will start the server, or reset\/restart it if already started from a previous test.\n        self._slapd.start()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls._slapd.stop()\n\n\n    def test_create_group(self):\n        RCLdapGroup.objects.create(\n            name='newgroup',\n            gid=1010,\n            members=['ucb_user'],\n        )\n\n        # check group was created\n        new = RCLdapGroup.objects.get(name='newgroup')\n        self.assertEqual(new.name, 'newgroup')\n        self.assertEqual(new.gid, 1010)\n        self.assertCountEqual(new.members, ['ucb_user'])\n\n    def test_create_user(self):\n        RCLdapUser.objects.create(\n            # organization='ucb',\n            first_name='user1 first name',\n            last_name='user1 last name',\n            full_name=\"user1 test\",\n            email='user1@test.com',\n            uid=1001,\n            group=10001,\n            gecos='Test Gecos',\n            home_directory='\/home\/user1',\n            login_shell='\/bin\/bash',\n            username='user1',\n        )\n\n        # check group was created\n        new = RCLdapUser.objects.get(username='user1')\n        self.assertEqual(new.uid, 1001)\n        self.assertEqual(new.first_name, 'user1 first name')\n        self.assertEqual(new.last_name, 'user1 last name')\n        self.assertEqual(new.full_name, 'user1 test')\n\n\nclass ScopedTestCase(BaseTestCase):\n    directory = dict([groups, people, foogroup, contacts])\n\n    def setUp(self):\n        super(ScopedTestCase, self).setUp()\n        self.scoped_model = LdapGroup.scoped(\"ou=contacts,%s\" %\n                                             LdapGroup.base_dn)\n\n    def test_scope(self):\n        ScopedGroup = self.scoped_model\n\n        qs = LdapGroup.objects.all()\n        self.assertEqual(qs.count(), 1)\n\n        qs = ScopedGroup.objects.all()\n        self.assertEqual(qs.count(), 0)\n\n        # create scoped group\n        g2 = ScopedGroup()\n        g2.name = \"scopedgroup\"\n        g2.gid = 5000\n        g2.save()\n\n        qs = LdapGroup.objects.all()\n        self.assertEqual(qs.count(), 2)\n\n        qs = ScopedGroup.objects.all()\n        self.assertEqual(qs.count(), 1)\n\n        g2 = ScopedGroup.objects.get(name=\"scopedgroup\")\n        self.assertEqual(g2.name, u'scopedgroup')\n        self.assertEqual(g2.gid, 5000)\n\n\nclass CompositePKTests(BaseTestCase):\n    directory = dict([rooms])\n\n    def test_create(self):\n        room = LdapMultiPKRoom(\n            name=\"Director office\",\n            number=\"42.01\",\n        )\n        room.save()\n        room = LdapMultiPKRoom.objects.get()\n        self.assertEqual(\"cn=Director office+roomNumber=42.01,ou=rooms,dc=example,dc=org\", room.dn)\n\n    def test_fetch(self):\n        room = LdapMultiPKRoom(\n            name=\"Director office\",\n            number=\"42.01\",\n        )\n        room.save()\n\n        room2 = LdapMultiPKRoom.objects.get(number=\"42.01\")\n        self.assertEqual(\"Director office\", room2.name)\n        self.assertEqual(\"42.01\", room2.number)\n\n    def test_move(self):\n        room = LdapMultiPKRoom.objects.create(\n            name=\"Director office\",\n            number=\"42.01\",\n        )\n\n        room.number = \"42.02\"\n        room.save()\n\n        qs = LdapMultiPKRoom.objects.all()\n        self.assertEqual(1, len(qs))\n        new_room = qs.get()\n        self.assertEqual(room, new_room)\n        self.assertEqual(\"42.02\", new_room.number)\n        self.assertEqual(\"cn=Director office+roomNumber=42.02,ou=rooms,dc=example,dc=org\", new_room.dn)\n\n    def test_update(self):\n        room = LdapMultiPKRoom.objects.create(\n            name=\"Director office\",\n            number=\"42.01\",\n            phone='+001234',\n        )\n\n        room.phone = '+004444'\n        room.save()\n\n        qs = LdapMultiPKRoom.objects.all()\n        self.assertEqual(1, len(qs))\n        new_room = qs.get()\n        self.assertEqual(room, new_room)\n        self.assertEqual(\"42.01\", new_room.number)\n        self.assertEqual('+004444', new_room.phone)\n        self.assertEqual(\"cn=Director office+roomNumber=42.01,ou=rooms,dc=example,dc=org\", new_room.dn)\n\n    def test_update_ambiguous_pk(self):\n        \"\"\"Updating an object where two entries with close pks exist shouldn't fail.\n\n        See #159.\n        \"\"\"\n        room1 = LdapMultiPKRoom.objects.create(\n            name=\"Director office\",\n            number=\"42.01\",\n            phone='+001234',\n        )\n        LdapMultiPKRoom.objects.create(\n            name=\"Director office\",\n            number=\"42.01b\",\n            phone='+001111',\n        )\n\n        room1.phone = '+004444'\n        room1.save()\n\n        qs = LdapMultiPKRoom.objects.all()\n        self.assertEqual(2, len(qs))\n        new_room = qs.get(number=\"42.01\")\n        self.assertEqual(\"42.01\", new_room.number)\n        self.assertEqual('+004444', new_room.phone)\n        self.assertEqual(\"cn=Director office+roomNumber=42.01,ou=rooms,dc=example,dc=org\", new_room.dn)\n\n\nclass AdminTestCase(BaseTestCase):\n    directory = dict([groups, people, foouser, foogroup, bargroup])\n\n    def setUp(self):\n        super(AdminTestCase, self).setUp()\n        self._user = UserFactory(\n            username='test_user',\n            cleartext_password='password',\n            superuser=True,\n        )\n        self.client.login(username=\"test_user\", password=\"password\")\n\n    def test_index(self):\n        response = self.client.get('\/admin\/examples\/')\n        self.assertContains(response, \"Ldap groups\")\n        self.assertContains(response, \"Ldap users\")\n\n    def test_group_list(self):\n        response = self.client.get('\/admin\/examples\/ldapgroup\/')\n        self.assertContains(response, \"Ldap groups\")\n        self.assertContains(response, \"foogroup\")\n        self.assertContains(response, \"1000\")\n\n        # order by name\n        response = self.client.get('\/admin\/examples\/ldapgroup\/?o=1')\n        self.assertContains(response, \"Ldap groups\")\n        self.assertContains(response, \"foogroup\")\n        self.assertContains(response, \"1000\")\n\n        # order by gid\n        response = self.client.get('\/admin\/examples\/ldapgroup\/?o=2')\n        self.assertContains(response, \"Ldap groups\")\n        self.assertContains(response, \"foogroup\")\n        self.assertContains(response, \"1000\")\n\n    def test_group_detail(self):\n        response = self.client.get('\/admin\/examples\/ldapgroup\/cn%3Dfoogroup%2Cou%3Dgroups%2Cdc%3Dexample%2Cdc%3Dorg\/', follow=True)\n        self.assertContains(response, \"foogroup\")\n        self.assertContains(response, \"1000\")\n\n    def test_group_add(self):\n        response = self.client.post('\/admin\/examples\/ldapgroup\/add\/',\n                                    {'gid': '1002', 'name': 'wizgroup'})\n        self.assertRedirects(response, '\/admin\/examples\/ldapgroup\/')\n        qs = LdapGroup.objects.all()\n        self.assertEqual(qs.count(), 3)\n\n    def test_group_delete(self):\n        response = self.client.post(\n            '\/admin\/examples\/ldapgroup\/cn%3Dfoogroup%2Cou%3Dgroups%2Cdc%3Dexample%2Cdc%3Dorg\/delete\/', {'yes': 'post'})\n        self.assertRedirects(response, '\/admin\/examples\/ldapgroup\/')\n        qs = LdapGroup.objects.all()\n        self.assertEqual(qs.count(), 1)\n\n    def test_group_search(self):\n        response = self.client.get('\/admin\/examples\/ldapgroup\/?q=foo')\n        self.assertContains(response, \"Ldap groups\")\n        self.assertContains(response, \"foogroup\")\n        self.assertContains(response, \"1000\")\n\n    def test_user_list(self):\n        response = self.client.get('\/admin\/examples\/ldapuser\/')\n        self.assertContains(response, \"Ldap users\")\n        self.assertContains(response, \"foouser\")\n        self.assertContains(response, \"2000\")\n\n        # order by username\n        response = self.client.get('\/admin\/examples\/ldapuser\/?o=1')\n        self.assertContains(response, \"Ldap users\")\n        self.assertContains(response, \"foouser\")\n        self.assertContains(response, \"2000\")\n\n        # order by uid\n        response = self.client.get('\/admin\/examples\/ldapuser\/?o=2')\n        self.assertContains(response, \"Ldap users\")\n        self.assertContains(response, \"foouser\")\n        self.assertContains(response, \"2000\")\n\n    def test_user_detail(self):\n        response = self.client.get('\/admin\/examples\/ldapuser\/uid%3Dfoouser%2Cou%3Dpeople%2Cdc%3Dexample%2Cdc%3Dorg\/', follow=True)\n        self.assertContains(response, \"foouser\")\n        self.assertContains(response, \"2000\")\n\n    def test_user_delete(self):\n        response = self.client.post('\/admin\/examples\/ldapuser\/uid%3Dfoouser%2Cou%3Dpeople%2Cdc%3Dexample%2Cdc%3Dorg\/delete\/',\n                                    {'yes': 'post'})\n        self.assertRedirects(response, '\/admin\/examples\/ldapuser\/')\n","label":1}
{"content":"import cv2\nfrom timeit import default_timer as timer\n\n\nclass FPS(object):\n    \"\"\" Calculate FPS.\n        example) fps = FPS()\n                 while(cap.isOpended()):\n                     # Your processing\n                     fps.calculate(draw)\n                     cv2.imshow('test', draw)\n    \"\"\"\n    def __init__(self):\n        self.accum_time = 0\n        self.curr_fps = 0\n        self.fps = \"FPS: ??\"\n        self.prev_time = timer()\n\n    def calculate(self, draw, show=True):\n        curr_time = timer()\n        exec_time = curr_time - self.prev_time\n        self.prev_time = curr_time\n        self.accum_time += exec_time\n        self.curr_fps += 1\n        if self.accum_time > 1:\n            self.accum_time -= 1\n            self.fps = \"FPS: \" + str(self.curr_fps)\n            self.curr_fps = 0\n        if show:\n            cv2.rectangle(draw, (0,0), (60,20), (255,255,255), -1)\n            cv2.putText(draw, self.fps, (3,13), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0,0,0), 1)\n        else:\n            print(self.fps)","label":1}
{"content":"#!\/usr\/bin\/env python3\n# -*- coding: utf-8 -*-\n#\n# CLARITE documentation build configuration file, created by\n# sphinx-quickstart on Thu Jun 28 12:35:56 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosectionlabel\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.githubpages\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.mathjax\",\n    \"sphinx.ext.viewcode\",\n    \"IPython.sphinxext.ipython_directive\",\n    \"IPython.sphinxext.ipython_console_highlighting\",\n    \"matplotlib.sphinxext.plot_directive\",\n    \"numpydoc\",\n    \"sphinx_copybutton\",\n    \"sphinx_click.ext\",\n]\n\n# AutoSectionLabel\nautosectionlabel_prefix_document = True\n\n# Configuration options for plot_directive. See:\n# https:\/\/github.com\/matplotlib\/matplotlib\/blob\/f3ed922d935751e08494e5fb5311d3050a3b637b\/lib\/matplotlib\/sphinxext\/plot_directive.py#L81\nplot_html_show_source_link = False\nplot_html_show_formats = False\n\n# Generate the API documentation when building\nautosummary_generate = True\nnumpydoc_show_class_members = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = \".rst\"\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = \"CLARITE\"\ncopyright = \"2020, Hall Lab\"\nauthor = \"Hall Lab\"\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\nimport clarite\n\n# The short X.Y version.\nversion = clarite.__version__\n# The full version, including alpha\/beta\/rc tags.\nrelease = clarite.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"sphinx_rtd_theme\"\nimport sphinx_rtd_theme\n\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_logo = \"_static\/clarite_logo.png\"\nhtml_theme_options = {\n    \"logo_only\": True,\n}\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http:\/\/alabaster.readthedocs.io\/en\/latest\/installation.html#sidebars\nhtml_sidebars = {\n    \"**\": [\n        \"relations.html\",  # needs 'show_related': True theme option to display\n        \"searchbox.html\",\n    ]\n}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"clarite\"\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \"clarite.tex\", \"CLARITE Documentation\", \"Contributors\", \"manual\"),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \"clarite\", \"CLARITE Documentation\", [author], 1)]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        \"clarite\",\n        \"CLARITE Documentation\",\n        author,\n        \"clarite\",\n        \"CLeaning to Analysis: Reproducibility-based Interface for Traits and Exposures\",\n        \"Miscellaneous\",\n    ),\n]\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \"python\": (\"https:\/\/docs.python.org\/3\/\", None),\n    \"numpy\": (\"https:\/\/docs.scipy.org\/doc\/numpy\/\", None),\n    \"scipy\": (\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/\", None),\n    \"pandas\": (\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\", None),\n    \"matplotlib\": (\"https:\/\/matplotlib.org\", None),\n}\n","label":1}
{"content":"# Copyright (C) 2019 Adek Maulana\r\n#\r\n# Licensed under the Raphielscape Public License, Version 1.d (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n#\r\n\r\nimport re\r\nimport hashlib\r\nimport asyncio\r\nimport datetime\r\nimport logging\r\nimport os\r\nimport math\r\nimport os.path\r\nimport sys\r\nimport time\r\nfrom typing import Tuple, Union\r\nfrom userbot import bot\r\n\r\nfrom telethon import errors\r\nfrom telethon.tl import types\r\nfrom telethon.utils import get_display_name\r\nfrom telethon import events\r\nfrom telethon.tl.functions.messages import GetPeerDialogsRequest\r\nfrom telethon.tl.functions.channels import GetParticipantRequest\r\nfrom telethon.tl.types import ChannelParticipantAdmin, ChannelParticipantCreator\r\n\r\n\r\nasync def md5(fname: str) -> str:\r\n    hash_md5 = hashlib.md5()\r\n    with open(fname, \"rb\") as f:\r\n        for chunk in iter(lambda: f.read(4096), b\"\"):\r\n            hash_md5.update(chunk)\r\n    return hash_md5.hexdigest()\r\n\r\n\r\ndef humanbytes(size: int) -> str:\r\n    if size is None or isinstance(size, str):\r\n        return \"\"\r\n\r\n    power = 2**10\r\n    raised_to_pow = 0\r\n    dict_power_n = {0: \"\", 1: \"Ki\", 2: \"Mi\", 3: \"Gi\", 4: \"Ti\"}\r\n    while size > power:\r\n        size \/= power\r\n        raised_to_pow += 1\r\n    return str(round(size, 2)) + \" \" + dict_power_n[raised_to_pow] + \"B\"\r\n\r\n\r\ndef time_formatter(seconds: int) -> str:\r\n    minutes, seconds = divmod(seconds, 60)\r\n    hours, minutes = divmod(minutes, 60)\r\n    days, hours = divmod(hours, 24)\r\n    tmp = (\r\n        ((str(days) + \" day(s), \") if days else \"\") +\r\n        ((str(hours) + \" hour(s), \") if hours else \"\") +\r\n        ((str(minutes) + \" minute(s), \") if minutes else \"\") +\r\n        ((str(seconds) + \" second(s), \") if seconds else \"\")\r\n    )\r\n    return tmp[:-2]\r\n\r\n\r\ndef human_to_bytes(size: str) -> int:\r\n    units = {\r\n        \"M\": 2**20, \"MB\": 2**20,\r\n        \"G\": 2**30, \"GB\": 2**30,\r\n        \"T\": 2**40, \"TB\": 2**40\r\n    }\r\n\r\n    size = size.upper()\r\n    if not re.match(r' ', size):\r\n        size = re.sub(r'([KMGT])', r' \\1', size)\r\n    number, unit = [string.strip() for string in size.split()]\r\n    return int(float(number)*units[unit])\r\n\r\nasync def is_admin(chat_id, user_id):\r\n    req_jo = await bot(GetParticipantRequest(\r\n        channel=chat_id,\r\n        user_id=user_id\r\n    ))\r\n    chat_participant = req_jo.participant\r\n    if isinstance(chat_participant, ChannelParticipantCreator) or isinstance(chat_participant, ChannelParticipantAdmin):\r\n        return True\r\n    return False\r\n","label":1}
{"content":"\"\"\"Provide functionality to stream video source.\n\nComponents use create_stream with a stream source (e.g. an rtsp url) to create\na new Stream object. Stream manages:\n  - Background work to fetch and decode a stream\n  - Desired output formats\n  - Home Assistant URLs for viewing a stream\n  - Access tokens for URLs for viewing a stream\n\nA Stream consists of a background worker, and one or more output formats each\nwith their own idle timeout managed by the stream component. When an output\nformat is no longer in use, the stream component will expire it. When there\nare no active output formats, the background worker is shut down and access\ntokens are expired. Alternatively, a Stream can be configured with keepalive\nto always keep workers active.\n\"\"\"\nfrom __future__ import annotations\n\nfrom collections.abc import Callable, Mapping\nimport logging\nimport re\nimport secrets\nimport threading\nimport time\nfrom types import MappingProxyType\nfrom typing import Any, cast\n\nimport voluptuous as vol\n\nfrom homeassistant.const import EVENT_HOMEASSISTANT_STOP\nfrom homeassistant.core import Event, HomeAssistant, callback\nfrom homeassistant.exceptions import HomeAssistantError\nimport homeassistant.helpers.config_validation as cv\nfrom homeassistant.helpers.typing import ConfigType\n\nfrom .const import (\n    ATTR_ENDPOINTS,\n    ATTR_SETTINGS,\n    ATTR_STREAMS,\n    CONF_LL_HLS,\n    CONF_PART_DURATION,\n    CONF_SEGMENT_DURATION,\n    DOMAIN,\n    HLS_PROVIDER,\n    MAX_SEGMENTS,\n    OUTPUT_IDLE_TIMEOUT,\n    RECORDER_PROVIDER,\n    SEGMENT_DURATION_ADJUSTER,\n    STREAM_RESTART_INCREMENT,\n    STREAM_RESTART_RESET_TIME,\n    TARGET_SEGMENT_DURATION_NON_LL_HLS,\n)\nfrom .core import PROVIDERS, IdleTimer, KeyFrameConverter, StreamOutput, StreamSettings\nfrom .diagnostics import Diagnostics\nfrom .hls import HlsStreamOutput, async_setup_hls\n\n_LOGGER = logging.getLogger(__name__)\n\nSTREAM_SOURCE_REDACT_PATTERN = [\n    (re.compile(r\"\/\/.*:.*@\"), \"\/\/****:****@\"),\n    (re.compile(r\"\\?auth=.*\"), \"?auth=****\"),\n]\n\n\ndef redact_credentials(data: str) -> str:\n    \"\"\"Redact credentials from string data.\"\"\"\n    for (pattern, repl) in STREAM_SOURCE_REDACT_PATTERN:\n        data = pattern.sub(repl, data)\n    return data\n\n\ndef create_stream(\n    hass: HomeAssistant,\n    stream_source: str,\n    options: dict[str, str],\n    stream_label: str | None = None,\n) -> Stream:\n    \"\"\"Create a stream with the specified identfier based on the source url.\n\n    The stream_source is typically an rtsp url (though any url accepted by ffmpeg is fine) and\n    options are passed into pyav \/ ffmpeg as options.\n\n    The stream_label is a string used as an additional message in logging.\n    \"\"\"\n    if DOMAIN not in hass.config.components:\n        raise HomeAssistantError(\"Stream integration is not set up.\")\n\n    # For RTSP streams, prefer TCP\n    if isinstance(stream_source, str) and stream_source[:7] == \"rtsp:\/\/\":\n        options = {\n            \"rtsp_flags\": \"prefer_tcp\",\n            \"stimeout\": \"5000000\",\n            **options,\n        }\n\n    stream = Stream(hass, stream_source, options=options, stream_label=stream_label)\n    hass.data[DOMAIN][ATTR_STREAMS].append(stream)\n    return stream\n\n\nDOMAIN_SCHEMA = vol.Schema(\n    {\n        vol.Optional(CONF_LL_HLS, default=True): cv.boolean,\n        vol.Optional(CONF_SEGMENT_DURATION, default=6): vol.All(\n            cv.positive_float, vol.Range(min=2, max=10)\n        ),\n        vol.Optional(CONF_PART_DURATION, default=1): vol.All(\n            cv.positive_float, vol.Range(min=0.2, max=1.5)\n        ),\n    }\n)\n\nCONFIG_SCHEMA = vol.Schema(\n    {\n        DOMAIN: DOMAIN_SCHEMA,\n    },\n    extra=vol.ALLOW_EXTRA,\n)\n\n\ndef filter_libav_logging() -> None:\n    \"\"\"Filter libav logging to only log when the stream logger is at DEBUG.\"\"\"\n\n    def libav_filter(record: logging.LogRecord) -> bool:\n        return logging.getLogger(__name__).isEnabledFor(logging.DEBUG)\n\n    for logging_namespace in (\n        \"libav.mp4\",\n        \"libav.h264\",\n        \"libav.hevc\",\n        \"libav.rtsp\",\n        \"libav.tcp\",\n        \"libav.tls\",\n        \"libav.mpegts\",\n        \"libav.NULL\",\n    ):\n        logging.getLogger(logging_namespace).addFilter(libav_filter)\n\n    # Set log level to error for libav.mp4\n    logging.getLogger(\"libav.mp4\").setLevel(logging.ERROR)\n    # Suppress \"deprecated pixel format\" WARNING\n    logging.getLogger(\"libav.swscaler\").setLevel(logging.ERROR)\n\n\nasync def async_setup(hass: HomeAssistant, config: ConfigType) -> bool:\n    \"\"\"Set up stream.\"\"\"\n\n    # Drop libav log messages if stream logging is above DEBUG\n    filter_libav_logging()\n\n    # Keep import here so that we can import stream integration without installing reqs\n    # pylint: disable=import-outside-toplevel\n    from .recorder import async_setup_recorder\n\n    hass.data[DOMAIN] = {}\n    hass.data[DOMAIN][ATTR_ENDPOINTS] = {}\n    hass.data[DOMAIN][ATTR_STREAMS] = []\n    conf = DOMAIN_SCHEMA(config.get(DOMAIN, {}))\n    if conf[CONF_LL_HLS]:\n        assert isinstance(conf[CONF_SEGMENT_DURATION], float)\n        assert isinstance(conf[CONF_PART_DURATION], float)\n        hass.data[DOMAIN][ATTR_SETTINGS] = StreamSettings(\n            ll_hls=True,\n            min_segment_duration=conf[CONF_SEGMENT_DURATION]\n            - SEGMENT_DURATION_ADJUSTER,\n            part_target_duration=conf[CONF_PART_DURATION],\n            hls_advance_part_limit=max(int(3 \/ conf[CONF_PART_DURATION]), 3),\n            hls_part_timeout=2 * conf[CONF_PART_DURATION],\n        )\n    else:\n        hass.data[DOMAIN][ATTR_SETTINGS] = StreamSettings(\n            ll_hls=False,\n            min_segment_duration=TARGET_SEGMENT_DURATION_NON_LL_HLS\n            - SEGMENT_DURATION_ADJUSTER,\n            part_target_duration=TARGET_SEGMENT_DURATION_NON_LL_HLS,\n            hls_advance_part_limit=3,\n            hls_part_timeout=TARGET_SEGMENT_DURATION_NON_LL_HLS,\n        )\n\n    # Setup HLS\n    hls_endpoint = async_setup_hls(hass)\n    hass.data[DOMAIN][ATTR_ENDPOINTS][HLS_PROVIDER] = hls_endpoint\n\n    # Setup Recorder\n    async_setup_recorder(hass)\n\n    @callback\n    def shutdown(event: Event) -> None:\n        \"\"\"Stop all stream workers.\"\"\"\n        for stream in hass.data[DOMAIN][ATTR_STREAMS]:\n            stream.keepalive = False\n            stream.stop()\n        _LOGGER.info(\"Stopped stream workers\")\n\n    hass.bus.async_listen_once(EVENT_HOMEASSISTANT_STOP, shutdown)\n\n    return True\n\n\nclass Stream:\n    \"\"\"Represents a single stream.\"\"\"\n\n    def __init__(\n        self,\n        hass: HomeAssistant,\n        source: str,\n        options: dict[str, str],\n        stream_label: str | None = None,\n    ) -> None:\n        \"\"\"Initialize a stream.\"\"\"\n        self.hass = hass\n        self.source = source\n        self.options = options\n        self._stream_label = stream_label\n        self.keepalive = False\n        self.access_token: str | None = None\n        self._thread: threading.Thread | None = None\n        self._thread_quit = threading.Event()\n        self._outputs: dict[str, StreamOutput] = {}\n        self._fast_restart_once = False\n        self._keyframe_converter = KeyFrameConverter(hass)\n        self._available: bool = True\n        self._update_callback: Callable[[], None] | None = None\n        self._logger = (\n            logging.getLogger(f\"{__package__}.stream.{stream_label}\")\n            if stream_label\n            else _LOGGER\n        )\n        self._diagnostics = Diagnostics()\n\n    def endpoint_url(self, fmt: str) -> str:\n        \"\"\"Start the stream and returns a url for the output format.\"\"\"\n        if fmt not in self._outputs:\n            raise ValueError(f\"Stream is not configured for format '{fmt}'\")\n        if not self.access_token:\n            self.access_token = secrets.token_hex()\n        endpoint_fmt: str = self.hass.data[DOMAIN][ATTR_ENDPOINTS][fmt]\n        return endpoint_fmt.format(self.access_token)\n\n    def outputs(self) -> Mapping[str, StreamOutput]:\n        \"\"\"Return a copy of the stream outputs.\"\"\"\n        # A copy is returned so the caller can iterate through the outputs\n        # without concern about self._outputs being modified from another thread.\n        return MappingProxyType(self._outputs.copy())\n\n    def add_provider(\n        self, fmt: str, timeout: int = OUTPUT_IDLE_TIMEOUT\n    ) -> StreamOutput:\n        \"\"\"Add provider output stream.\"\"\"\n        if not (provider := self._outputs.get(fmt)):\n\n            @callback\n            def idle_callback() -> None:\n                if (\n                    not self.keepalive or fmt == RECORDER_PROVIDER\n                ) and fmt in self._outputs:\n                    self.remove_provider(self._outputs[fmt])\n                self.check_idle()\n\n            provider = PROVIDERS[fmt](\n                self.hass, IdleTimer(self.hass, timeout, idle_callback)\n            )\n            self._outputs[fmt] = provider\n\n        return provider\n\n    def remove_provider(self, provider: StreamOutput) -> None:\n        \"\"\"Remove provider output stream.\"\"\"\n        if provider.name in self._outputs:\n            self._outputs[provider.name].cleanup()\n            del self._outputs[provider.name]\n\n        if not self._outputs:\n            self.stop()\n\n    def check_idle(self) -> None:\n        \"\"\"Reset access token if all providers are idle.\"\"\"\n        if all(p.idle for p in self._outputs.values()):\n            self.access_token = None\n\n    @property\n    def available(self) -> bool:\n        \"\"\"Return False if the stream is started and known to be unavailable.\"\"\"\n        return self._available\n\n    def set_update_callback(self, update_callback: Callable[[], None]) -> None:\n        \"\"\"Set callback to run when state changes.\"\"\"\n        self._update_callback = update_callback\n\n    @callback\n    def _async_update_state(self, available: bool) -> None:\n        \"\"\"Set state and Run callback to notify state has been updated.\"\"\"\n        self._available = available\n        if self._update_callback:\n            self._update_callback()\n\n    def start(self) -> None:\n        \"\"\"Start a stream.\"\"\"\n        if self._thread is None or not self._thread.is_alive():\n            if self._thread is not None:\n                # The thread must have crashed\/exited. Join to clean up the\n                # previous thread.\n                self._thread.join(timeout=0)\n            self._thread_quit.clear()\n            self._thread = threading.Thread(\n                name=\"stream_worker\",\n                target=self._run_worker,\n            )\n            self._thread.start()\n            self._logger.info(\n                \"Started stream: %s\", redact_credentials(str(self.source))\n            )\n\n    def update_source(self, new_source: str) -> None:\n        \"\"\"Restart the stream with a new stream source.\"\"\"\n        self._diagnostics.increment(\"update_source\")\n        self._logger.debug(\n            \"Updating stream source %s\", redact_credentials(str(new_source))\n        )\n        self.source = new_source\n        self._fast_restart_once = True\n        self._thread_quit.set()\n\n    def _run_worker(self) -> None:\n        \"\"\"Handle consuming streams and restart keepalive streams.\"\"\"\n        # Keep import here so that we can import stream integration without installing reqs\n        # pylint: disable=import-outside-toplevel\n        from .worker import StreamState, StreamWorkerError, stream_worker\n\n        stream_state = StreamState(self.hass, self.outputs, self._diagnostics)\n        wait_timeout = 0\n        while not self._thread_quit.wait(timeout=wait_timeout):\n            start_time = time.time()\n            self.hass.add_job(self._async_update_state, True)\n            self._diagnostics.set_value(\"keepalive\", self.keepalive)\n            self._diagnostics.increment(\"start_worker\")\n            try:\n                stream_worker(\n                    self.source,\n                    self.options,\n                    stream_state,\n                    self._keyframe_converter,\n                    self._thread_quit,\n                )\n            except StreamWorkerError as err:\n                self._diagnostics.increment(\"worker_error\")\n                self._logger.error(\"Error from stream worker: %s\", str(err))\n\n            stream_state.discontinuity()\n            if not _should_retry() or self._thread_quit.is_set():\n                if self._fast_restart_once:\n                    # The stream source is updated, restart without any delay and reset the retry\n                    # backoff for the new url.\n                    wait_timeout = 0\n                    self._fast_restart_once = False\n                    self._thread_quit.clear()\n                    continue\n                break\n\n            self.hass.add_job(self._async_update_state, False)\n            # To avoid excessive restarts, wait before restarting\n            # As the required recovery time may be different for different setups, start\n            # with trying a short wait_timeout and increase it on each reconnection attempt.\n            # Reset the wait_timeout after the worker has been up for several minutes\n            if time.time() - start_time > STREAM_RESTART_RESET_TIME:\n                wait_timeout = 0\n            wait_timeout += STREAM_RESTART_INCREMENT\n            self._diagnostics.set_value(\"retry_timeout\", wait_timeout)\n            self._logger.debug(\n                \"Restarting stream worker in %d seconds: %s\",\n                wait_timeout,\n                redact_credentials(str(self.source)),\n            )\n        self._worker_finished()\n\n    def _worker_finished(self) -> None:\n        \"\"\"Schedule cleanup of all outputs.\"\"\"\n\n        @callback\n        def remove_outputs() -> None:\n            for provider in self.outputs().values():\n                self.remove_provider(provider)\n\n        self.hass.loop.call_soon_threadsafe(remove_outputs)\n\n    def stop(self) -> None:\n        \"\"\"Remove outputs and access token.\"\"\"\n        self._outputs = {}\n        self.access_token = None\n\n        if not self.keepalive:\n            self._stop()\n\n    def _stop(self) -> None:\n        \"\"\"Stop worker thread.\"\"\"\n        if self._thread is not None:\n            self._thread_quit.set()\n            self._thread.join()\n            self._thread = None\n            self._logger.info(\n                \"Stopped stream: %s\", redact_credentials(str(self.source))\n            )\n\n    async def async_record(\n        self, video_path: str, duration: int = 30, lookback: int = 5\n    ) -> None:\n        \"\"\"Make a .mp4 recording from a provided stream.\"\"\"\n\n        # Keep import here so that we can import stream integration without installing reqs\n        # pylint: disable=import-outside-toplevel\n        from .recorder import RecorderOutput\n\n        # Check for file access\n        if not self.hass.config.is_allowed_path(video_path):\n            raise HomeAssistantError(f\"Can't write {video_path}, no access to path!\")\n\n        # Add recorder\n        if recorder := self.outputs().get(RECORDER_PROVIDER):\n            assert isinstance(recorder, RecorderOutput)\n            raise HomeAssistantError(\n                f\"Stream already recording to {recorder.video_path}!\"\n            )\n        recorder = cast(\n            RecorderOutput, self.add_provider(RECORDER_PROVIDER, timeout=duration)\n        )\n        recorder.video_path = video_path\n\n        self.start()\n        self._logger.debug(\"Started a stream recording of %s seconds\", duration)\n\n        # Take advantage of lookback\n        hls: HlsStreamOutput = cast(HlsStreamOutput, self.outputs().get(HLS_PROVIDER))\n        if lookback > 0 and hls:\n            num_segments = min(int(lookback \/\/ hls.target_duration), MAX_SEGMENTS)\n            # Wait for latest segment, then add the lookback\n            await hls.recv()\n            recorder.prepend(list(hls.get_segments())[-num_segments:])\n\n    async def async_get_image(\n        self,\n        width: int | None = None,\n        height: int | None = None,\n    ) -> bytes | None:\n        \"\"\"\n        Fetch an image from the Stream and return it as a jpeg in bytes.\n\n        Calls async_get_image from KeyFrameConverter. async_get_image should only be\n        called directly from the main loop and not from an executor thread as it uses\n        hass.add_executor_job underneath the hood.\n        \"\"\"\n\n        self.add_provider(HLS_PROVIDER)\n        self.start()\n        return await self._keyframe_converter.async_get_image(\n            width=width, height=height\n        )\n\n    def get_diagnostics(self) -> dict[str, Any]:\n        \"\"\"Return diagnostics information for the stream.\"\"\"\n        return self._diagnostics.as_dict()\n\n\ndef _should_retry() -> bool:\n    \"\"\"Return true if worker failures should be retried, for disabling during tests.\"\"\"\n    return True\n","label":1}
{"content":"import pytransform.rotations as pyrot\nfrom visualization import *\nfrom utils import parse_args\nfrom convert import trajectory_ik, trajectory_fk, timing_report, reaching_report\nfrom approxik import ApproxInvKin, ExactInvKin\n\n\ndef line1(P):\n    P[:, 0] = 0.5\n    P[:, 1] = np.linspace(-0.8, 0.8, P.shape[0])\n    P[:, 2] = 1.0\n    P[:, 3] = 1.0\n\n\ndef line2(P):\n    P[:, 0] = 0.5\n    P[:, 1] = np.linspace(-0.8, 0.8, P.shape[0])\n    P[:, 2] = 0.1\n    P[:, 3] = 1.0\n\n\nif __name__ == \"__main__\":\n    filename, base_link, ee_link = parse_args()\n\n    aik = ApproxInvKin(filename, base_link, ee_link, 1.0, 0.001, verbose=0)\n    eik = ExactInvKin(filename, base_link, ee_link, 1e-4, 200, verbose=0)\n\n    P = np.zeros((1000, 7))\n    line2(P)\n\n    Qa, timings = trajectory_ik(P, aik)\n    reaching_report(P, Qa, aik, label=\"Approximate IK\")\n    timing_report(timings, \"Approximate IK\")\n    Pa = trajectory_fk(Qa, aik)\n    Qe, timings, reachable = trajectory_ik(P, eik, return_reachable=True)\n    reaching_report(P, Qe, eik, label=\"Exact IK\")\n    timing_report(timings, \"Exact IK\")\n    Pe = trajectory_fk(Qe, eik)\n    Pe[np.logical_not(reachable)] = np.nan\n    Qe[np.logical_not(reachable)] = np.nan\n\n    ax = plot_pose_trajectory(P, Pa)\n    ax = plot_pose_trajectory(P, Pe)\n\n    plot_joint_trajectories([Qa, Qe], labels=[\"Approximation\", \"Exact\"])\n\n    plt.show()\n","label":1}
{"content":"\"\"\"\nASGI config for revolve project.\n\nIt exposes the ASGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps:\/\/docs.djangoproject.com\/en\/3.1\/howto\/deployment\/asgi\/\n\"\"\"\n\nimport os\n\nfrom django.core.asgi import get_asgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'revolve.settings')\n\napplication = get_asgi_application()\n","label":1}
{"content":"#! \/usr\/bin\/env python3\n\nimport argparse\nimport collections\nimport numpy as np\nimport pandas as pd\nimport sys\n\nimport bio_utils.bam_utils as bam_utils\nimport bio_utils.bed_utils as bed_utils\nimport misc.utils as utils\nimport misc.pandas_utils as pandas_utils\n\nimport logging\nimport misc.logging_utils as logging_utils\nlogger = logging.getLogger(__name__)\n\n\ndefault_num_cpus = 1\ndefault_lengths = []\n\ndefault_start_upstream = 300\ndefault_start_downstream = 300\ndefault_end_upstream = 300\ndefault_end_downstream = 300\n\n\ndef get_interval_df(start, end, seqname, strand):\n    interval_df = pd.DataFrame()\n    interval_df['start'] = start\n    interval_df['end'] = end\n    interval_df['seqname'] = seqname\n    interval_df['strand'] = strand\n    interval_df['id'] = \".\"\n    interval_df['score'] = 0\n    return interval_df\n\ndef get_length_strand_profiles(matches, profile_length):\n    init = lambda: np.zeros(profile_length, int)\n    length_strand_profiles = collections.defaultdict(init)\n    \n    for match in matches:\n        position_info = match.position_info\n        \n        relative_offset = int(match.relative_offset)\n        strand = position_info[5]\n        length = int(position_info[6])\n        \n        profile = length_strand_profiles[(length, strand)]\n        \n        profile[relative_offset] += 1\n        \n    return length_strand_profiles\n\ndef get_metagene_profile_df(length, type_label, length_strand_profiles, upstream, downstream):\n    reverse_metagene_profile = length_strand_profiles[(length, '-')]\n    forward_metagene_profile = length_strand_profiles[(length, '+')]\n    \n    metagene_profile = forward_metagene_profile + reverse_metagene_profile[::-1]\n    \n    if sum(metagene_profile) == 0:\n        return None\n\n    offset = range(-1*upstream, downstream+1)\n    \n    metagene_profile_df = pd.DataFrame()\n    metagene_profile_df['position'] = offset\n    metagene_profile_df['count'] = metagene_profile\n    metagene_profile_df['type'] = type_label\n    metagene_profile_df['length'] = length\n    \n    return metagene_profile_df\n     \ndef main():\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\"This script extracts the metagene profile from reads in a BAM \"\n        \"file, possibly filtering by length. It attempts to vectorize as many of the \"\n        \"counting operations as possible.\")\n    parser.add_argument('bam', help=\"The bam file\")\n    parser.add_argument('orfs', help=\"The annotated transcripts (bed) file\")\n    parser.add_argument('out', help=\"The (output) csv.gz counts file\")\n\n    parser.add_argument('-p', '--num-cpus', help=\"The number of processors to use\",\n        type=int, default=default_num_cpus)\n\n    parser.add_argument('--is-sam', help=\"If this flag is present, the alignment file will \"\n        \"be parsed as SAM rather than BAM\", action='store_true')\n\n    parser.add_argument('--lengths', help=\"If specified, then metagene profiles will be \"\n        \"created for reads of each length. Otherwise, profiles will be created for each \"\n        \"read length present in the bam file.\", type=int, nargs='*', default=default_lengths)\n\n    parser.add_argument('--start-upstream', type=int, default=default_start_upstream, \n        help=\"The number of bases upstream of the translation initiation site to begin \"\n        \"constructing the metagene profile.\")\n    parser.add_argument('--start-downstream', type=int, default=default_start_downstream,\n        help=\"The number of bases downstream of the translation initiation site to end \"\n        \"the metagene profile.\")\n    parser.add_argument('--end-upstream', type=int, default=default_end_upstream,\n        help=\"The number of bases upstream of the translation termination site to begin \"\n        \"constructing the metagene profile.\")\n    parser.add_argument('--end-downstream', type=int, default=default_end_downstream,\n        help=\"The number of bases downstream of the translation termination site to end \"\n        \"the metagene profile.\")\n    \n    logging_utils.add_logging_options(parser)\n    args = parser.parse_args()\n    logging_utils.update_logging(args)\n\n    msg = \"[extract-metagene-profiles]: {}\".format(' '.join(sys.argv))\n    logger.info(msg)\n\n    # first, get the 5' ends of the reads\n    alignment_df = bam_utils.get_five_prime_ends(args.bam, progress_bar=True, \n        count=True, logger=logger)\n\n    msg = \"Reading annotations\"\n    logger.info(msg)\n    annotations_df = bed_utils.read_bed(args.orfs)\n\n    msg = \"Constructing canonical translation initiation ORF data frames\"\n    logger.info(msg)\n\n    m_has_canonical = annotations_df['thick_start'] > -1\n    m_forward = annotations_df['strand'] == '+'\n\n    m_canonical_forward = m_has_canonical & m_forward\n    m_canonical_reverse = m_has_canonical & ~m_forward\n\n    # forward translation initiation\n    start = annotations_df.loc[m_canonical_forward, 'thick_start'] - args.start_upstream\n    end = annotations_df.loc[m_canonical_forward, 'thick_start'] + args.start_downstream\n    seqname = annotations_df.loc[m_canonical_forward, 'seqname']\n    strand = '+'\n\n    intervals_forward_initiation_bed = get_interval_df(start, end, seqname, strand)\n\n    # reverse translation initation\n    start = annotations_df.loc[m_canonical_reverse, 'thick_end'] - args.start_downstream\n    end = annotations_df.loc[m_canonical_reverse, 'thick_end'] + args.start_upstream\n    seqname = annotations_df.loc[m_canonical_reverse, 'seqname']\n    strand = '-'\n\n    intervals_reverse_initiation_bed = get_interval_df(start, end, seqname, strand)\n\n    # all translation initiation regions\n    intervals_initiation_bed = pd.concat([intervals_forward_initiation_bed, intervals_reverse_initiation_bed])\n\n    # make sure we do not double count isoforms with the same starts\n    intervals_initiation_bed = intervals_initiation_bed.drop_duplicates()\n\n    msg = \"Constructing canonical translation termination ORF data frames\"\n    logger.info(msg)\n    \n    # forward translation termination\n    start = annotations_df.loc[m_canonical_forward, 'thick_end'] - args.end_upstream\n    end = annotations_df.loc[m_canonical_forward, 'thick_end'] + args.end_downstream\n    seqname = annotations_df.loc[m_canonical_forward, 'seqname']\n    strand = '+'\n    \n    intervals_forward_termination_bed = get_interval_df(start, end, seqname, strand)\n\n    # reverse translation termination\n    start = annotations_df.loc[m_canonical_reverse, 'thick_start'] - args.end_downstream\n    end = annotations_df.loc[m_canonical_reverse, 'thick_start'] + args.end_upstream\n    seqname = annotations_df.loc[m_canonical_reverse, 'seqname']\n    strand = '-'\n    intervals_reverse_termination_bed = get_interval_df(start, end, seqname, strand)\n\n    # all translation termination regions\n    intervals_termination_bed = pd.concat([intervals_forward_termination_bed, intervals_reverse_termination_bed])\n\n    # make sure we do not double count isoforms with the same starts\n    intervals_termination_bed = intervals_termination_bed.drop_duplicates()\n\n    msg = \"Finding translation initiation site matches\"\n    logger.info(msg)\n\n    initiation_matches = bed_utils.get_all_position_intersections(alignment_df, intervals_initiation_bed)\n    profile_length = args.start_upstream + args.start_downstream + 1\n    initiation_length_strand_profiles = get_length_strand_profiles(initiation_matches, profile_length)\n\n    initiation_keys_str = ','.join(str(k) for k in initiation_length_strand_profiles.keys())\n    msg = \"Initiation keys: {}\".format(initiation_keys_str)\n    logger.debug(msg)\n\n    msg = \"Finding translation termination site matches\"\n    logger.info(msg)\n\n    termination_matches = bed_utils.get_all_position_intersections(alignment_df, intervals_termination_bed)\n    profile_length = args.end_upstream + args.end_downstream + 1\n    termination_length_strand_profiles = get_length_strand_profiles(termination_matches, profile_length)\n\n    termination_keys_str = ','.join(str(k) for k in termination_length_strand_profiles.keys())\n    msg = \"Termination keys: {}\".format(termination_keys_str)\n    logger.debug(msg)\n\n    msg = \"Extracting metagene profiles\"\n    logger.info(msg)\n\n    \n    if len(args.lengths) == 0:\n        args.lengths = list(alignment_df['length'].unique())\n\n    args.lengths = np.sort(args.lengths)\n    args.lengths = [int(l) for l in args.lengths]\n    length_str = ','.join(str(l) for l in args.lengths)\n    msg = \"Profiles will be created for lengths: {}\".format(length_str)\n    logger.info(msg)\n\n    all_metagene_profile_dfs = []\n\n    for length in args.lengths:\n        # first, the profile for this length around initiation sites\n        initiation_profile_df = get_metagene_profile_df(length, \n                                                        'start',\n                                                        initiation_length_strand_profiles, \n                                                        args.start_upstream, \n                                                        args.start_downstream)\n        \n        all_metagene_profile_dfs.append(initiation_profile_df)\n        \n        # and around termination sites\n        termination_profile_df = get_metagene_profile_df(length, \n                                                        'end',\n                                                        termination_length_strand_profiles, \n                                                        args.end_upstream, \n                                                        args.end_downstream)\n        \n        all_metagene_profile_dfs.append(termination_profile_df)\n        \n    # filter out all of the profiles which did not have any reads\n    all_metagene_profile_dfs = [df for df in all_metagene_profile_dfs if df is not None]\n\n    # join them together in one large data frame\n    all_metagene_profile_dfs = pd.concat(all_metagene_profile_dfs)\n\n    msg = \"Writing metagene profiles to disk\"\n    logger.info(msg)\n    pandas_utils.write_df(all_metagene_profile_dfs, args.out, index=False)\n\nif __name__ == '__main__':\n    main()\n","label":1}
{"content":"# coding: utf-8\n# Copyright (c) Pymatgen Development Team.\n# Distributed under the terms of the MIT License.\n\nfrom __future__ import division\n\nimport warnings\nimport numpy as np\nimport matplotlib.pylab as plt\n\nfrom pymatgen import Composition\nfrom pymatgen.analysis.phase_diagram import GrandPotentialPhaseDiagram\nfrom pymatgen.analysis.reaction_calculator import Reaction\n\n\"\"\"\nThis module provides class to generate and analyze interfacial reactions.\n\"\"\"\n\n__author__ = \"Yihan Xiao\"\n__copyright__ = \"Copyright 2011, The Materials Project\"\n__version__ = \"1.0\"\n__maintainer__ = \"Yihan Xiao\"\n__email__ = \"eric.xyh2011@gmail.com\"\n__status__ = \"Production\"\n__date__ = \"Aug 15 2017\"\n\n\nclass InterfacialReactivity:\n    \"\"\"\n    An object encompassing all relevant data for interface reactions.\n\n    Args:\n        c1 (Composition): Composition object for reactant 1.\n        c2 (Composition): Composition object for reactant 2.\n        pd (PhaseDiagram): PhaseDiagram object or GrandPotentialPhaseDiagram\n            object built from all elements in composition c1 and c2.\n        norm (bool): Whether or not the total number of atoms in composition\n            of reactant will be normalized to 1.\n        include_no_mixing_energy (bool): No_mixing_energy for a reactant is the\n            opposite number of its energy above grand potential convex hull. In\n            cases where reactions involve elements reservoir, this param\n            determines whether no_mixing_energy of reactants will be included\n            in the final reaction energy calculation. By definition, if pd is\n            not a GrandPotentialPhaseDiagram object, this param is False.\n        pd_non_grand (PhaseDiagram): PhaseDiagram object but not\n            GrandPotentialPhaseDiagram object built from elements in c1 and c2.\n        use_hull_energy (bool): Whether or not use the convex hull energy for\n            a given composition for reaction energy calculation. If false,\n            the energy of ground state structure will be used instead.\n            Note that in case when ground state can not be found for a\n            composition, convex hull energy will be used associated with a\n            warning message.\n    \"\"\"\n    EV_TO_KJ_PER_MOL = 96.4853\n\n    def __init__(self, c1, c2, pd, norm=True, include_no_mixing_energy=False,\n                 pd_non_grand=None, use_hull_energy=False):\n        self.grand = isinstance(pd, GrandPotentialPhaseDiagram)\n\n        # if include_no_mixing_energy is True, pd should be a\n        # GrandPotentialPhaseDiagram object and pd_non_grand should be given.\n        if include_no_mixing_energy and not self.grand:\n            raise ValueError('Please provide grand phase diagram to compute'\n                             ' no_mixing_energy!')\n        if include_no_mixing_energy and not pd_non_grand:\n            raise ValueError('Please provide non-grand phase diagram to '\n                             'compute no_mixing_energy!')\n        if self.grand and use_hull_energy and not pd_non_grand:\n            raise ValueError('Please provide non-grand phase diagram if'\n                             ' you want to use convex hull energy.')\n\n        # Keeps copy of original compositions.\n        self.c1_original = c1\n        self.c2_original = c2\n\n        # Two sets of composition attributes for two processing conditions:\n        # normalization with and without exluding element(s) from reservoir.\n        self.c1 = c1\n        self.c2 = c2\n        self.comp1 = c1\n        self.comp2 = c2\n\n        self.norm = norm\n        self.pd = pd\n        self.pd_non_grand = pd_non_grand\n        self.use_hull_energy = use_hull_energy\n\n        # Factor is the compositional ratio between composition self.c1 and\n        # processed composition self.comp1. E.g., factor for\n        # Composition('SiO2') and Composition('O') is 2.0. This factor will\n        # be used to convert mixing ratio in self.comp1 - self.comp2\n        # tie line to that in self.c1 - self.c2 tie line.\n        self.factor1 = 1\n        self.factor2 = 1\n\n        if self.grand:\n            # Excludes element(s) from reservoir.\n            self.comp1 = Composition({k: v for k, v in c1.items()\n                                      if k not in pd.chempots})\n            self.comp2 = Composition({k: v for k, v in c2.items()\n                                      if k not in pd.chempots})\n            # Calculate the factors in case where self.grand = True and\n            # self.norm = True.\n            factor1 = self.comp1.num_atoms \/ c1.num_atoms\n            factor2 = self.comp2.num_atoms \/ c2.num_atoms\n\n        if self.norm:\n            self.c1 = c1.fractional_composition\n            self.c2 = c2.fractional_composition\n            self.comp1 = self.comp1.fractional_composition\n            self.comp2 = self.comp2.fractional_composition\n            if self.grand:\n                # Only when self.grand = True and self.norm = True\n                # will self.factor be updated.\n                self.factor1 = factor1\n                self.factor2 = factor2\n\n        # Computes energies for reactants in different scenarios.\n        if not self.grand:\n            if self.use_hull_energy:\n                self.e1 = self.pd.get_hull_energy(self.comp1)\n                self.e2 = self.pd.get_hull_energy(self.comp2)\n            else:\n                # Use entry energy as reactant energy if no reservoir\n                # is present.\n                self.e1 = InterfacialReactivity._get_entry_energy(\n                    self.pd, self.comp1)\n                self.e2 = InterfacialReactivity._get_entry_energy(\n                    self.pd, self.comp2)\n        else:\n            if include_no_mixing_energy:\n                # Computing grand potentials needs compositions containing\n                # element(s) from reservoir, so self.c1 and self.c2 are used.\n                self.e1 = self._get_grand_potential(self.c1)\n                self.e2 = self._get_grand_potential(self.c2)\n            else:\n                self.e1 = self.pd.get_hull_energy(self.comp1)\n                self.e2 = self.pd.get_hull_energy(self.comp2)\n\n    @staticmethod\n    def _get_entry_energy(pd, composition):\n        \"\"\"\n        Finds the lowest entry energy for entries matching the composition.\n        Entries with non-negative formation energies are excluded. If no\n        entry is found, use the convex hull energy for the composition.\n\n        Args:\n            pd (PhaseDiagram): PhaseDiagram object.\n            composition (Composition): Composition object that the target\n            entry should match.\n\n        Returns:\n            The lowest entry energy among entries matching the composition.\n        \"\"\"\n        candidate = [i.energy_per_atom for i in pd.qhull_entries if\n                     i.composition.fractional_composition ==\n                     composition.fractional_composition]\n\n        if not candidate:\n            warnings.warn(\"The reactant \" + composition.reduced_formula +\n                          \" has no matching entry with negative formation\"\n                          \" energy, instead convex hull energy for this\"\n                          \" composition will be used for reaction energy \"\n                          \"calculation. \")\n            return pd.get_hull_energy(composition)\n        else:\n            min_entry_energy = min(candidate)\n            return min_entry_energy * composition.num_atoms\n\n    def _get_grand_potential(self, composition):\n        \"\"\"\n        Computes the grand potential Phi at a given composition and\n        chemical potential(s).\n\n        Args:\n            composition (Composition): Composition object.\n\n        Returns:\n            Grand potential at a given composition at chemical potential(s).\n        \"\"\"\n        if self.use_hull_energy:\n            grand_potential = self.pd_non_grand.get_hull_energy(composition)\n        else:\n            grand_potential = InterfacialReactivity._get_entry_energy(\n                self.pd_non_grand, composition)\n        grand_potential -= sum([composition[e] * mu\n                                for e, mu in self.pd.chempots.items()])\n        if self.norm:\n            # Normalizes energy to the composition excluding element(s)\n            # from reservoir.\n            grand_potential \/= sum([composition[el]\n                                    for el in composition\n                                    if el not in self.pd.chempots])\n        return grand_potential\n\n    def _get_energy(self, x):\n        \"\"\"\n        Computes reaction energy in eV\/atom at mixing ratio x : (1-x) for\n        self.comp1 : self.comp2.\n\n        Args:\n            x (float): Mixing ratio x of reactants, a float between 0 and 1.\n\n        Returns:\n            Reaction energy.\n        \"\"\"\n        return self.pd.get_hull_energy(self.comp1 * x + self.comp2 * (1-x)) - \\\n            self.e1 * x - self.e2 * (1-x)\n\n    def _get_reaction(self, x):\n        \"\"\"\n        Generates balanced reaction at mixing ratio x : (1-x) for\n        self.comp1 : self.comp2.\n\n        Args:\n            x (float): Mixing ratio x of reactants, a float between 0 and 1.\n\n        Returns:\n            Reaction object.\n        \"\"\"\n        mix_comp = self.comp1 * x + self.comp2 * (1-x)\n        decomp = self.pd.get_decomposition(mix_comp)\n\n        # Uses original composition for reactants.\n        if np.isclose(x, 0):\n            reactant = [self.c2_original]\n        elif np.isclose(x, 1):\n            reactant = [self.c1_original]\n        else:\n            reactant = list(set([self.c1_original, self.c2_original]))\n\n        if self.grand:\n            reactant += [Composition(e.symbol)\n                         for e, v in self.pd.chempots.items()]\n\n        product = [Composition(k.name) for k, v in decomp.items()]\n        reaction = Reaction(reactant, product)\n\n        if np.isclose(x, 1):\n            reaction.normalize_to(self.c1_original, 1)\n        else:\n            reaction.normalize_to(self.c2_original, 1)\n        return reaction\n\n    def _get_elmt_amt_in_rxt(self, rxt):\n        \"\"\"\n        Computes total number of atoms in a reaction formula for elements\n        not in external reservoir. This method is used in the calculation\n        of reaction energy per mol of reaction formula.\n\n        Args:\n            rxt (Reaction): a reaction.\n\n        Returns:\n            Total number of atoms for non_reservoir elements.\n        \"\"\"\n        return sum([rxt.get_el_amount(e) for e in self.pd.elements])\n\n    def get_products(self):\n        \"\"\"\n        List of formulas of potential products. E.g., ['Li','O2','Mn'].\n        \"\"\"\n        products = set()\n        for _, _, _, react, _ in self.get_kinks():\n            products = products.union(set([k.reduced_formula\n                                           for k in react.products]))\n        return list(products)\n\n    @staticmethod\n    def _convert(x, factor1, factor2):\n        \"\"\"\n        Converts mixing ratio x in comp1 - comp2 tie line to that in\n        c1 - c2 tie line.\n\n        Args:\n            x (float): Mixing ratio x in comp1 - comp2 tie line, a float\n                between 0 and 1.\n            factor1 (float): Compositional ratio between composition c1 and\n                processed composition comp1. E.g., factor for\n                Composition('SiO2') and Composition('O') is 2.0.\n            factor2 (float): Compositional ratio between composition c2 and\n                processed composition comp2.\n\n        Returns:\n            Mixing ratio in c1 - c2 tie line, a float between 0 and 1.\n        \"\"\"\n        return x * factor2 \/ ((1-x) * factor1 + x * factor2)\n\n    @staticmethod\n    def _reverse_convert(x, factor1, factor2):\n        \"\"\"\n        Converts mixing ratio x in c1 - c2 tie line to that in\n        comp1 - comp2 tie line.\n\n        Args:\n            x (float): Mixing ratio x in c1 - c2 tie line, a float between\n                0 and 1.\n            factor1 (float): Compositional ratio between composition c1 and\n                processed composition comp1. E.g., factor for\n                Composition('SiO2') and Composition('O') is 2.\n            factor2 (float): Compositional ratio between composition c2 and\n                processed composition comp2.\n\n        Returns:\n            Mixing ratio in comp1 - comp2 tie line, a float between 0 and 1.\n        \"\"\"\n        return x * factor1 \/ ((1-x) * factor2 + x * factor1)\n\n    def get_kinks(self):\n        \"\"\"\n        Finds all the kinks in mixing ratio where reaction products changes\n        along the tie line of composition self.c1 and composition self.c2.\n\n        Returns:\n            Zip object of tuples (index, mixing ratio,\n                                  reaction energy per atom in eV\/atom,\n                                  reaction formula,\n                                  reaction energy per mol of reaction\n                                  formula in kJ\/mol).\n        \"\"\"\n        c1_coord = self.pd.pd_coords(self.comp1)\n        c2_coord = self.pd.pd_coords(self.comp2)\n        n1 = self.comp1.num_atoms\n        n2 = self.comp2.num_atoms\n        critical_comp = self.pd.get_critical_compositions(self.comp1,\n                                                          self.comp2)\n        x_kink, energy_kink, react_kink, energy_per_rxt_formula = \\\n            [], [], [], []\n        if all(c1_coord == c2_coord):\n            x_kink = [0, 1]\n            energy_kink = [self._get_energy(x) for x in x_kink]\n            react_kink = [self._get_reaction(x) for x in x_kink]\n            num_atoms = [(x * self.comp1.num_atoms +\n                          (1-x) * self.comp2.num_atoms) for x in x_kink]\n            energy_per_rxt_formula = [energy_kink[i] *\n                                      self._get_elmt_amt_in_rxt(\n                                          react_kink[i]) \/\n                                      num_atoms[i] *\n                                      InterfacialReactivity.EV_TO_KJ_PER_MOL\n                                      for i in range(2)]\n        else:\n            for i in reversed(critical_comp):\n                # Gets mixing ratio x at kinks.\n                c = self.pd.pd_coords(i)\n                x = np.linalg.norm(c - c2_coord) \/ \\\n                    np.linalg.norm(c1_coord - c2_coord)\n                # Modifies mixing ratio in case compositions self.comp1 and\n                # self.comp2 are not normalized.\n                x = x * n2 \/ (n1 + x * (n2 - n1))\n                n_atoms = x * self.comp1.num_atoms \\\n                    + (1-x) * self.comp2.num_atoms\n                # Converts mixing ratio in comp1 - comp2 tie line to that in\n                # c1 - c2 tie line.\n                x_converted = InterfacialReactivity._convert(\n                    x, self.factor1, self.factor2)\n                x_kink.append(x_converted)\n                # Gets reaction energy at kinks\n                normalized_energy = self._get_energy(x)\n                energy_kink.append(normalized_energy)\n                # Gets balanced reaction at kinks\n                rxt = self._get_reaction(x)\n                react_kink.append(rxt)\n                rxt_energy = normalized_energy * \\\n                    self._get_elmt_amt_in_rxt(rxt) \/ \\\n                    n_atoms\n                energy_per_rxt_formula.append(\n                    rxt_energy *\n                    InterfacialReactivity.EV_TO_KJ_PER_MOL)\n        index_kink = range(1, len(critical_comp)+1)\n        return zip(index_kink, x_kink, energy_kink, react_kink,\n                   energy_per_rxt_formula)\n\n    def get_critical_original_kink_ratio(self):\n        \"\"\"\n        Returns a list of mixing ratio for each kink between ORIGINAL\n        (instead of processed) reactant compositions. This is the\n        same list as mixing ratio obtained from get_kinks method\n        if self.norm = False.\n\n        Returns:\n            A list of floats representing mixing ratios between original\n            reactant compositions for each kink.\n        \"\"\"\n        ratios = []\n        if self.c1_original == self.c2_original:\n            return [0, 1]\n        reaction_kink = [k[3] for k in self.get_kinks()]\n        for rxt in reaction_kink:\n            c1_coeff = rxt.get_coeff(self.c1_original) \\\n                if self.c1_original in rxt.reactants else 0\n            c2_coeff = rxt.get_coeff(self.c2_original) \\\n                if self.c2_original in rxt.reactants else 0\n            ratios.append(abs(c1_coeff \/ (c1_coeff + c2_coeff)))\n        return ratios\n\n    def labels(self):\n        \"\"\"\n        Returns a dictionary containing kink information:\n        {index: 'x= mixing_ratio energy= reaction_energy reaction_equation'}.\n        E.g., {1: 'x= 0.0 energy = 0.0 Mn -> Mn',\n               2: 'x= 0.5 energy = -15.0 O2 + Mn -> MnO2',\n               3: 'x= 1.0 energy = 0.0 O2 -> O2'}.\n        \"\"\"\n        return {j: 'x= ' + str(round(x, 4)) + ' energy in eV\/atom = ' +\n                   str(round(energy, 4)) + ' ' + str(reaction)\n                for j, x, energy, reaction, _ in self.get_kinks()}\n\n    def plot(self):\n        \"\"\"\n        Plots reaction energy as a function of mixing ratio x in\n        self.c1 - self.c2 tie line using pylab.\n\n        Returns:\n            Pylab object that plots reaction energy as a function of\n            mixing ratio x.\n        \"\"\"\n        plt.rcParams['xtick.major.pad'] = '6'\n        plt.rcParams['ytick.major.pad'] = '6'\n        plt.rcParams['axes.linewidth'] = 2\n        npoint = 1000\n        xs = np.linspace(0, 1, npoint)\n\n        # Converts sampling points in self.c1 - self.c2 tie line to those in\n        # self.comp1 - self.comp2 tie line.\n        xs_reverse_converted = InterfacialReactivity._reverse_convert(\n            xs, self.factor1, self.factor2)\n        energies = [self._get_energy(x) for x in xs_reverse_converted]\n        plt.plot(xs, energies, 'k-')\n\n        # Marks kinks and minimum energy point.\n        kinks = self.get_kinks()\n        _, x_kink, energy_kink, _, _ = zip(*kinks)\n        plt.scatter(x_kink, energy_kink, marker='o', c='blue', s=20)\n        plt.scatter(self.minimum()[0], self.minimum()[1], marker='*',\n                    c='red', s=300)\n\n        # Labels kinks with indices. Labels are made draggable\n        # in case of overlapping.\n        for index, x, energy, _, _ in kinks:\n            plt.annotate(\n                index,\n                xy=(x, energy), xytext=(5, 30),\n                textcoords='offset points', ha='right', va='bottom',\n                arrowprops=dict(arrowstyle='->',\n                                connectionstyle='arc3,rad=0')).draggable()\n        plt.xlim([-0.05, 1.05])\n        if self.norm:\n            plt.ylabel('Energy (eV\/atom)')\n        else:\n            plt.ylabel('Energy (eV\/f.u.)')\n        plt.xlabel('$x$ in $x$ {} + $(1-x)$ {}'.format(\n            self.c1.reduced_formula, self.c2.reduced_formula))\n        return plt\n\n    def minimum(self):\n        \"\"\"\n        Finds the minimum reaction energy E_min and corresponding\n        mixing ratio x_min.\n\n        Returns:\n            Tuple (x_min, E_min).\n        \"\"\"\n        return min([(x, energy) for _, x, energy, _, _ in self.get_kinks()],\n                   key=lambda i: i[1])\n\n    def get_no_mixing_energy(self):\n        \"\"\"\n        Generates the opposite number of energy above grand potential\n        convex hull for both reactants.\n\n        Returns:\n            [(reactant1, no_mixing_energy1),(reactant2,no_mixing_energy2)].\n        \"\"\"\n        assert self.grand == 1, \\\n            'Please provide grand potential phase diagram ' \\\n            'for computing no_mixing_energy!'\n\n        energy1 = self.pd.get_hull_energy(self.comp1) - \\\n            self._get_grand_potential(self.c1)\n        energy2 = self.pd.get_hull_energy(self.comp2) - \\\n            self._get_grand_potential(self.c2)\n        unit = 'eV\/f.u.'\n        if self.norm:\n            unit = 'eV\/atom'\n        return [(self.c1_original.reduced_formula +\n                 ' ({0})'.format(unit), energy1),\n                (self.c2_original.reduced_formula +\n                 ' ({0})'.format(unit), energy2)]\n\n    @staticmethod\n    def get_chempot_correction(element, temp, pres):\n        \"\"\"\n        Get the normalized correction term \u0394\u03bc for chemical potential of a gas\n        phase consisting of element at given temperature and pressure,\n        referenced to that in the standard state (T_std = 298.15 K,\n        T_std = 1 bar). The gas phase is limited to be one of O2, N2, Cl2,\n        F2, H2. Calculation formula can be found in the documentation of\n        Materials Project website.\n\n        Args:\n            element (string): The string representing the element.\n            temp (float): The temperature of the gas phase.\n            pres (float): The pressure of the gas phase.\n\n        Returns:\n            The correction of chemical potential in eV\/atom of the gas\n            phase at given temperature and pressure.\n        \"\"\"\n        if element not in [\"O\", \"N\", \"Cl\", \"F\", \"H\"]:\n            return 0\n        std_temp = 298.15\n        std_pres = 1E5\n        ideal_gas_const = 8.3144598\n        # Cp and S at standard state in J\/(K.mol). Data from\n        # https:\/\/janaf.nist.gov\/tables\/O-029.html\n        # https:\/\/janaf.nist.gov\/tables\/N-023.html\n        # https:\/\/janaf.nist.gov\/tables\/Cl-073.html\n        # https:\/\/janaf.nist.gov\/tables\/F-054.html\n        # https:\/\/janaf.nist.gov\/tables\/H-050.html\n        Cp_dict = {\"O\": 29.376,\n                   \"N\": 29.124,\n                   \"Cl\": 33.949,\n                   \"F\": 31.302,\n                   \"H\": 28.836}\n\n        S_dict = {\"O\": 205.147,\n                  \"N\": 191.609,\n                  \"Cl\": 223.079,\n                  \"F\": 202.789,\n                  \"H\": 130.680}\n        Cp_std = Cp_dict[element]\n        S_std = S_dict[element]\n        PV_correction = ideal_gas_const * temp * np.log(pres \/ std_pres)\n        TS_correction = - Cp_std * (temp * np.log(temp)\n                                    - std_temp * np.log(std_temp)) \\\n                        + Cp_std * (temp - std_temp) * (1 + np.log(std_temp)) \\\n                        - S_std * (temp - std_temp)\n\n        dG = PV_correction + TS_correction\n        # Convert to eV\/molecule unit.\n        dG \/= 1000 * InterfacialReactivity.EV_TO_KJ_PER_MOL\n        # Normalize by number of atoms in the gas molecule. For elements\n        # considered, the gas molecules are all diatomic.\n        dG \/= 2\n        return dG\n","label":1}
{"content":"#!\/usr\/bin\/env python3\n# -*- coding: utf-8 -*-\n\nimport argparse\nimport os\nimport shutil\n\nimport itertools\nfrom multiprocessing import Pool\nimport numpy as np\n\nfrom dmriqcpy.analysis.stats import stats_mean_in_tissues\nfrom dmriqcpy.io.report import Report\nfrom dmriqcpy.io.utils import (add_online_arg, add_overwrite_arg,\n                               assert_inputs_exist, assert_outputs_exist)\nfrom dmriqcpy.viz.graph import graph_mean_in_tissues\nfrom dmriqcpy.viz.screenshot import screenshot_mosaic_wrapper\nfrom dmriqcpy.viz.utils import analyse_qa, dataframe_to_html\n\n\nDESCRIPTION = \"\"\"\nCompute the FODF report in HTML format.\n\"\"\"\n\n\ndef _build_arg_parser():\n    p = argparse.ArgumentParser(description=DESCRIPTION,\n                                formatter_class=argparse.RawTextHelpFormatter)\n\n    p.add_argument('output_report',\n                   help='HTML report')\n\n    p.add_argument('--afd_max', nargs='+', required=True,\n                   help='AFD max images in Nifti format')\n\n    p.add_argument('--afd_sum', nargs='+', required=True,\n                   help='AFD sum images in Nifti format')\n\n    p.add_argument('--afd_total', nargs='+', required=True,\n                   help='AFD total images in Nifti format')\n\n    p.add_argument('--nufo', nargs='+', required=True,\n                   help='NUFO max images in Nifti format')\n\n    p.add_argument('--wm', nargs='+', required=True,\n                   help='WM mask in Nifti format')\n\n    p.add_argument('--gm', nargs='+', required=True,\n                   help='GM mask in Nifti format')\n\n    p.add_argument('--csf', nargs='+', required=True,\n                   help='CSF mask in Nifti format')\n\n    p.add_argument('--skip', default=2, type=int,\n                   help='Number of images skipped to build the '\n                        'mosaic. [%(default)s]')\n\n    p.add_argument('--nb_columns', default=12, type=int,\n                   help='Number of columns for the mosaic. [%(default)s]')\n\n    p.add_argument('--nb_threads', type=int, default=1,\n                   help='Number of threads. [%(default)s]')\n\n    add_online_arg(p)\n    add_overwrite_arg(p)\n\n    return p\n\n\ndef _subj_parralel(subj_metric, summary, name, skip, nb_columns):\n    subjects_dict = {}\n    screenshot_path = screenshot_mosaic_wrapper(subj_metric,\n                                                output_prefix=name,\n                                                directory=\"data\", skip=skip,\n                                                nb_columns=nb_columns)\n\n    summary_html = dataframe_to_html(summary.loc[subj_metric])\n    subjects_dict[subj_metric] = {}\n    subjects_dict[subj_metric]['screenshot'] = screenshot_path\n    subjects_dict[subj_metric]['stats'] = summary_html\n    return subjects_dict\n\n\ndef main():\n    parser = _build_arg_parser()\n    args = parser.parse_args()\n\n    if not len(args.afd_max) == len(args.afd_sum) == len(args.afd_total) ==\\\n        len(args.nufo) == len(args.wm) == len(args.gm) == len(args.csf):\n        parser.error(\"Not the same number of images in input.\")\n\n    all_images = np.concatenate([args.afd_max, args.afd_sum, args.afd_total,\n                                 args.nufo, args.wm, args.gm, args.csf])\n    assert_inputs_exist(parser, all_images)\n    assert_outputs_exist(parser, args, [args.output_report, \"data\", \"libs\"])\n\n    if os.path.exists(\"data\"):\n        shutil.rmtree(\"data\")\n    os.makedirs(\"data\")\n\n    if os.path.exists(\"libs\"):\n        shutil.rmtree(\"libs\")\n\n    metrics_names = [[args.afd_max, 'AFD_max'], [args.afd_sum, 'AFD_sum'],\n                     [args.afd_total, 'AFD_total'], [args.nufo, 'NUFO']]\n    metrics_dict = {}\n    summary_dict = {}\n    graphs = []\n    warning_dict = {}\n    for metrics, name in metrics_names:\n        subjects_dict = {}\n        curr_metrics = ['Mean {} in WM'.format(name),\n                        'Mean {} in GM'.format(name),\n                        'Mean {} in CSF'.format(name),\n                        'Max {} in WM'.format(name)]\n\n        summary, stats = stats_mean_in_tissues(curr_metrics, metrics, args.wm,\n                                               args.gm, args.csf)\n        warning_dict[name] = analyse_qa(summary, stats, curr_metrics[:3])\n        warning_list = np.concatenate([filenames for filenames in warning_dict[name].values()])\n        warning_dict[name]['nb_warnings'] = len(np.unique(warning_list))\n\n        graph = graph_mean_in_tissues('Mean {}'.format(name), curr_metrics[:3],\n                                      summary, args.online)\n        graphs.append(graph)\n\n        stats_html = dataframe_to_html(stats)\n        summary_dict[name] = stats_html\n        pool = Pool(args.nb_threads)\n        subjects_dict_pool = pool.starmap(_subj_parralel,\n                                          zip(metrics,\n                                              itertools.repeat(summary),\n                                              itertools.repeat(name),\n                                              itertools.repeat(args.skip),\n                                              itertools.repeat(args.nb_columns)))\n\n        pool.close()\n        pool.join()\n\n        for dict_sub in subjects_dict_pool:\n            for key in dict_sub:\n                subjects_dict[key] = dict_sub[key]\n        metrics_dict[name] = subjects_dict\n\n    nb_subjects = len(args.afd_max)\n    report = Report(args.output_report)\n    report.generate(title=\"Quality Assurance FODF metrics\",\n                    nb_subjects=nb_subjects, summary_dict=summary_dict,\n                    graph_array=graphs, metrics_dict=metrics_dict,\n                    warning_dict=warning_dict,\n                    online=args.online)\n\n\nif __name__ == '__main__':\n    main()\n","label":1}
{"content":"type(\"d\",Key.WIN)\ndoubleClick(\"1588404121792.png\")\nwait(3)\ntype(\"192.168.25.1\")\ntype(Key.ENTER)\nwait(8)\nclick(\"1588405355842.png\")\nwait(2)\nclick(\"1588405480600.png\")\nwait(1)\nclick(\"1596941744273.png\")\nwait(1)\ntype(\"admin\")\nwait(1)\ntype(Key.TAB)\nwait(1)\nif exists(\"1588405510110.png\", 2):\n    click(\"1588405510110.png\")\n    wait(2)\n    click(\"1588405549467.png\")\nelse:\n\tclick(\"1588405549467.png\")\n\t\nwait(4)\nclick(\"1592477656744.png\")","label":1}
{"content":"# Generated by Django 3.2.2 on 2021-06-14 07:42\n\nfrom django.db import migrations\n\n\ndef migrate_warehouse_address(apps, _schema_editor):\n    Warehouse = apps.get_model(\"warehouse\", \"Warehouse\")\n\n    for warehouse in Warehouse.objects.filter(\n        company_name__isnull=False, address__company_name=\"\"\n    ).iterator():\n        address = warehouse.address\n        address.company_name = warehouse.company_name\n        address.save(update_fields=[\"company_name\"])\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (\"warehouse\", \"0013_auto_20210308_1135\"),\n    ]\n\n    operations = [\n        migrations.RunPython(\n            migrate_warehouse_address, reverse_code=migrations.RunPython.noop\n        ),\n        migrations.RemoveField(\n            model_name=\"warehouse\",\n            name=\"company_name\",\n        ),\n    ]\n","label":1}
{"content":"from btchip.btchip import *\nfrom btchip.btchipComm import getDongle\nimport logging\nfrom btchip.btchipUtils import compress_public_key\nfrom hw_common import HardwareWalletCancelException, clean_bip32_path, HwSessionInfo\nfrom wnd_utils import WndUtils\nfrom dash_utils import *\nfrom PyQt5.QtWidgets import QMessageBox\nimport unicodedata\n\n\ndef process_ledger_exceptions(func):\n    \"\"\"\n    Catch exceptions for known user errors and expand the exception message with some suggestions.\n    :param func: function decorated.\n    \"\"\"\n    def process_ledger_exceptions_int(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except BTChipException as e:\n            logging.exception('Error while communicating with Ledger hardware wallet.')\n            if (e.sw in (0x6d00, 0x6700)):\n                e.message += '\\n\\nMake sure the NIX app is running on your Ledger device.'\n            elif (e.sw == 0x6982):\n                e.message += '\\n\\nMake sure you have entered the PIN on your Ledger device.'\n            raise\n    return process_ledger_exceptions_int\n\n\n@process_ledger_exceptions\ndef connect_ledgernano():\n    dongle = getDongle()\n    app = btchip(dongle)\n    try:\n        ver = app.getFirmwareVersion()\n        logging.info('Ledger Nano S connected. Firmware version: %s, specialVersion: %s, compressedKeys: %s' %\n                     (str(ver.get('version')), str(ver.get('specialVersion')), ver.get('compressedKeys')))\n\n        client = btchip(dongle)\n        return client\n    except:\n        dongle.close()\n        raise\n\n\nclass MessageSignature:\n    def __init__(self, address, signature):\n        self.address = address\n        self.signature = signature\n\n\n@process_ledger_exceptions\ndef sign_message(hw_session: HwSessionInfo, bip32_path, message):\n\n    client = hw_session.hw_client\n    # Ledger doesn't accept characters other that ascii printable:\n    # https:\/\/ledgerhq.github.io\/btchip-doc\/bitcoin-technical.html#_sign_message\n    message = message.encode('ascii', 'ignore')\n    bip32_path = clean_bip32_path(bip32_path)\n\n    ok = False\n    for i in range(1,4):\n        info = client.signMessagePrepare(bip32_path, message)\n        if info['confirmationNeeded'] and info['confirmationType'] == 34:\n            if i == 1 or \\\n                WndUtils.queryDlg('Another application (such as Ledger Wallet Bitcoin app) has probably taken over '\n                     'the communication with the Ledger device.'\n                     '\\n\\nTo continue, close that application and click the <b>Retry<\/b> button.'\n                     '\\nTo cancel, click the <b>Abort<\/b> button',\n                 buttons=QMessageBox.Retry | QMessageBox.Abort,\n                 default_button=QMessageBox.Retry, icon=QMessageBox.Warning) == QMessageBox.Retry:\n\n                # we need to reconnect the device; first, we'll try to reconnect to HW without closing the intefering\n                # application; it it doesn't help we'll display a message requesting the user to close the app\n                hw_session.hw_disconnect()\n                if hw_session.hw_connect():\n                    client = hw_session.hw_client\n                else:\n                    raise Exception('Hardware wallet reconnect error.')\n            else:\n                break\n        else:\n            ok = True\n            break\n\n    if not ok:\n        raise HardwareWalletCancelException('Cancelled')\n\n    try:\n        signature = client.signMessageSign()\n    except Exception as e:\n        logging.exception('Exception while signing message with Ledger Nano S')\n        raise Exception('Exception while signing message with Ledger Nano S. Details: ' + str(e))\n\n    try:\n        pubkey = client.getWalletPublicKey(bip32_path)\n    except Exception as e:\n        logging.exception('Could not get public key for BIP32 path from Ledger Nano S')\n        raise Exception('Could not get public key for BIP32 path from Ledger Nano S. Details: ' + str(e))\n\n    if len(signature) > 4:\n        r_length = signature[3]\n        r = signature[4: 4 + r_length]\n        if len(signature) > 4 + r_length + 1:\n            s_length = signature[4 + r_length + 1]\n            if len(signature) > 4 + r_length + 2:\n                s = signature[4 + r_length + 2:]\n                if r_length == 33:\n                    r = r[1:]\n                if s_length == 33:\n                    s = s[1:]\n            else:\n                logging.error('client.signMessageSign() returned invalid response (code 3): ' + signature.hex())\n                raise Exception('Invalid signature returned (code 3).')\n        else:\n            logging.error('client.signMessageSign() returned invalid response (code 2): ' + signature.hex())\n            raise Exception('Invalid signature returned (code 2).')\n    else:\n        logging.error('client.signMessageSign() returned invalid response (code 1): ' + signature.hex())\n        raise Exception('Invalid signature returned (code 1).')\n\n    return MessageSignature(\n        pubkey.get('address').decode('ascii'),\n        bytes(chr(27 + 4 + (signature[0] & 0x01)), \"utf-8\") + r + s\n    )\n\n\n@process_ledger_exceptions\ndef get_address_and_pubkey(client, bip32_path):\n    bip32_path = clean_bip32_path(bip32_path)\n    bip32_path.strip()\n    if bip32_path.lower().find('m\/') >= 0:\n        bip32_path = bip32_path[2:]\n\n    nodedata = client.getWalletPublicKey(bip32_path)\n\n    return {\n        'address': nodedata.get('address').decode('utf-8'),\n        'publicKey': compress_public_key(nodedata.get('publicKey'))\n    }\n\n\ndef load_device_by_mnemonic(mnemonic_words: str, pin: str, passphrase: str, secondary_pin: str):\n    \"\"\"\n    Initialise Ledger Nano S device with a list of mnemonic words.\n    :param mnemonic_words: 12, 18 or 24 mnemonic words separated with spaces to initialise device.\n    :param pin: PIN to be set in the device (4- or 8-character string)\n    :param passphrase: Passphrase to be set in the device or empty.\n    :param secondary_pin: Secondary PIN to activate passphrase. It's required if 'passphrase' is set.\n    \"\"\"\n\n    def process(ctrl, mnemonic_words, pin, passphrase, secondary_pin):\n        ctrl.dlg_config_fun(dlg_title=\"Please confirm\", show_progress_bar=False)\n        ctrl.display_msg_fun('<b>Please wait while initializing device...<\/b>')\n\n        dongle = getDongle()\n\n        # stage 1: initialize the hardware wallet with mnemonic words\n        apdudata = bytearray()\n        if pin:\n            apdudata += bytearray([len(pin)]) + bytearray(pin, 'utf8')\n        else:\n            apdudata += bytearray([0])\n\n        # empty prefix\n        apdudata += bytearray([0])\n\n        # empty passphrase in this phase\n        apdudata += bytearray([0])\n\n        if mnemonic_words:\n            apdudata += bytearray([len(mnemonic_words)]) + bytearray(mnemonic_words, 'utf8')\n        else:\n            apdudata += bytearray([0])\n\n        apdu = bytearray([0xE0, 0xD0, 0x00, 0x00, len(apdudata)]) + apdudata\n        dongle.exchange(apdu, timeout=3000)\n\n        # stage 2: setup the secondary pin and the passphrase if provided\n        if passphrase and secondary_pin:\n            ctrl.display_msg_fun('<b>Configuring the passphrase, enter the primary PIN on your <br>'\n                                 'hardware wallet when asked...<\/b>')\n\n            apdudata = bytearray()\n            if pin:\n                apdudata += bytearray([len(pin)]) + bytearray(secondary_pin, 'utf8')\n            else:\n                apdudata += bytearray([0])\n\n            # empty prefix\n            apdudata += bytearray([0])\n\n            if passphrase:\n                passphrase = unicodedata.normalize('NFKD', passphrase)\n                apdudata += bytearray([len(passphrase)]) + bytearray(passphrase, 'utf8')\n            else:\n                apdudata += bytearray([0])\n\n            # empty mnemonic words in this phase\n            apdudata += bytearray([0])\n\n            apdu = bytearray([0xE0, 0xD0, 0x01, 0x00, len(apdudata)]) + apdudata\n            dongle.exchange(apdu, timeout=3000)\n\n        dongle.close()\n        del dongle\n    try:\n        return WndUtils.run_thread_dialog(process, (mnemonic_words, pin, passphrase, secondary_pin), True)\n    except BTChipException as e:\n        if e.message == 'Invalid status 6982':\n            raise Exception('Operation failed with the following error: %s. \\n\\nMake sure you have reset the device '\n                            'and started it in recovery mode.' % e.message)\n        else:\n            raise\n    except Exception as e:\n        raise\n\n\n@process_ledger_exceptions\ndef prepare_transfer_tx(hw_session: HwSessionInfo, utxos_to_spend, dest_addresses, tx_fee, rawtransactions):\n    client = hw_session.hw_client\n\n    # Each of the UTXOs will become an input in the new transaction. For each of those inputs, create\n    # a Ledger's 'trusted input', that will be used by the the device to sign a transaction.\n    trusted_inputs = []\n\n    # arg_inputs: list of dicts\n    #  {\n    #    'locking_script': <Locking script of the UTXO used as an input. Used in the process of signing\n    #                       transaction.>,\n    #    'outputIndex': <index of the UTXO within the previus transaction>,\n    #    'txid': <hash of the previus transaction>,\n    #    'bip32_path': <BIP32 path of the HW key controlling UTXO's destination>,\n    #    'pubkey': <Public key obtained from the HW using the bip32_path.>\n    #    'signature' <Signature obtained as a result of processing the input. It will be used as a part of the\n    #               unlocking script.>\n    #  }\n    #  Why do we need a locking script of the previous transaction? When hashing a new transaction before creating its\n    #  signature, all placeholders for input's unlocking script has to be filled with locking script of the\n    #  corresponding UTXO. Look here for the details:\n    #    https:\/\/klmoney.wordpress.com\/bitcoin-dissecting-transactions-part-2-building-a-transaction-by-hand)\n    arg_inputs = []\n\n    # A dictionary mapping bip32 path to a pubkeys obtained from the Ledger device - used to avoid\n    # reading it multiple times for the same bip32 path\n    bip32_to_address = {}\n\n    amount = 0\n    starting = True\n    for idx, utxo in enumerate(utxos_to_spend):\n        amount += utxo['satoshis']\n\n        raw_tx = bytearray.fromhex(rawtransactions[utxo['txid']])\n        if not raw_tx:\n            raise Exception(\"Can't find raw transaction for txid: \" + rawtransactions[utxo['txid']])\n\n        # parse the raw transaction, so that we can extract the UTXO locking script we refer to\n        prev_transaction = bitcoinTransaction(raw_tx)\n\n        utxo_tx_index = utxo['outputIndex']\n        if utxo_tx_index < 0 or utxo_tx_index > len(prev_transaction.outputs):\n            raise Exception('Incorrent value of outputIndex for UTXO %s' % str(idx))\n\n        trusted_input = client.getTrustedInput(prev_transaction, utxo_tx_index)\n        trusted_inputs.append(trusted_input)\n\n        bip32_path = utxo['bip32_path']\n        bip32_path = clean_bip32_path(bip32_path)\n        pubkey = bip32_to_address.get(bip32_path)\n        if not pubkey:\n            pubkey = compress_public_key(client.getWalletPublicKey(bip32_path)['publicKey'])\n            bip32_to_address[bip32_path] = pubkey\n        pubkey_hash = bitcoin.bin_hash160(pubkey)\n\n        # verify if the public key hash of the wallet's bip32 path is the same as specified in the UTXO locking script\n        # if they differ, signature and public key we produce and are going to include in the unlocking script won't\n        # match the locking script conditions - transaction will be rejected by the network\n        pubkey_hash_from_script = extract_pkh_from_locking_script(prev_transaction.outputs[utxo_tx_index].script)\n        if pubkey_hash != pubkey_hash_from_script:\n            logging.error(\"Error: different public key hashes for the BIP32 path %s (UTXO %s) and the UTXO locking \"\n                          \"script. Your signed transaction will not be validated by the network.\" %\n              (bip32_path, str(idx)))\n\n        arg_inputs.append({\n            'locking_script': prev_transaction.outputs[utxo['outputIndex']].script,\n            'pubkey': pubkey,\n            'bip32_path': bip32_path,\n            'outputIndex': utxo['outputIndex'],\n            'txid': utxo['txid']\n        })\n\n    amount -= int(tx_fee)\n    amount = int(amount)\n\n    new_transaction = bitcoinTransaction()  # new transaction object to be used for serialization at the last stage\n    new_transaction.version = bytearray([0x01, 0x00, 0x00, 0x00])\n    for _addr, _amout, _path in dest_addresses:\n        output = bitcoinOutput()\n        output.script = compose_tx_locking_script(_addr, hw_session.app_config.dash_network)\n        output.amount = int.to_bytes(_amout, 8, byteorder='little')\n        new_transaction.outputs.append(output)\n\n    # join all outputs - will be used by Ledger for sigining transaction\n    all_outputs_raw = new_transaction.serializeOutputs()\n\n    # sign all inputs on Ledger and add inputs in the new_transaction object for serialization\n    for idx, new_input in enumerate(arg_inputs):\n\n        client.startUntrustedTransaction(starting, idx, trusted_inputs, new_input['locking_script'])\n        client.finalizeInputFull(all_outputs_raw)\n        sig = client.untrustedHashSign(new_input['bip32_path'], lockTime=0)\n        new_input['signature'] = sig\n\n        input = bitcoinInput()\n        input.prevOut = bytearray.fromhex(new_input['txid'])[::-1] + \\\n                        int.to_bytes(new_input['outputIndex'], 4, byteorder='little')\n        input.script = bytearray([len(sig)]) + sig + bytearray([0x21]) + new_input['pubkey']\n        input.sequence = bytearray([0xFF, 0xFF, 0xFF, 0xFF])\n        new_transaction.inputs.append(input)\n\n        starting = False\n\n    new_transaction.lockTime = bytearray([0, 0, 0, 0])\n\n    tx_raw = bytearray(new_transaction.serialize())\n    return tx_raw, amount\n","label":1}
{"content":"###############################################################################\n###############################################################################\n#Copyright (c) 2016, Andy Schroder\n#See the file README.md for licensing information.\n###############################################################################\n###############################################################################\n\n\nfrom numpy import loadtxt, lexsort, reshape, zeros, ones_like, around, abs\nfrom scipy.integrate import simps\nfrom scipy.interpolate import interp1d\nfrom FluidProperties.REFPROP import DensityFromTemperaturePressure,DynamicViscocityFromTemperatureDensity,ThermalConductivityFromTemperatureDensity,MolecularWeight\t#note, make sure don't use SetupFluid with another fluid because MolecularWeight may get changed and below it's assumed to be for CO2\nfrom helpers import SmartDictionary\nfrom HeatExchangers import GeneralRealRecuperator\n\n\n\n\n\nChannelIndices={}\nChannelIndices['Density']=0\nChannelIndices['DynamicViscocity']=1\nChannelIndices['ThermalConductivity']=2\nChannelIndices['StaticTemperature']=3\nChannelIndices['TotalTemperature']=4\nChannelIndices['TotalEnthalpy']=5\nChannelIndices['GaugeStaticPressure']=6\nChannelIndices['GaugeTotalPressure']=7\nChannelIndices['AxialVelocity']=8\nChannelIndices['X']=9\nChannelIndices['Y']=10\n\nWallIndices={}\nWallIndices['HeatFlux']=0\nWallIndices['X']=1\nWallIndices['Y']=2\n\n\n\n\n\n\ndef ReadStarCCMDataFile(RunName,Type,PointCounts,Type2='HalfChannel'):\n\tFileName=BaseDirectory+'\/'+RunName+'-'+Type+Type2+'.csv'\n\n\tRawData=loadtxt(FileName,skiprows=1,delimiter=',')\t\t#read in the data file\n\n\tif Type2=='':\t#assume it is a y+ file and no sorting is needed\n\t\tSortedData=RawData\n\telse:\n\n#for some reason lots of round off error in the data export from starccm+ when using the original grid and not a presentation grid derived part.\n#also, there was a huge spike for some reason in temperature and don't understand why\n#some other cases with presentation grid had some small spikes, but they weren't as terrible\n#anyway, need to round data to get the sort to work properly\n#note, this rounding mode is not the best because doesn't just truncate floating precision, but the fixed precision.\n#\t\tRawData[:,-3]=around(RawData[:,-3],decimals=11)\n\n\t\tSortedData=RawData[lexsort((RawData[:,-2],RawData[:,-3]))]\t#for some reason starccm+ does not sort the output, so do that.\n\n\n\tData=reshape(SortedData,PointCounts)\t\t\t\t#reshape to [x,y,parameter]\n\n\treturn Data\n\n\ndef ReadAndInterpolateRegularHeatFluxValues(RunName,Type,PointCounts,XValues):\n\n\tData=ReadStarCCMDataFile(RunName,Type,PointCounts,'Wall')\n\n\t#fudge some values so they don't give out or range errors\n#don't think this is needed if \"data on vertices\" is selected during export\n#\tData[0,0,WallIndices['X']]=XValues[0]\n#\tData[-1,0,WallIndices['X']]=XValues[-1]\n\n\tHeatFluxInterpolator=interp1d(Data[:,0,WallIndices['X']],Data[:,0,WallIndices['HeatFlux']])\t\t#for some reason LinearNDInterpolator requires 2D data, so using the less general \n\n\tRegularData=zeros((XValues.size,Data.shape[1],Data.shape[2]))\n\n\tRegularData[:,0,WallIndices['X']]=XValues\n\t#just leave the y and z values as zero for now because they aren't even used anyway\n\tRegularData[:,0,WallIndices['HeatFlux']]=HeatFluxInterpolator(XValues)\n\n\treturn RegularData\n\n\n\ndef ComputeAverage(Parameter,Data):\n\n#seems to be about 0.1% error created with this averaging technique, and not really sure why......maybe this was a grid or \"data on vertices\" issue that has now been resolved???\n\n\t#normal vector to the exiting side of the plane is 1,0,0\n\t#z component of velocitiy is not only zero but also meaningless in an averaging sense because there is no component of the normal vector in that direction\n\t#y component of velocity is not zero but it is also meaning less because there is no component of the normal vector in that direction\n\tMassFlux=Data[:,:,ChannelIndices['Density']]*Data[:,:,ChannelIndices['AxialVelocity']]\n\n\tif Parameter == 'MassFlux':\n\t\t#do a surface average for mass flux because it doesn't really make sense to weight it by itself\n\t\tParameterToAverage=MassFlux\n\t\tParameterToWeight=ones_like(Data[:,:,0])\n\telse:\n\t\t#for every thing else, weight by mass flux\n\t\tParameterToAverage=Data[:,:,ChannelIndices[Parameter]]\n\t\tParameterToWeight=MassFlux\n\n\n\txPoints,yPoints,_=Data.shape\n\n\tAveragedData=zeros(xPoints)\n\n\tfor xPoint in range(xPoints):\n\t\t#since no values are dependent upon the z direction, pretty much skipping those parts of the integral because it just cancels out.\n\t\t#simps function nomenclature\/use of x is actually y in this geometry\n\t\tAveragedData[xPoint]=simps(\n\t\t\t\t\t\ty=(\n\t\t\t\t\t\t\tParameterToWeight[xPoint,:]*ParameterToAverage[xPoint,:]\n\t\t\t\t\t\t),\n\t\t\t\t\t\tx=Data[xPoint,:,ChannelIndices['Y']]\n\t\t\t\t\t\t)\/simps(\n\t\t\t\t\t\ty=(\n\t\t\t\t\t\t\tParameterToWeight[xPoint,:]\n\t\t\t\t\t\t),\n\t\t\t\t\t\tx=Data[xPoint,:,ChannelIndices['Y']]\n\t\t\t\t\t\t)\n\n\tXValues=Data[:,0,ChannelIndices['X']]\t\t#x values are the same for all y, so just use point 0\n\n\treturn XValues,AveragedData\n\n\n\ndef ComputeIntegral(Parameter,Data):\n\n\tif Parameter == 'MassFlux':\n\t\t#also, as noted above, assumes axial velocity is normal to the plane\n\t\tParameterToIntegrate=Data[:,:,ChannelIndices['Density']]*Data[:,:,ChannelIndices['AxialVelocity']]\n\telif Parameter == 1:\n\t\tParameterToIntegrate=ones_like(Data[:,:,0])\n\telse:\n\t\tParameterToIntegrate=Data[:,:,ChannelIndices[Parameter]]\n\n\n\txPoints,yPoints,_=Data.shape\n\n\tIntegratedData=zeros(xPoints)\n\n\tfor xPoint in range(xPoints):\n\t\t#as noted above,\n\t\t#since no values are dependent upon the z direction, pretty much skipping those parts of the integral because it just cancels out.\n\t\t#simps function nomenclature\/use of x is actually y in this geometry\n\t\tIntegratedData[xPoint]=simps(y=(ParameterToIntegrate[xPoint,:]),x=Data[xPoint,:,ChannelIndices['Y']])\n\n\tXValues=Data[:,0,ChannelIndices['X']]\t\t#x values are the same for all y, so just use point 0\n\n\treturn XValues,IntegratedData\n\n\ndef ComputeReValues(Data,ReferencePressure):\n\tXValues,AveragedStaticTemperature=ComputeAverage('StaticTemperature',Data)\n\tXValues,AveragedGaugeStaticPressures=ComputeAverage('GaugeStaticPressure',Data)\n\tDensityWeightedByStaticTemperatureAndPressure=DensityFromTemperaturePressure(AveragedStaticTemperature,AveragedGaugeStaticPressures+ReferencePressure)\n\tDynamicViscocityWeightedByStaticTemperatureAndPressure=DynamicViscocityFromTemperatureDensity(AveragedStaticTemperature,DensityWeightedByStaticTemperatureAndPressure)\n\n\t#also calculate thermal conductivity since it's also needed later for Nu calculations\n\tThermalConductivityWeightedByStaticTemperatureAndPressure=ThermalConductivityFromTemperatureDensity(AveragedStaticTemperature,DensityWeightedByStaticTemperatureAndPressure)\n\n\tAveragedMassFlux=abs(ComputeAverage('MassFlux',Data)[1])\t\t#this should be a constant\n\n\tHalfChannelCrossSectionalArea=1*ComputeIntegral(1,Data)[1]\t\t#this should be a constant\n\tChannelHydraulicDiameter=2*(2*HalfChannelCrossSectionalArea\/1)\t\t#this should be a constant\n\n\n\tChannelReValues=AveragedMassFlux*ChannelHydraulicDiameter\/DynamicViscocityWeightedByStaticTemperatureAndPressure\n\n\treturn XValues,ChannelReValues,DynamicViscocityWeightedByStaticTemperatureAndPressure,ChannelHydraulicDiameter,ThermalConductivityWeightedByStaticTemperatureAndPressure,AveragedMassFlux\t\t#also return DynamicViscocityWeightedByStaticTemperatureAndPressure and some other stuff that are needed later for other things\n\n\ndef RecuperatorSimplifier(LowTemperature,HighTemperature,LowPressure,HighPressure,MassFraction,MinimumDeltaT):\n\n\tRecuperator=SmartDictionary()\n\tRecuperatorInputParameters=SmartDictionary()\n\tRecuperatorInputParameters['LowPressure']['MassFraction']=1.0\n\tRecuperatorInputParameters['HighPressure']['MassFraction']=MassFraction\n\tRecuperatorInputParameters['NumberofTemperatures']=200\n\tRecuperator['LowPressure']['StartingProperties']['Temperature']=HighTemperature\n\tRecuperator['LowPressure']['StartingProperties']['Pressure']=LowPressure\n\tRecuperator['HighPressure']['StartingProperties']['Temperature']=LowTemperature\n\tRecuperator['HighPressure']['StartingProperties']['Pressure']=HighPressure\n\n\tRecuperatorInputParameters['MinimumDeltaT']=MinimumDeltaT\n\tRecuperatorInputParameters['DeltaPPerDeltaT']=0\n\n\treturn GeneralRealRecuperator(Recuperator,RecuperatorInputParameters)\n\n\n\n\n\n\n\n\ndef SutherlandLaw(Temperature=273.,Fluid='CO2'):\n\n\t#setup this function to be general for any fluid, but actually only have constants for CO2 populated\n\tConstants={\n\t\t\t'CO2':{\n\t\t\t\t'ReferenceTemperature':\t\t273.,\n\t\t\t\t'ReferenceDynamicViscosity':\t1.370*10**(-5),\n\t\t\t\t'SutherlandConstant':\t\t222.,\n\t\t\t\t},\n\t\t}\n\n\tDynamicViscosity=Constants[Fluid]['ReferenceDynamicViscosity']*((Temperature\/Constants[Fluid]['ReferenceTemperature'])**(3.\/2.))*(Constants[Fluid]['ReferenceTemperature']+Constants[Fluid]['SutherlandConstant'])\/(Temperature+Constants[Fluid]['SutherlandConstant'])\n\n\treturn DynamicViscosity\n\n\n\ndef IdealGasDensity(Temperature,Pressure,Fluid='CO2'):\n\n\tR=8.3144598\t\t\t\t\t#J\/(mol*K), http:\/\/physics.nist.gov\/cgi-bin\/cuu\/Value?r\n\n\t#setup this function to be general for any fluid, but actually only have molar mass for CO2 populated\n\tMolarMass={'CO2':MolecularWeight}\t\t#g\/mole, see note above about MolecularWeight and SetupFluid\n\n\tRspecific=R\/(MolarMass[Fluid]\/1000)\t\t#J\/(kg*k)\n\n\tDensity=Pressure\/(Temperature*Rspecific)\t#kg\/m^3\n\n\treturn Density\n\n\n\n\n","label":1}
{"content":"# import Modules ----------------------------------\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create Functions for Rays ----------------------\n# rays are definied as ax+by+c=0\n#math ref: https:\/\/www.cuemath.com\/geometry\/intersection-of-two-lines\/\n\n# generate parrellel rays\ndef Generate_Rays(Theta,Spacing,Num):\n    # takes angle, spacing and number of rays \n    # return a matrix of a,b,c values for each ray\n    Rays = np.zeros([3,Num]) # empty matrix with a,b and c values ax+by+c=0\n    if (Theta == 0): #horizontal lines\n        Rays[0,:] = 0\n        Rays[1,:] = 1\n        Rays[2,:] = np.arange(0, Spacing*Num,Spacing) - Num*Spacing\/2 + Spacing\/2\n    elif (Theta == np.pi\/2): # vertical lines\n        Rays[0,:] = 1\n        Rays[1,:] = 0\n        Rays[2,:] = np.arange(0, Spacing*Num,Spacing) - Num*Spacing\/2 + Spacing\/2\n    else:\n        Rays[0,:] = -np.tan(Theta)\n        Rays[1,:] = 1\n        Rays[2,:] = (np.arange(0, Spacing*Num,Spacing) - Num*Spacing\/2 + Spacing\/2)\/np.abs(np.cos(Theta))\n    \n    return Rays\n\n# Generate rays from points\ndef Generator_Lines(P1,P2):\n    # takes to points P = [[x],[y]]\n    # returns vector of a,b,c for line\n    # ax+by+c=0\n    A =-(P2[1] - P1[1])\n    B = P2[0] - P1[0]\n    C = -P1[0]*A - P1[1]*B\n    return np.array([A,B,C])\n\n# find intercept points of rays\ndef Line_Intersection(Line1,Line2):\n    # takes P = [[a],[b],[c]] where ax+by+C=0\n    # returns points of intersection [x,y]\n    # for all possible line combose\n\n    # returns runtime error when lines are parrell (never intercept)\n    X = (np.outer(Line1[1],Line2[2])-np.outer(Line1[2],Line2[1]))\/(np.outer(Line1[0],Line2[1])-np.outer(Line1[1],Line2[0]))\n    Y = (np.outer(Line1[2],Line2[0])-np.outer(Line1[0],Line2[2]))\/(np.outer(Line1[0],Line2[1])-np.outer(Line1[1],Line2[0]))\n    return X,Y\n\n# Functions used for plotting results ------------------------\n# mostly usuful for debugging\n\n# Function used to plot layer\ndef Plot_Slice(P1,P2,color='b',width=None):\n    # takes set of points\n    X1 = P1[0]\n    X2 = P2[0]\n    Y1 = P1[1]\n    Y2 = P2[1]\n    plt.plot([X1,X2],[Y1,Y2],color,linewidth=width) \n\n# Plots rays within the x\/y limits\ndef Plot_Rays(Rays,X_lim,Y_lim,color='y'):\n    # takes ray parameters, x limit, and\n    # y limit and plots the lines\n    Y = np.zeros([2,len(Rays[0,:])])\n    X = np.zeros([2,len(Rays[0,:])])\n\n    M = -Rays[0,:]\/Rays[1,:] # slope of lines\n    idx_vert = (M >= 1) | (M < -1) # rays between 45-135 deg\n    idx_line = idx_vert == 0 # all other rays\n\n    # generate lines\n    Y[0,idx_vert] += Y_lim[0]\n    Y[1,idx_vert] += Y_lim[1]\n    X[0,idx_vert] = -(Rays[1,idx_vert]*Y[0,idx_vert]+Rays[2,idx_vert])\/Rays[0,idx_vert]\n    X[1,idx_vert] = -(Rays[1,idx_vert]*Y[1,idx_vert]+Rays[2,idx_vert])\/Rays[0,idx_vert]\n\n    # generate lines\n    X[0,idx_line] += X_lim[0]\n    X[1,idx_line] += X_lim[1]\n    Y[0,idx_line] = -(Rays[0,idx_line]*X[0,idx_line]+Rays[2,idx_line])\/Rays[1,idx_line]\n    Y[1,idx_line] = -(Rays[0,idx_line]*X[1,idx_line]+Rays[2,idx_line])\/Rays[1,idx_line]\n\n    # plot lines\n    plt.plot(X,Y,color)\n    plt.xlim(X_lim[0]*1.05,X_lim[1]*1.05)\n    plt.ylim(Y_lim[0]*1.05,Y_lim[1]*1.05)\n    return X,Y\n\n# Running Program ------------------------------\nP1 = np.load(\"Benchy_Set_1.npy\")\nP2 = np.load(\"Benchy_Set_2.npy\")\n\nlayers = np.load(\"Benchy_Layers.npy\")[30]\ni = P1[2,:] == layers\nP1 = P1[0:2,i]\nP2 = P2[0:2,i]\n\n#\nlines = Generator_Lines(P1,P2) # generate lines from slice 100\n\nrays = Generate_Rays(0*np.pi\/180,0.25,100) # Generate rays\n\nxp,yp = Line_Intersection(rays,lines) # Calculate intersection matriz reutrn x y points\n\nxpf,ypf = In_Bound_Points(xp,yp,P1,P2) # filter out points that dont lie inbtween points\n\n# plot results\n# fit around outline\nplt.close()\nPlot_Slice(P1,p2_100)\nx_lim = np.array([np.min(P1[0,:]),np.max(P1[0,:])])*1.05\ny_lim = np.array([np.min(P1[1,:]),np.max(P1[1,:])])*1.05\nPlot_Rays(rays,x_lim,y_lim)\nPlot_Rays(rays[:,idx],x_lim,y_lim,'r')\nplt.plot(xpf,ypf,'.g')\nplt.plot(xp,yp,'.')\n\nidx = 878 # slice(878,879)\nPlot_Slice(p1_100[:,idx],p2_100[:,idx],'r')\n\nplt.xlim([np.min(part_mesh.x),np.max(part_mesh.x)])\nplt.ylim([np.min(part_mesh.y),np.max(part_mesh.y)])\nplt.show()","label":1}
{"content":"\"\"\"\nThis module deals with interpreting the parse tree as Python\nwould have done, in the compiler.\n\nFor now this only covers parse tree to value conversion of\ncompile-time values.\n\"\"\"\n\nfrom __future__ import absolute_import\n\nfrom .Nodes import *\nfrom .ExprNodes import *\nfrom .Errors import CompileError\n\n\nclass EmptyScope(object):\n    def lookup(self, name):\n        return None\n\nempty_scope = EmptyScope()\n\ndef interpret_compiletime_options(optlist, optdict, type_env=None, type_args=()):\n    \"\"\"\n    Tries to interpret a list of compile time option nodes.\n    The result will be a tuple (optlist, optdict) but where\n    all expression nodes have been interpreted. The result is\n    in the form of tuples (value, pos).\n\n    optlist is a list of nodes, while optdict is a DictNode (the\n    result optdict is a dict)\n\n    If type_env is set, all type nodes will be analysed and the resulting\n    type set. Otherwise only interpretateable ExprNodes\n    are allowed, other nodes raises errors.\n\n    A CompileError will be raised if there are problems.\n    \"\"\"\n\n    def interpret(node, ix):\n        if ix in type_args:\n            if type_env:\n                type = node.analyse_as_type(type_env)\n                if not type:\n                    raise CompileError(node.pos, \"Invalid type.\")\n                return (type, node.pos)\n            else:\n                raise CompileError(node.pos, \"Type not allowed here.\")\n        else:\n            if (sys.version_info[0] >=3 and\n                isinstance(node, StringNode) and\n                node.unicode_value is not None):\n                return (node.unicode_value, node.pos)\n            return (node.compile_time_value(empty_scope), node.pos)\n\n    if optlist:\n        optlist = [interpret(x, ix) for ix, x in enumerate(optlist)]\n    if optdict:\n        assert isinstance(optdict, DictNode)\n        new_optdict = {}\n        for item in optdict.key_value_pairs:\n            new_key, dummy = interpret(item.key, None)\n            new_optdict[new_key] = interpret(item.value, item.key.value)\n        optdict = new_optdict\n    return (optlist, new_optdict)\n","label":1}
{"content":"#!\/usr\/bin\/env python\nimport setuptools\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"ppl-pkg-PINPEREPETTE\",\n    version=\"0.0.1\",\n    author=\"Pinperepette\",\n    author_email=\"pinperepette@gmail.com\",\n    description=\"Python Process Launch and kill\",\n    long_description=long_description,\n    long_description_content_type=\"text\/markdown\",\n    url=\"\",\n    project_urls={\n        \"Bug Tracker\": \"\",\n    },\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    package_dir={\"\": \"src\"},\n    packages=setuptools.find_packages(where=\"src\"),\n    python_requires=\">=3.6\",\n)\n","label":1}
{"content":"# -*- coding: utf-8 -*-\n# Generated by Django 1.11 on 2019-12-14 16:58\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('auction', '0002_auto_20191214_1654'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='bid',\n            name='bid_amount',\n            field=models.DecimalField(decimal_places=2, default=0, max_digits=9),\n        ),\n    ]\n","label":1}
{"content":"\"\"\"\nTests of various import related things that could not be tested with \"Black Box\nTests\".\n\"\"\"\n\nimport os\n\nimport pytest\nfrom jedi.file_io import FileIO, KnownContentFileIO\n\nfrom jedi._compatibility import find_module_py33, find_module\nfrom jedi.inference import compiled\nfrom jedi.inference import imports\nfrom jedi.api.project import Project\nfrom jedi.inference.gradual.conversion import _stub_to_python_value_set\nfrom jedi.inference.references import get_module_contexts_containing_name\nfrom ..helpers import cwd_at, get_example_dir, test_dir, root_dir\n\nTHIS_DIR = os.path.dirname(__file__)\n\n\n@pytest.mark.skipif('sys.version_info < (3,3)')\ndef test_find_module_py33():\n    \"\"\"Needs to work like the old find_module.\"\"\"\n    assert find_module_py33('_io') == (None, False)\n    with pytest.raises(ImportError):\n        assert find_module_py33('_DOESNTEXIST_') == (None, None)\n\n\ndef test_find_module_package():\n    file_io, is_package = find_module('json')\n    assert file_io.path.endswith(os.path.join('json', '__init__.py'))\n    assert is_package is True\n\n\ndef test_find_module_not_package():\n    file_io, is_package = find_module('io')\n    assert file_io.path.endswith('io.py')\n    assert is_package is False\n\n\npkg_zip_path = get_example_dir('zipped_imports', 'pkg.zip')\n\n\ndef test_find_module_package_zipped(Script, inference_state, environment):\n    sys_path = environment.get_sys_path() + [pkg_zip_path]\n    script = Script('import pkg; pkg.mod', sys_path=sys_path)\n    assert len(script.complete()) == 1\n\n    file_io, is_package = inference_state.compiled_subprocess.get_module_info(\n        sys_path=sys_path,\n        string=u'pkg',\n        full_name=u'pkg'\n    )\n    assert file_io is not None\n    assert file_io.path.endswith(os.path.join('pkg.zip', 'pkg', '__init__.py'))\n    assert file_io._zip_path.endswith('pkg.zip')\n    assert is_package is True\n\n\n@pytest.mark.parametrize(\n    'code, file, package, path', [\n        ('import pkg', '__init__.py', 'pkg', 'pkg'),\n        ('import pkg', '__init__.py', 'pkg', 'pkg'),\n\n        ('from pkg import module', 'module.py', 'pkg', None),\n        ('from pkg.module', 'module.py', 'pkg', None),\n\n        ('from pkg import nested', os.path.join('nested', '__init__.py'),\n         'pkg.nested', os.path.join('pkg', 'nested')),\n        ('from pkg.nested', os.path.join('nested', '__init__.py'),\n         'pkg.nested', os.path.join('pkg', 'nested')),\n\n        ('from pkg.nested import nested_module',\n         os.path.join('nested', 'nested_module.py'), 'pkg.nested', None),\n        ('from pkg.nested.nested_module',\n         os.path.join('nested', 'nested_module.py'), 'pkg.nested', None),\n\n        ('from pkg.namespace import namespace_module',\n         os.path.join('namespace', 'namespace_module.py'), 'pkg.namespace', None),\n        ('from pkg.namespace.namespace_module',\n         os.path.join('namespace', 'namespace_module.py'), 'pkg.namespace', None),\n    ]\n\n)\ndef test_correct_zip_package_behavior(Script, inference_state, environment, code,\n                                      file, package, path, skip_python2):\n    sys_path = environment.get_sys_path() + [pkg_zip_path]\n    pkg, = Script(code, sys_path=sys_path).infer()\n    value, = pkg._name.infer()\n    assert value.py__file__() == os.path.join(pkg_zip_path, 'pkg', file)\n    assert '.'.join(value.py__package__()) == package\n    assert value.is_package() is (path is not None)\n    if path is not None:\n        assert value.py__path__() == [os.path.join(pkg_zip_path, path)]\n\n\ndef test_find_module_not_package_zipped(Script, inference_state, environment):\n    path = get_example_dir('zipped_imports', 'not_pkg.zip')\n    sys_path = environment.get_sys_path() + [path]\n    script = Script('import not_pkg; not_pkg.val', sys_path=sys_path)\n    assert len(script.complete()) == 1\n\n    file_io, is_package = inference_state.compiled_subprocess.get_module_info(\n        sys_path=sys_path,\n        string=u'not_pkg',\n        full_name=u'not_pkg'\n    )\n    assert file_io.path.endswith(os.path.join('not_pkg.zip', 'not_pkg.py'))\n    assert is_package is False\n\n\n@cwd_at('test\/examples\/not_in_sys_path\/pkg')\ndef test_import_not_in_sys_path(Script):\n    \"\"\"\n    non-direct imports (not in sys.path)\n\n    This is in the end just a fallback.\n    \"\"\"\n    a = Script(path='module.py').infer(line=5)\n    assert a[0].name == 'int'\n\n    a = Script(path='module.py').infer(line=6)\n    assert a[0].name == 'str'\n    a = Script(path='module.py').infer(line=7)\n    assert a[0].name == 'str'\n\n\n@pytest.mark.parametrize(\"code,name\", [\n    (\"from flask.ext import foo; foo.\", \"Foo\"),  # flask_foo.py\n    (\"from flask.ext import bar; bar.\", \"Bar\"),  # flaskext\/bar.py\n    (\"from flask.ext import baz; baz.\", \"Baz\"),  # flask_baz\/__init__.py\n    (\"from flask.ext import moo; moo.\", \"Moo\"),  # flaskext\/moo\/__init__.py\n    (\"from flask.ext.\", \"foo\"),\n    (\"from flask.ext.\", \"bar\"),\n    (\"from flask.ext.\", \"baz\"),\n    (\"from flask.ext.\", \"moo\"),\n    pytest.param(\"import flask.ext.foo; flask.ext.foo.\", \"Foo\", marks=pytest.mark.xfail),\n    pytest.param(\"import flask.ext.bar; flask.ext.bar.\", \"Foo\", marks=pytest.mark.xfail),\n    pytest.param(\"import flask.ext.baz; flask.ext.baz.\", \"Foo\", marks=pytest.mark.xfail),\n    pytest.param(\"import flask.ext.moo; flask.ext.moo.\", \"Foo\", marks=pytest.mark.xfail),\n])\ndef test_flask_ext(Script, code, name):\n    \"\"\"flask.ext.foo is really imported from flaskext.foo or flask_foo.\n    \"\"\"\n    path = get_example_dir('flask-site-packages')\n    completions = Script(code, sys_path=[path]).complete()\n    assert name in [c.name for c in completions]\n\n\n@cwd_at('test\/test_inference\/')\ndef test_not_importable_file(Script):\n    src = 'import not_importable_file as x; x.'\n    assert not Script(src, path='example.py').complete()\n\n\ndef test_import_unique(Script):\n    src = \"import os; os.path\"\n    defs = Script(src, path='example.py').infer()\n    parent_contexts = [d._name._value for d in defs]\n    assert len(parent_contexts) == len(set(parent_contexts))\n\n\ndef test_cache_works_with_sys_path_param(Script, tmpdir):\n    foo_path = tmpdir.join('foo')\n    bar_path = tmpdir.join('bar')\n    foo_path.join('module.py').write('foo = 123', ensure=True)\n    bar_path.join('module.py').write('bar = 123', ensure=True)\n    foo_completions = Script('import module; module.',\n                             sys_path=[foo_path.strpath]).complete()\n    bar_completions = Script('import module; module.',\n                             sys_path=[bar_path.strpath]).complete()\n    assert 'foo' in [c.name for c in foo_completions]\n    assert 'bar' not in [c.name for c in foo_completions]\n\n    assert 'bar' in [c.name for c in bar_completions]\n    assert 'foo' not in [c.name for c in bar_completions]\n\n\ndef test_import_completion_docstring(Script):\n    import abc\n    s = Script('\"\"\"test\"\"\"\\nimport ab')\n    abc_completions = [c for c in s.complete() if c.name == 'abc']\n    assert len(abc_completions) == 1\n    assert abc_completions[0].docstring(fast=False) == abc.__doc__\n\n    # However for performance reasons not all modules are loaded and the\n    # docstring is empty in this case.\n    assert abc_completions[0].docstring() == ''\n\n\ndef test_goto_definition_on_import(Script):\n    assert Script(\"import sys_blabla\").infer(1, 8) == []\n    assert len(Script(\"import sys\").infer(1, 8)) == 1\n\n\n@cwd_at('jedi')\ndef test_complete_on_empty_import(Script):\n    assert Script(\"from datetime import\").complete()[0].name == 'import'\n    # should just list the files in the directory\n    assert 10 < len(Script(\"from .\", path='whatever.py').complete()) < 30\n\n    # Global import\n    assert len(Script(\"from . import\", 'whatever.py').complete(1, 5)) > 30\n    # relative import\n    assert 10 < len(Script(\"from . import\", 'whatever.py').complete(1, 6)) < 30\n\n    # Global import\n    assert len(Script(\"from . import classes\", 'whatever.py').complete(1, 5)) > 30\n    # relative import\n    assert 10 < len(Script(\"from . import classes\", 'whatever.py').complete(1, 6)) < 30\n\n    wanted = {'ImportError', 'import', 'ImportWarning'}\n    assert {c.name for c in Script(\"import\").complete()} == wanted\n    assert len(Script(\"import import\", path='').complete()) > 0\n\n    # 111\n    assert Script(\"from datetime import\").complete()[0].name == 'import'\n    assert Script(\"from datetime import \").complete()\n\n\ndef test_imports_on_global_namespace_without_path(Script):\n    \"\"\"If the path is None, there shouldn't be any import problem\"\"\"\n    completions = Script(\"import operator\").complete()\n    assert [c.name for c in completions] == ['operator']\n    completions = Script(\"import operator\", path='example.py').complete()\n    assert [c.name for c in completions] == ['operator']\n\n    # the first one has a path the second doesn't\n    completions = Script(\"import keyword\", path='example.py').complete()\n    assert [c.name for c in completions] == ['keyword']\n    completions = Script(\"import keyword\").complete()\n    assert [c.name for c in completions] == ['keyword']\n\n\ndef test_named_import(Script):\n    \"\"\"named import - jedi-vim issue #8\"\"\"\n    s = \"import time as dt\"\n    assert len(Script(s, path='\/').infer(1, 15)) == 1\n    assert len(Script(s, path='\/').infer(1, 10)) == 1\n\n\n@pytest.mark.skipif('True', reason='The nested import stuff is still very messy.')\ndef test_goto_following_on_imports(Script):\n    s = \"import multiprocessing.dummy; multiprocessing.dummy\"\n    g = Script(s).goto()\n    assert len(g) == 1\n    assert (g[0].line, g[0].column) != (0, 0)\n\n\ndef test_goto(Script):\n    sys, = Script(\"import sys\", 1, 10).goto(follow_imports=True)\n    assert sys.type == 'module'\n\n\ndef test_os_after_from(Script):\n    def check(source, result, column=None):\n        completions = Script(source).complete(column=column)\n        assert [c.name for c in completions] == result\n\n    check('\\nfrom os. ', ['path'])\n    check('\\nfrom os ', ['import'])\n    check('from os ', ['import'])\n    check('\\nfrom os import whatever', ['import'], len('from os im'))\n\n    check('from os\\\\\\n', ['import'])\n    check('from os \\\\\\n', ['import'])\n\n\ndef test_os_issues(Script):\n    def import_names(*args, **kwargs):\n        return [d.name for d in Script(*args).complete(**kwargs)]\n\n    # Github issue #759\n    s = 'import os, s'\n    assert 'sys' in import_names(s)\n    assert 'path' not in import_names(s, column=len(s) - 1)\n    assert 'os' in import_names(s, column=len(s) - 3)\n\n    # Some more checks\n    s = 'from os import path, e'\n    assert 'environ' in import_names(s)\n    assert 'json' not in import_names(s, column=len(s) - 1)\n    assert 'environ' in import_names(s, column=len(s) - 1)\n    assert 'path' in import_names(s, column=len(s) - 3)\n\n\ndef test_path_issues(Script):\n    \"\"\"\n    See pull request #684 for details.\n    \"\"\"\n    source = '''from datetime import '''\n    assert Script(source).complete()\n\n\ndef test_compiled_import_none(monkeypatch, Script):\n    \"\"\"\n    Related to #1079. An import might somehow fail and return None.\n    \"\"\"\n    script = Script('import sys')\n    monkeypatch.setattr(compiled, 'load_module', lambda *args, **kwargs: None)\n    def_, = script.infer()\n    assert def_.type == 'module'\n    value, = def_._name.infer()\n    assert not _stub_to_python_value_set(value)\n\n\n@pytest.mark.parametrize(\n    ('path', 'is_package', 'goal'), [\n        # Both of these tests used to return relative paths to the module\n        # context that was initially given, but now we just work with the file\n        # system.\n        (os.path.join(THIS_DIR, 'test_docstring.py'), False,\n         ('test', 'test_inference', 'test_imports')),\n        (os.path.join(THIS_DIR, '__init__.py'), True,\n         ('test', 'test_inference', 'test_imports')),\n    ]\n)\ndef test_get_modules_containing_name(inference_state, path, goal, is_package):\n    module = imports._load_python_module(\n        inference_state,\n        FileIO(path),\n        import_names=('ok', 'lala', 'x'),\n        is_package=is_package,\n    )\n    assert module\n    module_context = module.as_context()\n    input_module, found_module = get_module_contexts_containing_name(\n        inference_state,\n        [module_context],\n        'string_that_only_exists_here'\n    )\n    assert input_module is module_context\n    assert found_module.string_names == goal\n\n\n@pytest.mark.parametrize(\n    ('path', 'base_names', 'is_package', 'names'), [\n        ('\/foo\/bar.py', ('foo',), False, ('foo', 'bar')),\n        ('\/foo\/bar.py', ('foo', 'baz'), False, ('foo', 'baz', 'bar')),\n        ('\/foo\/__init__.py', ('foo',), True, ('foo',)),\n        ('\/__init__.py', ('foo',), True, ('foo',)),\n        ('\/foo\/bar\/__init__.py', ('foo',), True, ('foo',)),\n        ('\/foo\/bar\/__init__.py', ('foo', 'bar'), True, ('foo', 'bar')),\n    ]\n)\ndef test_load_module_from_path(inference_state, path, base_names, is_package, names):\n    file_io = KnownContentFileIO(path, '')\n    m = imports.load_module_from_path(inference_state, file_io, base_names)\n    assert m.is_package() == is_package\n    assert m.string_names == names\n\n\n@pytest.mark.parametrize(\n    'path', ('api\/whatever\/test_this.py', 'api\/whatever\/file'))\n@pytest.mark.parametrize('empty_sys_path', (False, True))\ndef test_relative_imports_with_multiple_similar_directories(Script, path, empty_sys_path):\n    dir = get_example_dir('issue1209')\n    if empty_sys_path:\n        project = Project(dir, sys_path=(), smart_sys_path=False)\n    else:\n        project = Project(dir)\n    script = Script(\n        \"from . \",\n        path=os.path.join(dir, path),\n        _project=project,\n    )\n    name, import_ = script.complete()\n    assert import_.name == 'import'\n    assert name.name == 'api_test1'\n\n\ndef test_relative_imports_with_outside_paths(Script):\n    dir = get_example_dir('issue1209')\n    project = Project(dir, sys_path=[], smart_sys_path=False)\n    script = Script(\n        \"from ...\",\n        path=os.path.join(dir, 'api\/whatever\/test_this.py'),\n        _project=project,\n    )\n    assert [c.name for c in script.complete()] == ['api', 'whatever']\n\n    script = Script(\n        \"from \" + '.' * 100,\n        path=os.path.join(dir, 'api\/whatever\/test_this.py'),\n        _project=project,\n    )\n    assert not script.complete()\n\n\n@cwd_at('test\/examples\/issue1209\/api\/whatever\/')\ndef test_relative_imports_without_path(Script):\n    project = Project('.', sys_path=[], smart_sys_path=False)\n    script = Script(\"from . \", _project=project)\n    assert [c.name for c in script.complete()] == ['api_test1', 'import']\n\n    script = Script(\"from .. \", _project=project)\n    assert [c.name for c in script.complete()] == ['import', 'whatever']\n\n    script = Script(\"from ... \", _project=project)\n    assert [c.name for c in script.complete()] == ['api', 'import', 'whatever']\n\n\ndef test_relative_import_out_of_file_system(Script):\n    code = \"from \" + '.' * 100\n    assert not Script(code).complete()\n    script = Script(code + ' ')\n    import_, = script.complete()\n    assert import_.name == 'import'\n\n    script = Script(\"from \" + '.' * 100 + 'abc import ABCMeta')\n    assert not script.infer()\n    assert not script.complete()\n\n\n@pytest.mark.parametrize(\n    'level, directory, project_path, result', [\n        (1, '\/a\/b\/c', '\/a', (['b', 'c'], '\/a')),\n        (2, '\/a\/b\/c', '\/a', (['b'], '\/a')),\n        (3, '\/a\/b\/c', '\/a', ([], '\/a')),\n        (4, '\/a\/b\/c', '\/a', (None, '\/')),\n        (5, '\/a\/b\/c', '\/a', (None, None)),\n        (1, '\/', '\/', ([], '\/')),\n        (2, '\/', '\/', (None, None)),\n        (1, '\/a\/b', '\/a\/b\/c', (None, '\/a\/b')),\n        (2, '\/a\/b', '\/a\/b\/c', (None, '\/a')),\n        (3, '\/a\/b', '\/a\/b\/c', (None, '\/')),\n    ]\n)\ndef test_level_to_import_path(level, directory, project_path, result):\n    assert imports._level_to_base_import_path(project_path, directory, level) == result\n\n\ndef test_import_name_calculation(Script):\n    s = Script(path=os.path.join(test_dir, 'completion', 'isinstance.py'))\n    m = s._get_module_context()\n    assert m.string_names == ('test', 'completion', 'isinstance')\n\n\n@pytest.mark.parametrize('name', ('builtins', 'typing'))\ndef test_pre_defined_imports_module(Script, environment, name):\n    if environment.version_info.major < 3 and name == 'builtins':\n        name = '__builtin__'\n\n    path = os.path.join(root_dir, name + '.py')\n    module = Script('', path=path)._get_module_context()\n    assert module.string_names == (name,)\n\n    assert module.inference_state.builtins_module.py__file__() != path\n    assert module.inference_state.typing_module.py__file__() != path\n\n\n@pytest.mark.parametrize('name', ('builtins', 'typing'))\ndef test_import_needed_modules_by_jedi(Script, environment, tmpdir, name):\n    if environment.version_info.major < 3 and name == 'builtins':\n        name = '__builtin__'\n\n    module_path = tmpdir.join(name + '.py')\n    module_path.write('int = ...')\n    script = Script(\n        'import ' + name,\n        path=tmpdir.join('something.py').strpath,\n        sys_path=[tmpdir.strpath] + environment.get_sys_path(),\n    )\n    module, = script.infer()\n    assert module._inference_state.builtins_module.py__file__() != module_path\n    assert module._inference_state.typing_module.py__file__() != module_path\n\n\ndef test_import_with_semicolon(Script):\n    names = [c.name for c in Script('xzy; from abc import ').complete()]\n    assert 'ABCMeta' in names\n    assert 'abc' not in names\n\n\ndef test_relative_import_star(Script):\n    # Coming from github #1235\n    source = \"\"\"\n    from . import *\n    furl.c\n    \"\"\"\n    script = Script(source, 'export.py')\n\n    assert script.complete(3, len(\"furl.c\"))\n","label":1}
{"content":"from __future__ import annotations\n\nimport os\nimport json\nimport pathlib\nimport sys\nfrom logging import getLogger\nfrom typing import Optional, Union, Dict, List\nfrom modules.data_type import PATH_OBJ, PATH_OBJ_LIST\nfrom modules.data_type import LOAD_DATA, DATA, NODE_LIST, NODE, EDGE_LIST, EDGE\n\nlogger = getLogger(__name__)\n\n\nclass CreateJson:\n\n    def __init__(self, input_dir: str) -> None:\n        self.input_dir: PATH_OBJ = pathlib.Path(input_dir)\n        self.data_dir_list: List[str] = [\n            \"lldp_neighbors\",\n            \"lldp_neighbors_detail\",\n            \"interface\",\n            \"chassis\",\n            \"config\",\n        ]\n        self.node_list: NODE_LIST = []\n        self.edge_list: EDGE_LIST = []\n        self.id_list: List[str] = []\n        self.source_list: List[str] = []\n        self.target_list: List[str] = []\n\n    def _glob_dir(self, regex: str = None) -> PATH_OBJ_LIST:\n        \"\"\"\u6307\u5b9a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b.\"\"\"\n        path_obj_list: Optional[PATH_OBJ_LIST] = []\n        if regex is None:\n            _glob_regex = \"[!.]*\"\n        else:\n            _glob_regex = regex\n        try:\n            for dir_name in self.data_dir_list:\n                dir_path = os.path.join(*[self.input_dir, dir_name])\n                obj = pathlib.Path(dir_path).glob(_glob_regex)\n                _path_obj_list = list(p for p in obj if p.is_file())\n                path_obj_list.extend(_path_obj_list)\n        except OSError as e:\n            msg = \"Cannot load file: %s\" % str(e)\n            logger.error(msg, exc_info=True)\n            raise Exception\n        return path_obj_list\n\n    def _update_node_list(self, node_name: str, interface: str, option=None) -> None:\n        _data: DATA = {\n            \"id\": f\"{node_name}_{interface}\",\n            \"label\": interface,\n            \"parent\": node_name\n        }\n        if option is not None:\n            _data.update(option)\n        new_node: NODE = {\n            \"data\": _data,\n            \"classes\": \"rectangle\",\n            \"group\": \"nodes\",\n        }\n\n        # Append node if not exist.\n        if node_name not in self.id_list:\n            parent_node: NODE = new_node.copy()\n            parent_node[\"data\"] = {\n                \"id\": node_name,\n                \"label\": node_name,\n                \"parent\": None,\n            }\n            self.node_list.append(parent_node)\n            self.id_list.append(node_name)\n\n        # Append interface if not exist.\n        if _data[\"id\"] not in self.id_list:\n            self.node_list.append(new_node)\n            self.id_list.append(_data[\"id\"])\n\n    def _update_edge_list(self, source_id: str, target_id: str, option=None) -> None:\n        _data = {\"source\": source_id, \"target\": target_id}\n        if option is not None:\n            _data.update(option)\n        new_edge = {\"data\": _data, \"group\": \"edges\"}\n\n        if source_id in self.source_list and target_id in self.target_list:\n            pass  # edge_list has already `new_edge`.\n        elif source_id in self.target_list and target_id in self.source_list:\n            pass  # edge_list has already `new_edge` by reverse.\n        else:\n            self.edge_list.append(new_edge)\n            self.source_list.append(source_id)\n            self.target_list.append(target_id)\n\n    def _generate_data(self, file_list: PATH_OBJ_LIST) -> None:\n\n        for _file in file_list:\n            print(f\"Loading {_file}\")\n            try:\n                with open(_file) as f:\n                    load_data = json.load(f)\n\n                source_name = _file.stem\n                for source_interface, target_info in load_data.items():\n                    source_id = f\"{source_name}_{source_interface}\"\n\n                    try:\n                        target_name = target_info[0].get(\"remote_system_name\")\n                        target_interface = target_info[0][\"remote_port_description\"]\n                    except KeyError:\n                        target_name = target_info[0][\"hostname\"]\n                        target_interface = target_info[0][\"port\"]\n                    except Exception:\n                        raise IndexError\n                    target_id = f\"{target_name}_{target_interface}\"\n\n                    # Node update for source device.\n                    print(f\" Updating Node: {source_name}\")\n                    self._update_node_list(\n                        node_name=source_name,\n                        interface=source_interface,\n                        option=None,  # add optional info. future release.\n                    )\n\n                    # Node update for target device.\n                    print(f\" Updating Node: {target_name}\")\n                    self._update_node_list(\n                        node_name=target_name,\n                        interface=target_interface,\n                        option=None,  # add optional info. future release.\n                    )\n\n                    # Edge update\n                    print(f\" Updating Edge: {source_id},{target_id}\")\n                    self._update_edge_list(\n                        source_id=source_id,\n                        target_id=target_id,\n                        option=None,  # add optional info. future release.\n                    )\n            except OSError as e:\n                logger.error(str(e))\n            except IndexError as e:\n                logger.error(str(e))\n\n    def run(self) -> None:\n        print(\"Loading data files.\")\n        file_list: PATH_OBJ_LIST = self._glob_dir()\n        self._generate_data(file_list)\n\n    def json_dumper(self, output_dir: str) -> int:\n        response_code: int = 1\n        try:\n            self.run()\n            node_file: str = os.path.join(*[output_dir, \"node.json\"])\n            edge_file: str = os.path.join(*[output_dir, \"edge.json\"])\n            pathlib.Path(node_file).write_text(json.dumps(self.node_list, indent=2))\n            pathlib.Path(edge_file).write_text(json.dumps(self.edge_list, indent=2))\n            print(\"Generating json files successfuly.\")\n        except OSError as e:\n            logger.error(str(e))\n        except Exception as e:\n            logger.error(str(e), exc_info=True)\n        else:\n            response_code = 0\n        finally:\n            return response_code\n\n    def data_dumper(self):\n        load_data = []\n        try:\n            self.run()\n            load_data.extend(self.node_list)\n            load_data.extend(self.edge_list)\n        except Exception as e:\n            logger.error(str(e), exc_info=True)\n            sys.exit(1)\n        else:\n            return load_data\n","label":1}
{"content":"from . import FixtureTest\n\n\n# ocean and sea labels should be in the water layer rather than the places\n# layer.\nclass SeaOceanLabelsWaterLayer(FixtureTest):\n\n    def test_gulf_of_california(self):\n        # Gulf of California: http:\/\/www.openstreetmap.org\/node\/305639734\n        self.load_fixtures([\n            'http:\/\/www.openstreetmap.org\/node\/305639734',\n        ])\n        self.assert_has_feature(\n            9, 97, 215, 'water',\n            {'kind': 'sea', 'name': 'Gulf of California',\n             'label_placement': True})\n        self.assert_no_matching_feature(\n            9, 97, 215, 'places',\n            {'kind': 'sea', 'name': 'Gulf of California'})\n\n    def test_greenland_sea(self):\n        # Greenland Sea: http:\/\/www.openstreetmap.org\/node\/305639396\n        self.load_fixtures([\n            'http:\/\/www.openstreetmap.org\/node\/305639396',\n        ])\n        self.assert_has_feature(\n            9, 241, 90, 'water',\n            {'kind': 'sea', 'name': 'Greenland Sea',\n             'label_placement': True})\n        self.assert_no_matching_feature(\n            9, 241, 90, 'places',\n            {'kind': 'sea', 'name': 'Greenland Sea'})\n\n# NOTE: No ocean points in the North America extract :-(\n","label":1}
{"content":"import os\nimport re\nfrom pathlib import Path\nfrom shutil import copyfile\n\nimport typer\nimport decamelize\nfrom git import Repo\nfrom jinja2 import PackageLoader, Environment, select_autoescape\n\njinja2_env = Environment(\n    loader=PackageLoader(\"cli\"),\n    autoescape=select_autoescape([\"jinja2\"]),\n)\njinja2_env.filters[\"decamelize\"] = decamelize.convert\n\n\ndef prepare_source_repo(work_dir: Path, repo_name: str) -> Path:\n    source_repo_dir = work_dir.parent \/ repo_name\n    source_repo = Repo(source_repo_dir)\n    source_repo.git.checkout(\"master\")\n    source_repo.git.pull()\n    source_repo.close()\n    return source_repo_dir\n\n\n# noinspection PyTypeChecker\ndef prepare_current_repo(work_dir: Path) -> Path:\n    dst_dir = work_dir \/ \"clients\" \/ \"intermediates\"\n    os.makedirs(dst_dir, exist_ok=True)\n    return dst_dir\n\n\ndef compile_proto_file(output_dir: Path, proto_file_name: str) -> None:\n    options = [f\"-I{output_dir}\", f\"--python_out={output_dir}\", f\"--grpc_python_out={output_dir}\"]\n    os.system(f\"python -m grpc_tools.protoc {' '.join(options)} {proto_file_name}\")\n\n\ndef compile_client_file(proto_path: Path, service_name: str):\n    service_pattern = re.compile(r\"^service\\s+(.*?)\\s+{$\")\n    rpc_pattern = re.compile(r\"\\s+rpc\\s+(.*?)[\\s(]+(.*?)[)\\s]+returns[\\s(]+(.*?)[)\\s]+{\")\n\n    # \u6309\u7167\u9879\u76ee\u7ea6\u5b9a\uff0c\u4e00\u4e2a proto \u6587\u4ef6\u4e2d\u53ea\u6709\u4e00\u4e2a service\n    service, methods = None, []\n    with proto_path.open(encoding='utf-8') as f:\n        for i in f.readlines():\n            _service = service_pattern.match(i)\n            _rpc = rpc_pattern.match(i)\n            if _service:\n                service = _service.group(1).strip()\n            if _rpc:\n                methods.append([i.strip() for i in _rpc.groups()])\n\n    template = jinja2_env.get_template(\"client.jinja2\")\n    content = template.render(service=service, methods=methods, filename=proto_path.stem)\n    with (proto_path.parent \/ f\"{service_name}_client.py\").open(mode=\"w\") as f:\n        f.write(content)\n\n\ndef create_config_file(config_path: Path):\n    if config_path.exists():\n        return\n\n    typer.echo(f\"{os.linesep} !!!!! The generated file needs to be rewritten: {config_path} !!!!!\")\n    template = jinja2_env.get_template(\"config.jinja2\")\n    content = template.render()\n    with config_path.open(mode=\"w\") as f:\n        f.write(content + os.linesep)\n\n\n# noinspection PyTypeChecker\ndef create_init_file(init_path: Path):\n\n    # get custom imports\n    custom_imports = []\n    try:\n        with init_path.open(mode=\"r\") as f:\n            while True:\n                # Read next line\n                line = f.readline()\n                # If line is blank, then you struck the EOF\n                if not line:\n                    break\n\n                if 'intermediates' not in line and not line.startswith('#'):\n                    if line.strip():\n                        custom_imports.append(line.strip() + os.linesep)\n    except FileNotFoundError:\n        pass\n\n    services = set()\n    for _1, _2, filenames in os.walk(init_path.parent \/ \"intermediates\"):\n        services |= {i.rsplit(\"_\", 1)[0] for i in filenames if i.endswith(\"_client.py\")}\n\n    template = jinja2_env.get_template(\"clients.init.jinja2\")\n    content = template.render(services=sorted(list(services)))\n    with init_path.open(mode=\"w\") as f:\n        f.write(content + os.linesep)\n\n        f.write('# Custom imports' + os.linesep)\n        for custom_import in custom_imports:\n            f.write(custom_import)\n\n\ndef create_utils_file(utils_path: Path):\n    template = jinja2_env.get_template(\"utils.jinja2\")\n    content = template.render()\n    with utils_path.open(mode=\"w\") as f:\n        f.write(content + os.linesep)\n\n\n# noinspection PyTypeChecker\ndef add_service(repo_name: str, service: str, target_dir: str) -> None:\n    work_dir = Path.cwd()\n    typer.echo(f'> Current work directory is `{work_dir}`')\n    typer.echo(f'> Target client path is `{target_dir or \".\"}`')\n\n    source_repo_dir = prepare_source_repo(work_dir, repo_name)\n\n    # target dir\n    client_dir = work_dir\n    if target_dir:\n        client_dir = client_dir \/ target_dir\n\n    dst_dir = prepare_current_repo(client_dir)\n\n    # copy proto files\n    proto_file_name = f\"{service}.proto\"\n    dst_proto_path = dst_dir \/ proto_file_name\n    copyfile(\n        source_repo_dir \/ service \/ proto_file_name,\n        dst_proto_path,\n    )\n\n    compile_proto_file(dst_dir, proto_file_name)\n    compile_client_file(dst_proto_path, service)\n    os.system(f\"pb2py {dst_dir \/ f'{service}_pb2.py'} > {dst_dir \/ f'{service}_schema.py'}\")\n    create_init_file(dst_dir.parent \/ \"__init__.py\")\n    create_config_file(dst_dir.parent \/ \"_config.py\")\n    create_utils_file(dst_dir.parent \/ \"_utils.py\")\n\n    typer.echo()\n\n\ndef build_service() -> None:\n    current_dir = Path.cwd()\n    work_dir = current_dir \/ \"services\" \/ \"rpc\"\n    base_command = \"python3 -m grpc_tools.protoc -I{dir} --python_out={dir} --grpc_python_out={dir} {file}\"\n\n    for _1, _2, filenames in os.walk(work_dir):\n        for i in filenames:\n            file = work_dir \/ i\n            if file.suffix == \".proto\":\n                command = base_command.format(dir=work_dir, file=i)\n                typer.echo(command)\n                os.system(command)\n","label":1}
{"content":"# -*- coding: utf-8 -*-\nimport datetime\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\n\nclass Migration(SchemaMigration):\n\n    def forwards(self, orm):\n        db.create_index('sentry_groupbookmark', ['user_id', 'group_id'])\n\n    def backwards(self, orm):\n        db.delete_index('sentry_groupbookmark', ['user_id', 'group_id'])\n\n    models = {\n        'sentry.user': {\n            'Meta': {'object_name': 'User', 'db_table': \"'auth_user'\"},\n            'date_joined': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'email': ('django.db.models.fields.EmailField', [], {'max_length': '75', 'blank': 'True'}),\n            'first_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'is_active': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'is_staff': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'is_superuser': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'last_login': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'last_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}),\n            'password': ('django.db.models.fields.CharField', [], {'max_length': '128'}),\n            'username': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '30'})\n        },\n        'contenttypes.contenttype': {\n            'Meta': {'ordering': \"('name',)\", 'unique_together': \"(('app_label', 'model'),)\", 'object_name': 'ContentType', 'db_table': \"'django_content_type'\"},\n            'app_label': ('django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'model': ('django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '100'})\n        },\n        'sentry.affecteduserbygroup': {\n            'Meta': {'unique_together': \"(('project', 'ident', 'group'),)\", 'object_name': 'AffectedUserByGroup'},\n            'first_seen': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now', 'db_index': 'True'}),\n            'group': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Group']\"}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'ident': ('django.db.models.fields.CharField', [], {'max_length': '200'}),\n            'last_seen': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now', 'db_index': 'True'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\"}),\n            'times_seen': ('django.db.models.fields.PositiveIntegerField', [], {'default': '0'})\n        },\n        'sentry.event': {\n            'Meta': {'unique_together': \"(('project', 'event_id'),)\", 'object_name': 'Event', 'db_table': \"'sentry_message'\"},\n            'checksum': ('django.db.models.fields.CharField', [], {'max_length': '32', 'db_index': 'True'}),\n            'culprit': ('django.db.models.fields.CharField', [], {'max_length': '200', 'null': 'True', 'db_column': \"'view'\", 'blank': 'True'}),\n            'data': ('django.db.models.fields.TextField', [], {'null': 'True', 'blank': 'True'}),\n            'datetime': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now', 'db_index': 'True'}),\n            'event_id': ('django.db.models.fields.CharField', [], {'max_length': '32', 'null': 'True', 'db_column': \"'message_id'\"}),\n            'group': ('sentry.db.models.fields.FlexibleForeignKey', [], {'blank': 'True', 'related_name': \"'event_set'\", 'null': 'True', 'to': \"orm['sentry.Group']\"}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'level': ('django.db.models.fields.PositiveIntegerField', [], {'default': '40', 'db_index': 'True', 'blank': 'True'}),\n            'logger': ('django.db.models.fields.CharField', [], {'default': \"'root'\", 'max_length': '64', 'db_index': 'True', 'blank': 'True'}),\n            'message': ('django.db.models.fields.TextField', [], {}),\n            'platform': ('django.db.models.fields.CharField', [], {'max_length': '64', 'null': 'True'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\", 'null': 'True'}),\n            'server_name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True', 'db_index': 'True'}),\n            'site': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True', 'db_index': 'True'}),\n            'time_spent': ('django.db.models.fields.FloatField', [], {'null': 'True'})\n        },\n        'sentry.filterkey': {\n            'Meta': {'unique_together': \"(('project', 'key'),)\", 'object_name': 'FilterKey'},\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'key': ('django.db.models.fields.CharField', [], {'max_length': '32'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\"})\n        },\n        'sentry.filtervalue': {\n            'Meta': {'unique_together': \"(('project', 'key', 'value'),)\", 'object_name': 'FilterValue'},\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'key': ('django.db.models.fields.CharField', [], {'max_length': '32'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\", 'null': 'True'}),\n            'value': ('django.db.models.fields.CharField', [], {'max_length': '200'})\n        },\n        'sentry.group': {\n            'Meta': {'unique_together': \"(('project', 'logger', 'culprit', 'checksum'),)\", 'object_name': 'Group', 'db_table': \"'sentry_groupedmessage'\"},\n            'active_at': ('django.db.models.fields.DateTimeField', [], {'null': 'True', 'db_index': 'True'}),\n            'checksum': ('django.db.models.fields.CharField', [], {'max_length': '32', 'db_index': 'True'}),\n            'culprit': ('django.db.models.fields.CharField', [], {'max_length': '200', 'null': 'True', 'db_column': \"'view'\", 'blank': 'True'}),\n            'data': ('django.db.models.fields.TextField', [], {'null': 'True', 'blank': 'True'}),\n            'first_seen': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now', 'db_index': 'True'}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'is_public': ('django.db.models.fields.NullBooleanField', [], {'default': 'False', 'null': 'True', 'blank': 'True'}),\n            'last_seen': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now', 'db_index': 'True'}),\n            'level': ('django.db.models.fields.PositiveIntegerField', [], {'default': '40', 'db_index': 'True', 'blank': 'True'}),\n            'logger': ('django.db.models.fields.CharField', [], {'default': \"'root'\", 'max_length': '64', 'db_index': 'True', 'blank': 'True'}),\n            'message': ('django.db.models.fields.TextField', [], {}),\n            'platform': ('django.db.models.fields.CharField', [], {'max_length': '64', 'null': 'True'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\", 'null': 'True'}),\n            'resolved_at': ('django.db.models.fields.DateTimeField', [], {'null': 'True', 'db_index': 'True'}),\n            'score': ('django.db.models.fields.IntegerField', [], {'default': '0'}),\n            'status': ('django.db.models.fields.PositiveIntegerField', [], {'default': '0', 'db_index': 'True'}),\n            'time_spent_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),\n            'time_spent_total': ('django.db.models.fields.FloatField', [], {'default': '0'}),\n            'times_seen': ('django.db.models.fields.PositiveIntegerField', [], {'default': '1', 'db_index': 'True'}),\n            'users_seen': ('django.db.models.fields.PositiveIntegerField', [], {'default': '0', 'db_index': 'True'})\n        },\n        'sentry.groupbookmark': {\n            'Meta': {'unique_together': \"(('project', 'user', 'group'),)\", 'object_name': 'GroupBookmark'},\n            'group': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'bookmark_set'\", 'to': \"orm['sentry.Group']\"}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'bookmark_set'\", 'to': \"orm['sentry.Project']\"}),\n            'user': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'sentry_bookmark_set'\", 'to': \"orm['sentry.User']\"})\n        },\n        'sentry.groupmeta': {\n            'Meta': {'unique_together': \"(('group', 'key'),)\", 'object_name': 'GroupMeta'},\n            'group': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Group']\"}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'key': ('django.db.models.fields.CharField', [], {'max_length': '64'}),\n            'value': ('django.db.models.fields.TextField', [], {})\n        },\n        'sentry.lostpasswordhash': {\n            'Meta': {'object_name': 'LostPasswordHash'},\n            'date_added': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'hash': ('django.db.models.fields.CharField', [], {'max_length': '32'}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'user': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.User']\", 'unique': 'True'})\n        },\n        'sentry.messagecountbyminute': {\n            'Meta': {'unique_together': \"(('project', 'group', 'date'),)\", 'object_name': 'MessageCountByMinute'},\n            'date': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True'}),\n            'group': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Group']\"}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\", 'null': 'True'}),\n            'time_spent_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),\n            'time_spent_total': ('django.db.models.fields.FloatField', [], {'default': '0'}),\n            'times_seen': ('django.db.models.fields.PositiveIntegerField', [], {'default': '0'})\n        },\n        'sentry.messagefiltervalue': {\n            'Meta': {'unique_together': \"(('project', 'key', 'value', 'group'),)\", 'object_name': 'MessageFilterValue'},\n            'first_seen': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now', 'null': 'True', 'db_index': 'True'}),\n            'group': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Group']\"}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'key': ('django.db.models.fields.CharField', [], {'max_length': '32'}),\n            'last_seen': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now', 'null': 'True', 'db_index': 'True'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\", 'null': 'True'}),\n            'times_seen': ('django.db.models.fields.PositiveIntegerField', [], {'default': '0'}),\n            'value': ('django.db.models.fields.CharField', [], {'max_length': '200'})\n        },\n        'sentry.messageindex': {\n            'Meta': {'unique_together': \"(('column', 'value', 'object_id'),)\", 'object_name': 'MessageIndex'},\n            'column': ('django.db.models.fields.CharField', [], {'max_length': '32'}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'object_id': ('django.db.models.fields.PositiveIntegerField', [], {}),\n            'value': ('django.db.models.fields.CharField', [], {'max_length': '128'})\n        },\n        'sentry.option': {\n            'Meta': {'object_name': 'Option'},\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'key': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '64'}),\n            'value': ('picklefield.fields.PickledObjectField', [], {})\n        },\n        'sentry.pendingteammember': {\n            'Meta': {'unique_together': \"(('team', 'email'),)\", 'object_name': 'PendingTeamMember'},\n            'date_added': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'email': ('django.db.models.fields.EmailField', [], {'max_length': '75'}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'team': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'pending_member_set'\", 'to': \"orm['sentry.Team']\"}),\n            'type': ('django.db.models.fields.IntegerField', [], {'default': '0'})\n        },\n        'sentry.project': {\n            'Meta': {'object_name': 'Project'},\n            'date_added': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '200'}),\n            'owner': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'sentry_owned_project_set'\", 'null': 'True', 'to': \"orm['sentry.User']\"}),\n            'platform': ('django.db.models.fields.CharField', [], {'max_length': '32', 'null': 'True'}),\n            'public': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'slug': ('django.db.models.fields.SlugField', [], {'max_length': '50', 'unique': 'True', 'null': 'True'}),\n            'status': ('django.db.models.fields.PositiveIntegerField', [], {'default': '0', 'db_index': 'True'}),\n            'team': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Team']\", 'null': 'True'})\n        },\n        'sentry.projectcountbyminute': {\n            'Meta': {'unique_together': \"(('project', 'date'),)\", 'object_name': 'ProjectCountByMinute'},\n            'date': ('django.db.models.fields.DateTimeField', [], {}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\", 'null': 'True'}),\n            'time_spent_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),\n            'time_spent_total': ('django.db.models.fields.FloatField', [], {'default': '0'}),\n            'times_seen': ('django.db.models.fields.PositiveIntegerField', [], {'default': '0'})\n        },\n        'sentry.projectkey': {\n            'Meta': {'object_name': 'ProjectKey'},\n            'date_added': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now', 'null': 'True'}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'key_set'\", 'to': \"orm['sentry.Project']\"}),\n            'public_key': ('django.db.models.fields.CharField', [], {'max_length': '32', 'unique': 'True', 'null': 'True'}),\n            'secret_key': ('django.db.models.fields.CharField', [], {'max_length': '32', 'unique': 'True', 'null': 'True'}),\n            'user': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.User']\", 'null': 'True'}),\n            'user_added': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'keys_added_set'\", 'null': 'True', 'to': \"orm['sentry.User']\"})\n        },\n        'sentry.projectoption': {\n            'Meta': {'unique_together': \"(('project', 'key'),)\", 'object_name': 'ProjectOption', 'db_table': \"'sentry_projectoptions'\"},\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'key': ('django.db.models.fields.CharField', [], {'max_length': '64'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\"}),\n            'value': ('picklefield.fields.PickledObjectField', [], {})\n        },\n        'sentry.searchdocument': {\n            'Meta': {'unique_together': \"(('project', 'group'),)\", 'object_name': 'SearchDocument'},\n            'date_added': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'date_changed': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'group': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Group']\"}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\"}),\n            'status': ('django.db.models.fields.PositiveIntegerField', [], {'default': '0'}),\n            'total_events': ('django.db.models.fields.PositiveIntegerField', [], {'default': '1'})\n        },\n        'sentry.searchtoken': {\n            'Meta': {'unique_together': \"(('document', 'field', 'token'),)\", 'object_name': 'SearchToken'},\n            'document': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'token_set'\", 'to': \"orm['sentry.SearchDocument']\"}),\n            'field': ('django.db.models.fields.CharField', [], {'default': \"'text'\", 'max_length': '64'}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'times_seen': ('django.db.models.fields.PositiveIntegerField', [], {'default': '1'}),\n            'token': ('django.db.models.fields.CharField', [], {'max_length': '128'})\n        },\n        'sentry.team': {\n            'Meta': {'object_name': 'Team'},\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '64'}),\n            'owner': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.User']\"}),\n            'slug': ('django.db.models.fields.SlugField', [], {'unique': 'True', 'max_length': '50'})\n        },\n        'sentry.teammember': {\n            'Meta': {'unique_together': \"(('team', 'user'),)\", 'object_name': 'TeamMember'},\n            'date_added': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'is_active': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'team': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'member_set'\", 'to': \"orm['sentry.Team']\"}),\n            'type': ('django.db.models.fields.IntegerField', [], {'default': '0'}),\n            'user': ('sentry.db.models.fields.FlexibleForeignKey', [], {'related_name': \"'sentry_teammember_set'\", 'to': \"orm['sentry.User']\"})\n        },\n        'sentry.useroption': {\n            'Meta': {'unique_together': \"(('user', 'project', 'key'),)\", 'object_name': 'UserOption'},\n            'id': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),\n            'key': ('django.db.models.fields.CharField', [], {'max_length': '64'}),\n            'project': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.Project']\", 'null': 'True'}),\n            'user': ('sentry.db.models.fields.FlexibleForeignKey', [], {'to': \"orm['sentry.User']\"}),\n            'value': ('picklefield.fields.PickledObjectField', [], {})\n        }\n    }\n\n    complete_apps = ['sentry']\n","label":1}
{"content":"import constants\r\nimport os\r\nfrom pathlib import Path\r\nimport torch\r\nimport utils\r\nimport unittest\r\n\r\n\r\nclass TestUtils(unittest.TestCase):\r\n    def test_load_dataset_success(self):\r\n        dataset_path = Path(os.path.normpath(os.path.join(utils.__file__, '..\/..\/data\/kitti_2d')))\r\n        train_ds, val_ds = utils.load_dataset(dataset_path)\r\n\r\n        self.assertEqual(len(train_ds), 6610)\r\n        self.assertEqual(len(val_ds), 871)\r\n\r\n        x, y = val_ds[0]\r\n\r\n        # Check type and shape of x\r\n        self.assertIsInstance(x, torch.Tensor)\r\n        self.assertEqual(x.dtype, torch.float32)\r\n        self.assertEqual(x.shape, (3, constants.TRANSFORMED_IMAGE_SIZE[0], constants.TRANSFORMED_IMAGE_SIZE[1]))\r\n\r\n        # Check that pixel values are normalized\r\n        self.assertGreater(x.min(), -3)\r\n        self.assertLess(x.max(), 3)\r\n\r\n        self.assertEqual(len(y), 2)\r\n\r\n        # Check type and shape of y[0] - boxes\r\n        self.assertIsInstance(y[0], torch.Tensor)\r\n        self.assertEqual(y[0].dtype, torch.float32)\r\n        self.assertEqual(len(y[0].shape), 2)\r\n        self.assertEqual(y[0].shape[1], 4)\r\n\r\n        # Check type and shape of y[1] - classes\r\n        self.assertIsInstance(y[1], torch.Tensor)\r\n        self.assertEqual(y[1].dtype, torch.int64)\r\n        self.assertEqual(len(y[1].shape), 1)\r\n\r\n        # Check that number of boxes and classes are equal\r\n        self.assertEqual(y[0].shape[0], y[1].shape[0])\r\n\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n","label":1}
{"content":"from .render import init_bp\n\n__all__ = [\"init_bp\"]\n","label":1}
{"content":"from setuptools import setup\n\nsetup(\n    name='txt_when_done',\n    version='0.0.5',\n    packages=['txt_when_done'],\n    url='https:\/\/github.com\/willhk\/txt-when-done',\n    license='MIT',\n    author='willhk',\n    author_email='will.haeck@gmail.com',\n    description='Magic command to send a text when a jupyter cell completes.',\n    install_requires=['ipython', 'jupyter', 'twilio'],\n    classifiers=[\n        'Programming Language :: Python :: 3.7',\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ]\n)","label":1}
{"content":"import discord\nfrom discord.ext import commands\nimport math\nimport random\n\n\nclass LevelSystemManager:\n\n    def __init__(self, bot):\n        self.bot = bot\n\n    async def level_system_enable(self, ctx):\n        select_query = 'SELECT * FROM guild_config WHERE \"guild_id\" = $1;'\n        update_query = 'UPDATE guild_config SET \"xp_enabled\" = $1 WHERE \"guild_id\" = $2;'\n\n        row = await self.bot.db.fetchrow(select_query, ctx.guild.id)\n\n        try:\n            if not row['xp_enabled']:\n                con = await self.bot.db.acquire()\n                async with con.transaction():\n                    await self.bot.db.execute(update_query, True, ctx.guild.id)\n                await self.bot.db.release(con)\n\n                await ctx.send(\n                    embed=discord.Embed(\n                        title='XP-System -> Aktivieren',\n                        description=\"Das XP-System wurde erfolgreich aktiviert!\",\n                        color=0x00fdfd\n                    )\n                )\n            else:\n                await ctx.send(\n                    embed=discord.Embed(\n                        title='XP-System -> Aktivieren',\n                        description=\"Das XP-System ist bereits aktiviert!\",\n                        color=0x00fdfd\n                    )\n                )\n        except Exception as error:\n            await ctx.send(\n                embed=discord.Embed(\n                    title='XP-System -> Aktivieren',\n                    description=f\"Ein Fehler ist aufgetreten: {error}\",\n                    color=0x00fdfd\n                )\n            )\n\n    async def level_system_disable(self, ctx):\n        select_query = f'SELECT * FROM guild_config WHERE \"guild_id\" = $1;'\n        update_query = 'UPDATE guild_config SET \"xp_enabled\" = $1 WHERE \"guild_id\" = $2;'\n\n        row = await self.bot.db.fetchrow(select_query, ctx.guild.id)\n\n        try:\n            if row['xp_enabled']:\n                con = await self.bot.db.acquire()\n                async with con.transaction():\n                    await self.bot.db.execute(update_query, False, ctx.guild.id)\n                await self.bot.db.release(con)\n\n                await ctx.send(\n                    embed=discord.Embed(\n                        title='XP-System -> Deaktivieren',\n                        description=\"Das XP-System wurde erfolgreich deaktiviert!\",\n                        color=0x00fdfd\n                    )\n                )\n            else:\n                await ctx.send(\n                    embed=discord.Embed(\n                        title='XP-System -> Deaktivieren',\n                        description=\"Das XP-System ist bereits deaktiviert!\",\n                        color=0x00fdfd\n                    )\n                )\n        except Exception as error:\n            await ctx.send(\n                embed=discord.Embed(\n                    title='XP-System -> Deaktivieren',\n                    description=f\"Ein Fehler ist aufgetreten: {error}\",\n                    color=0x00fdfd\n                )\n            )\n\n    @commands.command(name='server-xp')\n    @commands.guild_only()\n    @commands.has_permissions(manage_guild=True)\n    async def _server_xp(self, ctx, *, keyword: str):\n        if keyword == 'enable':\n            await self.level_system_enable(ctx)\n        elif keyword == 'disable':\n            await self.level_system_disable(ctx)\n        else:\n            await ctx.send(\n                embed=discord.Embed(\n                    title=\"Falsche Nutzung des Commands\",\n                    description=\"Nur `enable` oder `disable` sind als Argument f\u00fcr den Command zugelassen.\",\n                    color=0x00fdfd\n                )\n            )\n\n    @_server_xp.error\n    async def server_xp_error(self, ctx, error):\n        if isinstance(error, commands.MissingPermissions):\n            await ctx.send(\n                embed=discord.Embed(\n                    title=\"Fehlende Permissions\",\n                    description=\"Um diesen Command verwenden zu k\u00f6nnen, musst du die Permission `Manage Guild` haben.\",\n                    color=0x00fdfd\n                )\n            )\n        if isinstance(error, commands.MissingRequiredArgument):\n            await ctx.send(\n                embed=discord.Embed(\n                    title=\"Falsche Nutzung des Commands\",\n                    description=\"Verwendung:\\n```\\nserver-xp <enable|disable>\\n```\",\n                    color=0x00fdfd\n                )\n            )\n\n\nclass LevelSystem:\n\n    def __init__(self, bot):\n        self.bot = bot\n        self.factor = (1 \/ 1.2)\n\n    def get_user_level(self, xp_value: float):\n        if xp_value > 0:\n            return self.factor * math.log(xp_value + 1)\n        else:\n            return 0\n\n    def xp_for_level(self, level: float):\n        base = level \/ self.factor\n        return math.pow(base, math.e)\n\n    def xp_for_next_level(self, xp_value: float):\n        current_level = self.get_user_level(xp_value)\n        next_level = math.floor(current_level) + 1\n        xp_for_next_level = self.xp_for_level(next_level)\n        xp_for_current_level = self.xp_for_level(current_level)\n\n        return xp_for_next_level - xp_for_current_level\n\n    async def generate_xp(self, message: discord.Message):\n        if message.author == self.bot.user:\n            return False\n        select_query = 'SELECT * FROM guild_config WHERE \"guild_id\" = $1;'\n        row = await self.bot.db.fetchrow(select_query, message.guild.id)\n\n        try:\n            if row['xp_enabled']:\n                if len(message.content) > 0:\n                    user_xp = random.randint(1, math.floor(10 * math.log(len(message.content) + 1)))\n                else:\n                    user_xp = random.randint(1, 5)\n            else:\n                return False\n\n            return user_xp\n\n        except Exception as error:\n            return await message.channel.send(\n                embed=discord.Embed(\n                    title='XP-System',\n                    description=f\"Ein Fehler ist aufgetreten: {error}\",\n                    color=0x00fdfd\n                )\n            )\n\n    async def add_user_xp(self, message: discord.Message, xp: float):\n        if message.author.id == self.bot.user.id:\n            return\n\n        try:\n            # Erstmal Queries schreiben, und die bisherigen User-Daten aus der Datenbank holen\n            select_xp_query = 'SELECT * FROM xp WHERE \"user_id\" = $1 AND \"guild_id\" = $2;'\n            select_query = 'SELECT * FROM guild_config WHERE \"guild_id\" = $1;'\n            update_xp_query = 'UPDATE xp SET \"user_xp\" = $1, \"user_level\" = $2 WHERE \"user_id\" = $3 AND \"guild_id\" = $4;'\n\n            xp_row = await self.bot.db.fetchrow(select_xp_query, message.author.id, message.guild.id)\n            config_row = await self.bot.db.fetchrow(select_query, message.guild.id)\n\n            if config_row['xp_enabled']:\n                new_xp = xp_row['user_xp'] + xp\n                current_level = self.get_user_level(new_xp)\n\n                # Aktualisiere Datenbank\n                con = await self.bot.db.acquire()\n                async with con.transaction():\n                    await self.bot.db.execute(update_xp_query, new_xp, math.floor(current_level), message.author.id, message.guild.id)\n                await self.bot.db.release(con)\n\n                if xp_row['user_level'] < math.floor(current_level):\n                    await message.channel.send(\n                        embed=discord.Embed(\n                            title=\"Levelup!\",\n                            description=f\"{message.author.mention}, du bist nun Level {math.floor(current_level)}!\",\n                            color=0x00fdfd\n                        )\n                    )\n            else:\n                pass\n        except Exception as error:\n            await message.channel.send(\n                embed=discord.Embed(\n                    title='XP-System',\n                    description=f\"Ein Fehler ist aufgetreten: {error}\",\n                    color=0x00fdfd\n                )\n            )\n\n    async def remove_user_xp(self, message: discord.Message, xp: float, grund: str):\n        if message.author.id == self.bot.user.id:\n            return\n\n        try:\n            select_xp_query = 'SELECT * FROM xp WHERE \"user_id\" = $1 AND \"guild_id\" = $2;'\n            select_config_query = 'SELECT * FROM guild_config WHERE \"guild_id\" = $1;'\n            update_query = 'UPDATE xp SET \"user_xp\" = $1 WHERE \"user_id\" = $2 AND \"guild_id\" = $3;'\n\n            xp_row = await self.bot.db.fetchrow(select_xp_query, message.author.id, message.guild.id)\n            config_row = await self.bot.db.fetchrow(select_config_query, message.guild.id)\n\n            if config_row['xp_enabled']:\n                new_xp = xp_row['user_xp'] - xp\n\n                con = await self.bot.db.acquire()\n                async with con.transaction():\n                    await self.bot.db.execute(update_query, new_xp, message.author.id, message.guild.id)\n                await self.bot.db.release(con)\n\n                await message.author.send(\n                    embed=discord.Embed(\n                        title=\"Dir wurden XP abgezogen!\",\n                        description=f\"Grund:\\n```\\n{grund}\\n```\",\n                        color=0x00fdfd\n                    )\n                )\n            else:\n                pass\n        except Exception as error:\n            await message.channel.send(\n                embed=discord.Embed(\n                    title='XP-System',\n                    description=f\"Ein Fehler ist aufgetreten: {error}\",\n                    color=0x00fdfd\n                )\n            )\n\n    async def get_user_xp(self, message: discord.Message):\n        select_xp_query = 'SELECT * FROM xp WHERE \"user_id\" = $1 AND \"guild_id\" = $2;'\n        try:\n            xp_row = await self.bot.db.fetchrow(select_xp_query, message.author.id, message.guild.id)\n            return xp_row['user_xp']\n        except Exception as error:\n            await message.channel.send(\n                embed=discord.Embed(\n                    title='XP-System',\n                    description=f\"Ein Fehler ist aufgetreten: {error}\",\n                    color=0x00fdfd\n                )\n            )\n\n    @commands.command(name='xp')\n    @commands.guild_only()\n    async def _xp(self, ctx):\n        async with ctx.typing():\n            current_xp = await self.get_user_xp(ctx)\n\n            await ctx.send(\n                embed=discord.Embed(\n                    description=f\"Du hast momentan {math.floor(current_xp)} XP.\",\n                    color=0x00fdfd\n                )\n            )\n\n    @commands.command(name='level')\n    @commands.guild_only()\n    async def _level(self, ctx):\n        async with ctx.typing():\n            current_xp = await self.get_user_xp(ctx.message)\n            current_level = self.get_user_level(current_xp)\n            needed_xp = self.xp_for_next_level(current_xp)\n\n            await ctx.send(\n                embed=discord.Embed(\n                    description=f\"Du bist momentan Level {math.floor(current_level)}. \"\n                                f\"Du brauchst noch {math.floor(needed_xp)} XP f\u00fcr ein Level-Up.\",\n                    color=0x00fdfd\n                )\n            )\n\n    @commands.command(name='leaderboard')\n    @commands.guild_only()\n    async def _leaderboard(self, ctx):\n        async with ctx.typing():\n            select_query = 'SELECT * FROM xp WHERE \"guild_id\" = $1 ORDER BY \"user_xp\" DESC LIMIT 10;'\n            results = await self.bot.db.fetch(select_query, ctx.guild.id)\n\n            lb_embed = discord.Embed(\n                title=\"Leaderboard\",\n                description=\"\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\",\n                color=0x00fdfd\n            )\n\n            count = 1\n            for result in results:\n                member = ctx.guild.get_member(result['user_id'])\n                current_level = self.get_user_level(result['user_xp'])\n                lb_embed.add_field(name=f\"{count}. Platz:\", value=f\"{member.mention}: XP: {math.floor(result['user_xp'])}, \"\n                                                                  f\"Level: {math.floor(current_level)}\", inline=False)\n                count += 1\n\n            await ctx.send(embed=lb_embed)\n","label":1}
{"content":"#!\/usr\/bin\/env python3\n\nfrom pathlib import Path\nimport sys\nimport cv2\nimport depthai as dai\nimport numpy as np\n\n# Get argument first\nnnPath = str((Path(__file__).parent \/ Path('..\/models\/mobilenet-ssd_openvino_2021.4_6shave.blob')).resolve().absolute())\nif len(sys.argv) > 1:\n    nnPath = sys.argv[1]\n\nif not Path(nnPath).exists():\n    import sys\n    raise FileNotFoundError(f'Required file\/s not found, please run \"{sys.executable} install_requirements.py\"')\n\n# MobilenetSSD label texts\nlabelMap = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n            \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n\n# Create pipeline\npipeline = dai.Pipeline()\n\n# Define sources and outputs\ncamRgb = pipeline.create(dai.node.ColorCamera)\nvideoEncoder = pipeline.create(dai.node.VideoEncoder)\nmonoRight = pipeline.create(dai.node.MonoCamera)\nmonoLeft = pipeline.create(dai.node.MonoCamera)\ndepth = pipeline.create(dai.node.StereoDepth)\nmanip = pipeline.create(dai.node.ImageManip)\nnn = pipeline.create(dai.node.MobileNetDetectionNetwork)\n\nvideoOut = pipeline.create(dai.node.XLinkOut)\nxoutRight = pipeline.create(dai.node.XLinkOut)\ndisparityOut = pipeline.create(dai.node.XLinkOut)\nmanipOut = pipeline.create(dai.node.XLinkOut)\nnnOut = pipeline.create(dai.node.XLinkOut)\n\nvideoOut.setStreamName('h265')\nxoutRight.setStreamName('right')\ndisparityOut.setStreamName('disparity')\nmanipOut.setStreamName('manip')\nnnOut.setStreamName('nn')\n\n# Properties\ncamRgb.setBoardSocket(dai.CameraBoardSocket.RGB)\ncamRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\nmonoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)\nmonoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\nmonoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\nmonoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\nvideoEncoder.setDefaultProfilePreset(1920, 1080, 30, dai.VideoEncoderProperties.Profile.H265_MAIN)\n\ndepth.initialConfig.setConfidenceThreshold(255)\ndepth.setRectifyEdgeFillColor(0) # Black, to better see the cutout\n\nnn.setConfidenceThreshold(0.5)\nnn.setBlobPath(nnPath)\nnn.setNumInferenceThreads(2)\nnn.input.setBlocking(False)\n\n# The NN model expects BGR input. By default ImageManip output type would be same as input (gray in this case)\nmanip.initialConfig.setFrameType(dai.ImgFrame.Type.BGR888p)\nmanip.initialConfig.setResize(300, 300)\n\n# Linking\ncamRgb.video.link(videoEncoder.input)\nvideoEncoder.bitstream.link(videoOut.input)\nmonoRight.out.link(xoutRight.input)\nmonoRight.out.link(depth.right)\nmonoLeft.out.link(depth.left)\ndepth.disparity.link(disparityOut.input)\ndepth.rectifiedRight.link(manip.inputImage)\nmanip.out.link(nn.input)\nmanip.out.link(manipOut.input)\nnn.out.link(nnOut.input)\n\n# Disparity range is used for normalization\ndisparityMultiplier = 255 \/ depth.initialConfig.getMaxDisparity()\n\n# Connect to device and start pipeline\nwith dai.Device(pipeline) as device:\n\n    queueSize = 8\n    qRight = device.getOutputQueue(\"right\", queueSize)\n    qDisparity = device.getOutputQueue(\"disparity\", queueSize)\n    qManip = device.getOutputQueue(\"manip\", queueSize)\n    qDet = device.getOutputQueue(\"nn\", queueSize)\n    qRgbEnc = device.getOutputQueue('h265', maxSize=30, blocking=True)\n\n    frame = None\n    frameManip = None\n    frameDisparity = None\n    detections = []\n    offsetX = (monoRight.getResolutionWidth() - monoRight.getResolutionHeight()) \/\/ 2\n    color = (255, 0, 0)\n    croppedFrame = np.zeros((monoRight.getResolutionHeight(), monoRight.getResolutionHeight()))\n\n    def frameNorm(frame, bbox):\n        normVals = np.full(len(bbox), frame.shape[0])\n        normVals[::2] = frame.shape[1]\n        return (np.clip(np.array(bbox), 0, 1) * normVals).astype(int)\n\n    videoFile = open('video.h265', 'wb')\n    cv2.namedWindow(\"right\", cv2.WINDOW_NORMAL)\n    cv2.namedWindow(\"manip\", cv2.WINDOW_NORMAL)\n\n    while True:\n        inRight = qRight.tryGet()\n        inManip = qManip.tryGet()\n        inDet = qDet.tryGet()\n        inDisparity = qDisparity.tryGet()\n\n        while qRgbEnc.has():\n            qRgbEnc.get().getData().tofile(videoFile)\n\n        if inRight is not None:\n            frame = inRight.getCvFrame()\n\n        if inManip is not None:\n            frameManip = inManip.getCvFrame()\n\n        if inDisparity is not None:\n            # Apply color map for better visualization\n            frameDisparity = inDisparity.getCvFrame()\n            frameDisparity = (frameDisparity*disparityMultiplier).astype(np.uint8)\n            frameDisparity = cv2.applyColorMap(frameDisparity, cv2.COLORMAP_JET)\n\n        if inDet is not None:\n            detections = inDet.detections\n\n        if frame is not None:\n            for detection in detections:\n                bbox = frameNorm(croppedFrame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))\n                bbox[::2] += offsetX\n                cv2.putText(frame, labelMap[detection.label], (bbox[0] + 10, bbox[1] + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, color)\n                cv2.putText(frame, f\"{int(detection.confidence * 100)}%\", (bbox[0] + 10, bbox[1] + 40), cv2.FONT_HERSHEY_TRIPLEX, 0.5, color)\n                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n            # Show the right cam frame\n            cv2.imshow(\"right\", frame)\n\n        if frameDisparity is not None:\n            for detection in detections:\n                bbox = frameNorm(croppedFrame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))\n                bbox[::2] += offsetX\n                cv2.rectangle(frameDisparity, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n                cv2.putText(frameDisparity, labelMap[detection.label], (bbox[0] + 10, bbox[1] + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, color)\n                cv2.putText(frameDisparity, f\"{int(detection.confidence * 100)}%\", (bbox[0] + 10, bbox[1] + 40), cv2.FONT_HERSHEY_TRIPLEX, 0.5, color)\n            # Show the disparity frame\n            cv2.imshow(\"disparity\", frameDisparity)\n\n        if frameManip is not None:\n            for detection in detections:\n                bbox = frameNorm(frameManip, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))\n                cv2.rectangle(frameManip, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n                cv2.putText(frameManip, labelMap[detection.label], (bbox[0] + 10, bbox[1] + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, color)\n                cv2.putText(frameManip, f\"{int(detection.confidence * 100)}%\", (bbox[0] + 10, bbox[1] + 40), cv2.FONT_HERSHEY_TRIPLEX, 0.5, color)\n            # Show the manip frame\n            cv2.imshow(\"manip\", frameManip)\n\n        if cv2.waitKey(1) == ord('q'):\n            break\n\n    print(\"To view the encoded data, convert the stream file (.h265) into a video file (.mp4) using a command below:\")\n    print(\"ffmpeg -framerate 30 -i video.h265 -c copy video.mp4\")\n","label":1}
{"content":"from django.shortcuts import render\nfrom django.core.paginator import Paginator, EmptyPage, PageNotAnInteger\n\nfrom wagtail.wagtailcore.models import Page\nfrom wagtail.wagtailsearch.models import Query\n\nfrom wagtail.contrib.wagtailsearchpromotions.models import SearchPromotion\n\n\ndef search(request):\n    # Search\n    search_query = request.GET.get('query', None)\n    if search_query:\n        search_results = Page.objects.live().search(search_query)\n        query = Query.get(search_query)\n\n        # Record hit\n        query.add_hit()\n\n        # Get search picks\n        search_picks = query.editors_picks.all()\n    else:\n        search_results = Page.objects.none()\n        search_picks = SearchPromotion.objects.none()\n\n    # Pagination\n    page = request.GET.get('page', 1)\n    paginator = Paginator(search_results, 10)\n    try:\n        search_results = paginator.page(page)\n    except PageNotAnInteger:\n        search_results = paginator.page(1)\n    except EmptyPage:\n        search_results = paginator.page(paginator.num_pages)\n\n    return render(request, 'home\/search_results.html', {\n        'search_query': search_query,\n        'search_results': search_results,\n        'search_picks': search_picks,\n    })\n","label":1}
{"content":"exempl\n","label":1}
{"content":"\"\"\"                 Working with calendars\nThe calendar module contains different classes that you can use to create calendars.\n\nA text based calendar  # 11\n# A HTML based calendar  # 32\n\nPart 2:  # 50\n\nPart 3:  # 120\n\"\"\"\nimport calendar\n# A text based calendar\n# cal = calendar.TextCalendar(calendar.SUNDAY)  # SUNDAY is the first day of the week. == If I want to use Monday instead than enter MONDAY.\n\n# str = cal.formatmonth(2020,1)\n\n# print(str)  #     January 2020\n#             # Su Mo Tu We Th Fr Sa\n#             #         1  2  3  4\n#             # 5  6  7  8  9 10 11\n#             # 12 13 14 15 16 17 18\n#             # 19 20 21 22 23 24 25\n#             # 26 27 28 29 30 31\n\n\n\n\n\n\n\n\n# A HTML based calendar\n# cal = calendar.HTMLCalendar(calendar.MONDAY)  # MONDAY is the first day of the week. == If I want to use Sunday instead than enter SUNDAY.\n\n# str = cal.formatmonth(2020,1)\n\n# print(str)  # \n# <table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"month\">\n# <tr><th colspan=\"7\" class=\"month\">January 2020<\/th><\/tr>\n# <tr><th class=\"mon\">Mon<\/th><th class=\"tue\">Tue<\/th><th class=\"wed\">Wed<\/th><th class=\"thu\">Thu<\/th><th class=\"fri\">Fri<\/th><th class=\"sat\">Sat<\/th><th class=\"sun\">Sun<\/th><\/tr>\n# <tr><td class=\"noday\">&nbsp;<\/td><td class=\"noday\">&nbsp;<\/td><td class=\"wed\">1<\/td><td class=\"thu\">2<\/td><td class=\"fri\">3<\/td><td class=\"sat\">4<\/td><td class=\"sun\">5<\/td><\/tr>\n# <tr><td class=\"mon\">6<\/td><td class=\"tue\">7<\/td><td class=\"wed\">8<\/td><td class=\"thu\">9<\/td><td class=\"fri\">10<\/td><td class=\"sat\">11<\/td><td class=\"sun\">12<\/td><\/tr>\n# <tr><td class=\"mon\">13<\/td><td class=\"tue\">14<\/td><td class=\"wed\">15<\/td><td class=\"thu\">16<\/td><td class=\"fri\">17<\/td><td class=\"sat\">18<\/td><td class=\"sun\">19<\/td><\/tr>\n# <tr><td class=\"mon\">20<\/td><td class=\"tue\">21<\/td><td class=\"wed\">22<\/td><td class=\"thu\">23<\/td><td class=\"fri\">24<\/td><td class=\"sat\">25<\/td><td class=\"sun\">26<\/td><\/tr>\n# <tr><td class=\"mon\">27<\/td><td class=\"tue\">28<\/td><td class=\"wed\">29<\/td><td class=\"thu\">30<\/td><td class=\"fri\">31<\/td><td class=\"noday\">&nbsp;<\/td><td class=\"noday\">&nbsp;<\/td><\/tr>\n# <\/table\n\n# The MyCal.html shows a calendar with Monday as the first day of the week.\n\n# Part 2  *********************************************************\ncal = calendar.TextCalendar(calendar.SUNDAY)\n\nfor days in cal.itermonthdates(2021,1):  \n\n    print(days)  # 2020-12-27\n                # 2020-12-28\n                # 2020-12-29\n                # 2020-12-30\n                # 2020-12-31\n                # 2021-01-01\n                # 2021-01-02\n                # 2021-01-03\n                # 2021-01-04\n                # 2021-01-05\n                # 2021-01-06\n                # 2021-01-07\n                # 2021-01-08\n                # 2021-01-09\n                # 2021-01-10\n                # 2021-01-11\n                # 2021-01-12\n                # 2021-01-13\n                # 2021-01-14\n                # 2021-01-15\n                # 2021-01-16\n                # 2021-01-17\n                # 2021-01-18\n                # 2021-01-19\n                # 2021-01-20\n                # 2021-01-21\n                # 2021-01-22\n                # 2021-01-23\n                # 2021-01-24\n                # 2021-01-25\n                # 2021-01-26\n                # 2021-01-27\n                # 2021-01-28\n                # 2021-01-29\n                # 2021-01-30\n                # 2021-01-31\n                # 2021-02-01\n                # 2021-02-02\n                # 2021-02-03\n                # 2021-02-04\n                # 2021-02-05\n                # 2021-02-06\n\nfor name in calendar.month_name:\n    print(name)  # January\n                # February\n                # March\n                # April\n                # May\n                # June\n                # July\n                # August\n                # September\n                # October\n                # November\n                # December\nfor day in calendar.day_name:\n    print(day)  # Monday\n                # Tuesday\n                # Wednesday\n                # Thursday\n                # Friday\n                # Saturday\n                # Sunday\n\n# Part 3\n\n\n","label":1}
{"content":"import dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport pandas as pd\nimport numpy as np\nfrom dash.dependencies import Output, Input\n\ndata = pd.read_csv(\"avocado.csv\")\ndata[\"Date\"] = pd.to_datetime(data[\"Date\"], format=\"%Y-%m-%d\")\ndata.sort_values(\"Date\", inplace=True)\n\nexternal_stylesheets = [\n    {\n        \"href\": \"https:\/\/fonts.googleapis.com\/css2?\"\n        \"family=Lato:wght@400;700&display=swap\",\n        \"rel\": \"stylesheet\",\n    },\n]\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\napp.title = \"Avocado Analytics: Understand Your Avocados!\"\n\napp.layout = html.Div(\n    children=[\n        html.Div(\n            children=[\n                html.P(children=\"\ud83e\udd51\", className=\"header-emoji\"),\n                html.H1(\n                    children=\"Avocado Analytics\", className=\"header-title\"\n                ),\n                html.P(\n                    children=\"Analyze the behavior of avocado prices\"\n                    \" and the number of avocados sold in the US\"\n                    \" between 2015 and 2018\",\n                    className=\"header-description\",\n                ),\n            ],\n            className=\"header\",\n        ),\n        html.Div(\n            children=[\n                html.Div(\n                    children=[\n                        html.Div(children=\"Region\", className=\"menu-title\"),\n                        dcc.Dropdown(\n                            id=\"region-filter\",\n                            options=[\n                                {\"label\": region, \"value\": region}\n                                for region in np.sort(data.region.unique())\n                            ],\n                            value=\"Albany\",\n                            clearable=False,\n                            className=\"dropdown\",\n                        ),\n                    ]\n                ),\n                html.Div(\n                    children=[\n                        html.Div(children=\"Type\", className=\"menu-title\"),\n                        dcc.Dropdown(\n                            id=\"type-filter\",\n                            options=[\n                                {\"label\": avocado_type, \"value\": avocado_type}\n                                for avocado_type in data.type.unique()\n                            ],\n                            value=\"organic\",\n                            clearable=False,\n                            searchable=False,\n                            className=\"dropdown\",\n                        ),\n                    ],\n                ),\n                html.Div(\n                    children=[\n                        html.Div(\n                            children=\"Date Range\",\n                            className=\"menu-title\"\n                            ),\n                        dcc.DatePickerRange(\n                            id=\"date-range\",\n                            min_date_allowed=data.Date.min().date(),\n                            max_date_allowed=data.Date.max().date(),\n                            start_date=data.Date.min().date(),\n                            end_date=data.Date.max().date(),\n                        ),\n                    ]\n                ),\n            ],\n            className=\"menu\",\n        ),\n        html.Div(\n            children=[\n                html.Div(\n                    children=dcc.Graph(\n                        id=\"price-chart\", config={\"displayModeBar\": False},\n                    ),\n                    className=\"card\",\n                ),\n                html.Div(\n                    children=dcc.Graph(\n                        id=\"volume-chart\", config={\"displayModeBar\": False},\n                    ),\n                    className=\"card\",\n                ),\n            ],\n            className=\"wrapper\",\n        ),\n    ]\n)\n\n\n@app.callback(\n    [Output(\"price-chart\", \"figure\"), Output(\"volume-chart\", \"figure\")],\n    [\n        Input(\"region-filter\", \"value\"),\n        Input(\"type-filter\", \"value\"),\n        Input(\"date-range\", \"start_date\"),\n        Input(\"date-range\", \"end_date\"),\n    ],\n)\ndef update_charts(region, avocado_type, start_date, end_date):\n    mask = (\n        (data.region == region)\n        & (data.type == avocado_type)\n        & (data.Date >= start_date)\n        & (data.Date <= end_date)\n    )\n    filtered_data = data.loc[mask, :]\n    price_chart_figure = {\n        \"data\": [\n            {\n                \"x\": filtered_data[\"Date\"],\n                \"y\": filtered_data[\"AveragePrice\"],\n                \"type\": \"lines\",\n                \"hovertemplate\": \"$%{y:.2f}<extra><\/extra>\",\n            },\n        ],\n        \"layout\": {\n            \"title\": {\n                \"text\": \"Average Price of Avocados\",\n                \"x\": 0.05,\n                \"xanchor\": \"left\",\n            },\n            \"xaxis\": {\"fixedrange\": True},\n            \"yaxis\": {\"tickprefix\": \"$\", \"fixedrange\": True},\n            \"colorway\": [\"#17B897\"],\n        },\n    }\n\n    volume_chart_figure = {\n        \"data\": [\n            {\n                \"x\": filtered_data[\"Date\"],\n                \"y\": filtered_data[\"Total Volume\"],\n                \"type\": \"lines\",\n            },\n        ],\n        \"layout\": {\n            \"title\": {\"text\": \"Avocados Sold\", \"x\": 0.05, \"xanchor\": \"left\"},\n            \"xaxis\": {\"fixedrange\": True},\n            \"yaxis\": {\"fixedrange\": True},\n            \"colorway\": [\"#E12D39\"],\n        },\n    }\n    return price_chart_figure, volume_chart_figure\n\nif __name__ == '__main__':\n    app.run_server(debug=True)","label":1}
{"content":"# this file contains templates which are imported into the model under the templates\n\n\nbutton = {\n    \"name\": \"button\",\n    \"type\": \"folder\",\n    \"children\": [\n        {\"name\": \"caption\", \"type\": \"const\", \"value\": \"mycaption\"},\n        {\"name\": \"counter\", \"type\": \"variable\", \"value\": 0},\n        {\"name\": \"onClick\", \"type\": \"referencer\"}\n    ]\n}\n\n\n\ntimeseriesWidget = {\n    \"name\": \"timeseriesWidget\",\n    \"type\": \"widget\",\n    \"children\":\n    [\n        {\"name\":\"widgetType\",\"type\":\"const\",\"value\":\"timeSeriesWidget\"},        #the type of the widget, this is used by the backend to choose the right service to run\n        {\"name\":\"selectableVariables\",\"type\":\"referencer\"},                     # the variables to appear in the select box\n        {\"name\":\"selectedVariables\",\"type\":\"referencer\"},                       # the currently selected variables to be shown in the plot\n        {\"name\":\"hasSelection\",\"type\":\"const\",\"value\":True},                    # show\/hide the variable selection area selectbox\n        {\"name\":\"startTime\",\"type\":\"variable\"},                                 # the current start time of the view zoom\n        {\"name\":\"endTime\",\"type\":\"variable\"},                                   # the current end time of the view zoom\n        {\"name\":\"bins\",\"type\":\"const\",\"value\":300},                             # the bins (number of points) used on the screen on the x-axis\n        {\"name\":\"hasAnnotation\",\"type\":\"const\",\"value\":True,\"children\":[        # show\/hide annotations and annotations buttons\n                 {\"name\":\"annotations\",\"type\":\"referencer\",\"references\":[       # the locations where to find annotations\n                     \"timeseriesWidget.hasAnnotation.newAnnotations\"]},\n                 {\"name\":\"newAnnotations\",\"type\":\"folder\"},                     # the folder where annotations created by the users are put\n                 {\"name\":\"tags\",\"type\":\"const\",\"value\":[\"one\",\"two\"]},          # the tags available for annotations\n                 {\"name\":\"colors\",\"type\":\"const\",\"value\":[\"yellow\",\"brown\",\"grey\",\"green\",\"red\"]}, # the colors for annotations, can also be a dict with {\"tag\":{\"color\":\"color\",\"pattern\":patter\"},\"tag2\":{..}}\n                 {\"name\":\"selectedAnnotations\",\"type\":\"referencer\"},            # the currently selected annotation(s)\n                 {\"name\":\"visibleTags\",\"type\":\"variable\",\"value\":{\n                     \"one\":True,\"two\":True,\"region\":True}}                      # a dict holding info which tags of annotations are currently visible\n            ]\n        },\n        {\"name\":\"table\",\"type\":\"referencer\"},                                   # the data table where the time series data resides to be used in this widget (it must be only one)\n        {\"name\":\"lineColors\",\"type\": \"const\", \"value\": [\n            \"blue\", \"yellow\", \"brown\", \"grey\", \"red\"]},                         # colors of the lines\n        {\"name\":\"buttons\",\"type\":\"folder\"},                                     # user buttons, place buttons-templates here\n        {\"name\":\"hasBackground\",\"type\": \"const\", \"value\": True},                # use background coloring or not\n        {\"name\":\"background\",\"type\":\"referencer\"},                              # the location of the background column (must be part of the table as well)\n        {\"name\":\"backgroundMap\",\"type\": \"const\", \"value\": {\n            \"1\": \"yellow\", \"0\": \"brown\", \"-1\": \"blue\", \"default\": \"white\"}},    # color map for the background\n        {\"name\":\"hasReloadButton\",\"type\":\"const\",\"value\":True},                 # show\/hide the reload button (show is only for dev-mode)\n        {\"name\":\"hasHover\",\"type\":\"const\",\"value\":False},                       # show\/hide the hover-tool of [False, 'mouse','vline','hline'\n        {\"name\":\"width\",\"type\":\"const\",\"value\":900},                            # the display width of the plot in pixel\n        {\"name\":\"height\",\"type\":\"const\",\"value\":600},                           # the display height of the plot in pixel\n        {\"name\":\"controlPosition\",\"type\":\"const\",\"value\":\"right\"}  ,            # the position of the button controls of [\"bottom\",\"right]\n        {\"name\":\"timeZone\",\"type\":\"const\",\"value\":\"Europe\/Berlin\"},             # the timezone of the display\n        {\"name\":\"hasThreshold\",\"type\":\"const\",\"value\":False},                   # horizontal thresholds (min\/max) are supported for the annotations\n        {\"name\":\"hasStreaming\", \"type\": \"const\", \"value\": False},               # horizontal thresholds (min\/max) are supported for the annotations\n        {\"name\":\"scoreVariables\", \"type\": \"referencer\"},                        # pointing to variables which are scores, (they are rendered with markers)\n        {\"name\": \"scoreMap\", \"type\": \"const\",\"value\":{                          # color map for score markers\n            \"0\": \"green\", \"1\": \"red\", \"default\": \"blue\"}},\n        {\"name\": \"theme\", \"type\": \"const\", \"value\": \"dark\"},                    # the theme color\n\n        {\"name\":\"observerVariables\",\"type\": \"observer\", \"children\": [           # observer for the selected variables (not the values)\n            {\"name\": \"enabled\", \"type\": \"const\", \"value\": True},                # on by default to enable drag + drop\n            {\"name\": \"triggerCounter\", \"type\": \"variable\", \"value\": 0},         # increased on each trigger\n            {\"name\": \"lastTriggerTime\", \"type\": \"variable\", \"value\": \"\"},       # last datetime when it was triggered\n            {\"name\": \"targets\", \"type\": \"referencer\", \"references\":[\"timeseriesWidget.selectedVariables\",\"timeseriesWidget.selectedVariablesY2\"]},  # pointing to the nodes observed\n            {\"name\": \"properties\", \"type\": \"const\", \"value\": [\"forwardRefs\"]},  # properties to observe [\u201cchildren\u201d,\u201cvalue\u201d, \u201cforwardRefs\u201d]\n            {\"name\": \"onTriggerFunction\", \"type\": \"referencer\"},                # the function(s) to be called when triggering\n            {\"name\": \"hasEvent\", \"type\": \"const\", \"value\": True},               # set to true if we want an event as well\n            {\"name\": \"eventString\", \"type\": \"const\", \"value\": \"timeSeriesWidget.variables\"}  # the string of the event\n            ]\n        },\n        {\"name\": \"observerBack\", \"type\": \"observer\", \"children\": [              # observer for the change of the background value\n            {\"name\": \"enabled\", \"type\": \"const\", \"value\": False},                # turn on\/off the observer\n            {\"name\": \"triggerCounter\", \"type\": \"variable\", \"value\": 0},         # increased on each trigger\n            {\"name\": \"lastTriggerTime\", \"type\": \"variable\", \"value\": \"\"},       # last datetime when it was triggered\n            {\"name\": \"targets\", \"type\": \"referencer\",\"references\":[\"timeseriesWidget.background\"]},                          # pointing to the nodes observed, must be set correctly, typically the execution counter of the function\n            {\"name\": \"properties\", \"type\": \"const\", \"value\": [\"value\"]},        # properties to observe [\u201cchildren\u201d,\u201cvalue\u201d, \u201cforwardRefs\u201d]\n            {\"name\": \"onTriggerFunction\", \"type\": \"referencer\"},                # the function(s) to be called when triggering\n            {\"name\": \"hasEvent\", \"type\": \"const\", \"value\": True},               # set to true if we want an event as well\n            {\"name\": \"eventString\", \"type\": \"const\", \"value\": \"timeSeriesWidget.background\"}  # the string of the event\n            ]\n        },\n        {\"name\": \"observerStream\", \"type\": \"observer\", \"children\": [            # observer for the value change of selected vars (especialy for streaming)\n            {\"name\": \"enabled\", \"type\": \"const\", \"value\": False},                # turn on\/off the observer\n            {\"name\": \"triggerCounter\", \"type\": \"variable\", \"value\": 0},         # increased on each trigger\n            {\"name\": \"lastTriggerTime\", \"type\": \"variable\", \"value\": \"\"},       # last datetime when it was triggered\n            {\"name\": \"targets\", \"type\": \"referencer\",\"references\":[\"timeseriesWidget.selectableVariables\",\"timeseriesWidget.selectedVariablesY2\"]},    # pointing to the all nodes,\n            {\"name\": \"properties\", \"type\": \"const\", \"value\": [\"value\"]},        # look for value change properties to observe [\u201cchildren\u201d,\u201cvalue\u201d, \u201cforwardRefs\u201d]\n            {\"name\": \"onTriggerFunction\", \"type\": \"referencer\"},                # the function(s) to be called when triggering\n            {\"name\": \"hasEvent\", \"type\": \"const\", \"value\": True},               # set to true if we want an event as well\n            {\"name\": \"eventString\", \"type\": \"const\", \"value\": \"timeSeriesWidget.stream\"}  # the string of the event\n            ]\n        },\n        {\"name\":\"visibleElements\",\"type\":\"variable\",\"value\":{                   # which elements of the widget are currently visible\n            \"thresholds\":True,\n            \"annotations\":True,\n            \"scores\":True,\n            \"background\":True,\n            \"variables\":True\n            }\n        },\n        {\"name\":\"contextMenuFunctions\",\"type\":\"referencer\"},                    # pointing to functions which should be displayed and callable from context menu\n        {\"name\": \"observerAnnotations\", \"type\": \"observer\", \"children\": [       # observer for the value change of selected vars (especialy for streaming)\n            {\"name\": \"enabled\", \"type\": \"const\", \"value\": True},                # turn on\/off the observer\n            {\"name\": \"triggerCounter\", \"type\": \"variable\", \"value\": 0},         # increased on each trigger\n            {\"name\": \"lastTriggerTime\", \"type\": \"variable\", \"value\": \"\"},       # last datetime when it was triggered\n            {\"name\": \"targets\", \"type\": \"referencer\",\"references\":[\"timeseriesWidget.hasAnnotation.annotations\"]},    # pointing to the all nodes,\n            {\"name\": \"properties\", \"type\": \"const\", \"value\": [\"children\",\"value\"]},        # look for value change properties to observe [\u201cchildren\u201d,\u201cvalue\u201d, \u201cforwardRefs\u201d]\n            {\"name\": \"onTriggerFunction\", \"type\": \"referencer\"},                # the function(s) to be called when triggering\n            {\"name\": \"hasEvent\", \"type\": \"const\", \"value\": True},               # set to true if we want an event as well\n            {\"name\": \"eventString\", \"type\": \"const\", \"value\": \"timeSeriesWidget.annotations\"}  # the string of the event\n            ]\n        },\n        {\"name\": \"observerVisibleElements\", \"type\": \"observer\", \"children\": [   # observer for the value change of selected vars (especialy for streaming)\n            {\"name\": \"enabled\", \"type\": \"const\", \"value\": True},                # turn on\/off the observer\n            {\"name\": \"triggerCounter\", \"type\": \"variable\", \"value\": 0},         # increased on each trigger\n            {\"name\": \"lastTriggerTime\", \"type\": \"variable\", \"value\": \"\"},       # last datetime when it was triggered\n            {\"name\": \"targets\", \"type\": \"referencer\",\"references\":[\n                \"timeseriesWidget.visibleElements\",\n                \"timeseriesWidget.hasAnnotation.visibleTags\",\n                \"timeseriesWidget.autoScaleY\"\n                ]\n            },\n            {\"name\": \"properties\", \"type\": \"const\", \"value\": [\"value\"]},        # look for value change properties to observe [\u201cchildren\u201d,\u201cvalue\u201d, \u201cforwardRefs\u201d]\n            {\"name\": \"onTriggerFunction\", \"type\": \"referencer\"},                # the function(s) to be called when triggering\n            {\"name\": \"hasEvent\", \"type\": \"const\", \"value\": True},               # set to true if we want an event as well\n            {\"name\": \"eventString\", \"type\": \"const\", \"value\": \"timeSeriesWidget.visibleElements\"}  # the string of the event\n            ]\n        },\n        {\"name\": \"nextNewAnnotation\", \"type\": \"variable\", \"value\":{}},          # the value must be a dict like {\"type\":\"time\",\"tag\":\"one\"}\n        {\"name\": \"observerNewAnnotation\", \"type\": \"observer\", \"children\": [     # observer for the value change of selected vars (especialy for streaming)\n             {\"name\": \"enabled\", \"type\": \"const\", \"value\": True},              # turn on\/off the observer\n             {\"name\": \"triggerCounter\", \"type\": \"variable\", \"value\": 0},        # increased on each trigger\n             {\"name\": \"lastTriggerTime\", \"type\": \"variable\", \"value\": \"\"},      # last datetime when it was triggered\n             {\"name\": \"targets\", \"type\": \"referencer\", \"references\": [\"timeseriesWidget.nextNewAnnotation\"]},\n             {\"name\": \"properties\", \"type\": \"const\", \"value\": [\"value\"]},\n             {\"name\": \"onTriggerFunction\", \"type\": \"referencer\"},               # the function(s) to be called when triggering\n             {\"name\": \"hasEvent\", \"type\": \"const\", \"value\": True},              # set to true if we want an event as well\n             {\"name\": \"eventString\", \"type\": \"const\", \"value\": \"timeSeriesWidget.newAnnotation\"}  # the string of the event\n            ]\n        },\n        {\"name\": \"currentColors\", \"type\": \"variable\", \"value\":{\"entry\":{\"lineColor\":\"red\"}}},   #put the current colors here\n        {\"name\": \"autoScaleY\", \"type\": \"variable\", \"value\":True},                   # turn y axis autoscale on\/off during runtime\n        {\"name\": \"panOnlyX\",\"type\":\"const\",\"value\":False},                              # if this is set true, the pan and wheel zoom only zooms in x-direction\n        {\"name\": \"streamingMode\",\"type\":\"const\",\"value\":False},                       # if this is set true, the pan and wheel zoom only zooms in x-direction\n        {\"name\": \"showMarker\",\"type\":\"const\",\"value\":False},\n        {\"name\": \"contextMenuSettings\",\"type\":\"folder\"},                         # configure here the \"settings\" submenu of the context menu: each child should be a referencer pointing to a bool variable\n                                                                                # the entry displayed in the context menu will be the name of the referencder\n        {\"name\": \"contextMenuPipelines\",\"type\":\"referencer\"},\n        {\"name\": \"showLegend\",\"type\":\"const\",\"value\":True},\n        {\"name\": \"hasEvents\", \"type\": \"const\", \"value\": False, \"children\": [  # show\/hide annotations and annotations buttons\n            {\"name\": \"events\", \"type\": \"referencer\"},\n            {\"name\": \"newEvents\", \"type\": \"eventseries\"},  # the timeseries where manually created goes\n            {\"name\": \"colors\", \"type\": \"const\", \"value\": {}}, # the colors for events {\"myeveent1\":{\"color\":\"red\"},\"myev2\":{\"color\":\"blue\"}\n            {\"name\": \"visibleEvents\", \"type\": \"variable\", \"value\": {}}  # {\"myevent1\":true,\"myev2\":false}\n            ]\n        },\n        {\"name\": \"observerEventSeries\", \"type\": \"observer\",\n         \"children\": [  # observer for the value change of selected vars (especialy for streaming)\n             {\"name\": \"enabled\", \"type\": \"const\", \"value\": True},  # turn on\/off the observer\n             {\"name\": \"triggerCounter\", \"type\": \"variable\", \"value\": 0},  # increased on each trigger\n             {\"name\": \"lastTriggerTime\", \"type\": \"variable\", \"value\": \"\"},  # last datetime when it was triggered\n             {\"name\": \"targets\", \"type\": \"referencer\", \"references\": [\"timeseriesWidget.hasEvents.events\"]},\n             {\"name\": \"properties\", \"type\": \"const\", \"value\": [\"value\"]},\n             {\"name\": \"onTriggerFunction\", \"type\": \"referencer\"},  # the function(s) to be called when triggering\n             {\"name\": \"hasEvent\", \"type\": \"const\", \"value\": True},  # set to true if we want an event as well\n             {\"name\": \"eventString\", \"type\": \"const\", \"value\": \"timeSeriesWidget.eventSeries\"}\n             # the string of the event\n         ]\n         },\n        {\"name\": \"backgroundHighlight\", \"type\": \"variable\"},\n        {\"name\": \"aggregation\", \"type\": \"const\", \"value\": None},    #options: \"outlier\"\n        {\"name\": \"hasY2Axis\",\"type\":\"const\",\"value\":False},         #set true for second y axis on the right\n        {\"name\": \"selectedVariablesY2\",\"type\":\"referencer\"}         #contains the selected vars on the y2 axis, make sure you also connect the observer variables and observer stream to this\n    ]\n}\n","label":1}
{"content":"from typing import List\n\n\nclass Board:\n    squares: List[List[str]]\n\n    def __init__(self) -> None:\n        self.squares = [['r', 'n', 'b', 'q', 'k', 'b', 'n', 'r'],\n                        ['p', 'p', 'p', 'p', 'p', 'p', 'p', 'p'],\n                        ['.', '.', '.', '.', '.', '.', '.', '.'],\n                        ['.', '.', '.', '.', '.', '.', '.', '.'],\n                        ['.', '.', '.', '.', '.', '.', '.', '.'],\n                        ['.', '.', '.', '.', '.', '.', '.', '.'],\n                        ['P', 'P', 'P', 'P', 'P', 'P', 'P', 'P'],\n                        ['R', 'N', 'B', 'Q', 'K', 'B', 'N', 'R']\n                        ]\n","label":1}
{"content":"\"\"\"\nShamefully copied from\nhttps:\/\/www.reddit.com\/r\/adventofcode\/comments\/7lte5z\/2017_day_24_solutions\/droyesm\/\n\"\"\"\n\nfrom collections import defaultdict\n\ndef gen_bridges(library, bridge=None):\n    l, s, components, a = bridge or (0, 0, set(), 0)\n    for b in library[a]:\n        next = (a, b) if a <= b else (b, a)\n        if next not in components:\n            new = l+1, s+a+b, (components | {next}), b\n            yield new; yield from gen_bridges(library, new)\n\ndef solve(input):\n    library = defaultdict(set)\n    for l in input.strip().splitlines():\n        a, b = [int(x) for x in l.split('\/')]\n        library[a].add(b); library[b].add(a)\n    return [b[:2] for b in gen_bridges(library)]\n\nwith open('components.txt') as f:\n    input = f.read()\n\nbridges = solve(input)  # A list of (length, strength) tuples\npart1 = sorted(bridges, key=lambda x: x[1])[-1][1]  # Sort by strength only\npart2 = sorted(bridges)[-1][1]  # Sort by length, then by strength\nprint(part1)\nprint(part2)\n","label":1}
{"content":"\"\"\"_g_l_y_f.py -- Converter classes for the 'glyf' table.\"\"\"\n\nfrom collections import namedtuple\nfrom fontTools.misc import sstruct\nfrom fontTools import ttLib\nfrom fontTools import version\nfrom fontTools.misc.textTools import tostr, safeEval, pad\nfrom fontTools.misc.arrayTools import calcIntBounds, pointInRect\nfrom fontTools.misc.bezierTools import calcQuadraticBounds\nfrom fontTools.misc.fixedTools import (\n\tfixedToFloat as fi2fl,\n\tfloatToFixed as fl2fi,\n\tfloatToFixedToStr as fl2str,\n\tstrToFixedToFloat as str2fl,\n\totRound,\n)\nfrom numbers import Number\nfrom . import DefaultTable\nfrom . import ttProgram\nimport sys\nimport struct\nimport array\nimport logging\nimport os\nfrom fontTools.misc import xmlWriter\nfrom fontTools.misc.filenames import userNameToFileName\nfrom fontTools.misc.loggingTools import deprecateFunction\n\nlog = logging.getLogger(__name__)\n\n# We compute the version the same as is computed in ttlib\/__init__\n# so that we can write 'ttLibVersion' attribute of the glyf TTX files\n# when glyf is written to separate files.\nversion = \".\".join(version.split('.')[:2])\n\n#\n# The Apple and MS rasterizers behave differently for\n# scaled composite components: one does scale first and then translate\n# and the other does it vice versa. MS defined some flags to indicate\n# the difference, but it seems nobody actually _sets_ those flags.\n#\n# Funny thing: Apple seems to _only_ do their thing in the\n# WE_HAVE_A_SCALE (eg. Chicago) case, and not when it's WE_HAVE_AN_X_AND_Y_SCALE\n# (eg. Charcoal)...\n#\nSCALE_COMPONENT_OFFSET_DEFAULT = 0   # 0 == MS, 1 == Apple\n\n\nclass table__g_l_y_f(DefaultTable.DefaultTable):\n\t\"\"\"Glyph Data Table\n\n\tThis class represents the `glyf <https:\/\/docs.microsoft.com\/en-us\/typography\/opentype\/spec\/glyf>`_\n \ttable, which contains outlines for glyphs in TrueType format. In many cases,\n \tit is easier to access and manipulate glyph outlines through the ``GlyphSet``\n \tobject returned from :py:meth:`fontTools.ttLib.ttFont.getGlyphSet`::\n\n \t\t\t>> from fontTools.pens.boundsPen import BoundsPen\n \t\t\t>> glyphset = font.getGlyphSet()\n\t\t\t>> bp = BoundsPen(glyphset)\n\t\t\t>> glyphset[\"A\"].draw(bp)\n\t\t\t>> bp.bounds\n\t\t\t(19, 0, 633, 716)\n\n\tHowever, this class can be used for low-level access to the ``glyf`` table data.\n\tObjects of this class support dictionary-like access, mapping glyph names to\n\t:py:class:`Glyph` objects::\n\n\t\t\t>> glyf = font[\"glyf\"]\n\t\t\t>> len(glyf[\"Aacute\"].components)\n\t\t\t2\n\n\tNote that when adding glyphs to the font via low-level access to the ``glyf``\n\ttable, the new glyphs must also be added to the ``hmtx``\/``vmtx`` table::\n\n\t\t\t>> font[\"glyf\"][\"divisionslash\"] = Glyph()\n\t\t\t>> font[\"hmtx\"][\"divisionslash\"] = (640, 0)\n\n\t\"\"\"\n\n\t# this attribute controls the amount of padding applied to glyph data upon compile.\n\t# Glyph lenghts are aligned to multiples of the specified value. \n\t# Allowed values are (0, 1, 2, 4). '0' means no padding; '1' (default) also means\n\t# no padding, except for when padding would allow to use short loca offsets.\n\tpadding = 1\n\n\tdef decompile(self, data, ttFont):\n\t\tloca = ttFont['loca']\n\t\tpos = int(loca[0])\n\t\tnextPos = 0\n\t\tnoname = 0\n\t\tself.glyphs = {}\n\t\tself.glyphOrder = glyphOrder = ttFont.getGlyphOrder()\n\t\tfor i in range(0, len(loca)-1):\n\t\t\ttry:\n\t\t\t\tglyphName = glyphOrder[i]\n\t\t\texcept IndexError:\n\t\t\t\tnoname = noname + 1\n\t\t\t\tglyphName = 'ttxautoglyph%s' % i\n\t\t\tnextPos = int(loca[i+1])\n\t\t\tglyphdata = data[pos:nextPos]\n\t\t\tif len(glyphdata) != (nextPos - pos):\n\t\t\t\traise ttLib.TTLibError(\"not enough 'glyf' table data\")\n\t\t\tglyph = Glyph(glyphdata)\n\t\t\tself.glyphs[glyphName] = glyph\n\t\t\tpos = nextPos\n\t\tif len(data) - nextPos >= 4:\n\t\t\tlog.warning(\n\t\t\t\t\"too much 'glyf' table data: expected %d, received %d bytes\",\n\t\t\t\tnextPos, len(data))\n\t\tif noname:\n\t\t\tlog.warning('%s glyphs have no name', noname)\n\t\tif ttFont.lazy is False: # Be lazy for None and True\n\t\t\tfor glyph in self.glyphs.values():\n\t\t\t\tglyph.expand(self)\n\n\tdef compile(self, ttFont):\n\t\tif not hasattr(self, \"glyphOrder\"):\n\t\t\tself.glyphOrder = ttFont.getGlyphOrder()\n\t\tpadding = self.padding\n\t\tassert padding in (0, 1, 2, 4)\n\t\tlocations = []\n\t\tcurrentLocation = 0\n\t\tdataList = []\n\t\trecalcBBoxes = ttFont.recalcBBoxes\n\t\tfor glyphName in self.glyphOrder:\n\t\t\tglyph = self.glyphs[glyphName]\n\t\t\tglyphData = glyph.compile(self, recalcBBoxes)\n\t\t\tif padding > 1:\n\t\t\t\tglyphData = pad(glyphData, size=padding)\n\t\t\tlocations.append(currentLocation)\n\t\t\tcurrentLocation = currentLocation + len(glyphData)\n\t\t\tdataList.append(glyphData)\n\t\tlocations.append(currentLocation)\n\n\t\tif padding == 1 and currentLocation < 0x20000:\n\t\t\t# See if we can pad any odd-lengthed glyphs to allow loca\n\t\t\t# table to use the short offsets.\n\t\t\tindices = [i for i,glyphData in enumerate(dataList) if len(glyphData) % 2 == 1]\n\t\t\tif indices and currentLocation + len(indices) < 0x20000:\n\t\t\t\t# It fits.  Do it.\n\t\t\t\tfor i in indices:\n\t\t\t\t\tdataList[i] += b'\\0'\n\t\t\t\tcurrentLocation = 0\n\t\t\t\tfor i,glyphData in enumerate(dataList):\n\t\t\t\t\tlocations[i] = currentLocation\n\t\t\t\t\tcurrentLocation += len(glyphData)\n\t\t\t\tlocations[len(dataList)] = currentLocation\n\n\t\tdata = b''.join(dataList)\n\t\tif 'loca' in ttFont:\n\t\t\tttFont['loca'].set(locations)\n\t\tif 'maxp' in ttFont:\n\t\t\tttFont['maxp'].numGlyphs = len(self.glyphs)\n\t\tif not data:\n\t\t# As a special case when all glyph in the font are empty, add a zero byte\n\t\t# to the table, so that OTS doesn\u2019t reject it, and to make the table work\n\t\t# on Windows as well.\n\t\t# See https:\/\/github.com\/khaledhosny\/ots\/issues\/52\n\t\t\tdata = b\"\\0\"\n\t\treturn data\n\n\tdef toXML(self, writer, ttFont, splitGlyphs=False):\n\t\tnotice = (\n\t\t\t\"The xMin, yMin, xMax and yMax values\\n\"\n\t\t\t\"will be recalculated by the compiler.\")\n\t\tglyphNames = ttFont.getGlyphNames()\n\t\tif not splitGlyphs:\n\t\t\twriter.newline()\n\t\t\twriter.comment(notice)\n\t\t\twriter.newline()\n\t\t\twriter.newline()\n\t\tnumGlyphs = len(glyphNames)\n\t\tif splitGlyphs:\n\t\t\tpath, ext = os.path.splitext(writer.file.name)\n\t\t\texistingGlyphFiles = set()\n\t\tfor glyphName in glyphNames:\n\t\t\tglyph = self.get(glyphName)\n\t\t\tif glyph is None:\n\t\t\t\tlog.warning(\"glyph '%s' does not exist in glyf table\", glyphName)\n\t\t\t\tcontinue\n\t\t\tif glyph.numberOfContours:\n\t\t\t\tif splitGlyphs:\n\t\t\t\t\tglyphPath = userNameToFileName(\n\t\t\t\t\t\ttostr(glyphName, 'utf-8'),\n\t\t\t\t\t\texistingGlyphFiles,\n\t\t\t\t\t\tprefix=path + \".\",\n\t\t\t\t\t\tsuffix=ext)\n\t\t\t\t\texistingGlyphFiles.add(glyphPath.lower())\n\t\t\t\t\tglyphWriter = xmlWriter.XMLWriter(\n\t\t\t\t\t\tglyphPath, idlefunc=writer.idlefunc,\n\t\t\t\t\t\tnewlinestr=writer.newlinestr)\n\t\t\t\t\tglyphWriter.begintag(\"ttFont\", ttLibVersion=version)\n\t\t\t\t\tglyphWriter.newline()\n\t\t\t\t\tglyphWriter.begintag(\"glyf\")\n\t\t\t\t\tglyphWriter.newline()\n\t\t\t\t\tglyphWriter.comment(notice)\n\t\t\t\t\tglyphWriter.newline()\n\t\t\t\t\twriter.simpletag(\"TTGlyph\", src=os.path.basename(glyphPath))\n\t\t\t\telse:\n\t\t\t\t\tglyphWriter = writer\n\t\t\t\tglyphWriter.begintag('TTGlyph', [\n\t\t\t\t\t\t\t(\"name\", glyphName),\n\t\t\t\t\t\t\t(\"xMin\", glyph.xMin),\n\t\t\t\t\t\t\t(\"yMin\", glyph.yMin),\n\t\t\t\t\t\t\t(\"xMax\", glyph.xMax),\n\t\t\t\t\t\t\t(\"yMax\", glyph.yMax),\n\t\t\t\t\t\t\t])\n\t\t\t\tglyphWriter.newline()\n\t\t\t\tglyph.toXML(glyphWriter, ttFont)\n\t\t\t\tglyphWriter.endtag('TTGlyph')\n\t\t\t\tglyphWriter.newline()\n\t\t\t\tif splitGlyphs:\n\t\t\t\t\tglyphWriter.endtag(\"glyf\")\n\t\t\t\t\tglyphWriter.newline()\n\t\t\t\t\tglyphWriter.endtag(\"ttFont\")\n\t\t\t\t\tglyphWriter.newline()\n\t\t\t\t\tglyphWriter.close()\n\t\t\telse:\n\t\t\t\twriter.simpletag('TTGlyph', name=glyphName)\n\t\t\t\twriter.comment(\"contains no outline data\")\n\t\t\t\tif not splitGlyphs:\n\t\t\t\t\twriter.newline()\n\t\t\twriter.newline()\n\n\tdef fromXML(self, name, attrs, content, ttFont):\n\t\tif name != \"TTGlyph\":\n\t\t\treturn\n\t\tif not hasattr(self, \"glyphs\"):\n\t\t\tself.glyphs = {}\n\t\tif not hasattr(self, \"glyphOrder\"):\n\t\t\tself.glyphOrder = ttFont.getGlyphOrder()\n\t\tglyphName = attrs[\"name\"]\n\t\tlog.debug(\"unpacking glyph '%s'\", glyphName)\n\t\tglyph = Glyph()\n\t\tfor attr in ['xMin', 'yMin', 'xMax', 'yMax']:\n\t\t\tsetattr(glyph, attr, safeEval(attrs.get(attr, '0')))\n\t\tself.glyphs[glyphName] = glyph\n\t\tfor element in content:\n\t\t\tif not isinstance(element, tuple):\n\t\t\t\tcontinue\n\t\t\tname, attrs, content = element\n\t\t\tglyph.fromXML(name, attrs, content, ttFont)\n\t\tif not ttFont.recalcBBoxes:\n\t\t\tglyph.compact(self, 0)\n\n\tdef setGlyphOrder(self, glyphOrder):\n\t\t\"\"\"Sets the glyph order\n\n\t\tArgs:\n\t\t\tglyphOrder ([str]): List of glyph names in order.\n\t\t\"\"\"\n\t\tself.glyphOrder = glyphOrder\n\n\tdef getGlyphName(self, glyphID):\n\t\t\"\"\"Returns the name for the glyph with the given ID.\n\n\t\tRaises a ``KeyError`` if the glyph name is not found in the font.\n\t\t\"\"\"\n\t\treturn self.glyphOrder[glyphID]\n\n\tdef getGlyphID(self, glyphName):\n\t\t\"\"\"Returns the ID of the glyph with the given name.\n\n\t\tRaises a ``ValueError`` if the glyph is not found in the font.\n\t\t\"\"\"\n\t\t# XXX optimize with reverse dict!!!\n\t\treturn self.glyphOrder.index(glyphName)\n\n\tdef removeHinting(self):\n\t\t\"\"\"Removes TrueType hints from all glyphs in the glyphset.\n\n\t\tSee :py:meth:`Glyph.removeHinting`.\n\t\t\"\"\"\n\t\tfor glyph in self.glyphs.values():\n\t\t\tglyph.removeHinting()\n\n\tdef keys(self):\n\t\treturn self.glyphs.keys()\n\n\tdef has_key(self, glyphName):\n\t\treturn glyphName in self.glyphs\n\n\t__contains__ = has_key\n\n\tdef get(self, glyphName, default=None):\n\t\tglyph = self.glyphs.get(glyphName, default)\n\t\tif glyph is not None:\n\t\t\tglyph.expand(self)\n\t\treturn glyph\n\n\tdef __getitem__(self, glyphName):\n\t\tglyph = self.glyphs[glyphName]\n\t\tglyph.expand(self)\n\t\treturn glyph\n\n\tdef __setitem__(self, glyphName, glyph):\n\t\tself.glyphs[glyphName] = glyph\n\t\tif glyphName not in self.glyphOrder:\n\t\t\tself.glyphOrder.append(glyphName)\n\n\tdef __delitem__(self, glyphName):\n\t\tdel self.glyphs[glyphName]\n\t\tself.glyphOrder.remove(glyphName)\n\n\tdef __len__(self):\n\t\tassert len(self.glyphOrder) == len(self.glyphs)\n\t\treturn len(self.glyphs)\n\n\tdef _getPhantomPoints(self, glyphName, hMetrics, vMetrics=None):\n\t\t\"\"\"Compute the four \"phantom points\" for the given glyph from its bounding box\n\t\tand the horizontal and vertical advance widths and sidebearings stored in the\n\t\tttFont's \"hmtx\" and \"vmtx\" tables.\n\n\t\t'hMetrics' should be ttFont['hmtx'].metrics.\n\n\t\t'vMetrics' should be ttFont['vmtx'].metrics if there is \"vmtx\" or None otherwise.\n\t\tIf there is no vMetrics passed in, vertical phantom points are set to the zero coordinate.\n\n\t\thttps:\/\/docs.microsoft.com\/en-us\/typography\/opentype\/spec\/tt_instructing_glyphs#phantoms\n\t\t\"\"\"\n\t\tglyph = self[glyphName]\n\t\tif not hasattr(glyph, 'xMin'):\n\t\t\tglyph.recalcBounds(self)\n\n\t\thorizontalAdvanceWidth, leftSideBearing = hMetrics[glyphName]\n\t\tleftSideX = glyph.xMin - leftSideBearing\n\t\trightSideX = leftSideX + horizontalAdvanceWidth\n\n\t\tif vMetrics:\n\t\t\tverticalAdvanceWidth, topSideBearing = vMetrics[glyphName]\n\t\t\ttopSideY = topSideBearing + glyph.yMax\n\t\t\tbottomSideY = topSideY - verticalAdvanceWidth\n\t\telse:\n\t\t\tbottomSideY = topSideY = 0\n\n\t\treturn [\n\t\t\t(leftSideX, 0),\n\t\t\t(rightSideX, 0),\n\t\t\t(0, topSideY),\n\t\t\t(0, bottomSideY),\n\t\t]\n\n\tdef _getCoordinatesAndControls(self, glyphName, hMetrics, vMetrics=None):\n\t\t\"\"\"Return glyph coordinates and controls as expected by \"gvar\" table.\n\n\t\tThe coordinates includes four \"phantom points\" for the glyph metrics,\n\t\tas mandated by the \"gvar\" spec.\n\n\t\tThe glyph controls is a namedtuple with the following attributes:\n\t\t\t- numberOfContours: -1 for composite glyphs.\n\t\t\t- endPts: list of indices of end points for each contour in simple\n\t\t\tglyphs, or component indices in composite glyphs (used for IUP\n\t\t\toptimization).\n\t\t\t- flags: array of contour point flags for simple glyphs (None for\n\t\t\tcomposite glyphs).\n\t\t\t- components: list of base glyph names (str) for each component in\n\t\t\tcomposite glyphs (None for simple glyphs).\n\n\t\tThe \"hMetrics\" and vMetrics are used to compute the \"phantom points\" (see\n\t\tthe \"_getPhantomPoints\" method).\n\n\t\tReturn None if the requested glyphName is not present.\n\t\t\"\"\"\n\t\tglyph = self.get(glyphName)\n\t\tif glyph is None:\n\t\t\treturn None\n\t\tif glyph.isComposite():\n\t\t\tcoords = GlyphCoordinates(\n\t\t\t\t[(getattr(c, 'x', 0), getattr(c, 'y', 0)) for c in glyph.components]\n\t\t\t)\n\t\t\tcontrols = _GlyphControls(\n\t\t\t\tnumberOfContours=glyph.numberOfContours,\n\t\t\t\tendPts=list(range(len(glyph.components))),\n\t\t\t\tflags=None,\n\t\t\t\tcomponents=[c.glyphName for c in glyph.components],\n\t\t\t)\n\t\telse:\n\t\t\tcoords, endPts, flags = glyph.getCoordinates(self)\n\t\t\tcoords = coords.copy()\n\t\t\tcontrols = _GlyphControls(\n\t\t\t\tnumberOfContours=glyph.numberOfContours,\n\t\t\t\tendPts=endPts,\n\t\t\t\tflags=flags,\n\t\t\t\tcomponents=None,\n\t\t\t)\n\t\t# Add phantom points for (left, right, top, bottom) positions.\n\t\tphantomPoints = self._getPhantomPoints(glyphName, hMetrics, vMetrics)\n\t\tcoords.extend(phantomPoints)\n\t\treturn coords, controls\n\n\tdef _setCoordinates(self, glyphName, coord, hMetrics, vMetrics=None):\n\t\t\"\"\"Set coordinates and metrics for the given glyph.\n\n\t\t\"coord\" is an array of GlyphCoordinates which must include the \"phantom\n\t\tpoints\" as the last four coordinates.\n\n\t\tBoth the horizontal\/vertical advances and left\/top sidebearings in \"hmtx\"\n\t\tand \"vmtx\" tables (if any) are updated from four phantom points and\n\t\tthe glyph's bounding boxes.\n\n\t\tThe \"hMetrics\" and vMetrics are used to propagate \"phantom points\"\n\t\tinto \"hmtx\" and \"vmtx\" tables if desired.  (see the \"_getPhantomPoints\"\n\t\tmethod).\n\t\t\"\"\"\n\t\tglyph = self[glyphName]\n\n\t\t# Handle phantom points for (left, right, top, bottom) positions.\n\t\tassert len(coord) >= 4\n\t\tleftSideX = coord[-4][0]\n\t\trightSideX = coord[-3][0]\n\t\ttopSideY = coord[-2][1]\n\t\tbottomSideY = coord[-1][1]\n\n\t\tcoord = coord[:-4]\n\n\t\tif glyph.isComposite():\n\t\t\tassert len(coord) == len(glyph.components)\n\t\t\tfor p, comp in zip(coord, glyph.components):\n\t\t\t\tif hasattr(comp, 'x'):\n\t\t\t\t\tcomp.x, comp.y = p\n\t\telif glyph.numberOfContours == 0:\n\t\t\tassert len(coord) == 0\n\t\telse:\n\t\t\tassert len(coord) == len(glyph.coordinates)\n\t\t\tglyph.coordinates = GlyphCoordinates(coord)\n\n\t\tglyph.recalcBounds(self)\n\n\t\thorizontalAdvanceWidth = otRound(rightSideX - leftSideX)\n\t\tif horizontalAdvanceWidth < 0:\n\t\t\t# unlikely, but it can happen, see:\n\t\t\t# https:\/\/github.com\/fonttools\/fonttools\/pull\/1198\n\t\t\thorizontalAdvanceWidth = 0\n\t\tleftSideBearing = otRound(glyph.xMin - leftSideX)\n\t\thMetrics[glyphName] = horizontalAdvanceWidth, leftSideBearing\n\n\t\tif vMetrics is not None:\n\t\t\tverticalAdvanceWidth = otRound(topSideY - bottomSideY)\n\t\t\tif verticalAdvanceWidth < 0:  # unlikely but do the same as horizontal\n\t\t\t\tverticalAdvanceWidth = 0\n\t\t\ttopSideBearing = otRound(topSideY - glyph.yMax)\n\t\t\tvMetrics[glyphName] = verticalAdvanceWidth, topSideBearing\n\n\n\t# Deprecated\n\n\tdef _synthesizeVMetrics(self, glyphName, ttFont, defaultVerticalOrigin):\n\t\t\"\"\"This method is wrong and deprecated.\n\t\tFor rationale see:\n\t\thttps:\/\/github.com\/fonttools\/fonttools\/pull\/2266\/files#r613569473\n\t\t\"\"\"\n\t\tvMetrics = getattr(ttFont.get('vmtx'), 'metrics', None)\n\t\tif vMetrics is None:\n\t\t\tverticalAdvanceWidth = ttFont[\"head\"].unitsPerEm\n\t\t\ttopSideY = getattr(ttFont.get('hhea'), 'ascent', None)\n\t\t\tif topSideY is None:\n\t\t\t\tif defaultVerticalOrigin is not None:\n\t\t\t\t\ttopSideY = defaultVerticalOrigin\n\t\t\t\telse:\n\t\t\t\t\ttopSideY = verticalAdvanceWidth\n\t\t\tglyph = self[glyphName]\n\t\t\tglyph.recalcBounds(self)\n\t\t\ttopSideBearing = otRound(topSideY - glyph.yMax)\n\t\t\tvMetrics = {glyphName: (verticalAdvanceWidth, topSideBearing)}\n\t\treturn vMetrics\n\n\t@deprecateFunction(\"use '_getPhantomPoints' instead\", category=DeprecationWarning)\n\tdef getPhantomPoints(self, glyphName, ttFont, defaultVerticalOrigin=None):\n\t\t\"\"\"Old public name for self._getPhantomPoints().\n\t\tSee: https:\/\/github.com\/fonttools\/fonttools\/pull\/2266\"\"\"\n\t\thMetrics = ttFont['hmtx'].metrics\n\t\tvMetrics = self._synthesizeVMetrics(glyphName, ttFont, defaultVerticalOrigin)\n\t\treturn self._getPhantomPoints(glyphName, hMetrics, vMetrics)\n\n\t@deprecateFunction(\"use '_getCoordinatesAndControls' instead\", category=DeprecationWarning)\n\tdef getCoordinatesAndControls(self, glyphName, ttFont, defaultVerticalOrigin=None):\n\t\t\"\"\"Old public name for self._getCoordinatesAndControls().\n\t\tSee: https:\/\/github.com\/fonttools\/fonttools\/pull\/2266\"\"\"\n\t\thMetrics = ttFont['hmtx'].metrics\n\t\tvMetrics = self._synthesizeVMetrics(glyphName, ttFont, defaultVerticalOrigin)\n\t\treturn self._getCoordinatesAndControls(glyphName, hMetrics, vMetrics)\n\n\t@deprecateFunction(\"use '_setCoordinates' instead\", category=DeprecationWarning)\n\tdef setCoordinates(self, glyphName, ttFont):\n\t\t\"\"\"Old public name for self._setCoordinates().\n\t\tSee: https:\/\/github.com\/fonttools\/fonttools\/pull\/2266\"\"\"\n\t\thMetrics = ttFont['hmtx'].metrics\n\t\tvMetrics = getattr(ttFont.get('vmtx'), 'metrics', None)\n\t\tself._setCoordinates(glyphName, hMetrics, vMetrics)\n\n\n_GlyphControls = namedtuple(\n\t\"_GlyphControls\", \"numberOfContours endPts flags components\"\n)\n\n\nglyphHeaderFormat = \"\"\"\n\t\t>\t# big endian\n\t\tnumberOfContours:\th\n\t\txMin:\t\t\t\th\n\t\tyMin:\t\t\t\th\n\t\txMax:\t\t\t\th\n\t\tyMax:\t\t\t\th\n\"\"\"\n\n# flags\nflagOnCurve = 0x01\nflagXShort = 0x02\nflagYShort = 0x04\nflagRepeat = 0x08\nflagXsame =  0x10\nflagYsame = 0x20\nflagOverlapSimple = 0x40\nflagReserved = 0x80\n\n# These flags are kept for XML output after decompiling the coordinates\nkeepFlags = flagOnCurve + flagOverlapSimple\n\n_flagSignBytes = {\n\t0: 2,\n\tflagXsame: 0,\n\tflagXShort|flagXsame: +1,\n\tflagXShort: -1,\n\tflagYsame: 0,\n\tflagYShort|flagYsame: +1,\n\tflagYShort: -1,\n}\n\ndef flagBest(x, y, onCurve):\n\t\"\"\"For a given x,y delta pair, returns the flag that packs this pair\n\tmost efficiently, as well as the number of byte cost of such flag.\"\"\"\n\n\tflag = flagOnCurve if onCurve else 0\n\tcost = 0\n\t# do x\n\tif x == 0:\n\t\tflag = flag | flagXsame\n\telif -255 <= x <= 255:\n\t\tflag = flag | flagXShort\n\t\tif x > 0:\n\t\t\tflag = flag | flagXsame\n\t\tcost += 1\n\telse:\n\t\tcost += 2\n\t# do y\n\tif y == 0:\n\t\tflag = flag | flagYsame\n\telif -255 <= y <= 255:\n\t\tflag = flag | flagYShort\n\t\tif y > 0:\n\t\t\tflag = flag | flagYsame\n\t\tcost += 1\n\telse:\n\t\tcost += 2\n\treturn flag, cost\n\ndef flagFits(newFlag, oldFlag, mask):\n\tnewBytes = _flagSignBytes[newFlag & mask]\n\toldBytes = _flagSignBytes[oldFlag & mask]\n\treturn newBytes == oldBytes or abs(newBytes) > abs(oldBytes)\n\ndef flagSupports(newFlag, oldFlag):\n\treturn ((oldFlag & flagOnCurve) == (newFlag & flagOnCurve) and\n\t\tflagFits(newFlag, oldFlag, flagXsame|flagXShort) and\n\t\tflagFits(newFlag, oldFlag, flagYsame|flagYShort))\n\ndef flagEncodeCoord(flag, mask, coord, coordBytes):\n\tbyteCount = _flagSignBytes[flag & mask]\n\tif byteCount == 1:\n\t\tcoordBytes.append(coord)\n\telif byteCount == -1:\n\t\tcoordBytes.append(-coord)\n\telif byteCount == 2:\n\t\tcoordBytes.extend(struct.pack('>h', coord))\n\ndef flagEncodeCoords(flag, x, y, xBytes, yBytes):\n\tflagEncodeCoord(flag, flagXsame|flagXShort, x, xBytes)\n\tflagEncodeCoord(flag, flagYsame|flagYShort, y, yBytes)\n\n\nARG_1_AND_2_ARE_WORDS\t\t= 0x0001  # if set args are words otherwise they are bytes\nARGS_ARE_XY_VALUES\t\t= 0x0002  # if set args are xy values, otherwise they are points\nROUND_XY_TO_GRID\t\t= 0x0004  # for the xy values if above is true\nWE_HAVE_A_SCALE\t\t\t= 0x0008  # Sx = Sy, otherwise scale == 1.0\nNON_OVERLAPPING\t\t\t= 0x0010  # set to same value for all components (obsolete!)\nMORE_COMPONENTS\t\t\t= 0x0020  # indicates at least one more glyph after this one\nWE_HAVE_AN_X_AND_Y_SCALE\t= 0x0040  # Sx, Sy\nWE_HAVE_A_TWO_BY_TWO\t\t= 0x0080  # t00, t01, t10, t11\nWE_HAVE_INSTRUCTIONS\t\t= 0x0100  # instructions follow\nUSE_MY_METRICS\t\t\t= 0x0200  # apply these metrics to parent glyph\nOVERLAP_COMPOUND\t\t= 0x0400  # used by Apple in GX fonts\nSCALED_COMPONENT_OFFSET\t\t= 0x0800  # composite designed to have the component offset scaled (designed for Apple)\nUNSCALED_COMPONENT_OFFSET\t= 0x1000  # composite designed not to have the component offset scaled (designed for MS)\n\n\nCompositeMaxpValues = namedtuple('CompositeMaxpValues', ['nPoints', 'nContours', 'maxComponentDepth'])\n\n\nclass Glyph(object):\n\t\"\"\"This class represents an individual TrueType glyph.\n\n\tTrueType glyph objects come in two flavours: simple and composite. Simple\n\tglyph objects contain contours, represented via the ``.coordinates``,\n\t``.flags``, ``.numberOfContours``, and ``.endPtsOfContours`` attributes;\n\tcomposite glyphs contain components, available through the ``.components``\n\tattributes.\n\n\tBecause the ``.coordinates`` attribute (and other simple glyph attributes mentioned\n\tabove) is only set on simple glyphs and the ``.components`` attribute is only\n\tset on composite glyphs, it is necessary to use the :py:meth:`isComposite`\n\tmethod to test whether a glyph is simple or composite before attempting to\n\taccess its data.\n\n\tFor a composite glyph, the components can also be accessed via array-like access::\n\n\t\t>> assert(font[\"glyf\"][\"Aacute\"].isComposite())\n\t\t>> font[\"glyf\"][\"Aacute\"][0]\n\t\t<fontTools.ttLib.tables._g_l_y_f.GlyphComponent at 0x1027b2ee0>\n\n\t\"\"\"\n\n\tdef __init__(self, data=b\"\"):\n\t\tif not data:\n\t\t\t# empty char\n\t\t\tself.numberOfContours = 0\n\t\t\treturn\n\t\tself.data = data\n\n\tdef compact(self, glyfTable, recalcBBoxes=True):\n\t\tdata = self.compile(glyfTable, recalcBBoxes)\n\t\tself.__dict__.clear()\n\t\tself.data = data\n\n\tdef expand(self, glyfTable):\n\t\tif not hasattr(self, \"data\"):\n\t\t\t# already unpacked\n\t\t\treturn\n\t\tif not self.data:\n\t\t\t# empty char\n\t\t\tdel self.data\n\t\t\tself.numberOfContours = 0\n\t\t\treturn\n\t\tdummy, data = sstruct.unpack2(glyphHeaderFormat, self.data, self)\n\t\tdel self.data\n\t\t# Some fonts (eg. Neirizi.ttf) have a 0 for numberOfContours in\n\t\t# some glyphs; decompileCoordinates assumes that there's at least\n\t\t# one, so short-circuit here.\n\t\tif self.numberOfContours == 0:\n\t\t\treturn\n\t\tif self.isComposite():\n\t\t\tself.decompileComponents(data, glyfTable)\n\t\telse:\n\t\t\tself.decompileCoordinates(data)\n\n\tdef compile(self, glyfTable, recalcBBoxes=True):\n\t\tif hasattr(self, \"data\"):\n\t\t\tif recalcBBoxes:\n\t\t\t\t# must unpack glyph in order to recalculate bounding box\n\t\t\t\tself.expand(glyfTable)\n\t\t\telse:\n\t\t\t\treturn self.data\n\t\tif self.numberOfContours == 0:\n\t\t\treturn b''\n\t\tif recalcBBoxes:\n\t\t\tself.recalcBounds(glyfTable)\n\t\tdata = sstruct.pack(glyphHeaderFormat, self)\n\t\tif self.isComposite():\n\t\t\tdata = data + self.compileComponents(glyfTable)\n\t\telse:\n\t\t\tdata = data + self.compileCoordinates()\n\t\treturn data\n\n\tdef toXML(self, writer, ttFont):\n\t\tif self.isComposite():\n\t\t\tfor compo in self.components:\n\t\t\t\tcompo.toXML(writer, ttFont)\n\t\t\thaveInstructions = hasattr(self, \"program\")\n\t\telse:\n\t\t\tlast = 0\n\t\t\tfor i in range(self.numberOfContours):\n\t\t\t\twriter.begintag(\"contour\")\n\t\t\t\twriter.newline()\n\t\t\t\tfor j in range(last, self.endPtsOfContours[i] + 1):\n\t\t\t\t\tattrs = [\n\t\t\t\t\t\t\t(\"x\", self.coordinates[j][0]),\n\t\t\t\t\t\t\t(\"y\", self.coordinates[j][1]),\n\t\t\t\t\t\t\t(\"on\", self.flags[j] & flagOnCurve),\n\t\t\t\t\t\t]\n\t\t\t\t\tif self.flags[j] & flagOverlapSimple:\n\t\t\t\t\t\t# Apple's rasterizer uses flagOverlapSimple in the first contour\/first pt to flag glyphs that contain overlapping contours\n\t\t\t\t\t\tattrs.append((\"overlap\", 1))\n\t\t\t\t\twriter.simpletag(\"pt\", attrs)\n\t\t\t\t\twriter.newline()\n\t\t\t\tlast = self.endPtsOfContours[i] + 1\n\t\t\t\twriter.endtag(\"contour\")\n\t\t\t\twriter.newline()\n\t\t\thaveInstructions = self.numberOfContours > 0\n\t\tif haveInstructions:\n\t\t\tif self.program:\n\t\t\t\twriter.begintag(\"instructions\")\n\t\t\t\twriter.newline()\n\t\t\t\tself.program.toXML(writer, ttFont)\n\t\t\t\twriter.endtag(\"instructions\")\n\t\t\telse:\n\t\t\t\twriter.simpletag(\"instructions\")\n\t\t\twriter.newline()\n\n\tdef fromXML(self, name, attrs, content, ttFont):\n\t\tif name == \"contour\":\n\t\t\tif self.numberOfContours < 0:\n\t\t\t\traise ttLib.TTLibError(\"can't mix composites and contours in glyph\")\n\t\t\tself.numberOfContours = self.numberOfContours + 1\n\t\t\tcoordinates = GlyphCoordinates()\n\t\t\tflags = bytearray()\n\t\t\tfor element in content:\n\t\t\t\tif not isinstance(element, tuple):\n\t\t\t\t\tcontinue\n\t\t\t\tname, attrs, content = element\n\t\t\t\tif name != \"pt\":\n\t\t\t\t\tcontinue  # ignore anything but \"pt\"\n\t\t\t\tcoordinates.append((safeEval(attrs[\"x\"]), safeEval(attrs[\"y\"])))\n\t\t\t\tflag = bool(safeEval(attrs[\"on\"]))\n\t\t\t\tif \"overlap\" in attrs and bool(safeEval(attrs[\"overlap\"])):\n\t\t\t\t\tflag |= flagOverlapSimple\n\t\t\t\tflags.append(flag)\n\t\t\tif not hasattr(self, \"coordinates\"):\n\t\t\t\tself.coordinates = coordinates\n\t\t\t\tself.flags = flags\n\t\t\t\tself.endPtsOfContours = [len(coordinates)-1]\n\t\t\telse:\n\t\t\t\tself.coordinates.extend (coordinates)\n\t\t\t\tself.flags.extend(flags)\n\t\t\t\tself.endPtsOfContours.append(len(self.coordinates)-1)\n\t\telif name == \"component\":\n\t\t\tif self.numberOfContours > 0:\n\t\t\t\traise ttLib.TTLibError(\"can't mix composites and contours in glyph\")\n\t\t\tself.numberOfContours = -1\n\t\t\tif not hasattr(self, \"components\"):\n\t\t\t\tself.components = []\n\t\t\tcomponent = GlyphComponent()\n\t\t\tself.components.append(component)\n\t\t\tcomponent.fromXML(name, attrs, content, ttFont)\n\t\telif name == \"instructions\":\n\t\t\tself.program = ttProgram.Program()\n\t\t\tfor element in content:\n\t\t\t\tif not isinstance(element, tuple):\n\t\t\t\t\tcontinue\n\t\t\t\tname, attrs, content = element\n\t\t\t\tself.program.fromXML(name, attrs, content, ttFont)\n\n\tdef getCompositeMaxpValues(self, glyfTable, maxComponentDepth=1):\n\t\tassert self.isComposite()\n\t\tnContours = 0\n\t\tnPoints = 0\n\t\tinitialMaxComponentDepth = maxComponentDepth\n\t\tfor compo in self.components:\n\t\t\tbaseGlyph = glyfTable[compo.glyphName]\n\t\t\tif baseGlyph.numberOfContours == 0:\n\t\t\t\tcontinue\n\t\t\telif baseGlyph.numberOfContours > 0:\n\t\t\t\tnP, nC = baseGlyph.getMaxpValues()\n\t\t\telse:\n\t\t\t\tnP, nC, componentDepth = baseGlyph.getCompositeMaxpValues(\n\t\t\t\t\t\tglyfTable, initialMaxComponentDepth + 1)\n\t\t\t\tmaxComponentDepth = max(maxComponentDepth, componentDepth)\n\t\t\tnPoints = nPoints + nP\n\t\t\tnContours = nContours + nC\n\t\treturn CompositeMaxpValues(nPoints, nContours, maxComponentDepth)\n\n\tdef getMaxpValues(self):\n\t\tassert self.numberOfContours > 0\n\t\treturn len(self.coordinates), len(self.endPtsOfContours)\n\n\tdef decompileComponents(self, data, glyfTable):\n\t\tself.components = []\n\t\tmore = 1\n\t\thaveInstructions = 0\n\t\twhile more:\n\t\t\tcomponent = GlyphComponent()\n\t\t\tmore, haveInstr, data = component.decompile(data, glyfTable)\n\t\t\thaveInstructions = haveInstructions | haveInstr\n\t\t\tself.components.append(component)\n\t\tif haveInstructions:\n\t\t\tnumInstructions, = struct.unpack(\">h\", data[:2])\n\t\t\tdata = data[2:]\n\t\t\tself.program = ttProgram.Program()\n\t\t\tself.program.fromBytecode(data[:numInstructions])\n\t\t\tdata = data[numInstructions:]\n\t\t\tif len(data) >= 4:\n\t\t\t\tlog.warning(\n\t\t\t\t\t\"too much glyph data at the end of composite glyph: %d excess bytes\",\n\t\t\t\t\tlen(data))\n\n\tdef decompileCoordinates(self, data):\n\t\tendPtsOfContours = array.array(\"h\")\n\t\tendPtsOfContours.frombytes(data[:2*self.numberOfContours])\n\t\tif sys.byteorder != \"big\": endPtsOfContours.byteswap()\n\t\tself.endPtsOfContours = endPtsOfContours.tolist()\n\n\t\tpos = 2*self.numberOfContours\n\t\tinstructionLength, = struct.unpack(\">h\", data[pos:pos+2])\n\t\tself.program = ttProgram.Program()\n\t\tself.program.fromBytecode(data[pos+2:pos+2+instructionLength])\n\t\tpos += 2 + instructionLength\n\t\tnCoordinates = self.endPtsOfContours[-1] + 1\n\t\tflags, xCoordinates, yCoordinates = \\\n\t\t\t\tself.decompileCoordinatesRaw(nCoordinates, data, pos)\n\n\t\t# fill in repetitions and apply signs\n\t\tself.coordinates = coordinates = GlyphCoordinates.zeros(nCoordinates)\n\t\txIndex = 0\n\t\tyIndex = 0\n\t\tfor i in range(nCoordinates):\n\t\t\tflag = flags[i]\n\t\t\t# x coordinate\n\t\t\tif flag & flagXShort:\n\t\t\t\tif flag & flagXsame:\n\t\t\t\t\tx = xCoordinates[xIndex]\n\t\t\t\telse:\n\t\t\t\t\tx = -xCoordinates[xIndex]\n\t\t\t\txIndex = xIndex + 1\n\t\t\telif flag & flagXsame:\n\t\t\t\tx = 0\n\t\t\telse:\n\t\t\t\tx = xCoordinates[xIndex]\n\t\t\t\txIndex = xIndex + 1\n\t\t\t# y coordinate\n\t\t\tif flag & flagYShort:\n\t\t\t\tif flag & flagYsame:\n\t\t\t\t\ty = yCoordinates[yIndex]\n\t\t\t\telse:\n\t\t\t\t\ty = -yCoordinates[yIndex]\n\t\t\t\tyIndex = yIndex + 1\n\t\t\telif flag & flagYsame:\n\t\t\t\ty = 0\n\t\t\telse:\n\t\t\t\ty = yCoordinates[yIndex]\n\t\t\t\tyIndex = yIndex + 1\n\t\t\tcoordinates[i] = (x, y)\n\t\tassert xIndex == len(xCoordinates)\n\t\tassert yIndex == len(yCoordinates)\n\t\tcoordinates.relativeToAbsolute()\n\t\t# discard all flags except \"keepFlags\"\n\t\tfor i in range(len(flags)):\n\t\t\tflags[i] &= keepFlags\n\t\tself.flags = flags\n\n\tdef decompileCoordinatesRaw(self, nCoordinates, data, pos=0):\n\t\t# unpack flags and prepare unpacking of coordinates\n\t\tflags = bytearray(nCoordinates)\n\t\t# Warning: deep Python trickery going on. We use the struct module to unpack\n\t\t# the coordinates. We build a format string based on the flags, so we can\n\t\t# unpack the coordinates in one struct.unpack() call.\n\t\txFormat = \">\" # big endian\n\t\tyFormat = \">\" # big endian\n\t\tj = 0\n\t\twhile True:\n\t\t\tflag = data[pos]\n\t\t\tpos += 1\n\t\t\trepeat = 1\n\t\t\tif flag & flagRepeat:\n\t\t\t\trepeat = data[pos] + 1\n\t\t\t\tpos += 1\n\t\t\tfor k in range(repeat):\n\t\t\t\tif flag & flagXShort:\n\t\t\t\t\txFormat = xFormat + 'B'\n\t\t\t\telif not (flag & flagXsame):\n\t\t\t\t\txFormat = xFormat + 'h'\n\t\t\t\tif flag & flagYShort:\n\t\t\t\t\tyFormat = yFormat + 'B'\n\t\t\t\telif not (flag & flagYsame):\n\t\t\t\t\tyFormat = yFormat + 'h'\n\t\t\t\tflags[j] = flag\n\t\t\t\tj = j + 1\n\t\t\tif j >= nCoordinates:\n\t\t\t\tbreak\n\t\tassert j == nCoordinates, \"bad glyph flags\"\n\t\t# unpack raw coordinates, krrrrrr-tching!\n\t\txDataLen = struct.calcsize(xFormat)\n\t\tyDataLen = struct.calcsize(yFormat)\n\t\tif len(data) - pos - (xDataLen + yDataLen) >= 4:\n\t\t\tlog.warning(\n\t\t\t\t\"too much glyph data: %d excess bytes\", len(data) - pos - (xDataLen + yDataLen))\n\t\txCoordinates = struct.unpack(xFormat, data[pos:pos+xDataLen])\n\t\tyCoordinates = struct.unpack(yFormat, data[pos+xDataLen:pos+xDataLen+yDataLen])\n\t\treturn flags, xCoordinates, yCoordinates\n\n\tdef compileComponents(self, glyfTable):\n\t\tdata = b\"\"\n\t\tlastcomponent = len(self.components) - 1\n\t\tmore = 1\n\t\thaveInstructions = 0\n\t\tfor i in range(len(self.components)):\n\t\t\tif i == lastcomponent:\n\t\t\t\thaveInstructions = hasattr(self, \"program\")\n\t\t\t\tmore = 0\n\t\t\tcompo = self.components[i]\n\t\t\tdata = data + compo.compile(more, haveInstructions, glyfTable)\n\t\tif haveInstructions:\n\t\t\tinstructions = self.program.getBytecode()\n\t\t\tdata = data + struct.pack(\">h\", len(instructions)) + instructions\n\t\treturn data\n\n\tdef compileCoordinates(self):\n\t\tassert len(self.coordinates) == len(self.flags)\n\t\tdata = []\n\t\tendPtsOfContours = array.array(\"h\", self.endPtsOfContours)\n\t\tif sys.byteorder != \"big\": endPtsOfContours.byteswap()\n\t\tdata.append(endPtsOfContours.tobytes())\n\t\tinstructions = self.program.getBytecode()\n\t\tdata.append(struct.pack(\">h\", len(instructions)))\n\t\tdata.append(instructions)\n\n\t\tdeltas = self.coordinates.copy()\n\t\tdeltas.toInt()\n\t\tdeltas.absoluteToRelative()\n\n\t\t# TODO(behdad): Add a configuration option for this?\n\t\tdeltas = self.compileDeltasGreedy(self.flags, deltas)\n\t\t#deltas = self.compileDeltasOptimal(self.flags, deltas)\n\n\t\tdata.extend(deltas)\n\t\treturn b''.join(data)\n\n\tdef compileDeltasGreedy(self, flags, deltas):\n\t\t# Implements greedy algorithm for packing coordinate deltas:\n\t\t# uses shortest representation one coordinate at a time.\n\t\tcompressedFlags = bytearray()\n\t\tcompressedXs = bytearray()\n\t\tcompressedYs = bytearray()\n\t\tlastflag = None\n\t\trepeat = 0\n\t\tfor flag,(x,y) in zip(flags, deltas):\n\t\t\t# Oh, the horrors of TrueType\n\t\t\t# do x\n\t\t\tif x == 0:\n\t\t\t\tflag = flag | flagXsame\n\t\t\telif -255 <= x <= 255:\n\t\t\t\tflag = flag | flagXShort\n\t\t\t\tif x > 0:\n\t\t\t\t\tflag = flag | flagXsame\n\t\t\t\telse:\n\t\t\t\t\tx = -x\n\t\t\t\tcompressedXs.append(x)\n\t\t\telse:\n\t\t\t\tcompressedXs.extend(struct.pack('>h', x))\n\t\t\t# do y\n\t\t\tif y == 0:\n\t\t\t\tflag = flag | flagYsame\n\t\t\telif -255 <= y <= 255:\n\t\t\t\tflag = flag | flagYShort\n\t\t\t\tif y > 0:\n\t\t\t\t\tflag = flag | flagYsame\n\t\t\t\telse:\n\t\t\t\t\ty = -y\n\t\t\t\tcompressedYs.append(y)\n\t\t\telse:\n\t\t\t\tcompressedYs.extend(struct.pack('>h', y))\n\t\t\t# handle repeating flags\n\t\t\tif flag == lastflag and repeat != 255:\n\t\t\t\trepeat = repeat + 1\n\t\t\t\tif repeat == 1:\n\t\t\t\t\tcompressedFlags.append(flag)\n\t\t\t\telse:\n\t\t\t\t\tcompressedFlags[-2] = flag | flagRepeat\n\t\t\t\t\tcompressedFlags[-1] = repeat\n\t\t\telse:\n\t\t\t\trepeat = 0\n\t\t\t\tcompressedFlags.append(flag)\n\t\t\tlastflag = flag\n\t\treturn (compressedFlags, compressedXs, compressedYs)\n\n\tdef compileDeltasOptimal(self, flags, deltas):\n\t\t# Implements optimal, dynaic-programming, algorithm for packing coordinate\n\t\t# deltas.  The savings are negligible :(.\n\t\tcandidates = []\n\t\tbestTuple = None\n\t\tbestCost = 0\n\t\trepeat = 0\n\t\tfor flag,(x,y) in zip(flags, deltas):\n\t\t\t# Oh, the horrors of TrueType\n\t\t\tflag, coordBytes = flagBest(x, y, flag)\n\t\t\tbestCost += 1 + coordBytes\n\t\t\tnewCandidates = [(bestCost, bestTuple, flag, coordBytes),\n\t\t\t\t\t\t\t(bestCost+1, bestTuple, (flag|flagRepeat), coordBytes)]\n\t\t\tfor lastCost,lastTuple,lastFlag,coordBytes in candidates:\n\t\t\t\tif lastCost + coordBytes <= bestCost + 1 and (lastFlag & flagRepeat) and (lastFlag < 0xff00) and flagSupports(lastFlag, flag):\n\t\t\t\t\tif (lastFlag & 0xFF) == (flag|flagRepeat) and lastCost == bestCost + 1:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tnewCandidates.append((lastCost + coordBytes, lastTuple, lastFlag+256, coordBytes))\n\t\t\tcandidates = newCandidates\n\t\t\tbestTuple = min(candidates, key=lambda t:t[0])\n\t\t\tbestCost = bestTuple[0]\n\n\t\tflags = []\n\t\twhile bestTuple:\n\t\t\tcost, bestTuple, flag, coordBytes = bestTuple\n\t\t\tflags.append(flag)\n\t\tflags.reverse()\n\n\t\tcompressedFlags = bytearray()\n\t\tcompressedXs = bytearray()\n\t\tcompressedYs = bytearray()\n\t\tcoords = iter(deltas)\n\t\tff = []\n\t\tfor flag in flags:\n\t\t\trepeatCount, flag = flag >> 8, flag & 0xFF\n\t\t\tcompressedFlags.append(flag)\n\t\t\tif flag & flagRepeat:\n\t\t\t\tassert(repeatCount > 0)\n\t\t\t\tcompressedFlags.append(repeatCount)\n\t\t\telse:\n\t\t\t\tassert(repeatCount == 0)\n\t\t\tfor i in range(1 + repeatCount):\n\t\t\t\tx,y = next(coords)\n\t\t\t\tflagEncodeCoords(flag, x, y, compressedXs, compressedYs)\n\t\t\t\tff.append(flag)\n\t\ttry:\n\t\t\tnext(coords)\n\t\t\traise Exception(\"internal error\")\n\t\texcept StopIteration:\n\t\t\tpass\n\n\t\treturn (compressedFlags, compressedXs, compressedYs)\n\n\tdef recalcBounds(self, glyfTable):\n\t\t\"\"\"Recalculates the bounds of the glyph.\n\n\t\tEach glyph object stores its bounding box in the\n\t\t``xMin``\/``yMin``\/``xMax``\/``yMax`` attributes. These bounds must be\n\t\trecomputed when the ``coordinates`` change. The ``table__g_l_y_f`` bounds\n\t\tmust be provided to resolve component bounds.\n\t\t\"\"\"\n\t\tcoords, endPts, flags = self.getCoordinates(glyfTable)\n\t\tself.xMin, self.yMin, self.xMax, self.yMax = calcIntBounds(coords)\n\n\tdef isComposite(self):\n\t\t\"\"\"Test whether a glyph has components\"\"\"\n\t\tif hasattr(self, \"data\") and self.data:\n\t\t\treturn struct.unpack(\">h\", self.data[:2])[0] == -1\n\t\telse:\n\t\t\treturn self.numberOfContours == -1\n\n\tdef __getitem__(self, componentIndex):\n\t\tif not self.isComposite():\n\t\t\traise ttLib.TTLibError(\"can't use glyph as sequence\")\n\t\treturn self.components[componentIndex]\n\n\tdef getCoordinates(self, glyfTable):\n\t\t\"\"\"Return the coordinates, end points and flags\n\n\t\tThis method returns three values: A :py:class:`GlyphCoordinates` object,\n\t\ta list of the indexes of the final points of each contour (allowing you\n\t\tto split up the coordinates list into contours) and a list of flags.\n\n\t\tOn simple glyphs, this method returns information from the glyph's own\n\t\tcontours; on composite glyphs, it \"flattens\" all components recursively\n\t\tto return a list of coordinates representing all the components involved\n\t\tin the glyph.\n\n\t\tTo interpret the flags for each point, see the \"Simple Glyph Flags\"\n\t\tsection of the `glyf table specification <https:\/\/docs.microsoft.com\/en-us\/typography\/opentype\/spec\/glyf#simple-glyph-description>`.\n\t\t\"\"\"\n\n\t\tif self.numberOfContours > 0:\n\t\t\treturn self.coordinates, self.endPtsOfContours, self.flags\n\t\telif self.isComposite():\n\t\t\t# it's a composite\n\t\t\tallCoords = GlyphCoordinates()\n\t\t\tallFlags = bytearray()\n\t\t\tallEndPts = []\n\t\t\tfor compo in self.components:\n\t\t\t\tg = glyfTable[compo.glyphName]\n\t\t\t\ttry:\n\t\t\t\t\tcoordinates, endPts, flags = g.getCoordinates(glyfTable)\n\t\t\t\texcept RecursionError:\n\t\t\t\t\traise ttLib.TTLibError(\"glyph '%s' contains a recursive component reference\" % compo.glyphName)\n\t\t\t\tcoordinates = GlyphCoordinates(coordinates)\n\t\t\t\tif hasattr(compo, \"firstPt\"):\n\t\t\t\t\t# component uses two reference points: we apply the transform _before_\n\t\t\t\t\t# computing the offset between the points\n\t\t\t\t\tif hasattr(compo, \"transform\"):\n\t\t\t\t\t\tcoordinates.transform(compo.transform)\n\t\t\t\t\tx1,y1 = allCoords[compo.firstPt]\n\t\t\t\t\tx2,y2 = coordinates[compo.secondPt]\n\t\t\t\t\tmove = x1-x2, y1-y2\n\t\t\t\t\tcoordinates.translate(move)\n\t\t\t\telse:\n\t\t\t\t\t# component uses XY offsets\n\t\t\t\t\tmove = compo.x, compo.y\n\t\t\t\t\tif not hasattr(compo, \"transform\"):\n\t\t\t\t\t\tcoordinates.translate(move)\n\t\t\t\t\telse:\n\t\t\t\t\t\tapple_way = compo.flags & SCALED_COMPONENT_OFFSET\n\t\t\t\t\t\tms_way = compo.flags & UNSCALED_COMPONENT_OFFSET\n\t\t\t\t\t\tassert not (apple_way and ms_way)\n\t\t\t\t\t\tif not (apple_way or ms_way):\n\t\t\t\t\t\t\tscale_component_offset = SCALE_COMPONENT_OFFSET_DEFAULT  # see top of this file\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tscale_component_offset = apple_way\n\t\t\t\t\t\tif scale_component_offset:\n\t\t\t\t\t\t\t# the Apple way: first move, then scale (ie. scale the component offset)\n\t\t\t\t\t\t\tcoordinates.translate(move)\n\t\t\t\t\t\t\tcoordinates.transform(compo.transform)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t# the MS way: first scale, then move\n\t\t\t\t\t\t\tcoordinates.transform(compo.transform)\n\t\t\t\t\t\t\tcoordinates.translate(move)\n\t\t\t\toffset = len(allCoords)\n\t\t\t\tallEndPts.extend(e + offset for e in endPts)\n\t\t\t\tallCoords.extend(coordinates)\n\t\t\t\tallFlags.extend(flags)\n\t\t\treturn allCoords, allEndPts, allFlags\n\t\telse:\n\t\t\treturn GlyphCoordinates(), [], bytearray()\n\n\tdef getComponentNames(self, glyfTable):\n\t\t\"\"\"Returns a list of names of component glyphs used in this glyph\n\n\t\tThis method can be used on simple glyphs (in which case it returns an\n\t\tempty list) or composite glyphs.\n\t\t\"\"\"\n\t\tif not hasattr(self, \"data\"):\n\t\t\tif self.isComposite():\n\t\t\t\treturn [c.glyphName for c in self.components]\n\t\t\telse:\n\t\t\t\treturn []\n\n\t\t# Extract components without expanding glyph\n\n\t\tif not self.data or struct.unpack(\">h\", self.data[:2])[0] >= 0:\n\t\t\treturn []  # Not composite\n\n\t\tdata = self.data\n\t\ti = 10\n\t\tcomponents = []\n\t\tmore = 1\n\t\twhile more:\n\t\t\tflags, glyphID = struct.unpack(\">HH\", data[i:i+4])\n\t\t\ti += 4\n\t\t\tflags = int(flags)\n\t\t\tcomponents.append(glyfTable.getGlyphName(int(glyphID)))\n\n\t\t\tif flags & ARG_1_AND_2_ARE_WORDS: i += 4\n\t\t\telse: i += 2\n\t\t\tif flags & WE_HAVE_A_SCALE: i += 2\n\t\t\telif flags & WE_HAVE_AN_X_AND_Y_SCALE: i += 4\n\t\t\telif flags & WE_HAVE_A_TWO_BY_TWO: i += 8\n\t\t\tmore = flags & MORE_COMPONENTS\n\n\t\treturn components\n\n\tdef trim(self, remove_hinting=False):\n\t\t\"\"\" Remove padding and, if requested, hinting, from a glyph.\n\t\t\tThis works on both expanded and compacted glyphs, without\n\t\t\texpanding it.\"\"\"\n\t\tif not hasattr(self, \"data\"):\n\t\t\tif remove_hinting:\n\t\t\t\tif self.isComposite():\n\t\t\t\t\tif hasattr(self, \"program\"):\n\t\t\t\t\t\tdel self.program\n\t\t\t\telse:\n\t\t\t\t\tself.program = ttProgram.Program()\n\t\t\t\t\tself.program.fromBytecode([])\n\t\t\t# No padding to trim.\n\t\t\treturn\n\t\tif not self.data:\n\t\t\treturn\n\t\tnumContours = struct.unpack(\">h\", self.data[:2])[0]\n\t\tdata = bytearray(self.data)\n\t\ti = 10\n\t\tif numContours >= 0:\n\t\t\ti += 2 * numContours # endPtsOfContours\n\t\t\tnCoordinates = ((data[i-2] << 8) | data[i-1]) + 1\n\t\t\tinstructionLen = (data[i] << 8) | data[i+1]\n\t\t\tif remove_hinting:\n\t\t\t\t# Zero instruction length\n\t\t\t\tdata[i] = data [i+1] = 0\n\t\t\t\ti += 2\n\t\t\t\tif instructionLen:\n\t\t\t\t\t# Splice it out\n\t\t\t\t\tdata = data[:i] + data[i+instructionLen:]\n\t\t\t\tinstructionLen = 0\n\t\t\telse:\n\t\t\t\ti += 2 + instructionLen\n\n\t\t\tcoordBytes = 0\n\t\t\tj = 0\n\t\t\twhile True:\n\t\t\t\tflag = data[i]\n\t\t\t\ti = i + 1\n\t\t\t\trepeat = 1\n\t\t\t\tif flag & flagRepeat:\n\t\t\t\t\trepeat = data[i] + 1\n\t\t\t\t\ti = i + 1\n\t\t\t\txBytes = yBytes = 0\n\t\t\t\tif flag & flagXShort:\n\t\t\t\t\txBytes = 1\n\t\t\t\telif not (flag & flagXsame):\n\t\t\t\t\txBytes = 2\n\t\t\t\tif flag & flagYShort:\n\t\t\t\t\tyBytes = 1\n\t\t\t\telif not (flag & flagYsame):\n\t\t\t\t\tyBytes = 2\n\t\t\t\tcoordBytes += (xBytes + yBytes) * repeat\n\t\t\t\tj += repeat\n\t\t\t\tif j >= nCoordinates:\n\t\t\t\t\tbreak\n\t\t\tassert j == nCoordinates, \"bad glyph flags\"\n\t\t\ti += coordBytes\n\t\t\t# Remove padding\n\t\t\tdata = data[:i]\n\t\telse:\n\t\t\tmore = 1\n\t\t\twe_have_instructions = False\n\t\t\twhile more:\n\t\t\t\tflags =(data[i] << 8) | data[i+1]\n\t\t\t\tif remove_hinting:\n\t\t\t\t\tflags &= ~WE_HAVE_INSTRUCTIONS\n\t\t\t\tif flags & WE_HAVE_INSTRUCTIONS:\n\t\t\t\t\twe_have_instructions = True\n\t\t\t\tdata[i+0] = flags >> 8\n\t\t\t\tdata[i+1] = flags & 0xFF\n\t\t\t\ti += 4\n\t\t\t\tflags = int(flags)\n\n\t\t\t\tif flags & ARG_1_AND_2_ARE_WORDS: i += 4\n\t\t\t\telse: i += 2\n\t\t\t\tif flags & WE_HAVE_A_SCALE: i += 2\n\t\t\t\telif flags & WE_HAVE_AN_X_AND_Y_SCALE: i += 4\n\t\t\t\telif flags & WE_HAVE_A_TWO_BY_TWO: i += 8\n\t\t\t\tmore = flags & MORE_COMPONENTS\n\t\t\tif we_have_instructions:\n\t\t\t\tinstructionLen = (data[i] << 8) | data[i+1]\n\t\t\t\ti += 2 + instructionLen\n\t\t\t# Remove padding\n\t\t\tdata = data[:i]\n\n\t\tself.data = data\n\n\tdef removeHinting(self):\n\t\t\"\"\"Removes TrueType hinting instructions from the glyph.\"\"\"\n\t\tself.trim (remove_hinting=True)\n\n\tdef draw(self, pen, glyfTable, offset=0):\n\t\t\"\"\"Draws the glyph using the supplied pen object.\n\n\t\tArguments:\n\t\t\tpen: An object conforming to the pen protocol.\n\t\t\tglyfTable: A :py:class:`table__g_l_y_f` object, to resolve components.\n\t\t\toffset (int): A horizontal offset. If provided, all coordinates are\n\t\t\t\ttranslated by this offset.\n\t\t\"\"\"\n\n\t\tif self.isComposite():\n\t\t\tfor component in self.components:\n\t\t\t\tglyphName, transform = component.getComponentInfo()\n\t\t\t\tpen.addComponent(glyphName, transform)\n\t\t\treturn\n\n\t\tcoordinates, endPts, flags = self.getCoordinates(glyfTable)\n\t\tif offset:\n\t\t\tcoordinates = coordinates.copy()\n\t\t\tcoordinates.translate((offset, 0))\n\t\tstart = 0\n\t\tfor end in endPts:\n\t\t\tend = end + 1\n\t\t\tcontour = coordinates[start:end]\n\t\t\tcFlags = [flagOnCurve & f for f in flags[start:end]]\n\t\t\tstart = end\n\t\t\tif 1 not in cFlags:\n\t\t\t\t# There is not a single on-curve point on the curve,\n\t\t\t\t# use pen.qCurveTo's special case by specifying None\n\t\t\t\t# as the on-curve point.\n\t\t\t\tcontour.append(None)\n\t\t\t\tpen.qCurveTo(*contour)\n\t\t\telse:\n\t\t\t\t# Shuffle the points so that contour the is guaranteed\n\t\t\t\t# to *end* in an on-curve point, which we'll use for\n\t\t\t\t# the moveTo.\n\t\t\t\tfirstOnCurve = cFlags.index(1) + 1\n\t\t\t\tcontour = contour[firstOnCurve:] + contour[:firstOnCurve]\n\t\t\t\tcFlags = cFlags[firstOnCurve:] + cFlags[:firstOnCurve]\n\t\t\t\tpen.moveTo(contour[-1])\n\t\t\t\twhile contour:\n\t\t\t\t\tnextOnCurve = cFlags.index(1) + 1\n\t\t\t\t\tif nextOnCurve == 1:\n\t\t\t\t\t\t# Skip a final lineTo(), as it is implied by\n\t\t\t\t\t\t# pen.closePath()\n\t\t\t\t\t\tif len(contour) > 1:\n\t\t\t\t\t\t\tpen.lineTo(contour[0])\n\t\t\t\t\telse:\n\t\t\t\t\t\tpen.qCurveTo(*contour[:nextOnCurve])\n\t\t\t\t\tcontour = contour[nextOnCurve:]\n\t\t\t\t\tcFlags = cFlags[nextOnCurve:]\n\t\t\tpen.closePath()\n\n\tdef drawPoints(self, pen, glyfTable, offset=0):\n\t\t\"\"\"Draw the glyph using the supplied pointPen. As opposed to Glyph.draw(),\n\t\tthis will not change the point indices.\n\t\t\"\"\"\n\n\t\tif self.isComposite():\n\t\t\tfor component in self.components:\n\t\t\t\tglyphName, transform = component.getComponentInfo()\n\t\t\t\tpen.addComponent(glyphName, transform)\n\t\t\treturn\n\n\t\tcoordinates, endPts, flags = self.getCoordinates(glyfTable)\n\t\tif offset:\n\t\t\tcoordinates = coordinates.copy()\n\t\t\tcoordinates.translate((offset, 0))\n\t\tstart = 0\n\t\tfor end in endPts:\n\t\t\tend = end + 1\n\t\t\tcontour = coordinates[start:end]\n\t\t\tcFlags = flags[start:end]\n\t\t\tstart = end\n\t\t\tpen.beginPath()\n\t\t\t# Start with the appropriate segment type based on the final segment\n\t\t\tsegmentType = \"line\" if cFlags[-1] == 1 else \"qcurve\"\n\t\t\tfor i, pt in enumerate(contour):\n\t\t\t\tif cFlags[i] & flagOnCurve == 1:\n\t\t\t\t\tpen.addPoint(pt, segmentType=segmentType)\n\t\t\t\t\tsegmentType = \"line\"\n\t\t\t\telse:\n\t\t\t\t\tpen.addPoint(pt)\n\t\t\t\t\tsegmentType = \"qcurve\"\n\t\t\tpen.endPath()\n\n\tdef __eq__(self, other):\n\t\tif type(self) != type(other):\n\t\t\treturn NotImplemented\n\t\treturn self.__dict__ == other.__dict__\n\n\tdef __ne__(self, other):\n\t\tresult = self.__eq__(other)\n\t\treturn result if result is NotImplemented else not result\n\nclass GlyphComponent(object):\n\t\"\"\"Represents a component within a composite glyph.\n\n\tThe component is represented internally with four attributes: ``glyphName``,\n\t``x``, ``y`` and ``transform``. If there is no \"two-by-two\" matrix (i.e\n\tno scaling, reflection, or rotation; only translation), the ``transform``\n\tattribute is not present.\n\t\"\"\"\n\t# The above documentation is not *completely* true, but is *true enough* because\n\t# the rare firstPt\/lastPt attributes are not totally supported and nobody seems to\n\t# mind - see below.\n\n\tdef __init__(self):\n\t\tpass\n\n\tdef getComponentInfo(self):\n\t\t\"\"\"Return information about the component\n\n\t\tThis method returns a tuple of two values: the glyph name of the component's\n\t\tbase glyph, and a transformation matrix. As opposed to accessing the attributes\n\t\tdirectly, ``getComponentInfo`` always returns a six-element tuple of the\n\t\tcomponent's transformation matrix, even when the two-by-two ``.transform``\n\t\tmatrix is not present.\n\t\t\"\"\"\n\t\t# XXX Ignoring self.firstPt & self.lastpt for now: I need to implement\n\t\t# something equivalent in fontTools.objects.glyph (I'd rather not\n\t\t# convert it to an absolute offset, since it is valuable information).\n\t\t# This method will now raise \"AttributeError: x\" on glyphs that use\n\t\t# this TT feature.\n\t\tif hasattr(self, \"transform\"):\n\t\t\t[[xx, xy], [yx, yy]] = self.transform\n\t\t\ttrans = (xx, xy, yx, yy, self.x, self.y)\n\t\telse:\n\t\t\ttrans = (1, 0, 0, 1, self.x, self.y)\n\t\treturn self.glyphName, trans\n\n\tdef decompile(self, data, glyfTable):\n\t\tflags, glyphID = struct.unpack(\">HH\", data[:4])\n\t\tself.flags = int(flags)\n\t\tglyphID = int(glyphID)\n\t\tself.glyphName = glyfTable.getGlyphName(int(glyphID))\n\t\tdata = data[4:]\n\n\t\tif self.flags & ARG_1_AND_2_ARE_WORDS:\n\t\t\tif self.flags & ARGS_ARE_XY_VALUES:\n\t\t\t\tself.x, self.y = struct.unpack(\">hh\", data[:4])\n\t\t\telse:\n\t\t\t\tx, y = struct.unpack(\">HH\", data[:4])\n\t\t\t\tself.firstPt, self.secondPt = int(x), int(y)\n\t\t\tdata = data[4:]\n\t\telse:\n\t\t\tif self.flags & ARGS_ARE_XY_VALUES:\n\t\t\t\tself.x, self.y = struct.unpack(\">bb\", data[:2])\n\t\t\telse:\n\t\t\t\tx, y = struct.unpack(\">BB\", data[:2])\n\t\t\t\tself.firstPt, self.secondPt = int(x), int(y)\n\t\t\tdata = data[2:]\n\n\t\tif self.flags & WE_HAVE_A_SCALE:\n\t\t\tscale, = struct.unpack(\">h\", data[:2])\n\t\t\tself.transform = [[fi2fl(scale,14), 0], [0, fi2fl(scale,14)]]  # fixed 2.14\n\t\t\tdata = data[2:]\n\t\telif self.flags & WE_HAVE_AN_X_AND_Y_SCALE:\n\t\t\txscale, yscale = struct.unpack(\">hh\", data[:4])\n\t\t\tself.transform = [[fi2fl(xscale,14), 0], [0, fi2fl(yscale,14)]]  # fixed 2.14\n\t\t\tdata = data[4:]\n\t\telif self.flags & WE_HAVE_A_TWO_BY_TWO:\n\t\t\t(xscale, scale01,\n\t\t\t\t\tscale10, yscale) = struct.unpack(\">hhhh\", data[:8])\n\t\t\tself.transform = [[fi2fl(xscale,14), fi2fl(scale01,14)],\n\t\t\t\t\t\t\t[fi2fl(scale10,14), fi2fl(yscale,14)]] # fixed 2.14\n\t\t\tdata = data[8:]\n\t\tmore = self.flags & MORE_COMPONENTS\n\t\thaveInstructions = self.flags & WE_HAVE_INSTRUCTIONS\n\t\tself.flags = self.flags & (ROUND_XY_TO_GRID | USE_MY_METRICS |\n\t\t\t\tSCALED_COMPONENT_OFFSET | UNSCALED_COMPONENT_OFFSET |\n\t\t\t\tNON_OVERLAPPING | OVERLAP_COMPOUND)\n\t\treturn more, haveInstructions, data\n\n\tdef compile(self, more, haveInstructions, glyfTable):\n\t\tdata = b\"\"\n\n\t\t# reset all flags we will calculate ourselves\n\t\tflags = self.flags & (ROUND_XY_TO_GRID | USE_MY_METRICS |\n\t\t\t\tSCALED_COMPONENT_OFFSET | UNSCALED_COMPONENT_OFFSET |\n\t\t\t\tNON_OVERLAPPING | OVERLAP_COMPOUND)\n\t\tif more:\n\t\t\tflags = flags | MORE_COMPONENTS\n\t\tif haveInstructions:\n\t\t\tflags = flags | WE_HAVE_INSTRUCTIONS\n\n\t\tif hasattr(self, \"firstPt\"):\n\t\t\tif (0 <= self.firstPt <= 255) and (0 <= self.secondPt <= 255):\n\t\t\t\tdata = data + struct.pack(\">BB\", self.firstPt, self.secondPt)\n\t\t\telse:\n\t\t\t\tdata = data + struct.pack(\">HH\", self.firstPt, self.secondPt)\n\t\t\t\tflags = flags | ARG_1_AND_2_ARE_WORDS\n\t\telse:\n\t\t\tx = otRound(self.x)\n\t\t\ty = otRound(self.y)\n\t\t\tflags = flags | ARGS_ARE_XY_VALUES\n\t\t\tif (-128 <= x <= 127) and (-128 <= y <= 127):\n\t\t\t\tdata = data + struct.pack(\">bb\", x, y)\n\t\t\telse:\n\t\t\t\tdata = data + struct.pack(\">hh\", x, y)\n\t\t\t\tflags = flags | ARG_1_AND_2_ARE_WORDS\n\n\t\tif hasattr(self, \"transform\"):\n\t\t\ttransform = [[fl2fi(x,14) for x in row] for row in self.transform]\n\t\t\tif transform[0][1] or transform[1][0]:\n\t\t\t\tflags = flags | WE_HAVE_A_TWO_BY_TWO\n\t\t\t\tdata = data + struct.pack(\">hhhh\",\n\t\t\t\t\t\ttransform[0][0], transform[0][1],\n\t\t\t\t\t\ttransform[1][0], transform[1][1])\n\t\t\telif transform[0][0] != transform[1][1]:\n\t\t\t\tflags = flags | WE_HAVE_AN_X_AND_Y_SCALE\n\t\t\t\tdata = data + struct.pack(\">hh\",\n\t\t\t\t\t\ttransform[0][0], transform[1][1])\n\t\t\telse:\n\t\t\t\tflags = flags | WE_HAVE_A_SCALE\n\t\t\t\tdata = data + struct.pack(\">h\",\n\t\t\t\t\t\ttransform[0][0])\n\n\t\tglyphID = glyfTable.getGlyphID(self.glyphName)\n\t\treturn struct.pack(\">HH\", flags, glyphID) + data\n\n\tdef toXML(self, writer, ttFont):\n\t\tattrs = [(\"glyphName\", self.glyphName)]\n\t\tif not hasattr(self, \"firstPt\"):\n\t\t\tattrs = attrs + [(\"x\", self.x), (\"y\", self.y)]\n\t\telse:\n\t\t\tattrs = attrs + [(\"firstPt\", self.firstPt), (\"secondPt\", self.secondPt)]\n\n\t\tif hasattr(self, \"transform\"):\n\t\t\ttransform = self.transform\n\t\t\tif transform[0][1] or transform[1][0]:\n\t\t\t\tattrs = attrs + [\n\t\t\t\t\t(\"scalex\", fl2str(transform[0][0], 14)),\n\t\t\t\t\t(\"scale01\", fl2str(transform[0][1], 14)),\n\t\t\t\t\t(\"scale10\", fl2str(transform[1][0], 14)),\n\t\t\t\t\t(\"scaley\", fl2str(transform[1][1], 14)),\n\t\t\t\t]\n\t\t\telif transform[0][0] != transform[1][1]:\n\t\t\t\tattrs = attrs + [\n\t\t\t\t\t(\"scalex\", fl2str(transform[0][0], 14)),\n\t\t\t\t\t(\"scaley\", fl2str(transform[1][1], 14)),\n\t\t\t\t]\n\t\t\telse:\n\t\t\t\tattrs = attrs + [(\"scale\", fl2str(transform[0][0], 14))]\n\t\tattrs = attrs + [(\"flags\", hex(self.flags))]\n\t\twriter.simpletag(\"component\", attrs)\n\t\twriter.newline()\n\n\tdef fromXML(self, name, attrs, content, ttFont):\n\t\tself.glyphName = attrs[\"glyphName\"]\n\t\tif \"firstPt\" in attrs:\n\t\t\tself.firstPt = safeEval(attrs[\"firstPt\"])\n\t\t\tself.secondPt = safeEval(attrs[\"secondPt\"])\n\t\telse:\n\t\t\tself.x = safeEval(attrs[\"x\"])\n\t\t\tself.y = safeEval(attrs[\"y\"])\n\t\tif \"scale01\" in attrs:\n\t\t\tscalex = str2fl(attrs[\"scalex\"], 14)\n\t\t\tscale01 = str2fl(attrs[\"scale01\"], 14)\n\t\t\tscale10 = str2fl(attrs[\"scale10\"], 14)\n\t\t\tscaley = str2fl(attrs[\"scaley\"], 14)\n\t\t\tself.transform = [[scalex, scale01], [scale10, scaley]]\n\t\telif \"scalex\" in attrs:\n\t\t\tscalex = str2fl(attrs[\"scalex\"], 14)\n\t\t\tscaley = str2fl(attrs[\"scaley\"], 14)\n\t\t\tself.transform = [[scalex, 0], [0, scaley]]\n\t\telif \"scale\" in attrs:\n\t\t\tscale = str2fl(attrs[\"scale\"], 14)\n\t\t\tself.transform = [[scale, 0], [0, scale]]\n\t\tself.flags = safeEval(attrs[\"flags\"])\n\n\tdef __eq__(self, other):\n\t\tif type(self) != type(other):\n\t\t\treturn NotImplemented\n\t\treturn self.__dict__ == other.__dict__\n\n\tdef __ne__(self, other):\n\t\tresult = self.__eq__(other)\n\t\treturn result if result is NotImplemented else not result\n\nclass GlyphCoordinates(object):\n\t\"\"\"A list of glyph coordinates.\n\n\tUnlike an ordinary list, this is a numpy-like matrix object which supports\n\tmatrix addition, scalar multiplication and other operations described below.\n\t\"\"\"\n\tdef __init__(self, iterable=[]):\n\t\tself._a = array.array('d')\n\t\tself.extend(iterable)\n\n\t@property\n\tdef array(self):\n\t\t\"\"\"Returns the underlying array of coordinates\"\"\"\n\t\treturn self._a\n\n\t@staticmethod\n\tdef zeros(count):\n\t\t\"\"\"Creates a new ``GlyphCoordinates`` object with all coordinates set to (0,0)\"\"\"\n\t\tg = GlyphCoordinates()\n\t\tg._a.frombytes(bytes(count * 2 * g._a.itemsize))\n\t\treturn g\n\n\tdef copy(self):\n\t\t\"\"\"Creates a new ``GlyphCoordinates`` object which is a copy of the current one.\"\"\"\n\t\tc = GlyphCoordinates()\n\t\tc._a.extend(self._a)\n\t\treturn c\n\n\tdef __len__(self):\n\t\t\"\"\"Returns the number of coordinates in the array.\"\"\"\n\t\treturn len(self._a) \/\/ 2\n\n\tdef __getitem__(self, k):\n\t\t\"\"\"Returns a two element tuple (x,y)\"\"\"\n\t\tif isinstance(k, slice):\n\t\t\tindices = range(*k.indices(len(self)))\n\t\t\treturn [self[i] for i in indices]\n\t\ta = self._a\n\t\tx = a[2*k]\n\t\ty = a[2*k+1]\n\t\treturn (int(x) if x.is_integer() else x,\n\t\t\tint(y) if y.is_integer() else y)\n\n\tdef __setitem__(self, k, v):\n\t\t\"\"\"Sets a point's coordinates to a two element tuple (x,y)\"\"\"\n\t\tif isinstance(k, slice):\n\t\t\tindices = range(*k.indices(len(self)))\n\t\t\t# XXX This only works if len(v) == len(indices)\n\t\t\tfor j,i in enumerate(indices):\n\t\t\t\tself[i] = v[j]\n\t\t\treturn\n\t\tself._a[2*k],self._a[2*k+1] = v\n\n\tdef __delitem__(self, i):\n\t\t\"\"\"Removes a point from the list\"\"\"\n\t\ti = (2*i) % len(self._a)\n\t\tdel self._a[i]\n\t\tdel self._a[i]\n\n\tdef __repr__(self):\n\t\treturn 'GlyphCoordinates(['+','.join(str(c) for c in self)+'])'\n\n\tdef append(self, p):\n\t\tself._a.extend(tuple(p))\n\n\tdef extend(self, iterable):\n\t\tfor p in iterable:\n\t\t\tself._a.extend(p)\n\n\tdef toInt(self, *, round=otRound):\n\t\ta = self._a\n\t\tfor i in range(len(a)):\n\t\t\ta[i] = round(a[i])\n\n\tdef relativeToAbsolute(self):\n\t\ta = self._a\n\t\tx,y = 0,0\n\t\tfor i in range(0, len(a), 2):\n\t\t\ta[i  ] = x = a[i  ] + x\n\t\t\ta[i+1] = y = a[i+1] + y\n\n\tdef absoluteToRelative(self):\n\t\ta = self._a\n\t\tx,y = 0,0\n\t\tfor i in range(0, len(a), 2):\n\t\t\tnx = a[i  ]\n\t\t\tny = a[i+1]\n\t\t\ta[i]   = nx - x\n\t\t\ta[i+1] = ny - y\n\t\t\tx = nx\n\t\t\ty = ny\n\n\tdef translate(self, p):\n\t\t\"\"\"\n\t\t>>> GlyphCoordinates([(1,2)]).translate((.5,0))\n\t\t\"\"\"\n\t\tx,y = p\n\t\tif x == 0 and y == 0:\n\t\t\treturn\n\t\ta = self._a\n\t\tfor i in range(0, len(a), 2):\n\t\t\ta[i]   += x\n\t\t\ta[i+1] += y\n\n\tdef scale(self, p):\n\t\t\"\"\"\n\t\t>>> GlyphCoordinates([(1,2)]).scale((.5,0))\n\t\t\"\"\"\n\t\tx,y = p\n\t\tif x == 1 and y == 1:\n\t\t\treturn\n\t\ta = self._a\n\t\tfor i in range(0, len(a), 2):\n\t\t\ta[i]   *= x\n\t\t\ta[i+1] *= y\n\n\tdef transform(self, t):\n\t\t\"\"\"\n\t\t>>> GlyphCoordinates([(1,2)]).transform(((.5,0),(.2,.5)))\n\t\t\"\"\"\n\t\ta = self._a\n\t\tfor i in range(0, len(a), 2):\n\t\t\tx = a[i  ]\n\t\t\ty = a[i+1]\n\t\t\tpx = x * t[0][0] + y * t[1][0]\n\t\t\tpy = x * t[0][1] + y * t[1][1]\n\t\t\ta[i]   = px\n\t\t\ta[i+1] = py\n\n\tdef __eq__(self, other):\n\t\t\"\"\"\n\t\t>>> g = GlyphCoordinates([(1,2)])\n\t\t>>> g2 = GlyphCoordinates([(1.0,2)])\n\t\t>>> g3 = GlyphCoordinates([(1.5,2)])\n\t\t>>> g == g2\n\t\tTrue\n\t\t>>> g == g3\n\t\tFalse\n\t\t>>> g2 == g3\n\t\tFalse\n\t\t\"\"\"\n\t\tif type(self) != type(other):\n\t\t\treturn NotImplemented\n\t\treturn self._a == other._a\n\n\tdef __ne__(self, other):\n\t\t\"\"\"\n\t\t>>> g = GlyphCoordinates([(1,2)])\n\t\t>>> g2 = GlyphCoordinates([(1.0,2)])\n\t\t>>> g3 = GlyphCoordinates([(1.5,2)])\n\t\t>>> g != g2\n\t\tFalse\n\t\t>>> g != g3\n\t\tTrue\n\t\t>>> g2 != g3\n\t\tTrue\n\t\t\"\"\"\n\t\tresult = self.__eq__(other)\n\t\treturn result if result is NotImplemented else not result\n\n\t# Math operations\n\n\tdef __pos__(self):\n\t\t\"\"\"\n\t\t>>> g = GlyphCoordinates([(1,2)])\n\t\t>>> g\n\t\tGlyphCoordinates([(1, 2)])\n\t\t>>> g2 = +g\n\t\t>>> g2\n\t\tGlyphCoordinates([(1, 2)])\n\t\t>>> g2.translate((1,0))\n\t\t>>> g2\n\t\tGlyphCoordinates([(2, 2)])\n\t\t>>> g\n\t\tGlyphCoordinates([(1, 2)])\n\t\t\"\"\"\n\t\treturn self.copy()\n\tdef __neg__(self):\n\t\t\"\"\"\n\t\t>>> g = GlyphCoordinates([(1,2)])\n\t\t>>> g\n\t\tGlyphCoordinates([(1, 2)])\n\t\t>>> g2 = -g\n\t\t>>> g2\n\t\tGlyphCoordinates([(-1, -2)])\n\t\t>>> g\n\t\tGlyphCoordinates([(1, 2)])\n\t\t\"\"\"\n\t\tr = self.copy()\n\t\ta = r._a\n\t\tfor i in range(len(a)):\n\t\t\ta[i] = -a[i]\n\t\treturn r\n\tdef __round__(self, *, round=otRound):\n\t\tr = self.copy()\n\t\tr.toInt(round=round)\n\t\treturn r\n\n\tdef __add__(self, other): return self.copy().__iadd__(other)\n\tdef __sub__(self, other): return self.copy().__isub__(other)\n\tdef __mul__(self, other): return self.copy().__imul__(other)\n\tdef __truediv__(self, other): return self.copy().__itruediv__(other)\n\n\t__radd__ = __add__\n\t__rmul__ = __mul__\n\tdef __rsub__(self, other): return other + (-self)\n\n\tdef __iadd__(self, other):\n\t\t\"\"\"\n\t\t>>> g = GlyphCoordinates([(1,2)])\n\t\t>>> g += (.5,0)\n\t\t>>> g\n\t\tGlyphCoordinates([(1.5, 2)])\n\t\t>>> g2 = GlyphCoordinates([(3,4)])\n\t\t>>> g += g2\n\t\t>>> g\n\t\tGlyphCoordinates([(4.5, 6)])\n\t\t\"\"\"\n\t\tif isinstance(other, tuple):\n\t\t\tassert len(other) ==  2\n\t\t\tself.translate(other)\n\t\t\treturn self\n\t\tif isinstance(other, GlyphCoordinates):\n\t\t\tother = other._a\n\t\t\ta = self._a\n\t\t\tassert len(a) == len(other)\n\t\t\tfor i in range(len(a)):\n\t\t\t\ta[i] += other[i]\n\t\t\treturn self\n\t\treturn NotImplemented\n\n\tdef __isub__(self, other):\n\t\t\"\"\"\n\t\t>>> g = GlyphCoordinates([(1,2)])\n\t\t>>> g -= (.5,0)\n\t\t>>> g\n\t\tGlyphCoordinates([(0.5, 2)])\n\t\t>>> g2 = GlyphCoordinates([(3,4)])\n\t\t>>> g -= g2\n\t\t>>> g\n\t\tGlyphCoordinates([(-2.5, -2)])\n\t\t\"\"\"\n\t\tif isinstance(other, tuple):\n\t\t\tassert len(other) ==  2\n\t\t\tself.translate((-other[0],-other[1]))\n\t\t\treturn self\n\t\tif isinstance(other, GlyphCoordinates):\n\t\t\tother = other._a\n\t\t\ta = self._a\n\t\t\tassert len(a) == len(other)\n\t\t\tfor i in range(len(a)):\n\t\t\t\ta[i] -= other[i]\n\t\t\treturn self\n\t\treturn NotImplemented\n\n\tdef __imul__(self, other):\n\t\t\"\"\"\n\t\t>>> g = GlyphCoordinates([(1,2)])\n\t\t>>> g *= (2,.5)\n\t\t>>> g *= 2\n\t\t>>> g\n\t\tGlyphCoordinates([(4, 2)])\n\t\t>>> g = GlyphCoordinates([(1,2)])\n\t\t>>> g *= 2\n\t\t>>> g\n\t\tGlyphCoordinates([(2, 4)])\n\t\t\"\"\"\n\t\tif isinstance(other, tuple):\n\t\t\tassert len(other) ==  2\n\t\t\tself.scale(other)\n\t\t\treturn self\n\t\tif isinstance(other, Number):\n\t\t\tif other == 1:\n\t\t\t\treturn self\n\t\t\ta = self._a\n\t\t\tfor i in range(len(a)):\n\t\t\t\ta[i] *= other\n\t\t\treturn self\n\t\treturn NotImplemented\n\n\tdef __itruediv__(self, other):\n\t\t\"\"\"\n\t\t>>> g = GlyphCoordinates([(1,3)])\n\t\t>>> g \/= (.5,1.5)\n\t\t>>> g \/= 2\n\t\t>>> g\n\t\tGlyphCoordinates([(1, 1)])\n\t\t\"\"\"\n\t\tif isinstance(other, Number):\n\t\t\tother = (other, other)\n\t\tif isinstance(other, tuple):\n\t\t\tif other == (1,1):\n\t\t\t\treturn self\n\t\t\tassert len(other) ==  2\n\t\t\tself.scale((1.\/other[0],1.\/other[1]))\n\t\t\treturn self\n\t\treturn NotImplemented\n\n\tdef __bool__(self):\n\t\t\"\"\"\n\t\t>>> g = GlyphCoordinates([])\n\t\t>>> bool(g)\n\t\tFalse\n\t\t>>> g = GlyphCoordinates([(0,0), (0.,0)])\n\t\t>>> bool(g)\n\t\tTrue\n\t\t>>> g = GlyphCoordinates([(0,0), (1,0)])\n\t\t>>> bool(g)\n\t\tTrue\n\t\t>>> g = GlyphCoordinates([(0,.5), (0,0)])\n\t\t>>> bool(g)\n\t\tTrue\n\t\t\"\"\"\n\t\treturn bool(self._a)\n\n\t__nonzero__ = __bool__\n\n\nif __name__ == \"__main__\":\n\timport doctest, sys\n\tsys.exit(doctest.testmod().failed)\n","label":1}
{"content":"#!\/usr\/bin\/python\n\"\"\"\nprogram to get haproxy statistics data and to put it into shape for datalogger\n\nyou have to customize config file to put necessary data into it\n\"\"\"\nimport sys\nimport urllib2\nimport time\nimport datetime\nimport os\nimport argparse\nimport json\nfrom collections import OrderedDict\nimport logging\nlogging.basicConfig(level=logging.ERROR)\n\ndef get_haproxy_csv(servername, username, password):\n    # for basic authentication\n    password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()\n    # add authentication information\n    password_mgr.add_password(None, servername, username, password)\n    # create handler\n    handler = urllib2.HTTPBasicAuthHandler(password_mgr)\n    # create opener\n    opener = urllib2.build_opener(handler)\n    # open url\n    res = opener.open(\"http:\/\/%s\/haproxy?stats;csv\" % servername)\n    csv = res.read().split('\\n')\n    return(csv)\n\ndef get_data(haproxies, index_keynames):\n    ret_data = OrderedDict()\n    timestamp = time.time()\n    for clustername, server_dict in haproxies.items():\n        for servername, credentials in server_dict.items():\n            csv = get_haproxy_csv(servername, credentials[\"username\"], credentials[\"password\"])\n            headers = csv[0].split(\",\")\n            headers[0] = headers[0][2:] # cut off trailing #\n            #print headers\n            for row in csv[1:]:\n                if len(row) > 0:\n                    #print row\n                    values = row.split(\",\")\n                    dict_values = OrderedDict()\n                    dict_values[u\"ts\"] = str(timestamp)\n                    dict_values[u\"servername\"] = servername\n                    dict_values[u\"clustername\"] = clustername\n                    for key, value in dict(zip(headers, values)).items():\n                        if key == \"\":\n                            continue\n                        dict_values[key] = value\n                    #print dict_values\n                    index_value = tuple((dict_values[keyname] for keyname in index_keynames))\n                    try:\n                        assert index_value not in ret_data.keys() # index must be unique\n                    except AssertionError as exc:\n                        logging.error(\"AssertionError at %s\/%s index_value=%s, ret_data=%s\", clustername, servername, index_value, ret_data)\n                        raise exc\n                    ret_data[index_value] = dict_values\n    return ret_data\n\ndef save_data(basedir_raw, tablename, data):\n    \"\"\"\n    save data in data to raw file with isoformat extention in basedir_raw\n    \"\"\"\n    # output full dataset\n    filename = os.path.join(basedir_raw, \"%s_%s.csv\" % (tablename, datetime.date.today().isoformat()))\n    outfile = None\n    headers = True\n    if os.path.exists(filename):\n        # open in append mode if file exists\n        outfile = open(filename, \"ab\")\n        # and print headers\n        headers = False\n    else:\n        outfile = open(filename, \"wb\")\n    lines = 0\n    for keyid, values in data.items():\n        if headers is True:\n            outfile.write(\"%s\\n\" % \"\\t\".join(values.keys()))\n            headers = False\n        try:\n            outfile.write(\"%s\\n\" % \"\\t\".join(values.values()))\n        except KeyError as exc:\n            logging.error(\"DataformatError skipping %s : %s\", keyid, data[keyid])\n        except TypeError as exc:\n            logging.exception(exc)\n            logging.error(data[keyid])\n        lines += 1\n    logging.info(\"got %d datasets\", lines)\n    logging.info(\"got %d unique keys\", len(data.keys()))\n    outfile.close()\n\n\ndef main():\n    project = \"haproxy\"\n    # tablename will be auto generated\n    # initialize project\n    basedir = os.path.join(\"\/var\/rrd\", project)\n    raw_basedir = os.path.join(basedir, \"raw\")\n    meta_basedir = os.path.join(basedir, \"meta\")\n    if not os.path.exists(basedir):\n        os.mkdir(basedir)\n        os.mkdir(raw_basedir)\n        os.mkdir(meta_basedir)\n    # get DATA\n    configfilename = \"%s.json\" % sys.argv[0].split(\".\")[0]\n    logging.info(\"configfilename: %s\", configfilename)\n    if not os.path.isfile(configfilename):\n        logging.error(\"config file %s not found\", configfilename)\n        sys.exit(1)\n    haproxies = json.load(open(configfilename))\n    index_keynames = (\"clustername\", \"servername\", \"pxname\", \"svname\")\n    data = get_data(haproxies, index_keynames)\n    # get data in shape\n    tablename_dict = {\n        \"0\" : {\n            \"ts_keyname\" : u\"ts\",\n            \"delimiter\" : u\"\\t\",\n            \"tablename\" : u\"frontend\",\n            \"values\" : {},\n            \"index_keynames\" : index_keynames,\n            \"headers\" : None,\n            \"value_keynames\" : [],\n            \"blacklist\" : [],\n            \"deleted\" : [],\n        },\n        \"1\" : {\n            \"ts_keyname\" : u\"ts\",\n            \"delimiter\" : u\"\\t\",\n            \"tablename\" : u\"backend\",\n            \"values\" : {},\n            \"index_keynames\" : index_keynames,\n            \"headers\" : None,\n            \"value_keynames\" : [],\n            \"blacklist\" : [],\n            \"deleted\" : [],\n        },\n        \"2\" : {\n            \"ts_keyname\" : u\"ts\",\n            \"delimiter\" : u\"\\t\",\n            \"tablename\" : u\"server\",\n            \"values\" : {},\n            \"index_keynames\" : index_keynames,\n            \"headers\" : None,\n            \"value_keynames\" : [],\n            \"blacklist\" : [],\n            \"deleted\" : [],\n        },\n        \"3\" : {\n            \"ts_keyname\" : u\"ts\",\n            \"delimiter\" : u\"\\t\",\n            \"tablename\" : u\"listener\",\n            \"values\" : {},\n            \"index_keynames\" : index_keynames,\n            \"headers\" : None,\n            \"value_keynames\" : [],\n            \"blacklist\" : [],\n            \"deleted\" : [],\n        },\n    }\n    # split into different tables, get rid of empty fields for special\n    # types, generate blacklist keys for every type\n    # only numeric fields are values, all other keys appear on blacklist\n    for key, values in data.items():\n        tablename_dict[values[\"type\"]][\"headers\"] = values.keys()\n        tablename_dict[values[\"type\"]][\"values\"][key] = values\n        for v_key, v_value in values.items():\n            if v_value == \"\":\n                if not v_key in tablename_dict[values[\"type\"]][\"deleted\"]:\n                    tablename_dict[values[\"type\"]][\"deleted\"].append(v_key)\n                del tablename_dict[values[\"type\"]][\"values\"][key][v_key]\n                tablename_dict[values[\"type\"]][\"headers\"].remove(v_key)\n            try:\n                float(v_value)\n                if not v_key in tablename_dict[values[\"type\"]][\"value_keynames\"]:\n                    tablename_dict[values[\"type\"]][\"value_keynames\"].append(v_key)\n            except ValueError:\n                if not v_key in tablename_dict[values[\"type\"]][\"blacklist\"] and not v_key in tablename_dict[values[\"type\"]][\"index_keynames\"]:\n                    tablename_dict[values[\"type\"]][\"blacklist\"].append(v_key)\n    # output part\n    for tablekey, metainfo in tablename_dict.items():\n        #print metainfo[\"tablename\"]\n        #print metainfo[\"headers\"]\n        logging.debug(\"Type: \", metainfo[\"tablename\"])\n        meta = {\n            \"ts_keyname\" : metainfo[\"ts_keyname\"],\n            \"blacklist\" : metainfo[\"blacklist\"],\n            \"headers\" : metainfo[\"headers\"],\n            \"delimiter\" : metainfo[\"delimiter\"],\n            \"value_keynames\" : metainfo[\"value_keynames\"],\n            \"index_keynames\" : metainfo[\"index_keynames\"],\n        }\n        logging.debug(meta)\n        for key, series in metainfo[\"values\"].items():\n            logging.debug(series.values())\n            # is some servers are down, this assertion is a mismatch\n            #try:\n            #    assert  metainfo[\"headers\"] == series.keys()\n            #except AssertionError as exc:\n            #    logging.exception(exc)\n            #    logging.error(\"%s : metainfo[headers] : %s and series.keys() : %s, are not the same\", key, metainfo[\"headers\"], series.keys())\n        # dump data\n        save_data(raw_basedir, metainfo[\"tablename\"], metainfo[\"values\"])\n        # dump meta information\n        metafile = os.path.join(meta_basedir, \"%s.json\" % metainfo[\"tablename\"])\n        if not os.path.isfile(metafile):\n            logging.info(\"auto generating datalogger table configuration to %s\", metafile)\n            json.dump(meta, open(metafile, \"wb\"))\n\nif __name__ == \"__main__\":\n    main()\n","label":1}
{"content":"#!\/usr\/bin\/env python\n# -*- coding: utf-8 -*-\nimport pytest\nimport torch\n\nfrom combustion.vision.filters import CLAHE\n\n\n@pytest.mark.parametrize(\"num_channels\", [1, 3])\n@pytest.mark.parametrize(\"dtype\", [\"uint8\", \"float\"])\ndef test_clahe(num_channels, cuda, dtype):\n    torch.random.manual_seed(42)\n\n    inputs = torch.rand(1, num_channels, 32, 64)\n    if dtype == \"byte\":\n        inputs = inputs.mul_(255).byte()\n\n    if cuda:\n        inputs = inputs.cuda()\n\n    xform = CLAHE(clipLimit=2.0, tileGridSize=(5, 5))\n    output = xform(inputs)\n\n    assert output.shape == inputs.shape\n    assert output.device == inputs.device\n    assert not torch.allclose(inputs, output)\n\n\ndef test_repr():\n    xform = CLAHE(clipLimit=2.0, tileGridSize=(5, 5))\n    print(xform)\n","label":1}
{"content":"# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nimport cv2\n\nimport mmcv\nimport numpy as np\nfrom mmcv.image import imwrite\nfrom mmcv.utils.misc import deprecated_api_warning\nfrom mmcv.visualization.image import imshow\n\nfrom mmpose.core import imshow_bboxes, imshow_keypoints\nfrom .. import builder\nfrom ..builder import POSENETS\nfrom .base import BasePose\n\ntry:\n    from mmcv.runner import auto_fp16\nexcept ImportError:\n    warnings.warn('auto_fp16 from mmpose will be deprecated from v0.15.0'\n                  'Please install mmcv>=1.1.4')\n    from mmpose.core import auto_fp16\n\n\n@POSENETS.register_module()\nclass TopDown(BasePose):\n    \"\"\"Top-down pose detectors.\n\n    Args:\n        backbone (dict): Backbone modules to extract feature.\n        keypoint_head (dict): Keypoint head to process feature.\n        train_cfg (dict): Config for training. Default: None.\n        test_cfg (dict): Config for testing. Default: None.\n        pretrained (str): Path to the pretrained models.\n        loss_pose (None): Deprecated arguments. Please use\n            `loss_keypoint` for heads instead.\n    \"\"\"\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 keypoint_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None,\n                 loss_pose=None):\n        super().__init__()\n        self.fp16_enabled = False\n\n        self.backbone = builder.build_backbone(backbone)\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        if neck is not None:\n            self.neck = builder.build_neck(neck)\n\n        if keypoint_head is not None:\n            keypoint_head['train_cfg'] = train_cfg\n            keypoint_head['test_cfg'] = test_cfg\n\n            if 'loss_keypoint' not in keypoint_head and loss_pose is not None:\n                warnings.warn(\n                    '`loss_pose` for TopDown is deprecated, '\n                    'use `loss_keypoint` for heads instead. See '\n                    'https:\/\/github.com\/open-mmlab\/mmpose\/pull\/382'\n                    ' for more information.', DeprecationWarning)\n                keypoint_head['loss_keypoint'] = loss_pose\n\n            self.keypoint_head = builder.build_head(keypoint_head)\n\n        self.init_weights(pretrained=pretrained)\n\n    @property\n    def with_neck(self):\n        \"\"\"Check if has keypoint_head.\"\"\"\n        return hasattr(self, 'neck')\n\n    @property\n    def with_keypoint(self):\n        \"\"\"Check if has keypoint_head.\"\"\"\n        return hasattr(self, 'keypoint_head')\n\n    def init_weights(self, pretrained=None):\n        \"\"\"Weight initialization for model.\"\"\"\n        self.backbone.init_weights(pretrained)\n        if self.with_neck:\n            self.neck.init_weights()\n        if self.with_keypoint:\n            self.keypoint_head.init_weights()\n\n    @auto_fp16(apply_to=('img', ))\n    def forward(self,\n                img,\n                target=None,\n                target_weight=None,\n                img_metas=None,\n                return_loss=True,\n                return_heatmap=False,\n                **kwargs):\n        \"\"\"Calls either forward_train or forward_test depending on whether\n        return_loss=True. Note this setting will change the expected inputs.\n        When `return_loss=True`, img and img_meta are single-nested (i.e.\n        Tensor and List[dict]), and when `resturn_loss=False`, img and img_meta\n        should be double nested (i.e.  List[Tensor], List[List[dict]]), with\n        the outer list indicating test time augmentations.\n\n        Note:\n            batch_size: N\n            num_keypoints: K\n            num_img_channel: C (Default: 3)\n            img height: imgH\n            img width: imgW\n            heatmaps height: H\n            heatmaps weight: W\n\n        Args:\n            img (torch.Tensor[NxCximgHximgW]): Input images.\n            target (torch.Tensor[NxKxHxW]): Target heatmaps.\n            target_weight (torch.Tensor[NxKx1]): Weights across\n                different joint types.\n            img_metas (list(dict)): Information about data augmentation\n                By default this includes:\n                - \"image_file: path to the image file\n                - \"center\": center of the bbox\n                - \"scale\": scale of the bbox\n                - \"rotation\": rotation of the bbox\n                - \"bbox_score\": score of bbox\n            return_loss (bool): Option to `return loss`. `return loss=True`\n                for training, `return loss=False` for validation & test.\n            return_heatmap (bool) : Option to return heatmap.\n\n        Returns:\n            dict|tuple: if `return loss` is true, then return losses.\n              Otherwise, return predicted poses, boxes, image paths\n                  and heatmaps.\n        \"\"\"\n        if return_loss:\n            return self.forward_train(img, target, target_weight, img_metas,\n                                      **kwargs)\n        return self.forward_test(\n            img, img_metas, return_heatmap=return_heatmap, **kwargs)\n\n    def forward_train(self, img, target, target_weight, img_metas, **kwargs):\n        \"\"\"Defines the computation performed at every call when training.\"\"\"\n        output = self.backbone(img)\n        if self.with_neck:\n            output = self.neck(output)\n        if self.with_keypoint:\n            output = self.keypoint_head(output)\n\n        # if return loss\n        losses = dict()\n        if self.with_keypoint:\n            keypoint_losses = self.keypoint_head.get_loss(\n                output, target, target_weight)\n            losses.update(keypoint_losses)\n            keypoint_accuracy = self.keypoint_head.get_accuracy(\n                output, target, target_weight)\n            losses.update(keypoint_accuracy)\n\n        return losses\n\n    def forward_test(self, img, img_metas, return_heatmap=False, **kwargs):\n        \"\"\"Defines the computation performed at every call when testing.\"\"\"\n        assert img.size(0) == len(img_metas)\n        batch_size, _, img_height, img_width = img.shape\n        if batch_size > 1:\n            assert 'bbox_id' in img_metas[0]\n\n        result = {}\n\n        features = self.backbone(img)\n        if self.with_neck:\n            features = self.neck(features)\n        if self.with_keypoint:\n            output_heatmap = self.keypoint_head.inference_model(\n                features, flip_pairs=None)\n\n        if self.test_cfg.get('flip_test', True):\n            img_flipped = img.flip(3)\n            features_flipped = self.backbone(img_flipped)\n            if self.with_neck:\n                features_flipped = self.neck(features_flipped)\n            if self.with_keypoint:\n                output_flipped_heatmap = self.keypoint_head.inference_model(\n                    features_flipped, img_metas[0]['flip_pairs'])\n                output_heatmap = (output_heatmap +\n                                  output_flipped_heatmap) * 0.5\n\n        if self.with_keypoint:\n            keypoint_result = self.keypoint_head.decode(\n                img_metas, output_heatmap, img_size=[img_width, img_height])\n            result.update(keypoint_result)\n\n            if not return_heatmap:\n                output_heatmap = None\n\n            result['output_heatmap'] = output_heatmap\n\n        return result\n\n    def forward_dummy(self, img):\n        \"\"\"Used for computing network FLOPs.\n\n        See ``tools\/get_flops.py``.\n\n        Args:\n            img (torch.Tensor): Input image.\n\n        Returns:\n            Tensor: Output heatmaps.\n        \"\"\"\n        output = self.backbone(img)\n        if self.with_neck:\n            output = self.neck(output)\n        if self.with_keypoint:\n            output = self.keypoint_head(output)\n        return output\n\n    @deprecated_api_warning({'pose_limb_color': 'pose_link_color'},\n                            cls_name='TopDown')\n    def show_result(self,\n                    method,\n                    img,\n                    result,\n                    skeleton=None,\n                    kpt_score_thr=0.3,\n                    bbox_color='green',\n                    pose_kpt_color=None,\n                    pose_link_color=None,\n                    text_color='white',\n                    radius=4,\n                    thickness=1,\n                    font_scale=0.5,\n                    bbox_thickness=1,\n                    win_name='',\n                    show=False,\n                    show_keypoint_weight=False,\n                    wait_time=0,\n                    out_file=None):\n        \"\"\"Draw `result` over `img`.\n\n        Args:\n            img (str or Tensor): The image to be displayed.\n            result (list[dict]): The results to draw over `img`\n                (bbox_result, pose_result).\n            skeleton (list[list]): The connection of keypoints.\n                skeleton is 0-based indexing.\n            kpt_score_thr (float, optional): Minimum score of keypoints\n                to be shown. Default: 0.3.\n            bbox_color (str or tuple or :obj:`Color`): Color of bbox lines.\n            pose_kpt_color (np.array[Nx3]`): Color of N keypoints.\n                If None, do not draw keypoints.\n            pose_link_color (np.array[Mx3]): Color of M links.\n                If None, do not draw links.\n            text_color (str or tuple or :obj:`Color`): Color of texts.\n            radius (int): Radius of circles.\n            thickness (int): Thickness of lines.\n            font_scale (float): Font scales of texts.\n            win_name (str): The window name.\n            show (bool): Whether to show the image. Default: False.\n            show_keypoint_weight (bool): Whether to change the transparency\n                using the predicted confidence scores of keypoints.\n            wait_time (int): Value of waitKey param.\n                Default: 0.\n            out_file (str or None): The filename to write the image.\n                Default: None.\n\n        Returns:\n            Tensor: Visualized img, only if not `show` or `out_file`.\n        \"\"\"\n        print(f\"out_file: {out_file}\")\n        cam_no = out_file.split('\/')[-1].split('.')[0]\n        print(f\"cam no.: {cam_no}\")\n\n        img = mmcv.imread(img)\n        img = img.copy()\n\n        bbox_result = []\n        pose_result = []\n        for res in result:\n            bbox_result.append(res['bbox'])\n            pose_result.append(res['keypoints'])\n\n        if len(bbox_result) > 0:\n            bboxes = np.vstack(bbox_result)\n            labels = None\n            if 'label' in result[0]:\n                labels = [res['label'] for res in result]\n            # draw bounding boxes\n            imshow_bboxes(\n                img,\n                bboxes,\n                labels=labels,\n                colors=bbox_color,\n                text_color=text_color,\n                thickness=bbox_thickness,\n                font_scale=font_scale,\n                show=False)\n\n            imshow_keypoints(img, pose_result, skeleton, kpt_score_thr,\n                             pose_kpt_color, pose_link_color, radius,\n                             thickness)\n\n        cv2.putText(img, f\"{method}\", (30,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2, cv2.LINE_AA)\n        cv2.putText(img, f\"{cam_no}\", (30,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2, cv2.LINE_AA)\n        cv2.putText(img, f\"kp_thld:{kpt_score_thr}\", (30,150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2, cv2.LINE_AA)\n        \n        if show:\n            imshow(img, win_name, wait_time)\n\n        if out_file is not None:\n            if out_file.endswith('.jpg'):\n                imwrite(img, out_file)\n            elif out_file.endswith('.mp4'):   \n                print(\"We do not save file over here.\")\n\n        return img\n","label":1}
{"content":"\"\"\"Base class for DTA TA records\"\"\"\n\nfrom itertools import chain\nfrom typing import Tuple\n\nfrom swissdta.records.common import ValidationLogMixin\nfrom swissdta.records.header import DTAHeader\n\n\nclass DTARecord(ValidationLogMixin):\n    \"\"\"Base class for DTA TA records.\n\n    This class should not be instantiated directly but subclassed\n    instead. It automatically generates a empty header required for all\n    types of records.\n\n    The constructor (of this class and its children) should not accept\n    record values. All fields should be set after initialization and\n    all field attributes must use a subclass of `dta.fields.Field`.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.header = DTAHeader()\n\n    @property\n    def validation_warnings(self) -> Tuple[str, ...]:\n        \"\"\"~ValidationLog.validation_warnings\"\"\"\n        return tuple(warning for warning in chain(self.header.validation_warnings, super().validation_warnings))\n\n    @property\n    def validation_errors(self) -> Tuple[str, ...]:\n        \"\"\"~ValidationLog.validation_errors\"\"\"\n        return tuple(error for error in chain(self.header.validation_errors, super().validation_errors))\n\n    def has_warnings(self) -> bool:\n        \"\"\"~ValidationLog.has_warnings\"\"\"\n        return self.header.has_warnings() or super().has_warnings()\n\n    def has_errors(self) -> bool:\n        \"\"\"~ValidationLog.has_errors\"\"\"\n        return self.header.has_errors() or super().has_errors()\n\n    def validate(self) -> None:\n        \"\"\"Triggers the validation of the record.\n\n        This validate the data in the record according to the\n        validation defined in the `DTA Standards and Formats`_.\n\n        Warnings and errors are then exposed through the\n        ``validation_warnings`` and ``validation_errors`` properties.\n        The ``has_warnings`` and ``has_errors`` properties should\n        be used to test for the presence of warnings or errors.\n\n        .. _DTA Standards and Formats:\n            https:\/\/www.six-interbank-clearing.com\/dam\/downloads\/en\/standardization\/dta\/dta.pdf\n        \"\"\"\n        self.header.validate()\n","label":1}
{"content":"# python\n# This file is generated by a program (mib2py). Any edits will be lost.\n\nfrom pycopia.aid import Enum\nimport pycopia.SMI.Basetypes\nRange = pycopia.SMI.Basetypes.Range\nRanges = pycopia.SMI.Basetypes.Ranges\n\nfrom pycopia.SMI.Objects import ColumnObject, MacroObject, NotificationObject, RowObject, ScalarObject, NodeObject, ModuleObject, GroupObject\n\n# imports \nfrom SNMPv2_SMI import Counter32, Counter64, Integer32, Gauge32, TimeTicks, OBJECT_TYPE, MODULE_IDENTITY, NOTIFICATION_TYPE, mib_2\nfrom SNMPv2_CONF import OBJECT_GROUP, MODULE_COMPLIANCE\nfrom SNMPv2_TC import TimeStamp, DisplayString, MacAddress, TEXTUAL_CONVENTION, RowStatus, TestAndIncr\nfrom IF_MIB import OwnerString\n\nclass SNMP_REPEATER_MIB(ModuleObject):\n\tpath = '\/usr\/share\/mibs\/ietf\/SNMP-REPEATER-MIB'\n\tconformance = 3\n\tname = 'SNMP-REPEATER-MIB'\n\tlanguage = 2\n\tdescription = \"Management information for 802.3 repeaters.\\n\\nThe following references are used throughout\\nthis MIB module:\\n\\n[IEEE 802.3 Std]\\n    refers to IEEE 802.3\/ISO 8802-3 Information\\n    processing systems - Local area networks -\\n    Part 3: Carrier sense multiple access with\\n    collision detection (CSMA\/CD) access method\\n    and physical layer specifications (1993).\\n\\n[IEEE 802.3 Mgt]\\n    refers to IEEE 802.3u-1995, '10 Mb\/s &\\n    100 Mb\/s Management, Section 30,'\\n    Supplement to ANSI\/IEEE 802.3.\\n\\nThe following terms are used throughout this\\nMIB module.  For complete formal definitions,\\nthe IEEE 802.3 standards should be consulted\\nwherever possible:\\n\\nSystem - A managed entity compliant with this\\nMIB, and incorporating at least one managed\\n802.3 repeater.\\n\\nChassis - An enclosure for one managed repeater,\\npart of a managed repeater, or several managed\\nrepeaters.  It typically contains an integral\\npower supply and a variable number of available\\nmodule slots.\\n\\nRepeater-unit - The portion of the repeater set\\nthat is inboard of the physical media interfaces.\\nThe physical media interfaces (MAUs, AUIs) may be\\nphysically separated from the repeater-unit, or\\nthey may be integrated into the same physical\\npackage.\\n\\nTrivial repeater-unit - An isolated port that can\\ngather statistics.\\n\\nGroup - A recommended, but optional, entity\\ndefined by the IEEE 802.3 management standard,\\nin order to support a modular numbering scheme.\\nThe classical example allows an implementor to\\nrepresent field-replaceable units as groups of\\nports, with the port numbering matching the\\nmodular hardware implementation.\\n\\nSystem interconnect segment - An internal\\nsegment allowing interconnection of ports\\nbelonging to different physical entities\\ninto the same logical manageable repeater.\\nExamples of implementation might be\\nbackplane busses in modular hubs, or\\nchaining cables in stacks of hubs.\\nStack - A scalable system that may include\\nmanaged repeaters, in which modularity is\\nachieved by interconnecting a number of\\ndifferent chassis.\\n\\nModule - A building block in a modular\\nchassis.  It typically maps into one 'slot';\\nhowever, the range of configurations may be\\nvery large, with several modules entering\\none slot, or one module covering several\\nslots.\"\n\n# nodes\nclass snmpDot3RptrMgt(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22])\n\tname = 'snmpDot3RptrMgt'\n\nclass rptrBasicPackage(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1])\n\tname = 'rptrBasicPackage'\n\nclass rptrRptrInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 1])\n\tname = 'rptrRptrInfo'\n\nclass rptrGroupInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 2])\n\tname = 'rptrGroupInfo'\n\nclass rptrPortInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 3])\n\tname = 'rptrPortInfo'\n\nclass rptrAllRptrInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 4])\n\tname = 'rptrAllRptrInfo'\n\nclass rptrMonitorPackage(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2])\n\tname = 'rptrMonitorPackage'\n\nclass rptrMonitorRptrInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 1])\n\tname = 'rptrMonitorRptrInfo'\n\nclass rptrMonitorGroupInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 2])\n\tname = 'rptrMonitorGroupInfo'\n\nclass rptrMonitorPortInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3])\n\tname = 'rptrMonitorPortInfo'\n\nclass rptrMonitorAllRptrInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 4])\n\tname = 'rptrMonitorAllRptrInfo'\n\nclass rptrAddrTrackPackage(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3])\n\tname = 'rptrAddrTrackPackage'\n\nclass rptrAddrTrackRptrInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 1])\n\tname = 'rptrAddrTrackRptrInfo'\n\nclass rptrAddrTrackGroupInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 2])\n\tname = 'rptrAddrTrackGroupInfo'\n\nclass rptrAddrTrackPortInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3])\n\tname = 'rptrAddrTrackPortInfo'\n\nclass rptrTopNPackage(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4])\n\tname = 'rptrTopNPackage'\n\nclass rptrTopNRptrInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 1])\n\tname = 'rptrTopNRptrInfo'\n\nclass rptrTopNGroupInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 2])\n\tname = 'rptrTopNGroupInfo'\n\nclass rptrTopNPortInfo(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3])\n\tname = 'rptrTopNPortInfo'\n\nclass snmpRptrMod(NodeObject):\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5])\n\tname = 'snmpRptrMod'\n\nclass snmpRptrModConf(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1])\n\tname = 'snmpRptrModConf'\n\nclass snmpRptrModCompls(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 1])\n\tname = 'snmpRptrModCompls'\n\nclass snmpRptrModObjGrps(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2])\n\tname = 'snmpRptrModObjGrps'\n\nclass snmpRptrModNotGrps(NodeObject):\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 3])\n\tname = 'snmpRptrModNotGrps'\n\n\n# macros\n# types \n\nclass OptMacAddr(pycopia.SMI.Basetypes.OctetString):\n\tstatus = 1\n\tranges = Ranges(Range(0, 0), Range(6, 6))\n\tformat = '1x:'\n\n# scalars \nclass rptrGroupCapacity(ScalarObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrOperStatus(ScalarObject):\n\tstatus = 2\n\taccess = 4\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'other'), Enum(2, 'ok'), Enum(3, 'rptrFailure'), Enum(4, 'groupFailure'), Enum(5, 'portFailure'), Enum(6, 'generalFailure')]\n\n\nclass rptrHealthText(ScalarObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.DisplayString\n\n\nclass rptrReset(ScalarObject):\n\tstatus = 2\n\taccess = 5\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'noReset'), Enum(2, 'reset')]\n\n\nclass rptrNonDisruptTest(ScalarObject):\n\tstatus = 2\n\taccess = 5\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 1, 5])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'noSelfTest'), Enum(2, 'selfTest')]\n\n\nclass rptrTotalPartitionedPorts(ScalarObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 1, 6])\n\tsyntaxobject = pycopia.SMI.Basetypes.Gauge32\n\n\nclass rptrMonitorTransmitCollisions(ScalarObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\n# columns\nclass rptrGroupIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 2, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrGroupDescr(ColumnObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 2, 1, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.DisplayString\n\n\nclass rptrGroupObjectID(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 2, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.ObjectIdentifier\n\n\nclass rptrGroupOperStatus(ColumnObject):\n\tstatus = 1\n\taccess = 4\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 2, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'other'), Enum(2, 'operational'), Enum(3, 'malfunctioning'), Enum(4, 'notPresent'), Enum(5, 'underTest'), Enum(6, 'resetInProgress')]\n\n\nclass rptrGroupLastOperStatusChange(ColumnObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 2, 1, 1, 5])\n\tsyntaxobject = pycopia.SMI.Basetypes.TimeTicks\n\n\nclass rptrGroupPortCapacity(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 2, 1, 1, 6])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrPortGroupIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 3, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrPortIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 3, 1, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrPortAdminStatus(ColumnObject):\n\tstatus = 1\n\taccess = 5\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 3, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'enabled'), Enum(2, 'disabled')]\n\n\nclass rptrPortAutoPartitionState(ColumnObject):\n\tstatus = 1\n\taccess = 4\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 3, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'notAutoPartitioned'), Enum(2, 'autoPartitioned')]\n\n\nclass rptrPortOperStatus(ColumnObject):\n\tstatus = 1\n\taccess = 4\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 3, 1, 1, 5])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'operational'), Enum(2, 'notOperational'), Enum(3, 'notPresent')]\n\n\nclass rptrPortRptrId(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 3, 1, 1, 6])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrInfoId(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 4, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrInfoRptrType(ColumnObject):\n\tstatus = 1\n\taccess = 4\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 4, 1, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'other'), Enum(2, 'tenMb'), Enum(3, 'onehundredMbClassI'), Enum(4, 'onehundredMbClassII')]\n\n\nclass rptrInfoOperStatus(ColumnObject):\n\tstatus = 1\n\taccess = 4\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 4, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'other'), Enum(2, 'ok'), Enum(3, 'failure')]\n\n\nclass rptrInfoReset(ColumnObject):\n\tstatus = 1\n\taccess = 5\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 4, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'noReset'), Enum(2, 'reset')]\n\n\nclass rptrInfoPartitionedPorts(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 4, 1, 1, 5])\n\tsyntaxobject = pycopia.SMI.Basetypes.Gauge32\n\n\nclass rptrInfoLastChange(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 4, 1, 1, 6])\n\tsyntaxobject = pycopia.SMI.Basetypes.TimeStamp\n\n\nclass rptrMonitorGroupIndex(ColumnObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 2, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrMonitorGroupTotalFrames(ColumnObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 2, 1, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorGroupTotalOctets(ColumnObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 2, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorGroupTotalErrors(ColumnObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 2, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortGroupIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrMonitorPortIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrMonitorPortReadableFrames(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortReadableOctets(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortFCSErrors(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 5])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortAlignmentErrors(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 6])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortFrameTooLongs(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 7])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortShortEvents(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 8])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortRunts(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 9])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortCollisions(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 10])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortLateEvents(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 11])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortVeryLongEvents(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 12])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortDataRateMismatches(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 13])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortAutoPartitions(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 14])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortTotalErrors(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 15])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortLastChange(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1, 16])\n\tsyntaxobject = pycopia.SMI.Basetypes.TimeStamp\n\n\nclass rptrMonitorPortIsolates(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 2, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortSymbolErrors(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 2, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortUpper32Octets(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 2, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonitorPortHCReadableOctets(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 2, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter64\n\n\nclass rptrMonTxCollisions(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 4, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonTotalFrames(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 4, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonTotalErrors(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 4, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonTotalOctets(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 4, 1, 1, 5])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonUpper32TotalOctets(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 4, 2, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrMonHCTotalOctets(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 4, 2, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter64\n\n\nclass rptrAddrSearchLock(ColumnObject):\n\taccess = 5\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 1, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.TestAndIncr\n\n\nclass rptrAddrSearchStatus(ColumnObject):\n\tstatus = 1\n\taccess = 5\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 1, 1, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'notInUse'), Enum(2, 'inUse')]\n\n\nclass rptrAddrSearchAddress(ColumnObject):\n\taccess = 5\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 1, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.MacAddress\n\n\nclass rptrAddrSearchState(ColumnObject):\n\tstatus = 1\n\taccess = 4\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 1, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'none'), Enum(2, 'single'), Enum(3, 'multiple')]\n\n\nclass rptrAddrSearchGroup(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 1, 1, 1, 5])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrAddrSearchPort(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 1, 1, 1, 6])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrAddrSearchOwner(ColumnObject):\n\taccess = 5\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 1, 1, 1, 7])\n\tsyntaxobject = OwnerString\n\n\nclass rptrAddrTrackGroupIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrAddrTrackPortIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 1, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrAddrTrackLastSourceAddress(ColumnObject):\n\taccess = 4\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.MacAddress\n\n\nclass rptrAddrTrackSourceAddrChanges(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Counter32\n\n\nclass rptrAddrTrackNewLastSrcAddress(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 1, 1, 5])\n\tsyntaxobject = OptMacAddr\n\n\nclass rptrAddrTrackCapacity(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 1, 1, 6])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrExtAddrTrackMacIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 2, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrExtAddrTrackSourceAddress(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 2, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.MacAddress\n\n\nclass rptrTopNPortControlIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrTopNPortRepeaterId(ColumnObject):\n\taccess = 5\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrTopNPortRateBase(ColumnObject):\n\tstatus = 1\n\taccess = 5\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.Enumeration\n\tenumerations = [Enum(1, 'readableFrames'), Enum(2, 'readableOctets'), Enum(3, 'fcsErrors'), Enum(4, 'alignmentErrors'), Enum(5, 'frameTooLongs'), Enum(6, 'shortEvents'), Enum(7, 'runts'), Enum(8, 'collisions'), Enum(9, 'lateEvents'), Enum(10, 'veryLongEvents'), Enum(11, 'dataRateMismatches'), Enum(12, 'autoPartitions'), Enum(13, 'totalErrors'), Enum(14, 'isolates'), Enum(15, 'symbolErrors')]\n\n\nclass rptrTopNPortTimeRemaining(ColumnObject):\n\taccess = 5\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrTopNPortDuration(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 5])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrTopNPortRequestedSize(ColumnObject):\n\taccess = 5\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 6])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrTopNPortGrantedSize(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 7])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrTopNPortStartTime(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 8])\n\tsyntaxobject = pycopia.SMI.Basetypes.TimeStamp\n\n\nclass rptrTopNPortOwner(ColumnObject):\n\taccess = 5\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 9])\n\tsyntaxobject = OwnerString\n\n\nclass rptrTopNPortRowStatus(ColumnObject):\n\taccess = 5\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1, 10])\n\tsyntaxobject = pycopia.SMI.Basetypes.RowStatus\n\n\nclass rptrTopNPortIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 2, 1, 1])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrTopNPortGroupIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 2, 1, 2])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrTopNPortPortIndex(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 2, 1, 3])\n\tsyntaxobject = pycopia.SMI.Basetypes.Integer32\n\n\nclass rptrTopNPortRate(ColumnObject):\n\taccess = 4\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 2, 1, 4])\n\tsyntaxobject = pycopia.SMI.Basetypes.Gauge32\n\n\n# rows \nclass rptrGroupEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrGroupIndex], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 2, 1, 1])\n\taccess = 2\n\tcolumns = {'rptrGroupIndex': rptrGroupIndex, 'rptrGroupDescr': rptrGroupDescr, 'rptrGroupObjectID': rptrGroupObjectID, 'rptrGroupOperStatus': rptrGroupOperStatus, 'rptrGroupLastOperStatusChange': rptrGroupLastOperStatusChange, 'rptrGroupPortCapacity': rptrGroupPortCapacity}\n\n\nclass rptrPortEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrPortGroupIndex, rptrPortIndex], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 3, 1, 1])\n\taccess = 2\n\tcolumns = {'rptrPortGroupIndex': rptrPortGroupIndex, 'rptrPortIndex': rptrPortIndex, 'rptrPortAdminStatus': rptrPortAdminStatus, 'rptrPortAutoPartitionState': rptrPortAutoPartitionState, 'rptrPortOperStatus': rptrPortOperStatus, 'rptrPortRptrId': rptrPortRptrId}\n\n\nclass rptrInfoEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrInfoId], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 1, 4, 1, 1])\n\taccess = 2\n\tcolumns = {'rptrInfoId': rptrInfoId, 'rptrInfoRptrType': rptrInfoRptrType, 'rptrInfoOperStatus': rptrInfoOperStatus, 'rptrInfoReset': rptrInfoReset, 'rptrInfoPartitionedPorts': rptrInfoPartitionedPorts, 'rptrInfoLastChange': rptrInfoLastChange}\n\n\nclass rptrMonitorGroupEntry(RowObject):\n\tstatus = 2\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrMonitorGroupIndex], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 2, 1, 1])\n\taccess = 2\n\tcolumns = {'rptrMonitorGroupIndex': rptrMonitorGroupIndex, 'rptrMonitorGroupTotalFrames': rptrMonitorGroupTotalFrames, 'rptrMonitorGroupTotalOctets': rptrMonitorGroupTotalOctets, 'rptrMonitorGroupTotalErrors': rptrMonitorGroupTotalErrors}\n\n\nclass rptrMonitorPortEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrMonitorPortGroupIndex, rptrMonitorPortIndex], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 1, 1])\n\taccess = 2\n\tcolumns = {'rptrMonitorPortGroupIndex': rptrMonitorPortGroupIndex, 'rptrMonitorPortIndex': rptrMonitorPortIndex, 'rptrMonitorPortReadableFrames': rptrMonitorPortReadableFrames, 'rptrMonitorPortReadableOctets': rptrMonitorPortReadableOctets, 'rptrMonitorPortFCSErrors': rptrMonitorPortFCSErrors, 'rptrMonitorPortAlignmentErrors': rptrMonitorPortAlignmentErrors, 'rptrMonitorPortFrameTooLongs': rptrMonitorPortFrameTooLongs, 'rptrMonitorPortShortEvents': rptrMonitorPortShortEvents, 'rptrMonitorPortRunts': rptrMonitorPortRunts, 'rptrMonitorPortCollisions': rptrMonitorPortCollisions, 'rptrMonitorPortLateEvents': rptrMonitorPortLateEvents, 'rptrMonitorPortVeryLongEvents': rptrMonitorPortVeryLongEvents, 'rptrMonitorPortDataRateMismatches': rptrMonitorPortDataRateMismatches, 'rptrMonitorPortAutoPartitions': rptrMonitorPortAutoPartitions, 'rptrMonitorPortTotalErrors': rptrMonitorPortTotalErrors, 'rptrMonitorPortLastChange': rptrMonitorPortLastChange}\n\n\nclass rptrMonitor100PortEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrMonitorPortGroupIndex, rptrMonitorPortIndex], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 3, 2, 1])\n\taccess = 2\n\tcolumns = {'rptrMonitorPortIsolates': rptrMonitorPortIsolates, 'rptrMonitorPortSymbolErrors': rptrMonitorPortSymbolErrors, 'rptrMonitorPortUpper32Octets': rptrMonitorPortUpper32Octets, 'rptrMonitorPortHCReadableOctets': rptrMonitorPortHCReadableOctets}\n\n\nclass rptrMonEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrInfoId], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 4, 1, 1])\n\taccess = 2\n\tcolumns = {'rptrMonTxCollisions': rptrMonTxCollisions, 'rptrMonTotalFrames': rptrMonTotalFrames, 'rptrMonTotalErrors': rptrMonTotalErrors, 'rptrMonTotalOctets': rptrMonTotalOctets}\n\n\nclass rptrMon100Entry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrInfoId], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 2, 4, 2, 1])\n\taccess = 2\n\tcolumns = {'rptrMonUpper32TotalOctets': rptrMonUpper32TotalOctets, 'rptrMonHCTotalOctets': rptrMonHCTotalOctets}\n\n\nclass rptrAddrSearchEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrInfoId], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 1, 1, 1])\n\taccess = 2\n\tcolumns = {'rptrAddrSearchLock': rptrAddrSearchLock, 'rptrAddrSearchStatus': rptrAddrSearchStatus, 'rptrAddrSearchAddress': rptrAddrSearchAddress, 'rptrAddrSearchState': rptrAddrSearchState, 'rptrAddrSearchGroup': rptrAddrSearchGroup, 'rptrAddrSearchPort': rptrAddrSearchPort, 'rptrAddrSearchOwner': rptrAddrSearchOwner}\n\n\nclass rptrAddrTrackEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrAddrTrackGroupIndex, rptrAddrTrackPortIndex], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 1, 1])\n\taccess = 2\n\tcolumns = {'rptrAddrTrackGroupIndex': rptrAddrTrackGroupIndex, 'rptrAddrTrackPortIndex': rptrAddrTrackPortIndex, 'rptrAddrTrackLastSourceAddress': rptrAddrTrackLastSourceAddress, 'rptrAddrTrackSourceAddrChanges': rptrAddrTrackSourceAddrChanges, 'rptrAddrTrackNewLastSrcAddress': rptrAddrTrackNewLastSrcAddress, 'rptrAddrTrackCapacity': rptrAddrTrackCapacity}\n\n\nclass rptrExtAddrTrackEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrAddrTrackGroupIndex, rptrAddrTrackPortIndex, rptrExtAddrTrackMacIndex], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 3, 3, 2, 1])\n\taccess = 2\n\tcolumns = {'rptrExtAddrTrackMacIndex': rptrExtAddrTrackMacIndex, 'rptrExtAddrTrackSourceAddress': rptrExtAddrTrackSourceAddress}\n\n\nclass rptrTopNPortControlEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrTopNPortControlIndex], False)\n\tcreate = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 1, 1])\n\taccess = 2\n\trowstatus = rptrTopNPortRowStatus\n\tcolumns = {'rptrTopNPortControlIndex': rptrTopNPortControlIndex, 'rptrTopNPortRepeaterId': rptrTopNPortRepeaterId, 'rptrTopNPortRateBase': rptrTopNPortRateBase, 'rptrTopNPortTimeRemaining': rptrTopNPortTimeRemaining, 'rptrTopNPortDuration': rptrTopNPortDuration, 'rptrTopNPortRequestedSize': rptrTopNPortRequestedSize, 'rptrTopNPortGrantedSize': rptrTopNPortGrantedSize, 'rptrTopNPortStartTime': rptrTopNPortStartTime, 'rptrTopNPortOwner': rptrTopNPortOwner, 'rptrTopNPortRowStatus': rptrTopNPortRowStatus}\n\n\nclass rptrTopNPortEntry(RowObject):\n\tstatus = 1\n\tindex = pycopia.SMI.Objects.IndexObjects([rptrTopNPortControlIndex, rptrTopNPortIndex], False)\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 4, 3, 2, 1])\n\taccess = 2\n\tcolumns = {'rptrTopNPortIndex': rptrTopNPortIndex, 'rptrTopNPortGroupIndex': rptrTopNPortGroupIndex, 'rptrTopNPortPortIndex': rptrTopNPortPortIndex, 'rptrTopNPortRate': rptrTopNPortRate}\n\n\n# notifications (traps) \nclass rptrHealth(NotificationObject):\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 0, 1])\n\nclass rptrGroupChange(NotificationObject):\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 0, 2])\n\nclass rptrResetEvent(NotificationObject):\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 0, 3])\n\nclass rptrInfoHealth(NotificationObject):\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 0, 4])\n\nclass rptrInfoResetEvent(NotificationObject):\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 0, 5])\n\n# groups \nclass snmpRptrGrpBasic1516(GroupObject):\n\taccess = 2\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 1])\n\tgroup = [rptrGroupCapacity, rptrOperStatus, rptrHealthText, rptrReset, rptrNonDisruptTest, rptrTotalPartitionedPorts, rptrGroupIndex, rptrGroupDescr, rptrGroupObjectID, rptrGroupOperStatus, rptrGroupLastOperStatusChange, rptrGroupPortCapacity, rptrPortGroupIndex, rptrPortIndex, rptrPortAdminStatus, rptrPortAutoPartitionState, rptrPortOperStatus]\n\nclass snmpRptrGrpMonitor1516(GroupObject):\n\taccess = 2\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 2])\n\tgroup = [rptrMonitorTransmitCollisions, rptrMonitorGroupIndex, rptrMonitorGroupTotalFrames, rptrMonitorGroupTotalOctets, rptrMonitorGroupTotalErrors, rptrMonitorPortGroupIndex, rptrMonitorPortIndex, rptrMonitorPortReadableFrames, rptrMonitorPortReadableOctets, rptrMonitorPortFCSErrors, rptrMonitorPortAlignmentErrors, rptrMonitorPortFrameTooLongs, rptrMonitorPortShortEvents, rptrMonitorPortRunts, rptrMonitorPortCollisions, rptrMonitorPortLateEvents, rptrMonitorPortVeryLongEvents, rptrMonitorPortDataRateMismatches, rptrMonitorPortAutoPartitions, rptrMonitorPortTotalErrors]\n\nclass snmpRptrGrpAddrTrack1516(GroupObject):\n\taccess = 2\n\tstatus = 2\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 4])\n\tgroup = [rptrAddrTrackGroupIndex, rptrAddrTrackPortIndex, rptrAddrTrackLastSourceAddress, rptrAddrTrackSourceAddrChanges, rptrAddrTrackNewLastSrcAddress]\n\nclass snmpRptrGrpBasic(GroupObject):\n\taccess = 2\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 5])\n\tgroup = [rptrGroupIndex, rptrGroupObjectID, rptrGroupOperStatus, rptrGroupPortCapacity, rptrPortGroupIndex, rptrPortIndex, rptrPortAdminStatus, rptrPortAutoPartitionState, rptrPortOperStatus, rptrPortRptrId, rptrInfoId, rptrInfoRptrType, rptrInfoOperStatus, rptrInfoReset, rptrInfoPartitionedPorts, rptrInfoLastChange]\n\nclass snmpRptrGrpMonitor(GroupObject):\n\taccess = 2\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 6])\n\tgroup = [rptrMonitorPortGroupIndex, rptrMonitorPortIndex, rptrMonitorPortReadableFrames, rptrMonitorPortReadableOctets, rptrMonitorPortFCSErrors, rptrMonitorPortAlignmentErrors, rptrMonitorPortFrameTooLongs, rptrMonitorPortShortEvents, rptrMonitorPortRunts, rptrMonitorPortCollisions, rptrMonitorPortLateEvents, rptrMonitorPortVeryLongEvents, rptrMonitorPortDataRateMismatches, rptrMonitorPortAutoPartitions, rptrMonitorPortTotalErrors, rptrMonitorPortLastChange, rptrMonTxCollisions, rptrMonTotalFrames, rptrMonTotalErrors, rptrMonTotalOctets]\n\nclass snmpRptrGrpMonitor100(GroupObject):\n\taccess = 2\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 7])\n\tgroup = [rptrMonitorPortIsolates, rptrMonitorPortSymbolErrors, rptrMonitorPortUpper32Octets, rptrMonUpper32TotalOctets]\n\nclass snmpRptrGrpMonitor100w64(GroupObject):\n\taccess = 2\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 8])\n\tgroup = [rptrMonitorPortHCReadableOctets, rptrMonHCTotalOctets]\n\nclass snmpRptrGrpAddrTrack(GroupObject):\n\taccess = 2\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 9])\n\tgroup = [rptrAddrTrackGroupIndex, rptrAddrTrackPortIndex, rptrAddrTrackSourceAddrChanges, rptrAddrTrackNewLastSrcAddress, rptrAddrTrackCapacity]\n\nclass snmpRptrGrpExtAddrTrack(GroupObject):\n\taccess = 2\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 10])\n\tgroup = [rptrExtAddrTrackMacIndex, rptrExtAddrTrackSourceAddress]\n\nclass snmpRptrGrpRptrAddrSearch(GroupObject):\n\taccess = 2\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 11])\n\tgroup = [rptrAddrSearchLock, rptrAddrSearchStatus, rptrAddrSearchAddress, rptrAddrSearchState, rptrAddrSearchGroup, rptrAddrSearchPort, rptrAddrSearchOwner]\n\nclass snmpRptrGrpTopNPort(GroupObject):\n\taccess = 2\n\tstatus = 1\n\tOID = pycopia.SMI.Basetypes.ObjectIdentifier([1, 3, 6, 1, 2, 1, 22, 5, 1, 2, 12])\n\tgroup = [rptrTopNPortControlIndex, rptrTopNPortRepeaterId, rptrTopNPortRateBase, rptrTopNPortTimeRemaining, rptrTopNPortDuration, rptrTopNPortRequestedSize, rptrTopNPortGrantedSize, rptrTopNPortStartTime, rptrTopNPortOwner, rptrTopNPortRowStatus, rptrTopNPortIndex, rptrTopNPortGroupIndex, rptrTopNPortPortIndex, rptrTopNPortRate]\n\n# capabilities \n\n# special additions\n\n# Add to master OIDMAP.\nfrom pycopia import SMI\nSMI.update_oidmap(__name__)\n","label":1}
{"content":"\"\"\" Python Character Mapping Codec cp1250 generated from 'MAPPINGS\/VENDORS\/MICSFT\/WINDOWS\/CP1250.TXT' with gencodec.py.\n\n\"\"\"#\"\n\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    def encode(self,input,errors='strict'):\n        return codecs.charmap_encode(input,errors,encoding_table)\n\n    def decode(self,input,errors='strict'):\n        return codecs.charmap_decode(input,errors,decoding_table)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='cp1250',\n        encode=Codec().encode,\n        decode=Codec().decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n\n\n### Decoding Table\n\ndecoding_table = (\n    u'\\x00'     #  0x00 -> NULL\n    u'\\x01'     #  0x01 -> START OF HEADING\n    u'\\x02'     #  0x02 -> START OF TEXT\n    u'\\x03'     #  0x03 -> END OF TEXT\n    u'\\x04'     #  0x04 -> END OF TRANSMISSION\n    u'\\x05'     #  0x05 -> ENQUIRY\n    u'\\x06'     #  0x06 -> ACKNOWLEDGE\n    u'\\x07'     #  0x07 -> BELL\n    u'\\x08'     #  0x08 -> BACKSPACE\n    u'\\t'       #  0x09 -> HORIZONTAL TABULATION\n    u'\\n'       #  0x0A -> LINE FEED\n    u'\\x0b'     #  0x0B -> VERTICAL TABULATION\n    u'\\x0c'     #  0x0C -> FORM FEED\n    u'\\r'       #  0x0D -> CARRIAGE RETURN\n    u'\\x0e'     #  0x0E -> SHIFT OUT\n    u'\\x0f'     #  0x0F -> SHIFT IN\n    u'\\x10'     #  0x10 -> DATA LINK ESCAPE\n    u'\\x11'     #  0x11 -> DEVICE CONTROL ONE\n    u'\\x12'     #  0x12 -> DEVICE CONTROL TWO\n    u'\\x13'     #  0x13 -> DEVICE CONTROL THREE\n    u'\\x14'     #  0x14 -> DEVICE CONTROL FOUR\n    u'\\x15'     #  0x15 -> NEGATIVE ACKNOWLEDGE\n    u'\\x16'     #  0x16 -> SYNCHRONOUS IDLE\n    u'\\x17'     #  0x17 -> END OF TRANSMISSION BLOCK\n    u'\\x18'     #  0x18 -> CANCEL\n    u'\\x19'     #  0x19 -> END OF MEDIUM\n    u'\\x1a'     #  0x1A -> SUBSTITUTE\n    u'\\x1b'     #  0x1B -> ESCAPE\n    u'\\x1c'     #  0x1C -> FILE SEPARATOR\n    u'\\x1d'     #  0x1D -> GROUP SEPARATOR\n    u'\\x1e'     #  0x1E -> RECORD SEPARATOR\n    u'\\x1f'     #  0x1F -> UNIT SEPARATOR\n    u' '        #  0x20 -> SPACE\n    u'!'        #  0x21 -> EXCLAMATION MARK\n    u'\"'        #  0x22 -> QUOTATION MARK\n    u'#'        #  0x23 -> NUMBER SIGN\n    u'$'        #  0x24 -> DOLLAR SIGN\n    u'%'        #  0x25 -> PERCENT SIGN\n    u'&'        #  0x26 -> AMPERSAND\n    u\"'\"        #  0x27 -> APOSTROPHE\n    u'('        #  0x28 -> LEFT PARENTHESIS\n    u')'        #  0x29 -> RIGHT PARENTHESIS\n    u'*'        #  0x2A -> ASTERISK\n    u'+'        #  0x2B -> PLUS SIGN\n    u','        #  0x2C -> COMMA\n    u'-'        #  0x2D -> HYPHEN-MINUS\n    u'.'        #  0x2E -> FULL STOP\n    u'\/'        #  0x2F -> SOLIDUS\n    u'0'        #  0x30 -> DIGIT ZERO\n    u'1'        #  0x31 -> DIGIT ONE\n    u'2'        #  0x32 -> DIGIT TWO\n    u'3'        #  0x33 -> DIGIT THREE\n    u'4'        #  0x34 -> DIGIT FOUR\n    u'5'        #  0x35 -> DIGIT FIVE\n    u'6'        #  0x36 -> DIGIT SIX\n    u'7'        #  0x37 -> DIGIT SEVEN\n    u'8'        #  0x38 -> DIGIT EIGHT\n    u'9'        #  0x39 -> DIGIT NINE\n    u':'        #  0x3A -> COLON\n    u';'        #  0x3B -> SEMICOLON\n    u'<'        #  0x3C -> LESS-THAN SIGN\n    u'='        #  0x3D -> EQUALS SIGN\n    u'>'        #  0x3E -> GREATER-THAN SIGN\n    u'?'        #  0x3F -> QUESTION MARK\n    u'@'        #  0x40 -> COMMERCIAL AT\n    u'A'        #  0x41 -> LATIN CAPITAL LETTER A\n    u'B'        #  0x42 -> LATIN CAPITAL LETTER B\n    u'C'        #  0x43 -> LATIN CAPITAL LETTER C\n    u'D'        #  0x44 -> LATIN CAPITAL LETTER D\n    u'E'        #  0x45 -> LATIN CAPITAL LETTER E\n    u'F'        #  0x46 -> LATIN CAPITAL LETTER F\n    u'G'        #  0x47 -> LATIN CAPITAL LETTER G\n    u'H'        #  0x48 -> LATIN CAPITAL LETTER H\n    u'I'        #  0x49 -> LATIN CAPITAL LETTER I\n    u'J'        #  0x4A -> LATIN CAPITAL LETTER J\n    u'K'        #  0x4B -> LATIN CAPITAL LETTER K\n    u'L'        #  0x4C -> LATIN CAPITAL LETTER L\n    u'M'        #  0x4D -> LATIN CAPITAL LETTER M\n    u'N'        #  0x4E -> LATIN CAPITAL LETTER N\n    u'O'        #  0x4F -> LATIN CAPITAL LETTER O\n    u'P'        #  0x50 -> LATIN CAPITAL LETTER P\n    u'Q'        #  0x51 -> LATIN CAPITAL LETTER Q\n    u'R'        #  0x52 -> LATIN CAPITAL LETTER R\n    u'S'        #  0x53 -> LATIN CAPITAL LETTER S\n    u'T'        #  0x54 -> LATIN CAPITAL LETTER T\n    u'U'        #  0x55 -> LATIN CAPITAL LETTER U\n    u'V'        #  0x56 -> LATIN CAPITAL LETTER V\n    u'W'        #  0x57 -> LATIN CAPITAL LETTER W\n    u'X'        #  0x58 -> LATIN CAPITAL LETTER X\n    u'Y'        #  0x59 -> LATIN CAPITAL LETTER Y\n    u'Z'        #  0x5A -> LATIN CAPITAL LETTER Z\n    u'['        #  0x5B -> LEFT SQUARE BRACKET\n    u'\\\\'       #  0x5C -> REVERSE SOLIDUS\n    u']'        #  0x5D -> RIGHT SQUARE BRACKET\n    u'^'        #  0x5E -> CIRCUMFLEX ACCENT\n    u'_'        #  0x5F -> LOW LINE\n    u'`'        #  0x60 -> GRAVE ACCENT\n    u'a'        #  0x61 -> LATIN SMALL LETTER A\n    u'b'        #  0x62 -> LATIN SMALL LETTER B\n    u'c'        #  0x63 -> LATIN SMALL LETTER C\n    u'd'        #  0x64 -> LATIN SMALL LETTER D\n    u'e'        #  0x65 -> LATIN SMALL LETTER E\n    u'f'        #  0x66 -> LATIN SMALL LETTER F\n    u'g'        #  0x67 -> LATIN SMALL LETTER G\n    u'h'        #  0x68 -> LATIN SMALL LETTER H\n    u'i'        #  0x69 -> LATIN SMALL LETTER I\n    u'j'        #  0x6A -> LATIN SMALL LETTER J\n    u'k'        #  0x6B -> LATIN SMALL LETTER K\n    u'l'        #  0x6C -> LATIN SMALL LETTER L\n    u'm'        #  0x6D -> LATIN SMALL LETTER M\n    u'n'        #  0x6E -> LATIN SMALL LETTER N\n    u'o'        #  0x6F -> LATIN SMALL LETTER O\n    u'p'        #  0x70 -> LATIN SMALL LETTER P\n    u'q'        #  0x71 -> LATIN SMALL LETTER Q\n    u'r'        #  0x72 -> LATIN SMALL LETTER R\n    u's'        #  0x73 -> LATIN SMALL LETTER S\n    u't'        #  0x74 -> LATIN SMALL LETTER T\n    u'u'        #  0x75 -> LATIN SMALL LETTER U\n    u'v'        #  0x76 -> LATIN SMALL LETTER V\n    u'w'        #  0x77 -> LATIN SMALL LETTER W\n    u'x'        #  0x78 -> LATIN SMALL LETTER X\n    u'y'        #  0x79 -> LATIN SMALL LETTER Y\n    u'z'        #  0x7A -> LATIN SMALL LETTER Z\n    u'{'        #  0x7B -> LEFT CURLY BRACKET\n    u'|'        #  0x7C -> VERTICAL LINE\n    u'}'        #  0x7D -> RIGHT CURLY BRACKET\n    u'~'        #  0x7E -> TILDE\n    u'\\x7f'     #  0x7F -> DELETE\n    u'\\u20ac'   #  0x80 -> EURO SIGN\n    u'\\ufffe'   #  0x81 -> UNDEFINED\n    u'\\u201a'   #  0x82 -> SINGLE LOW-9 QUOTATION MARK\n    u'\\ufffe'   #  0x83 -> UNDEFINED\n    u'\\u201e'   #  0x84 -> DOUBLE LOW-9 QUOTATION MARK\n    u'\\u2026'   #  0x85 -> HORIZONTAL ELLIPSIS\n    u'\\u2020'   #  0x86 -> DAGGER\n    u'\\u2021'   #  0x87 -> DOUBLE DAGGER\n    u'\\ufffe'   #  0x88 -> UNDEFINED\n    u'\\u2030'   #  0x89 -> PER MILLE SIGN\n    u'\\u0160'   #  0x8A -> LATIN CAPITAL LETTER S WITH CARON\n    u'\\u2039'   #  0x8B -> SINGLE LEFT-POINTING ANGLE QUOTATION MARK\n    u'\\u015a'   #  0x8C -> LATIN CAPITAL LETTER S WITH ACUTE\n    u'\\u0164'   #  0x8D -> LATIN CAPITAL LETTER T WITH CARON\n    u'\\u017d'   #  0x8E -> LATIN CAPITAL LETTER Z WITH CARON\n    u'\\u0179'   #  0x8F -> LATIN CAPITAL LETTER Z WITH ACUTE\n    u'\\ufffe'   #  0x90 -> UNDEFINED\n    u'\\u2018'   #  0x91 -> LEFT SINGLE QUOTATION MARK\n    u'\\u2019'   #  0x92 -> RIGHT SINGLE QUOTATION MARK\n    u'\\u201c'   #  0x93 -> LEFT DOUBLE QUOTATION MARK\n    u'\\u201d'   #  0x94 -> RIGHT DOUBLE QUOTATION MARK\n    u'\\u2022'   #  0x95 -> BULLET\n    u'\\u2013'   #  0x96 -> EN DASH\n    u'\\u2014'   #  0x97 -> EM DASH\n    u'\\ufffe'   #  0x98 -> UNDEFINED\n    u'\\u2122'   #  0x99 -> TRADE MARK SIGN\n    u'\\u0161'   #  0x9A -> LATIN SMALL LETTER S WITH CARON\n    u'\\u203a'   #  0x9B -> SINGLE RIGHT-POINTING ANGLE QUOTATION MARK\n    u'\\u015b'   #  0x9C -> LATIN SMALL LETTER S WITH ACUTE\n    u'\\u0165'   #  0x9D -> LATIN SMALL LETTER T WITH CARON\n    u'\\u017e'   #  0x9E -> LATIN SMALL LETTER Z WITH CARON\n    u'\\u017a'   #  0x9F -> LATIN SMALL LETTER Z WITH ACUTE\n    u'\\xa0'     #  0xA0 -> NO-BREAK SPACE\n    u'\\u02c7'   #  0xA1 -> CARON\n    u'\\u02d8'   #  0xA2 -> BREVE\n    u'\\u0141'   #  0xA3 -> LATIN CAPITAL LETTER L WITH STROKE\n    u'\\xa4'     #  0xA4 -> CURRENCY SIGN\n    u'\\u0104'   #  0xA5 -> LATIN CAPITAL LETTER A WITH OGONEK\n    u'\\xa6'     #  0xA6 -> BROKEN BAR\n    u'\\xa7'     #  0xA7 -> SECTION SIGN\n    u'\\xa8'     #  0xA8 -> DIAERESIS\n    u'\\xa9'     #  0xA9 -> COPYRIGHT SIGN\n    u'\\u015e'   #  0xAA -> LATIN CAPITAL LETTER S WITH CEDILLA\n    u'\\xab'     #  0xAB -> LEFT-POINTING DOUBLE ANGLE QUOTATION MARK\n    u'\\xac'     #  0xAC -> NOT SIGN\n    u'\\xad'     #  0xAD -> SOFT HYPHEN\n    u'\\xae'     #  0xAE -> REGISTERED SIGN\n    u'\\u017b'   #  0xAF -> LATIN CAPITAL LETTER Z WITH DOT ABOVE\n    u'\\xb0'     #  0xB0 -> DEGREE SIGN\n    u'\\xb1'     #  0xB1 -> PLUS-MINUS SIGN\n    u'\\u02db'   #  0xB2 -> OGONEK\n    u'\\u0142'   #  0xB3 -> LATIN SMALL LETTER L WITH STROKE\n    u'\\xb4'     #  0xB4 -> ACUTE ACCENT\n    u'\\xb5'     #  0xB5 -> MICRO SIGN\n    u'\\xb6'     #  0xB6 -> PILCROW SIGN\n    u'\\xb7'     #  0xB7 -> MIDDLE DOT\n    u'\\xb8'     #  0xB8 -> CEDILLA\n    u'\\u0105'   #  0xB9 -> LATIN SMALL LETTER A WITH OGONEK\n    u'\\u015f'   #  0xBA -> LATIN SMALL LETTER S WITH CEDILLA\n    u'\\xbb'     #  0xBB -> RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK\n    u'\\u013d'   #  0xBC -> LATIN CAPITAL LETTER L WITH CARON\n    u'\\u02dd'   #  0xBD -> DOUBLE ACUTE ACCENT\n    u'\\u013e'   #  0xBE -> LATIN SMALL LETTER L WITH CARON\n    u'\\u017c'   #  0xBF -> LATIN SMALL LETTER Z WITH DOT ABOVE\n    u'\\u0154'   #  0xC0 -> LATIN CAPITAL LETTER R WITH ACUTE\n    u'\\xc1'     #  0xC1 -> LATIN CAPITAL LETTER A WITH ACUTE\n    u'\\xc2'     #  0xC2 -> LATIN CAPITAL LETTER A WITH CIRCUMFLEX\n    u'\\u0102'   #  0xC3 -> LATIN CAPITAL LETTER A WITH BREVE\n    u'\\xc4'     #  0xC4 -> LATIN CAPITAL LETTER A WITH DIAERESIS\n    u'\\u0139'   #  0xC5 -> LATIN CAPITAL LETTER L WITH ACUTE\n    u'\\u0106'   #  0xC6 -> LATIN CAPITAL LETTER C WITH ACUTE\n    u'\\xc7'     #  0xC7 -> LATIN CAPITAL LETTER C WITH CEDILLA\n    u'\\u010c'   #  0xC8 -> LATIN CAPITAL LETTER C WITH CARON\n    u'\\xc9'     #  0xC9 -> LATIN CAPITAL LETTER E WITH ACUTE\n    u'\\u0118'   #  0xCA -> LATIN CAPITAL LETTER E WITH OGONEK\n    u'\\xcb'     #  0xCB -> LATIN CAPITAL LETTER E WITH DIAERESIS\n    u'\\u011a'   #  0xCC -> LATIN CAPITAL LETTER E WITH CARON\n    u'\\xcd'     #  0xCD -> LATIN CAPITAL LETTER I WITH ACUTE\n    u'\\xce'     #  0xCE -> LATIN CAPITAL LETTER I WITH CIRCUMFLEX\n    u'\\u010e'   #  0xCF -> LATIN CAPITAL LETTER D WITH CARON\n    u'\\u0110'   #  0xD0 -> LATIN CAPITAL LETTER D WITH STROKE\n    u'\\u0143'   #  0xD1 -> LATIN CAPITAL LETTER N WITH ACUTE\n    u'\\u0147'   #  0xD2 -> LATIN CAPITAL LETTER N WITH CARON\n    u'\\xd3'     #  0xD3 -> LATIN CAPITAL LETTER O WITH ACUTE\n    u'\\xd4'     #  0xD4 -> LATIN CAPITAL LETTER O WITH CIRCUMFLEX\n    u'\\u0150'   #  0xD5 -> LATIN CAPITAL LETTER O WITH DOUBLE ACUTE\n    u'\\xd6'     #  0xD6 -> LATIN CAPITAL LETTER O WITH DIAERESIS\n    u'\\xd7'     #  0xD7 -> MULTIPLICATION SIGN\n    u'\\u0158'   #  0xD8 -> LATIN CAPITAL LETTER R WITH CARON\n    u'\\u016e'   #  0xD9 -> LATIN CAPITAL LETTER U WITH RING ABOVE\n    u'\\xda'     #  0xDA -> LATIN CAPITAL LETTER U WITH ACUTE\n    u'\\u0170'   #  0xDB -> LATIN CAPITAL LETTER U WITH DOUBLE ACUTE\n    u'\\xdc'     #  0xDC -> LATIN CAPITAL LETTER U WITH DIAERESIS\n    u'\\xdd'     #  0xDD -> LATIN CAPITAL LETTER Y WITH ACUTE\n    u'\\u0162'   #  0xDE -> LATIN CAPITAL LETTER T WITH CEDILLA\n    u'\\xdf'     #  0xDF -> LATIN SMALL LETTER SHARP S\n    u'\\u0155'   #  0xE0 -> LATIN SMALL LETTER R WITH ACUTE\n    u'\\xe1'     #  0xE1 -> LATIN SMALL LETTER A WITH ACUTE\n    u'\\xe2'     #  0xE2 -> LATIN SMALL LETTER A WITH CIRCUMFLEX\n    u'\\u0103'   #  0xE3 -> LATIN SMALL LETTER A WITH BREVE\n    u'\\xe4'     #  0xE4 -> LATIN SMALL LETTER A WITH DIAERESIS\n    u'\\u013a'   #  0xE5 -> LATIN SMALL LETTER L WITH ACUTE\n    u'\\u0107'   #  0xE6 -> LATIN SMALL LETTER C WITH ACUTE\n    u'\\xe7'     #  0xE7 -> LATIN SMALL LETTER C WITH CEDILLA\n    u'\\u010d'   #  0xE8 -> LATIN SMALL LETTER C WITH CARON\n    u'\\xe9'     #  0xE9 -> LATIN SMALL LETTER E WITH ACUTE\n    u'\\u0119'   #  0xEA -> LATIN SMALL LETTER E WITH OGONEK\n    u'\\xeb'     #  0xEB -> LATIN SMALL LETTER E WITH DIAERESIS\n    u'\\u011b'   #  0xEC -> LATIN SMALL LETTER E WITH CARON\n    u'\\xed'     #  0xED -> LATIN SMALL LETTER I WITH ACUTE\n    u'\\xee'     #  0xEE -> LATIN SMALL LETTER I WITH CIRCUMFLEX\n    u'\\u010f'   #  0xEF -> LATIN SMALL LETTER D WITH CARON\n    u'\\u0111'   #  0xF0 -> LATIN SMALL LETTER D WITH STROKE\n    u'\\u0144'   #  0xF1 -> LATIN SMALL LETTER N WITH ACUTE\n    u'\\u0148'   #  0xF2 -> LATIN SMALL LETTER N WITH CARON\n    u'\\xf3'     #  0xF3 -> LATIN SMALL LETTER O WITH ACUTE\n    u'\\xf4'     #  0xF4 -> LATIN SMALL LETTER O WITH CIRCUMFLEX\n    u'\\u0151'   #  0xF5 -> LATIN SMALL LETTER O WITH DOUBLE ACUTE\n    u'\\xf6'     #  0xF6 -> LATIN SMALL LETTER O WITH DIAERESIS\n    u'\\xf7'     #  0xF7 -> DIVISION SIGN\n    u'\\u0159'   #  0xF8 -> LATIN SMALL LETTER R WITH CARON\n    u'\\u016f'   #  0xF9 -> LATIN SMALL LETTER U WITH RING ABOVE\n    u'\\xfa'     #  0xFA -> LATIN SMALL LETTER U WITH ACUTE\n    u'\\u0171'   #  0xFB -> LATIN SMALL LETTER U WITH DOUBLE ACUTE\n    u'\\xfc'     #  0xFC -> LATIN SMALL LETTER U WITH DIAERESIS\n    u'\\xfd'     #  0xFD -> LATIN SMALL LETTER Y WITH ACUTE\n    u'\\u0163'   #  0xFE -> LATIN SMALL LETTER T WITH CEDILLA\n    u'\\u02d9'   #  0xFF -> DOT ABOVE\n)\n\n### Encoding table\nencoding_table=codecs.charmap_build(decoding_table)\n","label":0}
{"content":"#!\/usr\/bin\/python\n# encoding: utf-8 -*-\n\n# Copyright: (c) 2013, Matthias Vogelgesang <matthias.vogelgesang@gmail.com>\n# GNU General Public License v3.0+ (see COPYING or https:\/\/www.gnu.org\/licenses\/gpl-3.0.txt)\n\nfrom __future__ import absolute_import, division, print_function\n__metaclass__ = type\n\nANSIBLE_METADATA = {'metadata_version': '1.1',\n                    'status': ['preview'],\n                    'supported_by': 'community'}\n\nDOCUMENTATION = '''\n---\nmodule: kernel_blacklist\nauthor:\n- Matthias Vogelgesang (@matze)\nversion_added: '1.4'\nshort_description: Blacklist kernel modules\ndescription:\n    - Add or remove kernel modules from blacklist.\noptions:\n    name:\n        description:\n            - Name of kernel module to black- or whitelist.\n        required: true\n    state:\n        description:\n            - Whether the module should be present in the blacklist or absent.\n        choices: [ absent, present ]\n        default: present\n    blacklist_file:\n        description:\n            - If specified, use this blacklist file instead of\n              C(\/etc\/modprobe.d\/blacklist-ansible.conf).\n'''\n\nEXAMPLES = '''\n- name: Blacklist the nouveau driver module\n  kernel_blacklist:\n    name: nouveau\n    state: present\n'''\n\nimport os\nimport re\n\nfrom ansible.module_utils.basic import AnsibleModule\n\n\nclass Blacklist(object):\n    def __init__(self, module, filename, checkmode):\n        self.filename = filename\n        self.module = module\n        self.checkmode = checkmode\n\n    def create_file(self):\n        if not self.checkmode and not os.path.exists(self.filename):\n            open(self.filename, 'a').close()\n            return True\n        elif self.checkmode and not os.path.exists(self.filename):\n            self.filename = os.devnull\n            return True\n        else:\n            return False\n\n    def get_pattern(self):\n        return r'^blacklist\\s*' + self.module + '$'\n\n    def readlines(self):\n        f = open(self.filename, 'r')\n        lines = f.readlines()\n        f.close()\n        return lines\n\n    def module_listed(self):\n        lines = self.readlines()\n        pattern = self.get_pattern()\n\n        for line in lines:\n            stripped = line.strip()\n            if stripped.startswith('#'):\n                continue\n\n            if re.match(pattern, stripped):\n                return True\n\n        return False\n\n    def remove_module(self):\n        lines = self.readlines()\n        pattern = self.get_pattern()\n\n        if self.checkmode:\n            f = open(os.devnull, 'w')\n        else:\n            f = open(self.filename, 'w')\n\n        for line in lines:\n            if not re.match(pattern, line.strip()):\n                f.write(line)\n\n        f.close()\n\n    def add_module(self):\n        if self.checkmode:\n            f = open(os.devnull, 'a')\n        else:\n            f = open(self.filename, 'a')\n\n        f.write('blacklist %s\\n' % self.module)\n\n        f.close()\n\n\ndef main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            name=dict(type='str', required=True),\n            state=dict(type='str', default='present', choices=['absent', 'present']),\n            blacklist_file=dict(type='str')\n        ),\n        supports_check_mode=True,\n    )\n\n    args = dict(changed=False, failed=False,\n                name=module.params['name'], state=module.params['state'])\n\n    filename = '\/etc\/modprobe.d\/blacklist-ansible.conf'\n\n    if module.params['blacklist_file']:\n        filename = module.params['blacklist_file']\n\n    blacklist = Blacklist(args['name'], filename, module.check_mode)\n\n    if blacklist.create_file():\n        args['changed'] = True\n    else:\n        args['changed'] = False\n\n    if blacklist.module_listed():\n        if args['state'] == 'absent':\n            blacklist.remove_module()\n            args['changed'] = True\n    else:\n        if args['state'] == 'present':\n            blacklist.add_module()\n            args['changed'] = True\n\n    module.exit_json(**args)\n\n\nif __name__ == '__main__':\n    main()\n","label":0}
{"content":"# This file was created automatically by SWIG 1.3.29.\n# Don't modify this file, modify the SWIG interface instead.\n\nimport _controls_\nimport new\nnew_instancemethod = new.instancemethod\ndef _swig_setattr_nondynamic(self,class_type,name,value,static=1):\n    if (name == \"thisown\"): return self.this.own(value)\n    if (name == \"this\"):\n        if type(value).__name__ == 'PySwigObject':\n            self.__dict__[name] = value\n            return\n    method = class_type.__swig_setmethods__.get(name,None)\n    if method: return method(self,value)\n    if (not static) or hasattr(self,name):\n        self.__dict__[name] = value\n    else:\n        raise AttributeError(\"You cannot add attributes to %s\" % self)\n\ndef _swig_setattr(self,class_type,name,value):\n    return _swig_setattr_nondynamic(self,class_type,name,value,0)\n\ndef _swig_getattr(self,class_type,name):\n    if (name == \"thisown\"): return self.this.own()\n    method = class_type.__swig_getmethods__.get(name,None)\n    if method: return method(self)\n    raise AttributeError,name\n\ndef _swig_repr(self):\n    try: strthis = \"proxy of \" + self.this.__repr__()\n    except: strthis = \"\"\n    return \"<%s.%s; %s >\" % (self.__class__.__module__, self.__class__.__name__, strthis,)\n\nimport types\ntry:\n    _object = types.ObjectType\n    _newclass = 1\nexcept AttributeError:\n    class _object : pass\n    _newclass = 0\ndel types\n\n\ndef _swig_setattr_nondynamic_method(set):\n    def set_attr(self,name,value):\n        if (name == \"thisown\"): return self.this.own(value)\n        if hasattr(self,name) or (name == \"this\"):\n            set(self,name,value)\n        else:\n            raise AttributeError(\"You cannot add attributes to %s\" % self)\n    return set_attr\n\n\nimport _core\nwx = _core \n#---------------------------------------------------------------------------\n\nBU_LEFT = _controls_.BU_LEFT\nBU_TOP = _controls_.BU_TOP\nBU_RIGHT = _controls_.BU_RIGHT\nBU_BOTTOM = _controls_.BU_BOTTOM\nBU_ALIGN_MASK = _controls_.BU_ALIGN_MASK\nBU_EXACTFIT = _controls_.BU_EXACTFIT\nBU_AUTODRAW = _controls_.BU_AUTODRAW\nBU_NOTEXT = _controls_.BU_NOTEXT\nclass AnyButton(_core.Control):\n    \"\"\"Proxy of C++ AnyButton class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    def __init__(self): raise AttributeError, \"No constructor defined\"\n    __repr__ = _swig_repr\n    def SetBitmap(*args, **kwargs):\n        \"\"\"SetBitmap(self, Bitmap bitmap, int dir=LEFT)\"\"\"\n        return _controls_.AnyButton_SetBitmap(*args, **kwargs)\n\n    def GetBitmap(*args, **kwargs):\n        \"\"\"GetBitmap(self) -> Bitmap\"\"\"\n        return _controls_.AnyButton_GetBitmap(*args, **kwargs)\n\n    Bitmap = property(GetBitmap,SetBitmap) \n    def SetBitmapLabel(*args, **kwargs):\n        \"\"\"SetBitmapLabel(self, Bitmap bitmap)\"\"\"\n        return _controls_.AnyButton_SetBitmapLabel(*args, **kwargs)\n\n    def SetBitmapPressed(*args, **kwargs):\n        \"\"\"SetBitmapPressed(self, Bitmap bitmap)\"\"\"\n        return _controls_.AnyButton_SetBitmapPressed(*args, **kwargs)\n\n    def SetBitmapDisabled(*args, **kwargs):\n        \"\"\"SetBitmapDisabled(self, Bitmap bitmap)\"\"\"\n        return _controls_.AnyButton_SetBitmapDisabled(*args, **kwargs)\n\n    def SetBitmapCurrent(*args, **kwargs):\n        \"\"\"SetBitmapCurrent(self, Bitmap bitmap)\"\"\"\n        return _controls_.AnyButton_SetBitmapCurrent(*args, **kwargs)\n\n    def SetBitmapFocus(*args, **kwargs):\n        \"\"\"SetBitmapFocus(self, Bitmap bitmap)\"\"\"\n        return _controls_.AnyButton_SetBitmapFocus(*args, **kwargs)\n\n    def GetBitmapLabel(*args, **kwargs):\n        \"\"\"GetBitmapLabel(self) -> Bitmap\"\"\"\n        return _controls_.AnyButton_GetBitmapLabel(*args, **kwargs)\n\n    def GetBitmapPressed(*args, **kwargs):\n        \"\"\"GetBitmapPressed(self) -> Bitmap\"\"\"\n        return _controls_.AnyButton_GetBitmapPressed(*args, **kwargs)\n\n    def GetBitmapDisabled(*args, **kwargs):\n        \"\"\"GetBitmapDisabled(self) -> Bitmap\"\"\"\n        return _controls_.AnyButton_GetBitmapDisabled(*args, **kwargs)\n\n    def GetBitmapCurrent(*args, **kwargs):\n        \"\"\"GetBitmapCurrent(self) -> Bitmap\"\"\"\n        return _controls_.AnyButton_GetBitmapCurrent(*args, **kwargs)\n\n    def GetBitmapFocus(*args, **kwargs):\n        \"\"\"GetBitmapFocus(self) -> Bitmap\"\"\"\n        return _controls_.AnyButton_GetBitmapFocus(*args, **kwargs)\n\n    BitmapLabel = property(GetBitmapLabel,SetBitmapLabel) \n    BitmapPressed = property(GetBitmapPressed,SetBitmapPressed) \n    BitmapDisabled = property(GetBitmapDisabled,SetBitmapDisabled) \n    BitmapCurrent = property(GetBitmapCurrent,SetBitmapCurrent) \n    BitmapFocus = property(GetBitmapFocus,SetBitmapFocus) \n    def GetBitmapSelected(*args, **kwargs):\n        \"\"\"GetBitmapSelected(self) -> Bitmap\"\"\"\n        return _controls_.AnyButton_GetBitmapSelected(*args, **kwargs)\n\n    def GetBitmapHover(*args, **kwargs):\n        \"\"\"GetBitmapHover(self) -> Bitmap\"\"\"\n        return _controls_.AnyButton_GetBitmapHover(*args, **kwargs)\n\n    def SetBitmapSelected(*args, **kwargs):\n        \"\"\"SetBitmapSelected(self, Bitmap bitmap)\"\"\"\n        return _controls_.AnyButton_SetBitmapSelected(*args, **kwargs)\n\n    def SetBitmapHover(*args, **kwargs):\n        \"\"\"SetBitmapHover(self, Bitmap bitmap)\"\"\"\n        return _controls_.AnyButton_SetBitmapHover(*args, **kwargs)\n\n    BitmapSelected = property(GetBitmapSelected,SetBitmapSelected) \n    BitmapHover = property(GetBitmapHover,SetBitmapHover) \n    def SetBitmapMargins(*args):\n        \"\"\"\n        SetBitmapMargins(self, int x, int y)\n        SetBitmapMargins(self, Size sz)\n        \"\"\"\n        return _controls_.AnyButton_SetBitmapMargins(*args)\n\n    def GetBitmapMargins(*args, **kwargs):\n        \"\"\"GetBitmapMargins(self) -> Size\"\"\"\n        return _controls_.AnyButton_GetBitmapMargins(*args, **kwargs)\n\n    BitmapMargins = property(GetBitmapMargins,SetBitmapMargins) \n    def SetBitmapPosition(*args, **kwargs):\n        \"\"\"SetBitmapPosition(self, int dir)\"\"\"\n        return _controls_.AnyButton_SetBitmapPosition(*args, **kwargs)\n\n    def DontShowLabel(*args, **kwargs):\n        \"\"\"DontShowLabel(self) -> bool\"\"\"\n        return _controls_.AnyButton_DontShowLabel(*args, **kwargs)\n\n    def ShowsLabel(*args, **kwargs):\n        \"\"\"ShowsLabel(self) -> bool\"\"\"\n        return _controls_.AnyButton_ShowsLabel(*args, **kwargs)\n\n_controls_.AnyButton_swigregister(AnyButton)\ncvar = _controls_.cvar\nButtonNameStr = cvar.ButtonNameStr\n\nclass Button(AnyButton):\n    \"\"\"\n    A button is a control that contains a text string, and is one of the most\n    common elements of a GUI.  It may be placed on a dialog box or panel, or\n    indeed almost any other window.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=ButtonNameStr) -> Button\n\n        Create and show a button.  The preferred way to create standard\n        buttons is to use a standard ID and an empty label.  In this case\n        wxWigets will automatically use a stock label that corresponds to the\n        ID given.  These labels may vary across platforms as the platform\n        itself will provide the label if possible.  In addition, the button\n        will be decorated with stock icons under GTK+ 2.\n        \"\"\"\n        _controls_.Button_swiginit(self,_controls_.new_Button(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=ButtonNameStr) -> bool\n\n        Acutally create the GUI Button for 2-phase creation.\n        \"\"\"\n        return _controls_.Button_Create(*args, **kwargs)\n\n    def SetAuthNeeded(*args, **kwargs):\n        \"\"\"SetAuthNeeded(self, bool show=True)\"\"\"\n        return _controls_.Button_SetAuthNeeded(*args, **kwargs)\n\n    def GetAuthNeeded(*args, **kwargs):\n        \"\"\"GetAuthNeeded(self) -> bool\"\"\"\n        return _controls_.Button_GetAuthNeeded(*args, **kwargs)\n\n    def SetDefault(*args, **kwargs):\n        \"\"\"\n        SetDefault(self) -> Window\n\n        This sets the button to be the default item for the panel or dialog box.\n        \"\"\"\n        return _controls_.Button_SetDefault(*args, **kwargs)\n\n    def GetDefaultSize(*args, **kwargs):\n        \"\"\"\n        GetDefaultSize() -> Size\n\n        Returns the default button size for this platform.\n        \"\"\"\n        return _controls_.Button_GetDefaultSize(*args, **kwargs)\n\n    GetDefaultSize = staticmethod(GetDefaultSize)\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.Button_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n_controls_.Button_swigregister(Button)\n\ndef PreButton(*args, **kwargs):\n    \"\"\"\n    PreButton() -> Button\n\n    Precreate a Button for 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreButton(*args, **kwargs)\n    return val\n\ndef Button_GetDefaultSize(*args):\n  \"\"\"\n    Button_GetDefaultSize() -> Size\n\n    Returns the default button size for this platform.\n    \"\"\"\n  return _controls_.Button_GetDefaultSize(*args)\n\ndef Button_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    Button_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.Button_GetClassDefaultAttributes(*args, **kwargs)\n\nclass BitmapButton(Button):\n    \"\"\"\n    A Button that contains a bitmap.  A bitmap button can be supplied with a\n    single bitmap, and wxWidgets will draw all button states using this bitmap. If\n    the application needs more control, additional bitmaps for the selected state,\n    unpressed focused state, and greyed-out state may be supplied.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Bitmap bitmap=wxNullBitmap, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=BU_AUTODRAW, Validator validator=DefaultValidator, \n            String name=ButtonNameStr) -> BitmapButton\n\n        Create and show a button with a bitmap for the label.\n        \"\"\"\n        _controls_.BitmapButton_swiginit(self,_controls_.new_BitmapButton(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Bitmap bitmap=wxNullBitmap, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=BU_AUTODRAW, Validator validator=DefaultValidator, \n            String name=ButtonNameStr) -> bool\n\n        Acutally create the GUI BitmapButton for 2-phase creation.\n        \"\"\"\n        return _controls_.BitmapButton_Create(*args, **kwargs)\n\n_controls_.BitmapButton_swigregister(BitmapButton)\n\ndef PreBitmapButton(*args, **kwargs):\n    \"\"\"\n    PreBitmapButton() -> BitmapButton\n\n    Precreate a BitmapButton for 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreBitmapButton(*args, **kwargs)\n    return val\n\n#---------------------------------------------------------------------------\n\nCHK_2STATE = _controls_.CHK_2STATE\nCHK_3STATE = _controls_.CHK_3STATE\nCHK_ALLOW_3RD_STATE_FOR_USER = _controls_.CHK_ALLOW_3RD_STATE_FOR_USER\nclass CheckBox(_core.Control):\n    \"\"\"\n    A checkbox is a labelled box which by default is either on (the\n    checkmark is visible) or off (no checkmark). Optionally (When the\n    wx.CHK_3STATE style flag is set) it can have a third state, called the\n    mixed or undetermined state. Often this is used as a \"Does Not\n    Apply\" state.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=CheckBoxNameStr) -> CheckBox\n\n        Creates and shows a CheckBox control\n        \"\"\"\n        _controls_.CheckBox_swiginit(self,_controls_.new_CheckBox(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=CheckBoxNameStr) -> bool\n\n        Actually create the GUI CheckBox for 2-phase creation.\n        \"\"\"\n        return _controls_.CheckBox_Create(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"\n        GetValue(self) -> bool\n\n        Gets the state of a 2-state CheckBox.  Returns True if it is checked,\n        False otherwise.\n        \"\"\"\n        return _controls_.CheckBox_GetValue(*args, **kwargs)\n\n    def IsChecked(*args, **kwargs):\n        \"\"\"\n        IsChecked(self) -> bool\n\n        Similar to GetValue, but raises an exception if it is not a 2-state\n        CheckBox.\n        \"\"\"\n        return _controls_.CheckBox_IsChecked(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"\n        SetValue(self, bool state)\n\n        Set the state of a 2-state CheckBox.  Pass True for checked, False for\n        unchecked.\n        \"\"\"\n        return _controls_.CheckBox_SetValue(*args, **kwargs)\n\n    def Get3StateValue(*args, **kwargs):\n        \"\"\"\n        Get3StateValue(self) -> int\n\n        Returns wx.CHK_UNCHECKED when the CheckBox is unchecked,\n        wx.CHK_CHECKED when it is checked and wx.CHK_UNDETERMINED when it's in\n        the undetermined state.  Raises an exceptiion when the function is\n        used with a 2-state CheckBox.\n        \"\"\"\n        return _controls_.CheckBox_Get3StateValue(*args, **kwargs)\n\n    def Set3StateValue(*args, **kwargs):\n        \"\"\"\n        Set3StateValue(self, int state)\n\n        Sets the CheckBox to the given state.  The state parameter can be one\n        of the following: wx.CHK_UNCHECKED (Check is off), wx.CHK_CHECKED (the\n        Check is on) or wx.CHK_UNDETERMINED (Check is mixed). Raises an\n        exception when the CheckBox is a 2-state checkbox and setting the\n        state to wx.CHK_UNDETERMINED.\n        \"\"\"\n        return _controls_.CheckBox_Set3StateValue(*args, **kwargs)\n\n    def Is3State(*args, **kwargs):\n        \"\"\"\n        Is3State(self) -> bool\n\n        Returns whether or not the CheckBox is a 3-state CheckBox.\n        \"\"\"\n        return _controls_.CheckBox_Is3State(*args, **kwargs)\n\n    def Is3rdStateAllowedForUser(*args, **kwargs):\n        \"\"\"\n        Is3rdStateAllowedForUser(self) -> bool\n\n        Returns whether or not the user can set the CheckBox to the third\n        state.\n        \"\"\"\n        return _controls_.CheckBox_Is3rdStateAllowedForUser(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.CheckBox_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    ThreeStateValue = property(Get3StateValue,Set3StateValue,doc=\"See `Get3StateValue` and `Set3StateValue`\") \n    Value = property(GetValue,SetValue,doc=\"See `GetValue` and `SetValue`\") \n_controls_.CheckBox_swigregister(CheckBox)\nCheckBoxNameStr = cvar.CheckBoxNameStr\n\ndef PreCheckBox(*args, **kwargs):\n    \"\"\"\n    PreCheckBox() -> CheckBox\n\n    Precreate a CheckBox for 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreCheckBox(*args, **kwargs)\n    return val\n\ndef CheckBox_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    CheckBox_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.CheckBox_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nclass Choice(_core.ControlWithItems):\n    \"\"\"\n    A Choice control is used to select one of a list of strings.\n    Unlike a `wx.ListBox`, only the selection is visible until the\n    user pulls down the menu of choices.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(Window parent, int id, Point pos=DefaultPosition, Size size=DefaultSize,\n            List choices=EmptyList, long style=0, Validator validator=DefaultValidator,\n            String name=ChoiceNameStr) -> Choice\n\n        Create and show a Choice control\n        \"\"\"\n        _controls_.Choice_swiginit(self,_controls_.new_Choice(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(Window parent, int id, Point pos=DefaultPosition, Size size=DefaultSize,\n            List choices=EmptyList, long style=0, Validator validator=DefaultValidator,\n            String name=ChoiceNameStr) -> bool\n\n        Actually create the GUI Choice control for 2-phase creation\n        \"\"\"\n        return _controls_.Choice_Create(*args, **kwargs)\n\n    def GetCurrentSelection(*args, **kwargs):\n        \"\"\"\n        GetCurrentSelection(self) -> int\n\n        Unlike `GetSelection` which only returns the accepted selection value,\n        i.e. the selection in the control once the user closes the dropdown\n        list, this function returns the current selection.  That is, while the\n        dropdown list is shown, it returns the currently selected item in\n        it. When it is not shown, its result is the same as for the other\n        function.\n        \"\"\"\n        return _controls_.Choice_GetCurrentSelection(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.Choice_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    CurrentSelection = property(GetCurrentSelection,doc=\"See `GetCurrentSelection`\") \n_controls_.Choice_swigregister(Choice)\nChoiceNameStr = cvar.ChoiceNameStr\n\ndef PreChoice(*args, **kwargs):\n    \"\"\"\n    PreChoice() -> Choice\n\n    Precreate a Choice control for 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreChoice(*args, **kwargs)\n    return val\n\ndef Choice_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    Choice_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.Choice_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nclass ComboBox(_core.Control,_core.ItemContainer,_core.TextEntry):\n    \"\"\"\n    A combobox is like a combination of an edit control and a\n    listbox. It can be displayed as static list with editable or\n    read-only text field; or a drop-down list with text field.\n\n    A combobox permits a single selection only. Combobox items are\n    numbered from zero.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(Window parent, int id=-1, String value=EmptyString,\n            Point pos=DefaultPosition, Size size=DefaultSize,\n            List choices=EmptyList, long style=0, Validator validator=DefaultValidator,\n            String name=ComboBoxNameStr) -> ComboBox\n\n        Constructor, creates and shows a ComboBox control.\n        \"\"\"\n        _controls_.ComboBox_swiginit(self,_controls_.new_ComboBox(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(Window parent, int id=-1, String value=EmptyString,\n            Point pos=DefaultPosition, Size size=DefaultSize,\n            List choices=EmptyList, long style=0, Validator validator=DefaultValidator,\n            String name=ChoiceNameStr) -> bool\n\n        Actually create the GUI wxComboBox control for 2-phase creation\n        \"\"\"\n        return _controls_.ComboBox_Create(*args, **kwargs)\n\n    def SetMark(*args, **kwargs):\n        \"\"\"\n        SetMark(self, long from, long to)\n\n        Selects the text between the two positions in the combobox text field.\n        \"\"\"\n        return _controls_.ComboBox_SetMark(*args, **kwargs)\n\n    def GetMark(*args, **kwargs):\n        \"\"\"\n        GetMark(self) -> (from, to)\n\n        Gets the positions of the begining and ending of the selection mark in\n        the combobox text field.\n        \"\"\"\n        return _controls_.ComboBox_GetMark(*args, **kwargs)\n\n    def IsEmpty(*args, **kwargs):\n        \"\"\"IsEmpty(self) -> bool\"\"\"\n        return _controls_.ComboBox_IsEmpty(*args, **kwargs)\n\n    def IsListEmpty(*args, **kwargs):\n        \"\"\"IsListEmpty(self) -> bool\"\"\"\n        return _controls_.ComboBox_IsListEmpty(*args, **kwargs)\n\n    def IsTextEmpty(*args, **kwargs):\n        \"\"\"IsTextEmpty(self) -> bool\"\"\"\n        return _controls_.ComboBox_IsTextEmpty(*args, **kwargs)\n\n    def Popup(*args, **kwargs):\n        \"\"\"Popup(self)\"\"\"\n        return _controls_.ComboBox_Popup(*args, **kwargs)\n\n    def Dismiss(*args, **kwargs):\n        \"\"\"Dismiss(self)\"\"\"\n        return _controls_.ComboBox_Dismiss(*args, **kwargs)\n\n    def GetCurrentSelection(*args, **kwargs):\n        \"\"\"\n        GetCurrentSelection(self) -> int\n\n        Unlike `GetSelection` which only returns the accepted selection value,\n        i.e. the selection in the control once the user closes the dropdown\n        list, this function returns the current selection.  That is, while the\n        dropdown list is shown, it returns the currently selected item in\n        it. When it is not shown, its result is the same as for the other\n        function.\n        \"\"\"\n        return _controls_.ComboBox_GetCurrentSelection(*args, **kwargs)\n\n    def SetStringSelection(*args, **kwargs):\n        \"\"\"\n        SetStringSelection(self, String string) -> bool\n\n        Select the item with the specifed string\n        \"\"\"\n        return _controls_.ComboBox_SetStringSelection(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.ComboBox_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    CurrentSelection = property(GetCurrentSelection) \n    Mark = property(GetMark,SetMark) \n_controls_.ComboBox_swigregister(ComboBox)\nComboBoxNameStr = cvar.ComboBoxNameStr\n\ndef PreComboBox(*args, **kwargs):\n    \"\"\"\n    PreComboBox() -> ComboBox\n\n    Precreate a ComboBox control for 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreComboBox(*args, **kwargs)\n    return val\n\ndef ComboBox_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    ComboBox_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.ComboBox_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nGA_HORIZONTAL = _controls_.GA_HORIZONTAL\nGA_VERTICAL = _controls_.GA_VERTICAL\nGA_SMOOTH = _controls_.GA_SMOOTH\nGA_PROGRESSBAR = 0 # obsolete \nclass Gauge(_core.Control):\n    \"\"\"Proxy of C++ Gauge class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, int range=100, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=GA_HORIZONTAL, \n            Validator validator=DefaultValidator, \n            String name=GaugeNameStr) -> Gauge\n        \"\"\"\n        _controls_.Gauge_swiginit(self,_controls_.new_Gauge(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, int range=100, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=GA_HORIZONTAL, \n            Validator validator=DefaultValidator, \n            String name=GaugeNameStr) -> bool\n        \"\"\"\n        return _controls_.Gauge_Create(*args, **kwargs)\n\n    def SetRange(*args, **kwargs):\n        \"\"\"SetRange(self, int range)\"\"\"\n        return _controls_.Gauge_SetRange(*args, **kwargs)\n\n    def GetRange(*args, **kwargs):\n        \"\"\"GetRange(self) -> int\"\"\"\n        return _controls_.Gauge_GetRange(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"SetValue(self, int pos)\"\"\"\n        return _controls_.Gauge_SetValue(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"GetValue(self) -> int\"\"\"\n        return _controls_.Gauge_GetValue(*args, **kwargs)\n\n    def Pulse(*args, **kwargs):\n        \"\"\"Pulse(self)\"\"\"\n        return _controls_.Gauge_Pulse(*args, **kwargs)\n\n    def IsVertical(*args, **kwargs):\n        \"\"\"IsVertical(self) -> bool\"\"\"\n        return _controls_.Gauge_IsVertical(*args, **kwargs)\n\n    def SetShadowWidth(*args, **kwargs):\n        \"\"\"SetShadowWidth(self, int w)\"\"\"\n        return _controls_.Gauge_SetShadowWidth(*args, **kwargs)\n\n    def GetShadowWidth(*args, **kwargs):\n        \"\"\"GetShadowWidth(self) -> int\"\"\"\n        return _controls_.Gauge_GetShadowWidth(*args, **kwargs)\n\n    def SetBezelFace(*args, **kwargs):\n        \"\"\"SetBezelFace(self, int w)\"\"\"\n        return _controls_.Gauge_SetBezelFace(*args, **kwargs)\n\n    def GetBezelFace(*args, **kwargs):\n        \"\"\"GetBezelFace(self) -> int\"\"\"\n        return _controls_.Gauge_GetBezelFace(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.Gauge_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    BezelFace = property(GetBezelFace,SetBezelFace,doc=\"See `GetBezelFace` and `SetBezelFace`\") \n    Range = property(GetRange,SetRange,doc=\"See `GetRange` and `SetRange`\") \n    ShadowWidth = property(GetShadowWidth,SetShadowWidth,doc=\"See `GetShadowWidth` and `SetShadowWidth`\") \n    Value = property(GetValue,SetValue,doc=\"See `GetValue` and `SetValue`\") \n_controls_.Gauge_swigregister(Gauge)\nGaugeNameStr = cvar.GaugeNameStr\n\ndef PreGauge(*args, **kwargs):\n    \"\"\"PreGauge() -> Gauge\"\"\"\n    val = _controls_.new_PreGauge(*args, **kwargs)\n    return val\n\ndef Gauge_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    Gauge_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.Gauge_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nclass StaticBox(_core.Control):\n    \"\"\"Proxy of C++ StaticBox class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, String name=StaticBoxNameStr) -> StaticBox\n        \"\"\"\n        _controls_.StaticBox_swiginit(self,_controls_.new_StaticBox(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, String name=StaticBoxNameStr) -> bool\n        \"\"\"\n        return _controls_.StaticBox_Create(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.StaticBox_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n_controls_.StaticBox_swigregister(StaticBox)\nStaticBitmapNameStr = cvar.StaticBitmapNameStr\nStaticBoxNameStr = cvar.StaticBoxNameStr\nStaticTextNameStr = cvar.StaticTextNameStr\nStaticLineNameStr = cvar.StaticLineNameStr\n\ndef PreStaticBox(*args, **kwargs):\n    \"\"\"PreStaticBox() -> StaticBox\"\"\"\n    val = _controls_.new_PreStaticBox(*args, **kwargs)\n    return val\n\ndef StaticBox_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    StaticBox_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.StaticBox_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nclass StaticLine(_core.Control):\n    \"\"\"Proxy of C++ StaticLine class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=LI_HORIZONTAL, \n            String name=StaticLineNameStr) -> StaticLine\n        \"\"\"\n        _controls_.StaticLine_swiginit(self,_controls_.new_StaticLine(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=LI_HORIZONTAL, \n            String name=StaticLineNameStr) -> bool\n        \"\"\"\n        return _controls_.StaticLine_Create(*args, **kwargs)\n\n    def IsVertical(*args, **kwargs):\n        \"\"\"IsVertical(self) -> bool\"\"\"\n        return _controls_.StaticLine_IsVertical(*args, **kwargs)\n\n    def GetDefaultSize(*args, **kwargs):\n        \"\"\"GetDefaultSize() -> int\"\"\"\n        return _controls_.StaticLine_GetDefaultSize(*args, **kwargs)\n\n    GetDefaultSize = staticmethod(GetDefaultSize)\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.StaticLine_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n_controls_.StaticLine_swigregister(StaticLine)\n\ndef PreStaticLine(*args, **kwargs):\n    \"\"\"PreStaticLine() -> StaticLine\"\"\"\n    val = _controls_.new_PreStaticLine(*args, **kwargs)\n    return val\n\ndef StaticLine_GetDefaultSize(*args):\n  \"\"\"StaticLine_GetDefaultSize() -> int\"\"\"\n  return _controls_.StaticLine_GetDefaultSize(*args)\n\ndef StaticLine_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    StaticLine_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.StaticLine_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nST_NO_AUTORESIZE = _controls_.ST_NO_AUTORESIZE\nST_ELLIPSIZE_START = _controls_.ST_ELLIPSIZE_START\nST_ELLIPSIZE_MIDDLE = _controls_.ST_ELLIPSIZE_MIDDLE\nST_ELLIPSIZE_END = _controls_.ST_ELLIPSIZE_END\nclass StaticText(_core.Control):\n    \"\"\"Proxy of C++ StaticText class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, String name=StaticTextNameStr) -> StaticText\n        \"\"\"\n        _controls_.StaticText_swiginit(self,_controls_.new_StaticText(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, String name=StaticTextNameStr) -> bool\n        \"\"\"\n        return _controls_.StaticText_Create(*args, **kwargs)\n\n    def Wrap(*args, **kwargs):\n        \"\"\"\n        Wrap(self, int width)\n\n        This functions wraps the control's label so that each of its lines\n        becomes at most ``width`` pixels wide if possible (the lines are\n        broken at words boundaries so it might not be the case if words are\n        too long). If ``width`` is negative, no wrapping is done.\n        \"\"\"\n        return _controls_.StaticText_Wrap(*args, **kwargs)\n\n    def IsEllipsized(*args, **kwargs):\n        \"\"\"IsEllipsized(self) -> bool\"\"\"\n        return _controls_.StaticText_IsEllipsized(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.StaticText_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n_controls_.StaticText_swigregister(StaticText)\n\ndef PreStaticText(*args, **kwargs):\n    \"\"\"PreStaticText() -> StaticText\"\"\"\n    val = _controls_.new_PreStaticText(*args, **kwargs)\n    return val\n\ndef StaticText_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    StaticText_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.StaticText_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nclass StaticBitmap(_core.Control):\n    \"\"\"Proxy of C++ StaticBitmap class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Bitmap bitmap=wxNullBitmap, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, String name=StaticBitmapNameStr) -> StaticBitmap\n        \"\"\"\n        _controls_.StaticBitmap_swiginit(self,_controls_.new_StaticBitmap(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Bitmap bitmap=wxNullBitmap, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, String name=StaticBitmapNameStr) -> bool\n        \"\"\"\n        return _controls_.StaticBitmap_Create(*args, **kwargs)\n\n    def GetBitmap(*args, **kwargs):\n        \"\"\"GetBitmap(self) -> Bitmap\"\"\"\n        return _controls_.StaticBitmap_GetBitmap(*args, **kwargs)\n\n    def SetBitmap(*args, **kwargs):\n        \"\"\"SetBitmap(self, Bitmap bitmap)\"\"\"\n        return _controls_.StaticBitmap_SetBitmap(*args, **kwargs)\n\n    def SetIcon(*args, **kwargs):\n        \"\"\"SetIcon(self, Icon icon)\"\"\"\n        return _controls_.StaticBitmap_SetIcon(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.StaticBitmap_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n_controls_.StaticBitmap_swigregister(StaticBitmap)\n\ndef PreStaticBitmap(*args, **kwargs):\n    \"\"\"PreStaticBitmap() -> StaticBitmap\"\"\"\n    val = _controls_.new_PreStaticBitmap(*args, **kwargs)\n    return val\n\ndef StaticBitmap_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    StaticBitmap_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.StaticBitmap_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nclass ListBox(_core.ControlWithItems):\n    \"\"\"Proxy of C++ ListBox class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, wxArrayString choices=wxPyEmptyStringArray, \n            long style=0, Validator validator=DefaultValidator, \n            String name=ListBoxNameStr) -> ListBox\n        \"\"\"\n        _controls_.ListBox_swiginit(self,_controls_.new_ListBox(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, wxArrayString choices=wxPyEmptyStringArray, \n            long style=0, Validator validator=DefaultValidator, \n            String name=ListBoxNameStr) -> bool\n        \"\"\"\n        return _controls_.ListBox_Create(*args, **kwargs)\n\n    def Insert(*args, **kwargs):\n        \"\"\"\n        Insert(self, String item, int pos, PyObject clientData=None)\n\n        Insert an item into the control before the item at the ``pos`` index,\n        optionally associating some data object with the item.\n        \"\"\"\n        return _controls_.ListBox_Insert(*args, **kwargs)\n\n    def InsertItems(*args, **kwargs):\n        \"\"\"InsertItems(self, wxArrayString items, unsigned int pos)\"\"\"\n        return _controls_.ListBox_InsertItems(*args, **kwargs)\n\n    def Set(*args, **kwargs):\n        \"\"\"\n        Set(self, List strings)\n\n        Replace all the items in the control\n        \"\"\"\n        return _controls_.ListBox_Set(*args, **kwargs)\n\n    def IsSelected(*args, **kwargs):\n        \"\"\"IsSelected(self, int n) -> bool\"\"\"\n        return _controls_.ListBox_IsSelected(*args, **kwargs)\n\n    def SetSelection(*args, **kwargs):\n        \"\"\"SetSelection(self, int n, bool select=True)\"\"\"\n        return _controls_.ListBox_SetSelection(*args, **kwargs)\n\n    def Select(*args, **kwargs):\n        \"\"\"\n        Select(self, int n)\n\n        This is the same as `SetSelection` and exists only because it is\n        slightly more natural for controls which support multiple selection.\n        \"\"\"\n        return _controls_.ListBox_Select(*args, **kwargs)\n\n    def Deselect(*args, **kwargs):\n        \"\"\"Deselect(self, int n)\"\"\"\n        return _controls_.ListBox_Deselect(*args, **kwargs)\n\n    def DeselectAll(*args, **kwargs):\n        \"\"\"DeselectAll(self, int itemToLeaveSelected=-1)\"\"\"\n        return _controls_.ListBox_DeselectAll(*args, **kwargs)\n\n    def SetStringSelection(*args, **kwargs):\n        \"\"\"SetStringSelection(self, String s, bool select=True) -> bool\"\"\"\n        return _controls_.ListBox_SetStringSelection(*args, **kwargs)\n\n    def GetSelections(*args, **kwargs):\n        \"\"\"GetSelections(self) -> PyObject\"\"\"\n        return _controls_.ListBox_GetSelections(*args, **kwargs)\n\n    def SetFirstItem(*args, **kwargs):\n        \"\"\"SetFirstItem(self, int n)\"\"\"\n        return _controls_.ListBox_SetFirstItem(*args, **kwargs)\n\n    def SetFirstItemStr(*args, **kwargs):\n        \"\"\"SetFirstItemStr(self, String s)\"\"\"\n        return _controls_.ListBox_SetFirstItemStr(*args, **kwargs)\n\n    def EnsureVisible(*args, **kwargs):\n        \"\"\"EnsureVisible(self, int n)\"\"\"\n        return _controls_.ListBox_EnsureVisible(*args, **kwargs)\n\n    def AppendAndEnsureVisible(*args, **kwargs):\n        \"\"\"AppendAndEnsureVisible(self, String s)\"\"\"\n        return _controls_.ListBox_AppendAndEnsureVisible(*args, **kwargs)\n\n    def HitTest(*args, **kwargs):\n        \"\"\"\n        HitTest(self, Point pt) -> int\n\n        Test where the given (in client coords) point lies\n        \"\"\"\n        return _controls_.ListBox_HitTest(*args, **kwargs)\n\n    def SetItemForegroundColour(*args, **kwargs):\n        \"\"\"SetItemForegroundColour(self, int item, Colour c)\"\"\"\n        return _controls_.ListBox_SetItemForegroundColour(*args, **kwargs)\n\n    def SetItemBackgroundColour(*args, **kwargs):\n        \"\"\"SetItemBackgroundColour(self, int item, Colour c)\"\"\"\n        return _controls_.ListBox_SetItemBackgroundColour(*args, **kwargs)\n\n    def SetItemFont(*args, **kwargs):\n        \"\"\"SetItemFont(self, int item, Font f)\"\"\"\n        return _controls_.ListBox_SetItemFont(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.ListBox_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    Selections = property(GetSelections,doc=\"See `GetSelections`\") \n_controls_.ListBox_swigregister(ListBox)\nListBoxNameStr = cvar.ListBoxNameStr\n\ndef PreListBox(*args, **kwargs):\n    \"\"\"PreListBox() -> ListBox\"\"\"\n    val = _controls_.new_PreListBox(*args, **kwargs)\n    return val\n\ndef ListBox_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    ListBox_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.ListBox_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nclass CheckListBox(ListBox):\n    \"\"\"Proxy of C++ CheckListBox class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, wxArrayString choices=wxPyEmptyStringArray, \n            long style=0, Validator validator=DefaultValidator, \n            String name=ListBoxNameStr) -> CheckListBox\n        \"\"\"\n        _controls_.CheckListBox_swiginit(self,_controls_.new_CheckListBox(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, wxArrayString choices=wxPyEmptyStringArray, \n            long style=0, Validator validator=DefaultValidator, \n            String name=ListBoxNameStr) -> bool\n        \"\"\"\n        return _controls_.CheckListBox_Create(*args, **kwargs)\n\n    def IsChecked(*args, **kwargs):\n        \"\"\"IsChecked(self, unsigned int index) -> bool\"\"\"\n        return _controls_.CheckListBox_IsChecked(*args, **kwargs)\n\n    def Check(*args, **kwargs):\n        \"\"\"Check(self, unsigned int index, int check=True)\"\"\"\n        return _controls_.CheckListBox_Check(*args, **kwargs)\n\n    def GetChecked(self):\n        \"\"\"\n        GetChecked(self)\n\n        Return a tuple of integers corresponding to the checked items in\n        the control, based on `IsChecked`.\n        \"\"\"\n        return tuple([i for i in range(self.Count) if self.IsChecked(i)])\n\n    def GetCheckedStrings(self):\n        \"\"\"\n        GetCheckedStrings(self)\n\n        Return a tuple of strings corresponding to the checked\n        items of the control, based on `GetChecked`.\n        \"\"\"\n        return tuple([self.GetString(i) for i in self.GetChecked()])\n\n    def SetChecked(self, indexes):\n        \"\"\"\n        SetChecked(self, indexes)\n\n        Sets the checked state of items if the index of the item is \n        found in the indexes sequence.\n        \"\"\"\n        for i in indexes:\n            assert 0 <= i < self.Count, \"Index (%s) out of range\" % i\n        for i in range(self.Count):\n            self.Check(i, i in indexes)\n\n    def SetCheckedStrings(self, strings):\n        \"\"\"\n        SetCheckedStrings(self, indexes)\n\n        Sets the checked state of items if the item's string is found\n        in the strings sequence.\n        \"\"\"\n        for s in strings:\n            assert s in self.GetStrings(), \"String ('%s') not found\" % s\n        for i in range(self.Count):\n            self.Check(i, self.GetString(i) in strings)\n\n    Checked = property(GetChecked,SetChecked)\n    CheckedStrings = property(GetCheckedStrings,SetCheckedStrings)\n\n_controls_.CheckListBox_swigregister(CheckListBox)\n\ndef PreCheckListBox(*args, **kwargs):\n    \"\"\"PreCheckListBox() -> CheckListBox\"\"\"\n    val = _controls_.new_PreCheckListBox(*args, **kwargs)\n    return val\n\n#---------------------------------------------------------------------------\n\nTE_NO_VSCROLL = _controls_.TE_NO_VSCROLL\nTE_AUTO_SCROLL = _controls_.TE_AUTO_SCROLL\nTE_READONLY = _controls_.TE_READONLY\nTE_MULTILINE = _controls_.TE_MULTILINE\nTE_PROCESS_TAB = _controls_.TE_PROCESS_TAB\nTE_LEFT = _controls_.TE_LEFT\nTE_CENTER = _controls_.TE_CENTER\nTE_RIGHT = _controls_.TE_RIGHT\nTE_CENTRE = _controls_.TE_CENTRE\nTE_RICH = _controls_.TE_RICH\nTE_PROCESS_ENTER = _controls_.TE_PROCESS_ENTER\nTE_PASSWORD = _controls_.TE_PASSWORD\nTE_AUTO_URL = _controls_.TE_AUTO_URL\nTE_NOHIDESEL = _controls_.TE_NOHIDESEL\nTE_DONTWRAP = _controls_.TE_DONTWRAP\nTE_CHARWRAP = _controls_.TE_CHARWRAP\nTE_WORDWRAP = _controls_.TE_WORDWRAP\nTE_BESTWRAP = _controls_.TE_BESTWRAP\nTE_RICH2 = _controls_.TE_RICH2\nTE_CAPITALIZE = _controls_.TE_CAPITALIZE\nTE_LINEWRAP = TE_CHARWRAP \nPROCESS_ENTER = TE_PROCESS_ENTER\nPASSWORD = TE_PASSWORD\n\nTEXT_ALIGNMENT_DEFAULT = _controls_.TEXT_ALIGNMENT_DEFAULT\nTEXT_ALIGNMENT_LEFT = _controls_.TEXT_ALIGNMENT_LEFT\nTEXT_ALIGNMENT_CENTRE = _controls_.TEXT_ALIGNMENT_CENTRE\nTEXT_ALIGNMENT_CENTER = _controls_.TEXT_ALIGNMENT_CENTER\nTEXT_ALIGNMENT_RIGHT = _controls_.TEXT_ALIGNMENT_RIGHT\nTEXT_ALIGNMENT_JUSTIFIED = _controls_.TEXT_ALIGNMENT_JUSTIFIED\nTEXT_ATTR_TEXT_COLOUR = _controls_.TEXT_ATTR_TEXT_COLOUR\nTEXT_ATTR_BACKGROUND_COLOUR = _controls_.TEXT_ATTR_BACKGROUND_COLOUR\nTEXT_ATTR_FONT_FACE = _controls_.TEXT_ATTR_FONT_FACE\nTEXT_ATTR_FONT_SIZE = _controls_.TEXT_ATTR_FONT_SIZE\nTEXT_ATTR_FONT_WEIGHT = _controls_.TEXT_ATTR_FONT_WEIGHT\nTEXT_ATTR_FONT_ITALIC = _controls_.TEXT_ATTR_FONT_ITALIC\nTEXT_ATTR_FONT_UNDERLINE = _controls_.TEXT_ATTR_FONT_UNDERLINE\nTEXT_ATTR_FONT_STRIKETHROUGH = _controls_.TEXT_ATTR_FONT_STRIKETHROUGH\nTEXT_ATTR_FONT_ENCODING = _controls_.TEXT_ATTR_FONT_ENCODING\nTEXT_ATTR_FONT_FAMILY = _controls_.TEXT_ATTR_FONT_FAMILY\nTEXT_ATTR_FONT = _controls_.TEXT_ATTR_FONT\nTEXT_ATTR_ALIGNMENT = _controls_.TEXT_ATTR_ALIGNMENT\nTEXT_ATTR_LEFT_INDENT = _controls_.TEXT_ATTR_LEFT_INDENT\nTEXT_ATTR_RIGHT_INDENT = _controls_.TEXT_ATTR_RIGHT_INDENT\nTEXT_ATTR_TABS = _controls_.TEXT_ATTR_TABS\nTEXT_ATTR_PARA_SPACING_AFTER = _controls_.TEXT_ATTR_PARA_SPACING_AFTER\nTEXT_ATTR_LINE_SPACING = _controls_.TEXT_ATTR_LINE_SPACING\nTEXT_ATTR_CHARACTER_STYLE_NAME = _controls_.TEXT_ATTR_CHARACTER_STYLE_NAME\nTEXT_ATTR_PARAGRAPH_STYLE_NAME = _controls_.TEXT_ATTR_PARAGRAPH_STYLE_NAME\nTEXT_ATTR_LIST_STYLE_NAME = _controls_.TEXT_ATTR_LIST_STYLE_NAME\nTEXT_ATTR_BULLET_STYLE = _controls_.TEXT_ATTR_BULLET_STYLE\nTEXT_ATTR_BULLET_NUMBER = _controls_.TEXT_ATTR_BULLET_NUMBER\nTEXT_ATTR_BULLET_TEXT = _controls_.TEXT_ATTR_BULLET_TEXT\nTEXT_ATTR_BULLET_NAME = _controls_.TEXT_ATTR_BULLET_NAME\nTEXT_ATTR_BULLET = _controls_.TEXT_ATTR_BULLET\nTEXT_ATTR_URL = _controls_.TEXT_ATTR_URL\nTEXT_ATTR_PAGE_BREAK = _controls_.TEXT_ATTR_PAGE_BREAK\nTEXT_ATTR_EFFECTS = _controls_.TEXT_ATTR_EFFECTS\nTEXT_ATTR_OUTLINE_LEVEL = _controls_.TEXT_ATTR_OUTLINE_LEVEL\nTEXT_ATTR_CHARACTER = _controls_.TEXT_ATTR_CHARACTER\nTEXT_ATTR_PARAGRAPH = _controls_.TEXT_ATTR_PARAGRAPH\nTEXT_ATTR_ALL = _controls_.TEXT_ATTR_ALL\nTEXT_ATTR_BULLET_STYLE_NONE = _controls_.TEXT_ATTR_BULLET_STYLE_NONE\nTEXT_ATTR_BULLET_STYLE_ARABIC = _controls_.TEXT_ATTR_BULLET_STYLE_ARABIC\nTEXT_ATTR_BULLET_STYLE_LETTERS_UPPER = _controls_.TEXT_ATTR_BULLET_STYLE_LETTERS_UPPER\nTEXT_ATTR_BULLET_STYLE_LETTERS_LOWER = _controls_.TEXT_ATTR_BULLET_STYLE_LETTERS_LOWER\nTEXT_ATTR_BULLET_STYLE_ROMAN_UPPER = _controls_.TEXT_ATTR_BULLET_STYLE_ROMAN_UPPER\nTEXT_ATTR_BULLET_STYLE_ROMAN_LOWER = _controls_.TEXT_ATTR_BULLET_STYLE_ROMAN_LOWER\nTEXT_ATTR_BULLET_STYLE_SYMBOL = _controls_.TEXT_ATTR_BULLET_STYLE_SYMBOL\nTEXT_ATTR_BULLET_STYLE_BITMAP = _controls_.TEXT_ATTR_BULLET_STYLE_BITMAP\nTEXT_ATTR_BULLET_STYLE_PARENTHESES = _controls_.TEXT_ATTR_BULLET_STYLE_PARENTHESES\nTEXT_ATTR_BULLET_STYLE_PERIOD = _controls_.TEXT_ATTR_BULLET_STYLE_PERIOD\nTEXT_ATTR_BULLET_STYLE_STANDARD = _controls_.TEXT_ATTR_BULLET_STYLE_STANDARD\nTEXT_ATTR_BULLET_STYLE_RIGHT_PARENTHESIS = _controls_.TEXT_ATTR_BULLET_STYLE_RIGHT_PARENTHESIS\nTEXT_ATTR_BULLET_STYLE_OUTLINE = _controls_.TEXT_ATTR_BULLET_STYLE_OUTLINE\nTEXT_ATTR_BULLET_STYLE_ALIGN_LEFT = _controls_.TEXT_ATTR_BULLET_STYLE_ALIGN_LEFT\nTEXT_ATTR_BULLET_STYLE_ALIGN_RIGHT = _controls_.TEXT_ATTR_BULLET_STYLE_ALIGN_RIGHT\nTEXT_ATTR_BULLET_STYLE_ALIGN_CENTRE = _controls_.TEXT_ATTR_BULLET_STYLE_ALIGN_CENTRE\nTEXT_ATTR_EFFECT_NONE = _controls_.TEXT_ATTR_EFFECT_NONE\nTEXT_ATTR_EFFECT_CAPITALS = _controls_.TEXT_ATTR_EFFECT_CAPITALS\nTEXT_ATTR_EFFECT_SMALL_CAPITALS = _controls_.TEXT_ATTR_EFFECT_SMALL_CAPITALS\nTEXT_ATTR_EFFECT_STRIKETHROUGH = _controls_.TEXT_ATTR_EFFECT_STRIKETHROUGH\nTEXT_ATTR_EFFECT_DOUBLE_STRIKETHROUGH = _controls_.TEXT_ATTR_EFFECT_DOUBLE_STRIKETHROUGH\nTEXT_ATTR_EFFECT_SHADOW = _controls_.TEXT_ATTR_EFFECT_SHADOW\nTEXT_ATTR_EFFECT_EMBOSS = _controls_.TEXT_ATTR_EFFECT_EMBOSS\nTEXT_ATTR_EFFECT_OUTLINE = _controls_.TEXT_ATTR_EFFECT_OUTLINE\nTEXT_ATTR_EFFECT_ENGRAVE = _controls_.TEXT_ATTR_EFFECT_ENGRAVE\nTEXT_ATTR_EFFECT_SUPERSCRIPT = _controls_.TEXT_ATTR_EFFECT_SUPERSCRIPT\nTEXT_ATTR_EFFECT_SUBSCRIPT = _controls_.TEXT_ATTR_EFFECT_SUBSCRIPT\nTEXT_ATTR_LINE_SPACING_NORMAL = _controls_.TEXT_ATTR_LINE_SPACING_NORMAL\nTEXT_ATTR_LINE_SPACING_HALF = _controls_.TEXT_ATTR_LINE_SPACING_HALF\nTEXT_ATTR_LINE_SPACING_TWICE = _controls_.TEXT_ATTR_LINE_SPACING_TWICE\nOutOfRangeTextCoord = _controls_.OutOfRangeTextCoord\nInvalidTextCoord = _controls_.InvalidTextCoord\nTEXT_TYPE_ANY = _controls_.TEXT_TYPE_ANY\nclass TextAttr(object):\n    \"\"\"Proxy of C++ TextAttr class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Colour colText=wxNullColour, Colour colBack=wxNullColour, \n            Font font=wxNullFont, int alignment=TEXT_ALIGNMENT_DEFAULT) -> TextAttr\n        \"\"\"\n        _controls_.TextAttr_swiginit(self,_controls_.new_TextAttr(*args, **kwargs))\n    __swig_destroy__ = _controls_.delete_TextAttr\n    __del__ = lambda self : None;\n    def Init(*args, **kwargs):\n        \"\"\"Init(self)\"\"\"\n        return _controls_.TextAttr_Init(*args, **kwargs)\n\n    def Copy(*args, **kwargs):\n        \"\"\"Copy(self, TextAttr attr)\"\"\"\n        return _controls_.TextAttr_Copy(*args, **kwargs)\n\n    def EqPartial(*args, **kwargs):\n        \"\"\"EqPartial(self, TextAttr attr) -> bool\"\"\"\n        return _controls_.TextAttr_EqPartial(*args, **kwargs)\n\n    def GetFontAttributes(*args, **kwargs):\n        \"\"\"GetFontAttributes(self, Font font, int flags=TEXT_ATTR_FONT) -> bool\"\"\"\n        return _controls_.TextAttr_GetFontAttributes(*args, **kwargs)\n\n    def SetTextColour(*args, **kwargs):\n        \"\"\"SetTextColour(self, Colour colText)\"\"\"\n        return _controls_.TextAttr_SetTextColour(*args, **kwargs)\n\n    def SetBackgroundColour(*args, **kwargs):\n        \"\"\"SetBackgroundColour(self, Colour colBack)\"\"\"\n        return _controls_.TextAttr_SetBackgroundColour(*args, **kwargs)\n\n    def SetAlignment(*args, **kwargs):\n        \"\"\"SetAlignment(self, int alignment)\"\"\"\n        return _controls_.TextAttr_SetAlignment(*args, **kwargs)\n\n    def SetTabs(*args, **kwargs):\n        \"\"\"SetTabs(self, wxArrayInt tabs)\"\"\"\n        return _controls_.TextAttr_SetTabs(*args, **kwargs)\n\n    def SetLeftIndent(*args, **kwargs):\n        \"\"\"SetLeftIndent(self, int indent, int subIndent=0)\"\"\"\n        return _controls_.TextAttr_SetLeftIndent(*args, **kwargs)\n\n    def SetRightIndent(*args, **kwargs):\n        \"\"\"SetRightIndent(self, int indent)\"\"\"\n        return _controls_.TextAttr_SetRightIndent(*args, **kwargs)\n\n    def SetFontSize(*args, **kwargs):\n        \"\"\"SetFontSize(self, int pointSize)\"\"\"\n        return _controls_.TextAttr_SetFontSize(*args, **kwargs)\n\n    def SetFontStyle(*args, **kwargs):\n        \"\"\"SetFontStyle(self, int fontStyle)\"\"\"\n        return _controls_.TextAttr_SetFontStyle(*args, **kwargs)\n\n    def SetFontWeight(*args, **kwargs):\n        \"\"\"SetFontWeight(self, int fontWeight)\"\"\"\n        return _controls_.TextAttr_SetFontWeight(*args, **kwargs)\n\n    def SetFontFaceName(*args, **kwargs):\n        \"\"\"SetFontFaceName(self, String faceName)\"\"\"\n        return _controls_.TextAttr_SetFontFaceName(*args, **kwargs)\n\n    def SetFontUnderlined(*args, **kwargs):\n        \"\"\"SetFontUnderlined(self, bool underlined)\"\"\"\n        return _controls_.TextAttr_SetFontUnderlined(*args, **kwargs)\n\n    def SetFontStrikethrough(*args, **kwargs):\n        \"\"\"SetFontStrikethrough(self, bool strikethrough)\"\"\"\n        return _controls_.TextAttr_SetFontStrikethrough(*args, **kwargs)\n\n    def SetFontEncoding(*args, **kwargs):\n        \"\"\"SetFontEncoding(self, int encoding)\"\"\"\n        return _controls_.TextAttr_SetFontEncoding(*args, **kwargs)\n\n    def SetFontFamily(*args, **kwargs):\n        \"\"\"SetFontFamily(self, int family)\"\"\"\n        return _controls_.TextAttr_SetFontFamily(*args, **kwargs)\n\n    def SetFont(*args, **kwargs):\n        \"\"\"SetFont(self, Font font, int flags=TEXT_ATTR_FONT)\"\"\"\n        return _controls_.TextAttr_SetFont(*args, **kwargs)\n\n    def SetFlags(*args, **kwargs):\n        \"\"\"SetFlags(self, long flags)\"\"\"\n        return _controls_.TextAttr_SetFlags(*args, **kwargs)\n\n    def SetCharacterStyleName(*args, **kwargs):\n        \"\"\"SetCharacterStyleName(self, String name)\"\"\"\n        return _controls_.TextAttr_SetCharacterStyleName(*args, **kwargs)\n\n    def SetParagraphStyleName(*args, **kwargs):\n        \"\"\"SetParagraphStyleName(self, String name)\"\"\"\n        return _controls_.TextAttr_SetParagraphStyleName(*args, **kwargs)\n\n    def SetListStyleName(*args, **kwargs):\n        \"\"\"SetListStyleName(self, String name)\"\"\"\n        return _controls_.TextAttr_SetListStyleName(*args, **kwargs)\n\n    def SetParagraphSpacingAfter(*args, **kwargs):\n        \"\"\"SetParagraphSpacingAfter(self, int spacing)\"\"\"\n        return _controls_.TextAttr_SetParagraphSpacingAfter(*args, **kwargs)\n\n    def SetParagraphSpacingBefore(*args, **kwargs):\n        \"\"\"SetParagraphSpacingBefore(self, int spacing)\"\"\"\n        return _controls_.TextAttr_SetParagraphSpacingBefore(*args, **kwargs)\n\n    def SetLineSpacing(*args, **kwargs):\n        \"\"\"SetLineSpacing(self, int spacing)\"\"\"\n        return _controls_.TextAttr_SetLineSpacing(*args, **kwargs)\n\n    def SetBulletStyle(*args, **kwargs):\n        \"\"\"SetBulletStyle(self, int style)\"\"\"\n        return _controls_.TextAttr_SetBulletStyle(*args, **kwargs)\n\n    def SetBulletNumber(*args, **kwargs):\n        \"\"\"SetBulletNumber(self, int n)\"\"\"\n        return _controls_.TextAttr_SetBulletNumber(*args, **kwargs)\n\n    def SetBulletText(*args, **kwargs):\n        \"\"\"SetBulletText(self, String text)\"\"\"\n        return _controls_.TextAttr_SetBulletText(*args, **kwargs)\n\n    def SetBulletFont(*args, **kwargs):\n        \"\"\"SetBulletFont(self, String bulletFont)\"\"\"\n        return _controls_.TextAttr_SetBulletFont(*args, **kwargs)\n\n    def SetBulletName(*args, **kwargs):\n        \"\"\"SetBulletName(self, String name)\"\"\"\n        return _controls_.TextAttr_SetBulletName(*args, **kwargs)\n\n    def SetURL(*args, **kwargs):\n        \"\"\"SetURL(self, String url)\"\"\"\n        return _controls_.TextAttr_SetURL(*args, **kwargs)\n\n    def SetPageBreak(*args, **kwargs):\n        \"\"\"SetPageBreak(self, bool pageBreak=True)\"\"\"\n        return _controls_.TextAttr_SetPageBreak(*args, **kwargs)\n\n    def SetTextEffects(*args, **kwargs):\n        \"\"\"SetTextEffects(self, int effects)\"\"\"\n        return _controls_.TextAttr_SetTextEffects(*args, **kwargs)\n\n    def SetTextEffectFlags(*args, **kwargs):\n        \"\"\"SetTextEffectFlags(self, int effects)\"\"\"\n        return _controls_.TextAttr_SetTextEffectFlags(*args, **kwargs)\n\n    def SetOutlineLevel(*args, **kwargs):\n        \"\"\"SetOutlineLevel(self, int level)\"\"\"\n        return _controls_.TextAttr_SetOutlineLevel(*args, **kwargs)\n\n    def GetTextColour(*args, **kwargs):\n        \"\"\"GetTextColour(self) -> Colour\"\"\"\n        return _controls_.TextAttr_GetTextColour(*args, **kwargs)\n\n    def GetBackgroundColour(*args, **kwargs):\n        \"\"\"GetBackgroundColour(self) -> Colour\"\"\"\n        return _controls_.TextAttr_GetBackgroundColour(*args, **kwargs)\n\n    def GetAlignment(*args, **kwargs):\n        \"\"\"GetAlignment(self) -> int\"\"\"\n        return _controls_.TextAttr_GetAlignment(*args, **kwargs)\n\n    def GetTabs(*args, **kwargs):\n        \"\"\"GetTabs(self) -> wxArrayInt\"\"\"\n        return _controls_.TextAttr_GetTabs(*args, **kwargs)\n\n    def GetLeftIndent(*args, **kwargs):\n        \"\"\"GetLeftIndent(self) -> long\"\"\"\n        return _controls_.TextAttr_GetLeftIndent(*args, **kwargs)\n\n    def GetLeftSubIndent(*args, **kwargs):\n        \"\"\"GetLeftSubIndent(self) -> long\"\"\"\n        return _controls_.TextAttr_GetLeftSubIndent(*args, **kwargs)\n\n    def GetRightIndent(*args, **kwargs):\n        \"\"\"GetRightIndent(self) -> long\"\"\"\n        return _controls_.TextAttr_GetRightIndent(*args, **kwargs)\n\n    def GetFlags(*args, **kwargs):\n        \"\"\"GetFlags(self) -> long\"\"\"\n        return _controls_.TextAttr_GetFlags(*args, **kwargs)\n\n    def GetFontSize(*args, **kwargs):\n        \"\"\"GetFontSize(self) -> int\"\"\"\n        return _controls_.TextAttr_GetFontSize(*args, **kwargs)\n\n    def GetFontStyle(*args, **kwargs):\n        \"\"\"GetFontStyle(self) -> int\"\"\"\n        return _controls_.TextAttr_GetFontStyle(*args, **kwargs)\n\n    def GetFontWeight(*args, **kwargs):\n        \"\"\"GetFontWeight(self) -> int\"\"\"\n        return _controls_.TextAttr_GetFontWeight(*args, **kwargs)\n\n    def GetFontUnderlined(*args, **kwargs):\n        \"\"\"GetFontUnderlined(self) -> bool\"\"\"\n        return _controls_.TextAttr_GetFontUnderlined(*args, **kwargs)\n\n    def GetFontStrikethrough(*args, **kwargs):\n        \"\"\"GetFontStrikethrough(self) -> bool\"\"\"\n        return _controls_.TextAttr_GetFontStrikethrough(*args, **kwargs)\n\n    def GetFontFaceName(*args, **kwargs):\n        \"\"\"GetFontFaceName(self) -> String\"\"\"\n        return _controls_.TextAttr_GetFontFaceName(*args, **kwargs)\n\n    def GetFontEncoding(*args, **kwargs):\n        \"\"\"GetFontEncoding(self) -> int\"\"\"\n        return _controls_.TextAttr_GetFontEncoding(*args, **kwargs)\n\n    def GetFontFamily(*args, **kwargs):\n        \"\"\"GetFontFamily(self) -> int\"\"\"\n        return _controls_.TextAttr_GetFontFamily(*args, **kwargs)\n\n    def GetFont(*args, **kwargs):\n        \"\"\"GetFont(self) -> Font\"\"\"\n        return _controls_.TextAttr_GetFont(*args, **kwargs)\n\n    CreateFont = GetFont \n    def GetCharacterStyleName(*args, **kwargs):\n        \"\"\"GetCharacterStyleName(self) -> String\"\"\"\n        return _controls_.TextAttr_GetCharacterStyleName(*args, **kwargs)\n\n    def GetParagraphStyleName(*args, **kwargs):\n        \"\"\"GetParagraphStyleName(self) -> String\"\"\"\n        return _controls_.TextAttr_GetParagraphStyleName(*args, **kwargs)\n\n    def GetListStyleName(*args, **kwargs):\n        \"\"\"GetListStyleName(self) -> String\"\"\"\n        return _controls_.TextAttr_GetListStyleName(*args, **kwargs)\n\n    def GetParagraphSpacingAfter(*args, **kwargs):\n        \"\"\"GetParagraphSpacingAfter(self) -> int\"\"\"\n        return _controls_.TextAttr_GetParagraphSpacingAfter(*args, **kwargs)\n\n    def GetParagraphSpacingBefore(*args, **kwargs):\n        \"\"\"GetParagraphSpacingBefore(self) -> int\"\"\"\n        return _controls_.TextAttr_GetParagraphSpacingBefore(*args, **kwargs)\n\n    def GetLineSpacing(*args, **kwargs):\n        \"\"\"GetLineSpacing(self) -> int\"\"\"\n        return _controls_.TextAttr_GetLineSpacing(*args, **kwargs)\n\n    def GetBulletStyle(*args, **kwargs):\n        \"\"\"GetBulletStyle(self) -> int\"\"\"\n        return _controls_.TextAttr_GetBulletStyle(*args, **kwargs)\n\n    def GetBulletNumber(*args, **kwargs):\n        \"\"\"GetBulletNumber(self) -> int\"\"\"\n        return _controls_.TextAttr_GetBulletNumber(*args, **kwargs)\n\n    def GetBulletText(*args, **kwargs):\n        \"\"\"GetBulletText(self) -> String\"\"\"\n        return _controls_.TextAttr_GetBulletText(*args, **kwargs)\n\n    def GetBulletFont(*args, **kwargs):\n        \"\"\"GetBulletFont(self) -> String\"\"\"\n        return _controls_.TextAttr_GetBulletFont(*args, **kwargs)\n\n    def GetBulletName(*args, **kwargs):\n        \"\"\"GetBulletName(self) -> String\"\"\"\n        return _controls_.TextAttr_GetBulletName(*args, **kwargs)\n\n    def GetURL(*args, **kwargs):\n        \"\"\"GetURL(self) -> String\"\"\"\n        return _controls_.TextAttr_GetURL(*args, **kwargs)\n\n    def GetTextEffects(*args, **kwargs):\n        \"\"\"GetTextEffects(self) -> int\"\"\"\n        return _controls_.TextAttr_GetTextEffects(*args, **kwargs)\n\n    def GetTextEffectFlags(*args, **kwargs):\n        \"\"\"GetTextEffectFlags(self) -> int\"\"\"\n        return _controls_.TextAttr_GetTextEffectFlags(*args, **kwargs)\n\n    def GetOutlineLevel(*args, **kwargs):\n        \"\"\"GetOutlineLevel(self) -> int\"\"\"\n        return _controls_.TextAttr_GetOutlineLevel(*args, **kwargs)\n\n    def HasTextColour(*args, **kwargs):\n        \"\"\"HasTextColour(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasTextColour(*args, **kwargs)\n\n    def HasBackgroundColour(*args, **kwargs):\n        \"\"\"HasBackgroundColour(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasBackgroundColour(*args, **kwargs)\n\n    def HasAlignment(*args, **kwargs):\n        \"\"\"HasAlignment(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasAlignment(*args, **kwargs)\n\n    def HasTabs(*args, **kwargs):\n        \"\"\"HasTabs(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasTabs(*args, **kwargs)\n\n    def HasLeftIndent(*args, **kwargs):\n        \"\"\"HasLeftIndent(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasLeftIndent(*args, **kwargs)\n\n    def HasRightIndent(*args, **kwargs):\n        \"\"\"HasRightIndent(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasRightIndent(*args, **kwargs)\n\n    def HasFontWeight(*args, **kwargs):\n        \"\"\"HasFontWeight(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasFontWeight(*args, **kwargs)\n\n    def HasFontSize(*args, **kwargs):\n        \"\"\"HasFontSize(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasFontSize(*args, **kwargs)\n\n    def HasFontItalic(*args, **kwargs):\n        \"\"\"HasFontItalic(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasFontItalic(*args, **kwargs)\n\n    def HasFontUnderlined(*args, **kwargs):\n        \"\"\"HasFontUnderlined(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasFontUnderlined(*args, **kwargs)\n\n    def HasFontStrikethrough(*args, **kwargs):\n        \"\"\"HasFontStrikethrough(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasFontStrikethrough(*args, **kwargs)\n\n    def HasFontFaceName(*args, **kwargs):\n        \"\"\"HasFontFaceName(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasFontFaceName(*args, **kwargs)\n\n    def HasFontEncoding(*args, **kwargs):\n        \"\"\"HasFontEncoding(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasFontEncoding(*args, **kwargs)\n\n    def HasFontFamily(*args, **kwargs):\n        \"\"\"HasFontFamily(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasFontFamily(*args, **kwargs)\n\n    def HasFont(*args, **kwargs):\n        \"\"\"HasFont(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasFont(*args, **kwargs)\n\n    def HasParagraphSpacingAfter(*args, **kwargs):\n        \"\"\"HasParagraphSpacingAfter(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasParagraphSpacingAfter(*args, **kwargs)\n\n    def HasParagraphSpacingBefore(*args, **kwargs):\n        \"\"\"HasParagraphSpacingBefore(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasParagraphSpacingBefore(*args, **kwargs)\n\n    def HasLineSpacing(*args, **kwargs):\n        \"\"\"HasLineSpacing(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasLineSpacing(*args, **kwargs)\n\n    def HasCharacterStyleName(*args, **kwargs):\n        \"\"\"HasCharacterStyleName(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasCharacterStyleName(*args, **kwargs)\n\n    def HasParagraphStyleName(*args, **kwargs):\n        \"\"\"HasParagraphStyleName(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasParagraphStyleName(*args, **kwargs)\n\n    def HasListStyleName(*args, **kwargs):\n        \"\"\"HasListStyleName(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasListStyleName(*args, **kwargs)\n\n    def HasBulletStyle(*args, **kwargs):\n        \"\"\"HasBulletStyle(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasBulletStyle(*args, **kwargs)\n\n    def HasBulletNumber(*args, **kwargs):\n        \"\"\"HasBulletNumber(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasBulletNumber(*args, **kwargs)\n\n    def HasBulletText(*args, **kwargs):\n        \"\"\"HasBulletText(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasBulletText(*args, **kwargs)\n\n    def HasBulletName(*args, **kwargs):\n        \"\"\"HasBulletName(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasBulletName(*args, **kwargs)\n\n    def HasURL(*args, **kwargs):\n        \"\"\"HasURL(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasURL(*args, **kwargs)\n\n    def HasPageBreak(*args, **kwargs):\n        \"\"\"HasPageBreak(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasPageBreak(*args, **kwargs)\n\n    def HasTextEffects(*args, **kwargs):\n        \"\"\"HasTextEffects(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasTextEffects(*args, **kwargs)\n\n    def HasTextEffect(*args, **kwargs):\n        \"\"\"HasTextEffect(self, int effect) -> bool\"\"\"\n        return _controls_.TextAttr_HasTextEffect(*args, **kwargs)\n\n    def HasOutlineLevel(*args, **kwargs):\n        \"\"\"HasOutlineLevel(self) -> bool\"\"\"\n        return _controls_.TextAttr_HasOutlineLevel(*args, **kwargs)\n\n    def HasFlag(*args, **kwargs):\n        \"\"\"HasFlag(self, long flag) -> bool\"\"\"\n        return _controls_.TextAttr_HasFlag(*args, **kwargs)\n\n    def RemoveFlag(*args, **kwargs):\n        \"\"\"RemoveFlag(self, long flag)\"\"\"\n        return _controls_.TextAttr_RemoveFlag(*args, **kwargs)\n\n    def AddFlag(*args, **kwargs):\n        \"\"\"AddFlag(self, long flag)\"\"\"\n        return _controls_.TextAttr_AddFlag(*args, **kwargs)\n\n    def IsCharacterStyle(*args, **kwargs):\n        \"\"\"IsCharacterStyle(self) -> bool\"\"\"\n        return _controls_.TextAttr_IsCharacterStyle(*args, **kwargs)\n\n    def IsParagraphStyle(*args, **kwargs):\n        \"\"\"IsParagraphStyle(self) -> bool\"\"\"\n        return _controls_.TextAttr_IsParagraphStyle(*args, **kwargs)\n\n    def IsDefault(*args, **kwargs):\n        \"\"\"IsDefault(self) -> bool\"\"\"\n        return _controls_.TextAttr_IsDefault(*args, **kwargs)\n\n    def Apply(*args, **kwargs):\n        \"\"\"Apply(self, TextAttr style, TextAttr compareWith=None) -> bool\"\"\"\n        return _controls_.TextAttr_Apply(*args, **kwargs)\n\n    def Merge(*args, **kwargs):\n        \"\"\"Merge(self, TextAttr overlay)\"\"\"\n        return _controls_.TextAttr_Merge(*args, **kwargs)\n\n    def Combine(*args, **kwargs):\n        \"\"\"Combine(TextAttr attr, TextAttr attrDef, TextCtrl text) -> TextAttr\"\"\"\n        return _controls_.TextAttr_Combine(*args, **kwargs)\n\n    Combine = staticmethod(Combine)\n    def TabsEq(*args, **kwargs):\n        \"\"\"TabsEq(wxArrayInt tabs1, wxArrayInt tabs2) -> bool\"\"\"\n        return _controls_.TextAttr_TabsEq(*args, **kwargs)\n\n    TabsEq = staticmethod(TabsEq)\n    def RemoveStyle(*args, **kwargs):\n        \"\"\"RemoveStyle(TextAttr destStyle, TextAttr style) -> bool\"\"\"\n        return _controls_.TextAttr_RemoveStyle(*args, **kwargs)\n\n    RemoveStyle = staticmethod(RemoveStyle)\n    def CombineBitlists(*args, **kwargs):\n        \"\"\"CombineBitlists(int valueA, int valueB, int flagsA, int flagsB) -> bool\"\"\"\n        return _controls_.TextAttr_CombineBitlists(*args, **kwargs)\n\n    CombineBitlists = staticmethod(CombineBitlists)\n    def BitlistsEqPartial(*args, **kwargs):\n        \"\"\"BitlistsEqPartial(int valueA, int valueB, int flags) -> bool\"\"\"\n        return _controls_.TextAttr_BitlistsEqPartial(*args, **kwargs)\n\n    BitlistsEqPartial = staticmethod(BitlistsEqPartial)\n    def SplitParaCharStyles(*args, **kwargs):\n        \"\"\"SplitParaCharStyles(TextAttr style, TextAttr parStyle, TextAttr charStyle) -> bool\"\"\"\n        return _controls_.TextAttr_SplitParaCharStyles(*args, **kwargs)\n\n    SplitParaCharStyles = staticmethod(SplitParaCharStyles)\n    Alignment = property(GetAlignment,SetAlignment) \n    BackgroundColour = property(GetBackgroundColour,SetBackgroundColour) \n    Flags = property(GetFlags,SetFlags) \n    Font = property(GetFont,SetFont) \n    LeftIndent = property(GetLeftIndent,SetLeftIndent) \n    LeftSubIndent = property(GetLeftSubIndent) \n    RightIndent = property(GetRightIndent,SetRightIndent) \n    Tabs = property(GetTabs,SetTabs) \n    TextColour = property(GetTextColour,SetTextColour) \n    FontSize = property(GetFontSize,SetFontSize) \n    FontStyle = property(GetFontStyle,SetFontStyle) \n    FontWeight = property(GetFontWeight,SetFontWeight) \n    FontUnderlined = property(GetFontUnderlined,SetFontUnderlined) \n    FontFaceName = property(GetFontFaceName,SetFontFaceName) \n    FontEncoding = property(GetFontEncoding,SetFontEncoding) \n    FontFamily = property(GetFontFamily,SetFontFamily) \n    CharacterStyleName = property(GetCharacterStyleName,SetCharacterStyleName) \n    ParagraphStyleName = property(GetParagraphStyleName,SetParagraphStyleName) \n    ListStyleName = property(GetListStyleName,SetListStyleName) \n    ParagraphSpacingAfter = property(GetParagraphSpacingAfter,SetParagraphSpacingAfter) \n    ParagraphSpacingBefore = property(GetParagraphSpacingBefore,SetParagraphSpacingBefore) \n    LineSpacing = property(GetLineSpacing,SetLineSpacing) \n    BulletStyle = property(GetBulletStyle,SetBulletStyle) \n    BulletNumber = property(GetBulletNumber,SetBulletNumber) \n    BulletText = property(GetBulletText,SetBulletText) \n    BulletFont = property(GetBulletFont,SetBulletFont) \n    BulletName = property(GetBulletName,SetBulletName) \n    URL = property(GetURL,SetURL) \n    TextEffects = property(GetTextEffects,SetTextEffects) \n    TextEffectFlags = property(GetTextEffectFlags,SetTextEffectFlags) \n    OutlineLevel = property(GetOutlineLevel,SetOutlineLevel) \n_controls_.TextAttr_swigregister(TextAttr)\nTextCtrlNameStr = cvar.TextCtrlNameStr\n\ndef TextAttr_Combine(*args, **kwargs):\n  \"\"\"TextAttr_Combine(TextAttr attr, TextAttr attrDef, TextCtrl text) -> TextAttr\"\"\"\n  return _controls_.TextAttr_Combine(*args, **kwargs)\n\ndef TextAttr_TabsEq(*args, **kwargs):\n  \"\"\"TextAttr_TabsEq(wxArrayInt tabs1, wxArrayInt tabs2) -> bool\"\"\"\n  return _controls_.TextAttr_TabsEq(*args, **kwargs)\n\ndef TextAttr_RemoveStyle(*args, **kwargs):\n  \"\"\"TextAttr_RemoveStyle(TextAttr destStyle, TextAttr style) -> bool\"\"\"\n  return _controls_.TextAttr_RemoveStyle(*args, **kwargs)\n\ndef TextAttr_CombineBitlists(*args, **kwargs):\n  \"\"\"TextAttr_CombineBitlists(int valueA, int valueB, int flagsA, int flagsB) -> bool\"\"\"\n  return _controls_.TextAttr_CombineBitlists(*args, **kwargs)\n\ndef TextAttr_BitlistsEqPartial(*args, **kwargs):\n  \"\"\"TextAttr_BitlistsEqPartial(int valueA, int valueB, int flags) -> bool\"\"\"\n  return _controls_.TextAttr_BitlistsEqPartial(*args, **kwargs)\n\ndef TextAttr_SplitParaCharStyles(*args, **kwargs):\n  \"\"\"TextAttr_SplitParaCharStyles(TextAttr style, TextAttr parStyle, TextAttr charStyle) -> bool\"\"\"\n  return _controls_.TextAttr_SplitParaCharStyles(*args, **kwargs)\n\nclass TextCtrl(_core.TextCtrlBase):\n    \"\"\"Proxy of C++ TextCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String value=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=TextCtrlNameStr) -> TextCtrl\n        \"\"\"\n        _controls_.TextCtrl_swiginit(self,_controls_.new_TextCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String value=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=TextCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.TextCtrl_Create(*args, **kwargs)\n\n    def IsSingleLine(*args, **kwargs):\n        \"\"\"IsSingleLine(self) -> bool\"\"\"\n        return _controls_.TextCtrl_IsSingleLine(*args, **kwargs)\n\n    def IsMultiLine(*args, **kwargs):\n        \"\"\"IsMultiLine(self) -> bool\"\"\"\n        return _controls_.TextCtrl_IsMultiLine(*args, **kwargs)\n\n    def EmulateKeyPress(*args, **kwargs):\n        \"\"\"EmulateKeyPress(self, KeyEvent event) -> bool\"\"\"\n        return _controls_.TextCtrl_EmulateKeyPress(*args, **kwargs)\n\n    def MacCheckSpelling(*args, **kwargs):\n        \"\"\"MacCheckSpelling(self, bool check)\"\"\"\n        return _controls_.TextCtrl_MacCheckSpelling(*args, **kwargs)\n\n    def SendTextUpdatedEvent(*args, **kwargs):\n        \"\"\"SendTextUpdatedEvent(self)\"\"\"\n        return _controls_.TextCtrl_SendTextUpdatedEvent(*args, **kwargs)\n\n    def write(*args, **kwargs):\n        \"\"\"write(self, String text)\"\"\"\n        return _controls_.TextCtrl_write(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.TextCtrl_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n_controls_.TextCtrl_swigregister(TextCtrl)\n\ndef PreTextCtrl(*args, **kwargs):\n    \"\"\"PreTextCtrl() -> TextCtrl\"\"\"\n    val = _controls_.new_PreTextCtrl(*args, **kwargs)\n    return val\n\ndef TextCtrl_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    TextCtrl_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.TextCtrl_GetClassDefaultAttributes(*args, **kwargs)\n\nwxEVT_COMMAND_TEXT_UPDATED = _controls_.wxEVT_COMMAND_TEXT_UPDATED\nwxEVT_COMMAND_TEXT_ENTER = _controls_.wxEVT_COMMAND_TEXT_ENTER\nwxEVT_COMMAND_TEXT_URL = _controls_.wxEVT_COMMAND_TEXT_URL\nwxEVT_COMMAND_TEXT_MAXLEN = _controls_.wxEVT_COMMAND_TEXT_MAXLEN\nclass TextUrlEvent(_core.CommandEvent):\n    \"\"\"Proxy of C++ TextUrlEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, int winid, MouseEvent evtMouse, long start, long end) -> TextUrlEvent\"\"\"\n        _controls_.TextUrlEvent_swiginit(self,_controls_.new_TextUrlEvent(*args, **kwargs))\n    def GetMouseEvent(*args, **kwargs):\n        \"\"\"GetMouseEvent(self) -> MouseEvent\"\"\"\n        return _controls_.TextUrlEvent_GetMouseEvent(*args, **kwargs)\n\n    def GetURLStart(*args, **kwargs):\n        \"\"\"GetURLStart(self) -> long\"\"\"\n        return _controls_.TextUrlEvent_GetURLStart(*args, **kwargs)\n\n    def GetURLEnd(*args, **kwargs):\n        \"\"\"GetURLEnd(self) -> long\"\"\"\n        return _controls_.TextUrlEvent_GetURLEnd(*args, **kwargs)\n\n    MouseEvent = property(GetMouseEvent,doc=\"See `GetMouseEvent`\") \n    URLEnd = property(GetURLEnd,doc=\"See `GetURLEnd`\") \n    URLStart = property(GetURLStart,doc=\"See `GetURLStart`\") \n_controls_.TextUrlEvent_swigregister(TextUrlEvent)\n\nEVT_TEXT        = wx.PyEventBinder( wxEVT_COMMAND_TEXT_UPDATED, 1)\nEVT_TEXT_ENTER  = wx.PyEventBinder( wxEVT_COMMAND_TEXT_ENTER, 1)\nEVT_TEXT_URL    = wx.PyEventBinder( wxEVT_COMMAND_TEXT_URL, 1)\nEVT_TEXT_MAXLEN = wx.PyEventBinder( wxEVT_COMMAND_TEXT_MAXLEN, 1)\n\n#---------------------------------------------------------------------------\n\nclass ScrollBar(_core.Control):\n    \"\"\"Proxy of C++ ScrollBar class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=SB_HORIZONTAL, \n            Validator validator=DefaultValidator, String name=ScrollBarNameStr) -> ScrollBar\n        \"\"\"\n        _controls_.ScrollBar_swiginit(self,_controls_.new_ScrollBar(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=SB_HORIZONTAL, \n            Validator validator=DefaultValidator, String name=ScrollBarNameStr) -> bool\n\n        Do the 2nd phase and create the GUI control.\n        \"\"\"\n        return _controls_.ScrollBar_Create(*args, **kwargs)\n\n    def GetThumbPosition(*args, **kwargs):\n        \"\"\"GetThumbPosition(self) -> int\"\"\"\n        return _controls_.ScrollBar_GetThumbPosition(*args, **kwargs)\n\n    def GetThumbSize(*args, **kwargs):\n        \"\"\"GetThumbSize(self) -> int\"\"\"\n        return _controls_.ScrollBar_GetThumbSize(*args, **kwargs)\n\n    GetThumbLength = GetThumbSize \n    def GetPageSize(*args, **kwargs):\n        \"\"\"GetPageSize(self) -> int\"\"\"\n        return _controls_.ScrollBar_GetPageSize(*args, **kwargs)\n\n    def GetRange(*args, **kwargs):\n        \"\"\"GetRange(self) -> int\"\"\"\n        return _controls_.ScrollBar_GetRange(*args, **kwargs)\n\n    def IsVertical(*args, **kwargs):\n        \"\"\"IsVertical(self) -> bool\"\"\"\n        return _controls_.ScrollBar_IsVertical(*args, **kwargs)\n\n    def SetThumbPosition(*args, **kwargs):\n        \"\"\"SetThumbPosition(self, int viewStart)\"\"\"\n        return _controls_.ScrollBar_SetThumbPosition(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.ScrollBar_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    PageSize = property(GetPageSize,doc=\"See `GetPageSize`\") \n    Range = property(GetRange,doc=\"See `GetRange`\") \n    ThumbPosition = property(GetThumbPosition,SetThumbPosition,doc=\"See `GetThumbPosition` and `SetThumbPosition`\") \n    ThumbSize = property(GetThumbSize,doc=\"See `GetThumbSize`\") \n_controls_.ScrollBar_swigregister(ScrollBar)\nScrollBarNameStr = cvar.ScrollBarNameStr\n\ndef PreScrollBar(*args, **kwargs):\n    \"\"\"PreScrollBar() -> ScrollBar\"\"\"\n    val = _controls_.new_PreScrollBar(*args, **kwargs)\n    return val\n\ndef ScrollBar_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    ScrollBar_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.ScrollBar_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nSP_HORIZONTAL = _controls_.SP_HORIZONTAL\nSP_VERTICAL = _controls_.SP_VERTICAL\nSP_ARROW_KEYS = _controls_.SP_ARROW_KEYS\nSP_WRAP = _controls_.SP_WRAP\nclass SpinButton(_core.Control):\n    \"\"\"Proxy of C++ SpinButton class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=SP_HORIZONTAL, \n            String name=SPIN_BUTTON_NAME) -> SpinButton\n        \"\"\"\n        _controls_.SpinButton_swiginit(self,_controls_.new_SpinButton(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=SP_HORIZONTAL, \n            String name=SPIN_BUTTON_NAME) -> bool\n        \"\"\"\n        return _controls_.SpinButton_Create(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"GetValue(self) -> int\"\"\"\n        return _controls_.SpinButton_GetValue(*args, **kwargs)\n\n    def GetMin(*args, **kwargs):\n        \"\"\"GetMin(self) -> int\"\"\"\n        return _controls_.SpinButton_GetMin(*args, **kwargs)\n\n    def GetMax(*args, **kwargs):\n        \"\"\"GetMax(self) -> int\"\"\"\n        return _controls_.SpinButton_GetMax(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"SetValue(self, int val)\"\"\"\n        return _controls_.SpinButton_SetValue(*args, **kwargs)\n\n    def SetMin(*args, **kwargs):\n        \"\"\"SetMin(self, int minVal)\"\"\"\n        return _controls_.SpinButton_SetMin(*args, **kwargs)\n\n    def SetMax(*args, **kwargs):\n        \"\"\"SetMax(self, int maxVal)\"\"\"\n        return _controls_.SpinButton_SetMax(*args, **kwargs)\n\n    def SetRange(*args, **kwargs):\n        \"\"\"SetRange(self, int minVal, int maxVal)\"\"\"\n        return _controls_.SpinButton_SetRange(*args, **kwargs)\n\n    def IsVertical(*args, **kwargs):\n        \"\"\"IsVertical(self) -> bool\"\"\"\n        return _controls_.SpinButton_IsVertical(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.SpinButton_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    Max = property(GetMax,SetMax,doc=\"See `GetMax` and `SetMax`\") \n    Min = property(GetMin,SetMin,doc=\"See `GetMin` and `SetMin`\") \n    Value = property(GetValue,SetValue,doc=\"See `GetValue` and `SetValue`\") \n_controls_.SpinButton_swigregister(SpinButton)\nSPIN_BUTTON_NAME = cvar.SPIN_BUTTON_NAME\nSpinCtrlNameStr = cvar.SpinCtrlNameStr\n\ndef PreSpinButton(*args, **kwargs):\n    \"\"\"PreSpinButton() -> SpinButton\"\"\"\n    val = _controls_.new_PreSpinButton(*args, **kwargs)\n    return val\n\ndef SpinButton_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    SpinButton_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.SpinButton_GetClassDefaultAttributes(*args, **kwargs)\n\nclass SpinCtrl(_core.Control):\n    \"\"\"Proxy of C++ SpinCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String value=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=wxSP_ARROW_KEYS|wxALIGN_RIGHT, \n            int min=0, int max=100, int initial=0, String name=SpinCtrlNameStr) -> SpinCtrl\n        \"\"\"\n        _controls_.SpinCtrl_swiginit(self,_controls_.new_SpinCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String value=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=SP_ARROW_KEYS, int min=0, int max=100, \n            int initial=0, String name=SpinCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.SpinCtrl_Create(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"GetValue(self) -> int\"\"\"\n        return _controls_.SpinCtrl_GetValue(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"SetValue(self, int value)\"\"\"\n        return _controls_.SpinCtrl_SetValue(*args, **kwargs)\n\n    def SetValueString(*args, **kwargs):\n        \"\"\"SetValueString(self, String text)\"\"\"\n        return _controls_.SpinCtrl_SetValueString(*args, **kwargs)\n\n    def SetRange(*args, **kwargs):\n        \"\"\"SetRange(self, int minVal, int maxVal)\"\"\"\n        return _controls_.SpinCtrl_SetRange(*args, **kwargs)\n\n    def GetMin(*args, **kwargs):\n        \"\"\"GetMin(self) -> int\"\"\"\n        return _controls_.SpinCtrl_GetMin(*args, **kwargs)\n\n    def GetMax(*args, **kwargs):\n        \"\"\"GetMax(self) -> int\"\"\"\n        return _controls_.SpinCtrl_GetMax(*args, **kwargs)\n\n    def SetSelection(*args, **kwargs):\n        \"\"\"SetSelection(self, long from, long to)\"\"\"\n        return _controls_.SpinCtrl_SetSelection(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.SpinCtrl_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    Max = property(GetMax,doc=\"See `GetMax`\") \n    Min = property(GetMin,doc=\"See `GetMin`\") \n    Value = property(GetValue,SetValue,doc=\"See `GetValue` and `SetValue`\") \n_controls_.SpinCtrl_swigregister(SpinCtrl)\n\ndef PreSpinCtrl(*args, **kwargs):\n    \"\"\"PreSpinCtrl() -> SpinCtrl\"\"\"\n    val = _controls_.new_PreSpinCtrl(*args, **kwargs)\n    return val\n\ndef SpinCtrl_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    SpinCtrl_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.SpinCtrl_GetClassDefaultAttributes(*args, **kwargs)\n\nclass SpinEvent(_core.NotifyEvent):\n    \"\"\"Proxy of C++ SpinEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, EventType commandType=wxEVT_NULL, int winid=0) -> SpinEvent\"\"\"\n        _controls_.SpinEvent_swiginit(self,_controls_.new_SpinEvent(*args, **kwargs))\n    def GetPosition(*args, **kwargs):\n        \"\"\"GetPosition(self) -> int\"\"\"\n        return _controls_.SpinEvent_GetPosition(*args, **kwargs)\n\n    def SetPosition(*args, **kwargs):\n        \"\"\"SetPosition(self, int pos)\"\"\"\n        return _controls_.SpinEvent_SetPosition(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"GetValue(self) -> int\"\"\"\n        return _controls_.SpinEvent_GetValue(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"SetValue(self, int value)\"\"\"\n        return _controls_.SpinEvent_SetValue(*args, **kwargs)\n\n    Position = property(GetPosition,SetPosition) \n    Value = property(GetValue,SetValue) \n_controls_.SpinEvent_swigregister(SpinEvent)\n\nwxEVT_SPIN_UP = _controls_.wxEVT_SPIN_UP\nwxEVT_SPIN_DOWN = _controls_.wxEVT_SPIN_DOWN\nwxEVT_SPIN = _controls_.wxEVT_SPIN\nwxEVT_COMMAND_SPINCTRL_UPDATED = _controls_.wxEVT_COMMAND_SPINCTRL_UPDATED\nwxEVT_COMMAND_SPINCTRLDOUBLE_UPDATED = _controls_.wxEVT_COMMAND_SPINCTRLDOUBLE_UPDATED\nEVT_SPIN_UP   = wx.PyEventBinder( wxEVT_SPIN_UP, 1)\nEVT_SPIN_DOWN = wx.PyEventBinder( wxEVT_SPIN_DOWN, 1)\nEVT_SPIN      = wx.PyEventBinder( wxEVT_SPIN, 1)\nEVT_SPINCTRL  = wx.PyEventBinder( wxEVT_COMMAND_SPINCTRL_UPDATED, 1)\nEVT_SPINCTRLDOUBLE  = wx.PyEventBinder( wxEVT_COMMAND_SPINCTRLDOUBLE_UPDATED, 1)    \n\nclass SpinCtrlDouble(_core.Control):\n    \"\"\"Proxy of C++ SpinCtrlDouble class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=ID_ANY, String value=wxEmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=wxSP_ARROW_KEYS|wxALIGN_RIGHT, \n            double min=0, double max=100, double initial=0, \n            double inc=1, String name=\"wxSpinCtrlDouble\") -> SpinCtrlDouble\n        \"\"\"\n        _controls_.SpinCtrlDouble_swiginit(self,_controls_.new_SpinCtrlDouble(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=ID_ANY, String value=wxEmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=SP_ARROW_KEYS, double min=0, \n            double max=100, double initial=0, double inc=1, \n            String name=\"wxSpinCtrlDouble\") -> bool\n        \"\"\"\n        return _controls_.SpinCtrlDouble_Create(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"GetValue(self) -> double\"\"\"\n        return _controls_.SpinCtrlDouble_GetValue(*args, **kwargs)\n\n    def GetMin(*args, **kwargs):\n        \"\"\"GetMin(self) -> double\"\"\"\n        return _controls_.SpinCtrlDouble_GetMin(*args, **kwargs)\n\n    def GetMax(*args, **kwargs):\n        \"\"\"GetMax(self) -> double\"\"\"\n        return _controls_.SpinCtrlDouble_GetMax(*args, **kwargs)\n\n    def GetIncrement(*args, **kwargs):\n        \"\"\"GetIncrement(self) -> double\"\"\"\n        return _controls_.SpinCtrlDouble_GetIncrement(*args, **kwargs)\n\n    def GetDigits(*args, **kwargs):\n        \"\"\"GetDigits(self) -> unsigned int\"\"\"\n        return _controls_.SpinCtrlDouble_GetDigits(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"SetValue(self, double value)\"\"\"\n        return _controls_.SpinCtrlDouble_SetValue(*args, **kwargs)\n\n    def SetRange(*args, **kwargs):\n        \"\"\"SetRange(self, double minVal, double maxVal)\"\"\"\n        return _controls_.SpinCtrlDouble_SetRange(*args, **kwargs)\n\n    def SetMin(self, minVal):\n        self.SetRange(minVal, self.GetMax())\n    def SetMax(self, maxVal):\n        self.SetRange(self.GetMin(), maxVal)\n\n    def SetIncrement(*args, **kwargs):\n        \"\"\"SetIncrement(self, double inc)\"\"\"\n        return _controls_.SpinCtrlDouble_SetIncrement(*args, **kwargs)\n\n    def SetDigits(*args, **kwargs):\n        \"\"\"SetDigits(self, unsigned int digits)\"\"\"\n        return _controls_.SpinCtrlDouble_SetDigits(*args, **kwargs)\n\n    Value = property(GetValue,SetValue) \n    Min = property(GetMin,SetMin) \n    Max = property(GetMax,SetMax) \n    Increment = property(GetIncrement,SetIncrement) \n    Digits = property(GetDigits,SetDigits) \n_controls_.SpinCtrlDouble_swigregister(SpinCtrlDouble)\n\ndef PreSpinCtrlDouble(*args, **kwargs):\n    \"\"\"PreSpinCtrlDouble() -> SpinCtrlDouble\"\"\"\n    val = _controls_.new_PreSpinCtrlDouble(*args, **kwargs)\n    return val\n\nclass SpinDoubleEvent(_core.NotifyEvent):\n    \"\"\"Proxy of C++ SpinDoubleEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, EventType commandType=wxEVT_NULL, int winid=0, double value=0) -> SpinDoubleEvent\"\"\"\n        _controls_.SpinDoubleEvent_swiginit(self,_controls_.new_SpinDoubleEvent(*args, **kwargs))\n    def GetValue(*args, **kwargs):\n        \"\"\"GetValue(self) -> double\"\"\"\n        return _controls_.SpinDoubleEvent_GetValue(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"SetValue(self, double value)\"\"\"\n        return _controls_.SpinDoubleEvent_SetValue(*args, **kwargs)\n\n    Value = property(GetValue,SetValue) \n_controls_.SpinDoubleEvent_swigregister(SpinDoubleEvent)\n\nEVT_SPINCTRLDOUBLE = wx.PyEventBinder( wxEVT_COMMAND_SPINCTRLDOUBLE_UPDATED, 1 )\n\n#---------------------------------------------------------------------------\n\nclass RadioBox(_core.Control):\n    \"\"\"Proxy of C++ RadioBox class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            wxArrayString choices=wxPyEmptyStringArray, \n            int majorDimension=0, long style=RA_HORIZONTAL, \n            Validator validator=DefaultValidator, \n            String name=RadioBoxNameStr) -> RadioBox\n        \"\"\"\n        if kwargs.has_key('point'): kwargs['pos'] = kwargs['point'];del kwargs['point']\n        _controls_.RadioBox_swiginit(self,_controls_.new_RadioBox(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            wxArrayString choices=wxPyEmptyStringArray, \n            int majorDimension=0, long style=RA_HORIZONTAL, \n            Validator validator=DefaultValidator, \n            String name=RadioBoxNameStr) -> bool\n        \"\"\"\n        return _controls_.RadioBox_Create(*args, **kwargs)\n\n    def SetSelection(*args, **kwargs):\n        \"\"\"SetSelection(self, int n)\"\"\"\n        return _controls_.RadioBox_SetSelection(*args, **kwargs)\n\n    def GetSelection(*args, **kwargs):\n        \"\"\"GetSelection(self) -> int\"\"\"\n        return _controls_.RadioBox_GetSelection(*args, **kwargs)\n\n    def GetStringSelection(*args, **kwargs):\n        \"\"\"GetStringSelection(self) -> String\"\"\"\n        return _controls_.RadioBox_GetStringSelection(*args, **kwargs)\n\n    def SetStringSelection(*args, **kwargs):\n        \"\"\"SetStringSelection(self, String s) -> bool\"\"\"\n        return _controls_.RadioBox_SetStringSelection(*args, **kwargs)\n\n    def GetCount(*args, **kwargs):\n        \"\"\"GetCount(self) -> size_t\"\"\"\n        return _controls_.RadioBox_GetCount(*args, **kwargs)\n\n    def FindString(*args, **kwargs):\n        \"\"\"FindString(self, String s) -> int\"\"\"\n        return _controls_.RadioBox_FindString(*args, **kwargs)\n\n    def GetString(*args, **kwargs):\n        \"\"\"GetString(self, int n) -> String\"\"\"\n        return _controls_.RadioBox_GetString(*args, **kwargs)\n\n    def SetString(*args, **kwargs):\n        \"\"\"SetString(self, int n, String label)\"\"\"\n        return _controls_.RadioBox_SetString(*args, **kwargs)\n\n    GetItemLabel = GetString \n    SetItemLabel = SetString \n    def EnableItem(*args, **kwargs):\n        \"\"\"EnableItem(self, unsigned int n, bool enable=True)\"\"\"\n        return _controls_.RadioBox_EnableItem(*args, **kwargs)\n\n    def ShowItem(*args, **kwargs):\n        \"\"\"ShowItem(self, unsigned int n, bool show=True)\"\"\"\n        return _controls_.RadioBox_ShowItem(*args, **kwargs)\n\n    def IsItemEnabled(*args, **kwargs):\n        \"\"\"IsItemEnabled(self, unsigned int n) -> bool\"\"\"\n        return _controls_.RadioBox_IsItemEnabled(*args, **kwargs)\n\n    def IsItemShown(*args, **kwargs):\n        \"\"\"IsItemShown(self, unsigned int n) -> bool\"\"\"\n        return _controls_.RadioBox_IsItemShown(*args, **kwargs)\n\n    def GetColumnCount(*args, **kwargs):\n        \"\"\"GetColumnCount(self) -> unsigned int\"\"\"\n        return _controls_.RadioBox_GetColumnCount(*args, **kwargs)\n\n    def GetRowCount(*args, **kwargs):\n        \"\"\"GetRowCount(self) -> unsigned int\"\"\"\n        return _controls_.RadioBox_GetRowCount(*args, **kwargs)\n\n    def GetNextItem(*args, **kwargs):\n        \"\"\"GetNextItem(self, int item, int dir, long style) -> int\"\"\"\n        return _controls_.RadioBox_GetNextItem(*args, **kwargs)\n\n    def SetItemToolTip(*args, **kwargs):\n        \"\"\"SetItemToolTip(self, unsigned int item, String text)\"\"\"\n        return _controls_.RadioBox_SetItemToolTip(*args, **kwargs)\n\n    def GetItemToolTip(*args, **kwargs):\n        \"\"\"GetItemToolTip(self, unsigned int item) -> ToolTip\"\"\"\n        return _controls_.RadioBox_GetItemToolTip(*args, **kwargs)\n\n    def SetItemHelpText(*args, **kwargs):\n        \"\"\"SetItemHelpText(self, unsigned int n, String helpText)\"\"\"\n        return _controls_.RadioBox_SetItemHelpText(*args, **kwargs)\n\n    def GetItemHelpText(*args, **kwargs):\n        \"\"\"GetItemHelpText(self, unsigned int n) -> String\"\"\"\n        return _controls_.RadioBox_GetItemHelpText(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.RadioBox_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    ColumnCount = property(GetColumnCount,doc=\"See `GetColumnCount`\") \n    Count = property(GetCount,doc=\"See `GetCount`\") \n    RowCount = property(GetRowCount,doc=\"See `GetRowCount`\") \n    Selection = property(GetSelection,SetSelection,doc=\"See `GetSelection` and `SetSelection`\") \n    StringSelection = property(GetStringSelection,SetStringSelection,doc=\"See `GetStringSelection` and `SetStringSelection`\") \n_controls_.RadioBox_swigregister(RadioBox)\nRadioBoxNameStr = cvar.RadioBoxNameStr\nRadioButtonNameStr = cvar.RadioButtonNameStr\n\ndef PreRadioBox(*args, **kwargs):\n    \"\"\"PreRadioBox() -> RadioBox\"\"\"\n    val = _controls_.new_PreRadioBox(*args, **kwargs)\n    return val\n\ndef RadioBox_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    RadioBox_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.RadioBox_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nclass RadioButton(_core.Control):\n    \"\"\"Proxy of C++ RadioButton class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=RadioButtonNameStr) -> RadioButton\n        \"\"\"\n        _controls_.RadioButton_swiginit(self,_controls_.new_RadioButton(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=RadioButtonNameStr) -> bool\n        \"\"\"\n        return _controls_.RadioButton_Create(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"GetValue(self) -> bool\"\"\"\n        return _controls_.RadioButton_GetValue(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"SetValue(self, bool value)\"\"\"\n        return _controls_.RadioButton_SetValue(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.RadioButton_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    Value = property(GetValue,SetValue,doc=\"See `GetValue` and `SetValue`\") \n_controls_.RadioButton_swigregister(RadioButton)\n\ndef PreRadioButton(*args, **kwargs):\n    \"\"\"PreRadioButton() -> RadioButton\"\"\"\n    val = _controls_.new_PreRadioButton(*args, **kwargs)\n    return val\n\ndef RadioButton_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    RadioButton_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.RadioButton_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nSL_HORIZONTAL = _controls_.SL_HORIZONTAL\nSL_VERTICAL = _controls_.SL_VERTICAL\nSL_TICKS = _controls_.SL_TICKS\nSL_AUTOTICKS = _controls_.SL_AUTOTICKS\nSL_LEFT = _controls_.SL_LEFT\nSL_TOP = _controls_.SL_TOP\nSL_RIGHT = _controls_.SL_RIGHT\nSL_BOTTOM = _controls_.SL_BOTTOM\nSL_BOTH = _controls_.SL_BOTH\nSL_SELRANGE = _controls_.SL_SELRANGE\nSL_INVERSE = _controls_.SL_INVERSE\nSL_MIN_MAX_LABELS = _controls_.SL_MIN_MAX_LABELS\nSL_VALUE_LABEL = _controls_.SL_VALUE_LABEL\nSL_LABELS = _controls_.SL_LABELS\nclass Slider(_core.Control):\n    \"\"\"Proxy of C++ Slider class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, int value=0, int minValue=0, \n            int maxValue=100, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=SL_HORIZONTAL, \n            Validator validator=DefaultValidator, \n            String name=SliderNameStr) -> Slider\n        \"\"\"\n        if kwargs.has_key('point'): kwargs['pos'] = kwargs['point'];del kwargs['point']\n        _controls_.Slider_swiginit(self,_controls_.new_Slider(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, int value=0, int minValue=0, \n            int maxValue=100, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=SL_HORIZONTAL, \n            Validator validator=DefaultValidator, \n            String name=SliderNameStr) -> bool\n        \"\"\"\n        return _controls_.Slider_Create(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"GetValue(self) -> int\"\"\"\n        return _controls_.Slider_GetValue(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"SetValue(self, int value)\"\"\"\n        return _controls_.Slider_SetValue(*args, **kwargs)\n\n    def GetMin(*args, **kwargs):\n        \"\"\"GetMin(self) -> int\"\"\"\n        return _controls_.Slider_GetMin(*args, **kwargs)\n\n    def GetMax(*args, **kwargs):\n        \"\"\"GetMax(self) -> int\"\"\"\n        return _controls_.Slider_GetMax(*args, **kwargs)\n\n    def SetMin(*args, **kwargs):\n        \"\"\"SetMin(self, int minValue)\"\"\"\n        return _controls_.Slider_SetMin(*args, **kwargs)\n\n    def SetMax(*args, **kwargs):\n        \"\"\"SetMax(self, int maxValue)\"\"\"\n        return _controls_.Slider_SetMax(*args, **kwargs)\n\n    def SetRange(*args, **kwargs):\n        \"\"\"SetRange(self, int minValue, int maxValue)\"\"\"\n        return _controls_.Slider_SetRange(*args, **kwargs)\n\n    def GetRange(self):\n        return self.GetMin(), self.GetMax()\n\n    def SetLineSize(*args, **kwargs):\n        \"\"\"SetLineSize(self, int lineSize)\"\"\"\n        return _controls_.Slider_SetLineSize(*args, **kwargs)\n\n    def SetPageSize(*args, **kwargs):\n        \"\"\"SetPageSize(self, int pageSize)\"\"\"\n        return _controls_.Slider_SetPageSize(*args, **kwargs)\n\n    def GetLineSize(*args, **kwargs):\n        \"\"\"GetLineSize(self) -> int\"\"\"\n        return _controls_.Slider_GetLineSize(*args, **kwargs)\n\n    def GetPageSize(*args, **kwargs):\n        \"\"\"GetPageSize(self) -> int\"\"\"\n        return _controls_.Slider_GetPageSize(*args, **kwargs)\n\n    def SetThumbLength(*args, **kwargs):\n        \"\"\"SetThumbLength(self, int lenPixels)\"\"\"\n        return _controls_.Slider_SetThumbLength(*args, **kwargs)\n\n    def GetThumbLength(*args, **kwargs):\n        \"\"\"GetThumbLength(self) -> int\"\"\"\n        return _controls_.Slider_GetThumbLength(*args, **kwargs)\n\n    def SetTickFreq(*args, **kwargs):\n        \"\"\"SetTickFreq(self, int n, int pos=1)\"\"\"\n        return _controls_.Slider_SetTickFreq(*args, **kwargs)\n\n    def GetTickFreq(*args, **kwargs):\n        \"\"\"GetTickFreq(self) -> int\"\"\"\n        return _controls_.Slider_GetTickFreq(*args, **kwargs)\n\n    def ClearTicks(*args, **kwargs):\n        \"\"\"ClearTicks(self)\"\"\"\n        return _controls_.Slider_ClearTicks(*args, **kwargs)\n\n    def SetTick(*args, **kwargs):\n        \"\"\"SetTick(self, int tickPos)\"\"\"\n        return _controls_.Slider_SetTick(*args, **kwargs)\n\n    def ClearSel(*args, **kwargs):\n        \"\"\"ClearSel(self)\"\"\"\n        return _controls_.Slider_ClearSel(*args, **kwargs)\n\n    def GetSelEnd(*args, **kwargs):\n        \"\"\"GetSelEnd(self) -> int\"\"\"\n        return _controls_.Slider_GetSelEnd(*args, **kwargs)\n\n    def GetSelStart(*args, **kwargs):\n        \"\"\"GetSelStart(self) -> int\"\"\"\n        return _controls_.Slider_GetSelStart(*args, **kwargs)\n\n    def SetSelection(*args, **kwargs):\n        \"\"\"SetSelection(self, int min, int max)\"\"\"\n        return _controls_.Slider_SetSelection(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.Slider_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    LineSize = property(GetLineSize,SetLineSize,doc=\"See `GetLineSize` and `SetLineSize`\") \n    Max = property(GetMax,SetMax,doc=\"See `GetMax` and `SetMax`\") \n    Min = property(GetMin,SetMin,doc=\"See `GetMin` and `SetMin`\") \n    PageSize = property(GetPageSize,SetPageSize,doc=\"See `GetPageSize` and `SetPageSize`\") \n    SelEnd = property(GetSelEnd,doc=\"See `GetSelEnd`\") \n    SelStart = property(GetSelStart,doc=\"See `GetSelStart`\") \n    ThumbLength = property(GetThumbLength,SetThumbLength,doc=\"See `GetThumbLength` and `SetThumbLength`\") \n    TickFreq = property(GetTickFreq,SetTickFreq,doc=\"See `GetTickFreq` and `SetTickFreq`\") \n    Value = property(GetValue,SetValue,doc=\"See `GetValue` and `SetValue`\") \n_controls_.Slider_swigregister(Slider)\nSliderNameStr = cvar.SliderNameStr\n\ndef PreSlider(*args, **kwargs):\n    \"\"\"PreSlider() -> Slider\"\"\"\n    val = _controls_.new_PreSlider(*args, **kwargs)\n    return val\n\ndef Slider_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    Slider_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.Slider_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nwxEVT_COMMAND_TOGGLEBUTTON_CLICKED = _controls_.wxEVT_COMMAND_TOGGLEBUTTON_CLICKED\nEVT_TOGGLEBUTTON = wx.PyEventBinder( wxEVT_COMMAND_TOGGLEBUTTON_CLICKED, 1)\n\nclass ToggleButton(AnyButton):\n    \"\"\"Proxy of C++ ToggleButton class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=ToggleButtonNameStr) -> ToggleButton\n        \"\"\"\n        _controls_.ToggleButton_swiginit(self,_controls_.new_ToggleButton(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=ToggleButtonNameStr) -> bool\n        \"\"\"\n        return _controls_.ToggleButton_Create(*args, **kwargs)\n\n    def SetValue(*args, **kwargs):\n        \"\"\"SetValue(self, bool value)\"\"\"\n        return _controls_.ToggleButton_SetValue(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"GetValue(self) -> bool\"\"\"\n        return _controls_.ToggleButton_GetValue(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.ToggleButton_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    Value = property(GetValue,SetValue,doc=\"See `GetValue` and `SetValue`\") \n_controls_.ToggleButton_swigregister(ToggleButton)\nToggleButtonNameStr = cvar.ToggleButtonNameStr\n\ndef PreToggleButton(*args, **kwargs):\n    \"\"\"PreToggleButton() -> ToggleButton\"\"\"\n    val = _controls_.new_PreToggleButton(*args, **kwargs)\n    return val\n\ndef ToggleButton_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    ToggleButton_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.ToggleButton_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nNB_FIXEDWIDTH = _controls_.NB_FIXEDWIDTH\nNB_TOP = _controls_.NB_TOP\nNB_LEFT = _controls_.NB_LEFT\nNB_RIGHT = _controls_.NB_RIGHT\nNB_BOTTOM = _controls_.NB_BOTTOM\nNB_MULTILINE = _controls_.NB_MULTILINE\nNB_NOPAGETHEME = _controls_.NB_NOPAGETHEME\nNB_HITTEST_NOWHERE = _controls_.NB_HITTEST_NOWHERE\nNB_HITTEST_ONICON = _controls_.NB_HITTEST_ONICON\nNB_HITTEST_ONLABEL = _controls_.NB_HITTEST_ONLABEL\nNB_HITTEST_ONITEM = _controls_.NB_HITTEST_ONITEM\nNB_HITTEST_ONPAGE = _controls_.NB_HITTEST_ONPAGE\nclass Notebook(_core.BookCtrlBase):\n    \"\"\"Proxy of C++ Notebook class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=0, String name=NotebookNameStr) -> Notebook\n        \"\"\"\n        _controls_.Notebook_swiginit(self,_controls_.new_Notebook(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=0, String name=NotebookNameStr) -> bool\n        \"\"\"\n        return _controls_.Notebook_Create(*args, **kwargs)\n\n    def GetRowCount(*args, **kwargs):\n        \"\"\"GetRowCount(self) -> int\"\"\"\n        return _controls_.Notebook_GetRowCount(*args, **kwargs)\n\n    def SetPadding(*args, **kwargs):\n        \"\"\"SetPadding(self, Size padding)\"\"\"\n        return _controls_.Notebook_SetPadding(*args, **kwargs)\n\n    def SetTabSize(*args, **kwargs):\n        \"\"\"SetTabSize(self, Size sz)\"\"\"\n        return _controls_.Notebook_SetTabSize(*args, **kwargs)\n\n    def GetThemeBackgroundColour(*args, **kwargs):\n        \"\"\"GetThemeBackgroundColour(self) -> Colour\"\"\"\n        return _controls_.Notebook_GetThemeBackgroundColour(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.Notebook_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    def SendPageChangingEvent(*args, **kwargs):\n        \"\"\"SendPageChangingEvent(self, int nPage) -> bool\"\"\"\n        return _controls_.Notebook_SendPageChangingEvent(*args, **kwargs)\n\n    def SendPageChangedEvent(*args, **kwargs):\n        \"\"\"SendPageChangedEvent(self, int nPageOld, int nPageNew=-1)\"\"\"\n        return _controls_.Notebook_SendPageChangedEvent(*args, **kwargs)\n\n    RowCount = property(GetRowCount,doc=\"See `GetRowCount`\") \n    ThemeBackgroundColour = property(GetThemeBackgroundColour,doc=\"See `GetThemeBackgroundColour`\") \n_controls_.Notebook_swigregister(Notebook)\nNotebookNameStr = cvar.NotebookNameStr\n\ndef PreNotebook(*args, **kwargs):\n    \"\"\"PreNotebook() -> Notebook\"\"\"\n    val = _controls_.new_PreNotebook(*args, **kwargs)\n    return val\n\ndef Notebook_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    Notebook_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.Notebook_GetClassDefaultAttributes(*args, **kwargs)\n\nNotebookEvent = wx.BookCtrlEvent\n\nwxEVT_COMMAND_NOTEBOOK_PAGE_CHANGED = _controls_.wxEVT_COMMAND_NOTEBOOK_PAGE_CHANGED\nwxEVT_COMMAND_NOTEBOOK_PAGE_CHANGING = _controls_.wxEVT_COMMAND_NOTEBOOK_PAGE_CHANGING\n# wxNotebook events\nEVT_NOTEBOOK_PAGE_CHANGED  = wx.PyEventBinder( wxEVT_COMMAND_NOTEBOOK_PAGE_CHANGED, 1 )\nEVT_NOTEBOOK_PAGE_CHANGING = wx.PyEventBinder( wxEVT_COMMAND_NOTEBOOK_PAGE_CHANGING, 1 )\n\n#----------------------------------------------------------------------------\n\nclass NotebookPage(wx.Panel):\n    \"\"\"\n    There is an old (and apparently unsolvable) bug when placing a\n    window with a nonstandard background colour in a wx.Notebook on\n    wxGTK1, as the notbooks's background colour would always be used\n    when the window is refreshed.  The solution is to place a panel in\n    the notbook and the coloured window on the panel, sized to cover\n    the panel.  This simple class does that for you, just put an\n    instance of this in the notebook and make your regular window a\n    child of this one and it will handle the resize for you.\n    \"\"\"\n    def __init__(self, parent, id=-1,\n                 pos=wx.DefaultPosition, size=wx.DefaultSize,\n                 style=wx.TAB_TRAVERSAL, name=\"panel\"):\n        wx.Panel.__init__(self, parent, id, pos, size, style, name)\n        self.child = None\n        self.Bind(wx.EVT_SIZE, self.OnSize)\n\n    def OnSize(self, evt):\n        if self.child is None:\n            children = self.GetChildren()\n            if len(children):\n                self.child = children[0]\n        if self.child:\n            self.child.SetPosition((0,0))\n            self.child.SetSize(self.GetSize())\n\n\n#---------------------------------------------------------------------------\n\nLB_DEFAULT = _controls_.LB_DEFAULT\nLB_TOP = _controls_.LB_TOP\nLB_BOTTOM = _controls_.LB_BOTTOM\nLB_LEFT = _controls_.LB_LEFT\nLB_RIGHT = _controls_.LB_RIGHT\nLB_ALIGN_MASK = _controls_.LB_ALIGN_MASK\nclass Listbook(_core.BookCtrlBase):\n    \"\"\"Proxy of C++ Listbook class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=0, String name=EmptyString) -> Listbook\n        \"\"\"\n        _controls_.Listbook_swiginit(self,_controls_.new_Listbook(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=0, String name=EmptyString) -> bool\n        \"\"\"\n        return _controls_.Listbook_Create(*args, **kwargs)\n\n    def GetListView(*args, **kwargs):\n        \"\"\"GetListView(self) -> ListView\"\"\"\n        return _controls_.Listbook_GetListView(*args, **kwargs)\n\n    ListView = property(GetListView,doc=\"See `GetListView`\") \n_controls_.Listbook_swigregister(Listbook)\n\ndef PreListbook(*args, **kwargs):\n    \"\"\"PreListbook() -> Listbook\"\"\"\n    val = _controls_.new_PreListbook(*args, **kwargs)\n    return val\n\nListbookEvent = wx.BookCtrlEvent\n\nwxEVT_COMMAND_LISTBOOK_PAGE_CHANGED = _controls_.wxEVT_COMMAND_LISTBOOK_PAGE_CHANGED\nwxEVT_COMMAND_LISTBOOK_PAGE_CHANGING = _controls_.wxEVT_COMMAND_LISTBOOK_PAGE_CHANGING\nEVT_LISTBOOK_PAGE_CHANGED  = wx.PyEventBinder( wxEVT_COMMAND_LISTBOOK_PAGE_CHANGED, 1 )\nEVT_LISTBOOK_PAGE_CHANGING = wx.PyEventBinder( wxEVT_COMMAND_LISTBOOK_PAGE_CHANGING, 1 )\n\nCHB_DEFAULT = _controls_.CHB_DEFAULT\nCHB_TOP = _controls_.CHB_TOP\nCHB_BOTTOM = _controls_.CHB_BOTTOM\nCHB_LEFT = _controls_.CHB_LEFT\nCHB_RIGHT = _controls_.CHB_RIGHT\nCHB_ALIGN_MASK = _controls_.CHB_ALIGN_MASK\nclass Choicebook(_core.BookCtrlBase):\n    \"\"\"Proxy of C++ Choicebook class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id, Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, String name=EmptyString) -> Choicebook\n        \"\"\"\n        _controls_.Choicebook_swiginit(self,_controls_.new_Choicebook(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id, Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, String name=EmptyString) -> bool\n        \"\"\"\n        return _controls_.Choicebook_Create(*args, **kwargs)\n\n    def GetChoiceCtrl(*args, **kwargs):\n        \"\"\"GetChoiceCtrl(self) -> Choice\"\"\"\n        return _controls_.Choicebook_GetChoiceCtrl(*args, **kwargs)\n\n    ChoiceCtrl = property(GetChoiceCtrl,doc=\"See `GetChoiceCtrl`\") \n_controls_.Choicebook_swigregister(Choicebook)\n\ndef PreChoicebook(*args, **kwargs):\n    \"\"\"PreChoicebook() -> Choicebook\"\"\"\n    val = _controls_.new_PreChoicebook(*args, **kwargs)\n    return val\n\nChoicebookEvent = wx.BookCtrlEvent\n\nwxEVT_COMMAND_CHOICEBOOK_PAGE_CHANGED = _controls_.wxEVT_COMMAND_CHOICEBOOK_PAGE_CHANGED\nwxEVT_COMMAND_CHOICEBOOK_PAGE_CHANGING = _controls_.wxEVT_COMMAND_CHOICEBOOK_PAGE_CHANGING\nEVT_CHOICEBOOK_PAGE_CHANGED  = wx.PyEventBinder( wxEVT_COMMAND_CHOICEBOOK_PAGE_CHANGED, 1 )\nEVT_CHOICEBOOK_PAGE_CHANGING = wx.PyEventBinder( wxEVT_COMMAND_CHOICEBOOK_PAGE_CHANGING, 1 )\n\n#---------------------------------------------------------------------------\n\nclass Treebook(_core.BookCtrlBase):\n    \"\"\"Proxy of C++ Treebook class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id, Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=BK_DEFAULT, \n            String name=EmptyString) -> Treebook\n        \"\"\"\n        _controls_.Treebook_swiginit(self,_controls_.new_Treebook(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id, Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=BK_DEFAULT, \n            String name=EmptyString) -> bool\n        \"\"\"\n        return _controls_.Treebook_Create(*args, **kwargs)\n\n    def InsertSubPage(*args, **kwargs):\n        \"\"\"\n        InsertSubPage(self, size_t pos, Window page, String text, bool select=False, \n            int imageId=NOT_FOUND) -> bool\n        \"\"\"\n        return _controls_.Treebook_InsertSubPage(*args, **kwargs)\n\n    def AddSubPage(*args, **kwargs):\n        \"\"\"AddSubPage(self, Window page, String text, bool select=False, int imageId=NOT_FOUND) -> bool\"\"\"\n        return _controls_.Treebook_AddSubPage(*args, **kwargs)\n\n    def IsNodeExpanded(*args, **kwargs):\n        \"\"\"IsNodeExpanded(self, size_t pos) -> bool\"\"\"\n        return _controls_.Treebook_IsNodeExpanded(*args, **kwargs)\n\n    def ExpandNode(*args, **kwargs):\n        \"\"\"ExpandNode(self, size_t pos, bool expand=True) -> bool\"\"\"\n        return _controls_.Treebook_ExpandNode(*args, **kwargs)\n\n    def CollapseNode(*args, **kwargs):\n        \"\"\"CollapseNode(self, size_t pos) -> bool\"\"\"\n        return _controls_.Treebook_CollapseNode(*args, **kwargs)\n\n    def GetPageParent(*args, **kwargs):\n        \"\"\"GetPageParent(self, size_t pos) -> int\"\"\"\n        return _controls_.Treebook_GetPageParent(*args, **kwargs)\n\n    def GetTreeCtrl(*args, **kwargs):\n        \"\"\"GetTreeCtrl(self) -> TreeCtrl\"\"\"\n        return _controls_.Treebook_GetTreeCtrl(*args, **kwargs)\n\n    TreeCtrl = property(GetTreeCtrl,doc=\"See `GetTreeCtrl`\") \n_controls_.Treebook_swigregister(Treebook)\n\ndef PreTreebook(*args, **kwargs):\n    \"\"\"PreTreebook() -> Treebook\"\"\"\n    val = _controls_.new_PreTreebook(*args, **kwargs)\n    return val\n\nTreebookEvent = wx.BookCtrlEvent\n\nwxEVT_COMMAND_TREEBOOK_PAGE_CHANGED = _controls_.wxEVT_COMMAND_TREEBOOK_PAGE_CHANGED\nwxEVT_COMMAND_TREEBOOK_PAGE_CHANGING = _controls_.wxEVT_COMMAND_TREEBOOK_PAGE_CHANGING\nwxEVT_COMMAND_TREEBOOK_NODE_COLLAPSED = _controls_.wxEVT_COMMAND_TREEBOOK_NODE_COLLAPSED\nwxEVT_COMMAND_TREEBOOK_NODE_EXPANDED = _controls_.wxEVT_COMMAND_TREEBOOK_NODE_EXPANDED\nEVT_TREEBOOK_PAGE_CHANGED = wx.PyEventBinder( wxEVT_COMMAND_TREEBOOK_PAGE_CHANGED, 1 )\nEVT_TREEBOOK_PAGE_CHANGING = wx.PyEventBinder( wxEVT_COMMAND_TREEBOOK_PAGE_CHANGING, 1)\nEVT_TREEBOOK_NODE_COLLAPSED = wx.PyEventBinder( wxEVT_COMMAND_TREEBOOK_NODE_COLLAPSED, 1 )\nEVT_TREEBOOK_NODE_EXPANDED = wx.PyEventBinder( wxEVT_COMMAND_TREEBOOK_NODE_EXPANDED, 1 )\n\n#---------------------------------------------------------------------------\n\nclass Toolbook(_core.BookCtrlBase):\n    \"\"\"Proxy of C++ Toolbook class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id, Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=BK_DEFAULT, \n            String name=EmptyString) -> Toolbook\n        \"\"\"\n        _controls_.Toolbook_swiginit(self,_controls_.new_Toolbook(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id, Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, String name=wxEmptyString) -> bool\n        \"\"\"\n        return _controls_.Toolbook_Create(*args, **kwargs)\n\n    def GetToolBar(*args, **kwargs):\n        \"\"\"GetToolBar(self) -> ToolBarBase\"\"\"\n        return _controls_.Toolbook_GetToolBar(*args, **kwargs)\n\n    def Realize(*args, **kwargs):\n        \"\"\"Realize(self)\"\"\"\n        return _controls_.Toolbook_Realize(*args, **kwargs)\n\n    ToolBar = property(GetToolBar,doc=\"See `GetToolBar`\") \n_controls_.Toolbook_swigregister(Toolbook)\n\ndef PreToolbook(*args, **kwargs):\n    \"\"\"PreToolbook() -> Toolbook\"\"\"\n    val = _controls_.new_PreToolbook(*args, **kwargs)\n    return val\n\nToolbookEvent = wx.BookCtrlEvent\n\nwxEVT_COMMAND_TOOLBOOK_PAGE_CHANGED = _controls_.wxEVT_COMMAND_TOOLBOOK_PAGE_CHANGED\nwxEVT_COMMAND_TOOLBOOK_PAGE_CHANGING = _controls_.wxEVT_COMMAND_TOOLBOOK_PAGE_CHANGING\nEVT_TOOLBOOK_PAGE_CHANGED = wx.PyEventBinder( wxEVT_COMMAND_TOOLBOOK_PAGE_CHANGED, 1)\nEVT_TOOLBOOK_PAGE_CHANGING = wx.PyEventBinder( wxEVT_COMMAND_TOOLBOOK_PAGE_CHANGING, 1)\n\n#---------------------------------------------------------------------------\n\nTOOL_STYLE_BUTTON = _controls_.TOOL_STYLE_BUTTON\nTOOL_STYLE_SEPARATOR = _controls_.TOOL_STYLE_SEPARATOR\nTOOL_STYLE_CONTROL = _controls_.TOOL_STYLE_CONTROL\nTB_HORIZONTAL = _controls_.TB_HORIZONTAL\nTB_VERTICAL = _controls_.TB_VERTICAL\nTB_TOP = _controls_.TB_TOP\nTB_LEFT = _controls_.TB_LEFT\nTB_BOTTOM = _controls_.TB_BOTTOM\nTB_RIGHT = _controls_.TB_RIGHT\nTB_3DBUTTONS = _controls_.TB_3DBUTTONS\nTB_FLAT = _controls_.TB_FLAT\nTB_DOCKABLE = _controls_.TB_DOCKABLE\nTB_NOICONS = _controls_.TB_NOICONS\nTB_TEXT = _controls_.TB_TEXT\nTB_NODIVIDER = _controls_.TB_NODIVIDER\nTB_NOALIGN = _controls_.TB_NOALIGN\nTB_HORZ_LAYOUT = _controls_.TB_HORZ_LAYOUT\nTB_HORZ_TEXT = _controls_.TB_HORZ_TEXT\nTB_NO_TOOLTIPS = _controls_.TB_NO_TOOLTIPS\nclass ToolBarToolBase(_core.Object):\n    \"\"\"Proxy of C++ ToolBarToolBase class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    def __init__(self): raise AttributeError, \"No constructor defined\"\n    __repr__ = _swig_repr\n    def GetId(*args, **kwargs):\n        \"\"\"GetId(self) -> int\"\"\"\n        return _controls_.ToolBarToolBase_GetId(*args, **kwargs)\n\n    def GetControl(*args, **kwargs):\n        \"\"\"GetControl(self) -> Control\"\"\"\n        return _controls_.ToolBarToolBase_GetControl(*args, **kwargs)\n\n    def GetToolBar(*args, **kwargs):\n        \"\"\"GetToolBar(self) -> ToolBarBase\"\"\"\n        return _controls_.ToolBarToolBase_GetToolBar(*args, **kwargs)\n\n    def IsStretchable(*args, **kwargs):\n        \"\"\"IsStretchable(self) -> bool\"\"\"\n        return _controls_.ToolBarToolBase_IsStretchable(*args, **kwargs)\n\n    def IsButton(*args, **kwargs):\n        \"\"\"IsButton(self) -> int\"\"\"\n        return _controls_.ToolBarToolBase_IsButton(*args, **kwargs)\n\n    def IsControl(*args, **kwargs):\n        \"\"\"IsControl(self) -> int\"\"\"\n        return _controls_.ToolBarToolBase_IsControl(*args, **kwargs)\n\n    def IsSeparator(*args, **kwargs):\n        \"\"\"IsSeparator(self) -> int\"\"\"\n        return _controls_.ToolBarToolBase_IsSeparator(*args, **kwargs)\n\n    def IsStretchableSpace(*args, **kwargs):\n        \"\"\"IsStretchableSpace(self) -> bool\"\"\"\n        return _controls_.ToolBarToolBase_IsStretchableSpace(*args, **kwargs)\n\n    def GetStyle(*args, **kwargs):\n        \"\"\"GetStyle(self) -> int\"\"\"\n        return _controls_.ToolBarToolBase_GetStyle(*args, **kwargs)\n\n    def GetKind(*args, **kwargs):\n        \"\"\"GetKind(self) -> int\"\"\"\n        return _controls_.ToolBarToolBase_GetKind(*args, **kwargs)\n\n    def MakeStretchable(*args, **kwargs):\n        \"\"\"MakeStretchable(self)\"\"\"\n        return _controls_.ToolBarToolBase_MakeStretchable(*args, **kwargs)\n\n    def IsEnabled(*args, **kwargs):\n        \"\"\"IsEnabled(self) -> bool\"\"\"\n        return _controls_.ToolBarToolBase_IsEnabled(*args, **kwargs)\n\n    def IsToggled(*args, **kwargs):\n        \"\"\"IsToggled(self) -> bool\"\"\"\n        return _controls_.ToolBarToolBase_IsToggled(*args, **kwargs)\n\n    def CanBeToggled(*args, **kwargs):\n        \"\"\"CanBeToggled(self) -> bool\"\"\"\n        return _controls_.ToolBarToolBase_CanBeToggled(*args, **kwargs)\n\n    def GetNormalBitmap(*args, **kwargs):\n        \"\"\"GetNormalBitmap(self) -> Bitmap\"\"\"\n        return _controls_.ToolBarToolBase_GetNormalBitmap(*args, **kwargs)\n\n    def GetDisabledBitmap(*args, **kwargs):\n        \"\"\"GetDisabledBitmap(self) -> Bitmap\"\"\"\n        return _controls_.ToolBarToolBase_GetDisabledBitmap(*args, **kwargs)\n\n    def GetBitmap(*args, **kwargs):\n        \"\"\"GetBitmap(self) -> Bitmap\"\"\"\n        return _controls_.ToolBarToolBase_GetBitmap(*args, **kwargs)\n\n    def GetLabel(*args, **kwargs):\n        \"\"\"GetLabel(self) -> String\"\"\"\n        return _controls_.ToolBarToolBase_GetLabel(*args, **kwargs)\n\n    def GetShortHelp(*args, **kwargs):\n        \"\"\"GetShortHelp(self) -> String\"\"\"\n        return _controls_.ToolBarToolBase_GetShortHelp(*args, **kwargs)\n\n    def GetLongHelp(*args, **kwargs):\n        \"\"\"GetLongHelp(self) -> String\"\"\"\n        return _controls_.ToolBarToolBase_GetLongHelp(*args, **kwargs)\n\n    def Enable(*args, **kwargs):\n        \"\"\"Enable(self, bool enable) -> bool\"\"\"\n        return _controls_.ToolBarToolBase_Enable(*args, **kwargs)\n\n    def Toggle(*args, **kwargs):\n        \"\"\"Toggle(self)\"\"\"\n        return _controls_.ToolBarToolBase_Toggle(*args, **kwargs)\n\n    def SetToggle(*args, **kwargs):\n        \"\"\"SetToggle(self, bool toggle) -> bool\"\"\"\n        return _controls_.ToolBarToolBase_SetToggle(*args, **kwargs)\n\n    def SetShortHelp(*args, **kwargs):\n        \"\"\"SetShortHelp(self, String help) -> bool\"\"\"\n        return _controls_.ToolBarToolBase_SetShortHelp(*args, **kwargs)\n\n    def SetLongHelp(*args, **kwargs):\n        \"\"\"SetLongHelp(self, String help) -> bool\"\"\"\n        return _controls_.ToolBarToolBase_SetLongHelp(*args, **kwargs)\n\n    def SetNormalBitmap(*args, **kwargs):\n        \"\"\"SetNormalBitmap(self, Bitmap bmp)\"\"\"\n        return _controls_.ToolBarToolBase_SetNormalBitmap(*args, **kwargs)\n\n    def SetDisabledBitmap(*args, **kwargs):\n        \"\"\"SetDisabledBitmap(self, Bitmap bmp)\"\"\"\n        return _controls_.ToolBarToolBase_SetDisabledBitmap(*args, **kwargs)\n\n    def SetLabel(*args, **kwargs):\n        \"\"\"SetLabel(self, String label)\"\"\"\n        return _controls_.ToolBarToolBase_SetLabel(*args, **kwargs)\n\n    def Detach(*args, **kwargs):\n        \"\"\"Detach(self)\"\"\"\n        return _controls_.ToolBarToolBase_Detach(*args, **kwargs)\n\n    def Attach(*args, **kwargs):\n        \"\"\"Attach(self, ToolBarBase tbar)\"\"\"\n        return _controls_.ToolBarToolBase_Attach(*args, **kwargs)\n\n    def SetDropdownMenu(*args, **kwargs):\n        \"\"\"SetDropdownMenu(self, Menu menu)\"\"\"\n        return _controls_.ToolBarToolBase_SetDropdownMenu(*args, **kwargs)\n\n    def GetDropdownMenu(*args, **kwargs):\n        \"\"\"GetDropdownMenu(self) -> Menu\"\"\"\n        return _controls_.ToolBarToolBase_GetDropdownMenu(*args, **kwargs)\n\n    def GetClientData(*args, **kwargs):\n        \"\"\"GetClientData(self) -> PyObject\"\"\"\n        return _controls_.ToolBarToolBase_GetClientData(*args, **kwargs)\n\n    def SetClientData(*args, **kwargs):\n        \"\"\"SetClientData(self, PyObject clientData)\"\"\"\n        return _controls_.ToolBarToolBase_SetClientData(*args, **kwargs)\n\n    GetBitmap1 = GetNormalBitmap\n    GetBitmap2 = GetDisabledBitmap\n    SetBitmap1 = SetNormalBitmap\n    SetBitmap2 = SetDisabledBitmap\n\n    Bitmap = property(GetBitmap,doc=\"See `GetBitmap`\") \n    ClientData = property(GetClientData,SetClientData,doc=\"See `GetClientData` and `SetClientData`\") \n    Control = property(GetControl,doc=\"See `GetControl`\") \n    DisabledBitmap = property(GetDisabledBitmap,SetDisabledBitmap,doc=\"See `GetDisabledBitmap` and `SetDisabledBitmap`\") \n    Id = property(GetId,doc=\"See `GetId`\") \n    Kind = property(GetKind,doc=\"See `GetKind`\") \n    Label = property(GetLabel,SetLabel,doc=\"See `GetLabel` and `SetLabel`\") \n    LongHelp = property(GetLongHelp,SetLongHelp,doc=\"See `GetLongHelp` and `SetLongHelp`\") \n    NormalBitmap = property(GetNormalBitmap,SetNormalBitmap,doc=\"See `GetNormalBitmap` and `SetNormalBitmap`\") \n    ShortHelp = property(GetShortHelp,SetShortHelp,doc=\"See `GetShortHelp` and `SetShortHelp`\") \n    Style = property(GetStyle,doc=\"See `GetStyle`\") \n    ToolBar = property(GetToolBar,doc=\"See `GetToolBar`\") \n_controls_.ToolBarToolBase_swigregister(ToolBarToolBase)\n\nclass ToolBarBase(_core.Control):\n    \"\"\"Proxy of C++ ToolBarBase class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    def __init__(self): raise AttributeError, \"No constructor defined\"\n    __repr__ = _swig_repr\n    def DoAddTool(*args, **kwargs):\n        \"\"\"\n        DoAddTool(self, int id, String label, Bitmap bitmap, Bitmap bmpDisabled=wxNullBitmap, \n            int kind=ITEM_NORMAL, String shortHelp=EmptyString, \n            String longHelp=EmptyString, \n            PyObject clientData=None) -> ToolBarToolBase\n        \"\"\"\n        return _controls_.ToolBarBase_DoAddTool(*args, **kwargs)\n\n    def DoInsertTool(*args, **kwargs):\n        \"\"\"\n        DoInsertTool(self, size_t pos, int id, String label, Bitmap bitmap, Bitmap bmpDisabled=wxNullBitmap, \n            int kind=ITEM_NORMAL, \n            String shortHelp=EmptyString, String longHelp=EmptyString, \n            PyObject clientData=None) -> ToolBarToolBase\n        \"\"\"\n        return _controls_.ToolBarBase_DoInsertTool(*args, **kwargs)\n\n    # These match the original Add methods for this class, kept for\n    # backwards compatibility with versions < 2.3.3.\n\n\n    def AddTool(self, id, bitmap,\n                pushedBitmap = wx.NullBitmap,\n                isToggle = 0,\n                clientData = None,\n                shortHelpString = '',\n                longHelpString = '') :\n        '''Old style method to add a tool to the toolbar.'''\n        kind = wx.ITEM_NORMAL\n        if isToggle: kind = wx.ITEM_CHECK\n        return self.DoAddTool(id, '', bitmap, pushedBitmap, kind,\n                              shortHelpString, longHelpString, clientData)\n\n    def AddSimpleTool(self, id, bitmap,\n                      shortHelpString = '',\n                      longHelpString = '',\n                      isToggle = 0):\n        '''Old style method to add a tool to the toolbar.'''\n        kind = wx.ITEM_NORMAL\n        if isToggle: kind = wx.ITEM_CHECK\n        return self.DoAddTool(id, '', bitmap, wx.NullBitmap, kind,\n                              shortHelpString, longHelpString, None)\n\n    def InsertTool(self, pos, id, bitmap,\n                   pushedBitmap = wx.NullBitmap,\n                   isToggle = 0,\n                   clientData = None,\n                   shortHelpString = '',\n                   longHelpString = ''):\n        '''Old style method to insert a tool in the toolbar.'''\n        kind = wx.ITEM_NORMAL\n        if isToggle: kind = wx.ITEM_CHECK\n        return self.DoInsertTool(pos, id, '', bitmap, pushedBitmap, kind,\n                                 shortHelpString, longHelpString, clientData)\n\n    def InsertSimpleTool(self, pos, id, bitmap,\n                         shortHelpString = '',\n                         longHelpString = '',\n                         isToggle = 0):\n        '''Old style method to insert a tool in the toolbar.'''\n        kind = wx.ITEM_NORMAL\n        if isToggle: kind = wx.ITEM_CHECK\n        return self.DoInsertTool(pos, id, '', bitmap, wx.NullBitmap, kind,\n                                 shortHelpString, longHelpString, None)\n\n\n    # The following are the new toolbar Add methods starting with\n    # 2.3.3.  They are renamed to have 'Label' in the name so as to be\n    # able to keep backwards compatibility with using the above\n    # methods.  Eventually these should migrate to be the methods used\n    # primarily and lose the 'Label' in the name...\n\n    def AddLabelTool(self, id, label, bitmap,\n                     bmpDisabled = wx.NullBitmap,\n                     kind = wx.ITEM_NORMAL,\n                     shortHelp = '', longHelp = '',\n                     clientData = None):\n        '''\n        The full AddTool() function.\n\n        If bmpDisabled is wx.NullBitmap, a shadowed version of the normal bitmap\n        is created and used as the disabled image.\n        '''\n        return self.DoAddTool(id, label, bitmap, bmpDisabled, kind,\n                              shortHelp, longHelp, clientData)\n\n\n    def InsertLabelTool(self, pos, id, label, bitmap,\n                        bmpDisabled = wx.NullBitmap,\n                        kind = wx.ITEM_NORMAL,\n                        shortHelp = '', longHelp = '',\n                        clientData = None):\n        '''\n        Insert the new tool at the given position, if pos == GetToolsCount(), it\n        is equivalent to AddTool()\n        '''\n        return self.DoInsertTool(pos, id, label, bitmap, bmpDisabled, kind,\n                                 shortHelp, longHelp, clientData)\n\n    def AddCheckLabelTool(self, id, label, bitmap,\n                        bmpDisabled = wx.NullBitmap,\n                        shortHelp = '', longHelp = '',\n                        clientData = None):\n        '''Add a check tool, i.e. a tool which can be toggled'''\n        return self.DoAddTool(id, label, bitmap, bmpDisabled, wx.ITEM_CHECK,\n                              shortHelp, longHelp, clientData)\n\n    def AddRadioLabelTool(self, id, label, bitmap,\n                          bmpDisabled = wx.NullBitmap,\n                          shortHelp = '', longHelp = '',\n                          clientData = None):\n        '''\n        Add a radio tool, i.e. a tool which can be toggled and releases any\n        other toggled radio tools in the same group when it happens\n        '''\n        return self.DoAddTool(id, label, bitmap, bmpDisabled, wx.ITEM_RADIO,\n                              shortHelp, longHelp, clientData)\n\n\n    # For consistency with the backwards compatible methods above, here are\n    # some non-'Label' versions of the Check and Radio methods\n\n    def AddCheckTool(self, id, bitmap,\n                     bmpDisabled = wx.NullBitmap,\n                     shortHelp = '', longHelp = '',\n                     clientData = None):\n        '''Add a check tool, i.e. a tool which can be toggled'''\n        return self.DoAddTool(id, '', bitmap, bmpDisabled, wx.ITEM_CHECK,\n                              shortHelp, longHelp, clientData)\n\n    def AddRadioTool(self, id, bitmap,\n                     bmpDisabled = wx.NullBitmap,\n                     shortHelp = '', longHelp = '',\n                     clientData = None):\n        '''\n        Add a radio tool, i.e. a tool which can be toggled and releases any\n        other toggled radio tools in the same group when it happens\n        '''\n        return self.DoAddTool(id, '', bitmap, bmpDisabled, wx.ITEM_RADIO,\n                              shortHelp, longHelp, clientData)\n\n    def AddToolItem(*args, **kwargs):\n        \"\"\"AddToolItem(self, ToolBarToolBase tool) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_AddToolItem(*args, **kwargs)\n\n    def InsertToolItem(*args, **kwargs):\n        \"\"\"InsertToolItem(self, size_t pos, ToolBarToolBase tool) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_InsertToolItem(*args, **kwargs)\n\n    def AddControl(*args, **kwargs):\n        \"\"\"AddControl(self, Control control, String label=wxEmptyString) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_AddControl(*args, **kwargs)\n\n    def InsertControl(*args, **kwargs):\n        \"\"\"InsertControl(self, size_t pos, Control control, String label=wxEmptyString) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_InsertControl(*args, **kwargs)\n\n    def FindControl(*args, **kwargs):\n        \"\"\"FindControl(self, int id) -> Control\"\"\"\n        return _controls_.ToolBarBase_FindControl(*args, **kwargs)\n\n    def AddSeparator(*args, **kwargs):\n        \"\"\"AddSeparator(self) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_AddSeparator(*args, **kwargs)\n\n    def InsertSeparator(*args, **kwargs):\n        \"\"\"InsertSeparator(self, size_t pos) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_InsertSeparator(*args, **kwargs)\n\n    def AddStretchableSpace(*args, **kwargs):\n        \"\"\"AddStretchableSpace(self) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_AddStretchableSpace(*args, **kwargs)\n\n    def InsertStretchableSpace(*args, **kwargs):\n        \"\"\"InsertStretchableSpace(self, size_t pos) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_InsertStretchableSpace(*args, **kwargs)\n\n    def RemoveTool(*args, **kwargs):\n        \"\"\"RemoveTool(self, int id) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_RemoveTool(*args, **kwargs)\n\n    def DeleteToolByPos(*args, **kwargs):\n        \"\"\"DeleteToolByPos(self, size_t pos) -> bool\"\"\"\n        return _controls_.ToolBarBase_DeleteToolByPos(*args, **kwargs)\n\n    def DeleteTool(*args, **kwargs):\n        \"\"\"DeleteTool(self, int id) -> bool\"\"\"\n        return _controls_.ToolBarBase_DeleteTool(*args, **kwargs)\n\n    def ClearTools(*args, **kwargs):\n        \"\"\"ClearTools(self)\"\"\"\n        return _controls_.ToolBarBase_ClearTools(*args, **kwargs)\n\n    def Realize(*args, **kwargs):\n        \"\"\"Realize(self) -> bool\"\"\"\n        return _controls_.ToolBarBase_Realize(*args, **kwargs)\n\n    def EnableTool(*args, **kwargs):\n        \"\"\"EnableTool(self, int id, bool enable)\"\"\"\n        return _controls_.ToolBarBase_EnableTool(*args, **kwargs)\n\n    def ToggleTool(*args, **kwargs):\n        \"\"\"ToggleTool(self, int id, bool toggle)\"\"\"\n        return _controls_.ToolBarBase_ToggleTool(*args, **kwargs)\n\n    def SetToggle(*args, **kwargs):\n        \"\"\"SetToggle(self, int id, bool toggle)\"\"\"\n        return _controls_.ToolBarBase_SetToggle(*args, **kwargs)\n\n    def GetToolClientData(*args, **kwargs):\n        \"\"\"GetToolClientData(self, int id) -> PyObject\"\"\"\n        return _controls_.ToolBarBase_GetToolClientData(*args, **kwargs)\n\n    def SetToolClientData(*args, **kwargs):\n        \"\"\"SetToolClientData(self, int id, PyObject clientData)\"\"\"\n        return _controls_.ToolBarBase_SetToolClientData(*args, **kwargs)\n\n    def GetToolPos(*args, **kwargs):\n        \"\"\"GetToolPos(self, int id) -> int\"\"\"\n        return _controls_.ToolBarBase_GetToolPos(*args, **kwargs)\n\n    def GetToolState(*args, **kwargs):\n        \"\"\"GetToolState(self, int id) -> bool\"\"\"\n        return _controls_.ToolBarBase_GetToolState(*args, **kwargs)\n\n    def GetToolEnabled(*args, **kwargs):\n        \"\"\"GetToolEnabled(self, int id) -> bool\"\"\"\n        return _controls_.ToolBarBase_GetToolEnabled(*args, **kwargs)\n\n    def SetToolShortHelp(*args, **kwargs):\n        \"\"\"SetToolShortHelp(self, int id, String helpString)\"\"\"\n        return _controls_.ToolBarBase_SetToolShortHelp(*args, **kwargs)\n\n    def GetToolShortHelp(*args, **kwargs):\n        \"\"\"GetToolShortHelp(self, int id) -> String\"\"\"\n        return _controls_.ToolBarBase_GetToolShortHelp(*args, **kwargs)\n\n    def SetToolLongHelp(*args, **kwargs):\n        \"\"\"SetToolLongHelp(self, int id, String helpString)\"\"\"\n        return _controls_.ToolBarBase_SetToolLongHelp(*args, **kwargs)\n\n    def GetToolLongHelp(*args, **kwargs):\n        \"\"\"GetToolLongHelp(self, int id) -> String\"\"\"\n        return _controls_.ToolBarBase_GetToolLongHelp(*args, **kwargs)\n\n    def SetMarginsXY(*args, **kwargs):\n        \"\"\"SetMarginsXY(self, int x, int y)\"\"\"\n        return _controls_.ToolBarBase_SetMarginsXY(*args, **kwargs)\n\n    def SetMargins(*args, **kwargs):\n        \"\"\"SetMargins(self, Size size)\"\"\"\n        return _controls_.ToolBarBase_SetMargins(*args, **kwargs)\n\n    def SetToolPacking(*args, **kwargs):\n        \"\"\"SetToolPacking(self, int packing)\"\"\"\n        return _controls_.ToolBarBase_SetToolPacking(*args, **kwargs)\n\n    def SetToolSeparation(*args, **kwargs):\n        \"\"\"SetToolSeparation(self, int separation)\"\"\"\n        return _controls_.ToolBarBase_SetToolSeparation(*args, **kwargs)\n\n    def GetToolMargins(*args, **kwargs):\n        \"\"\"GetToolMargins(self) -> Size\"\"\"\n        return _controls_.ToolBarBase_GetToolMargins(*args, **kwargs)\n\n    def GetMargins(*args, **kwargs):\n        \"\"\"GetMargins(self) -> Size\"\"\"\n        return _controls_.ToolBarBase_GetMargins(*args, **kwargs)\n\n    def GetToolPacking(*args, **kwargs):\n        \"\"\"GetToolPacking(self) -> int\"\"\"\n        return _controls_.ToolBarBase_GetToolPacking(*args, **kwargs)\n\n    def GetToolSeparation(*args, **kwargs):\n        \"\"\"GetToolSeparation(self) -> int\"\"\"\n        return _controls_.ToolBarBase_GetToolSeparation(*args, **kwargs)\n\n    def SetRows(*args, **kwargs):\n        \"\"\"SetRows(self, int nRows)\"\"\"\n        return _controls_.ToolBarBase_SetRows(*args, **kwargs)\n\n    def SetMaxRowsCols(*args, **kwargs):\n        \"\"\"SetMaxRowsCols(self, int rows, int cols)\"\"\"\n        return _controls_.ToolBarBase_SetMaxRowsCols(*args, **kwargs)\n\n    def GetMaxRows(*args, **kwargs):\n        \"\"\"GetMaxRows(self) -> int\"\"\"\n        return _controls_.ToolBarBase_GetMaxRows(*args, **kwargs)\n\n    def GetMaxCols(*args, **kwargs):\n        \"\"\"GetMaxCols(self) -> int\"\"\"\n        return _controls_.ToolBarBase_GetMaxCols(*args, **kwargs)\n\n    def SetToolBitmapSize(*args, **kwargs):\n        \"\"\"SetToolBitmapSize(self, Size size)\"\"\"\n        return _controls_.ToolBarBase_SetToolBitmapSize(*args, **kwargs)\n\n    def GetToolBitmapSize(*args, **kwargs):\n        \"\"\"GetToolBitmapSize(self) -> Size\"\"\"\n        return _controls_.ToolBarBase_GetToolBitmapSize(*args, **kwargs)\n\n    def GetToolSize(*args, **kwargs):\n        \"\"\"GetToolSize(self) -> Size\"\"\"\n        return _controls_.ToolBarBase_GetToolSize(*args, **kwargs)\n\n    def FindToolForPosition(*args, **kwargs):\n        \"\"\"FindToolForPosition(self, int x, int y) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_FindToolForPosition(*args, **kwargs)\n\n    def FindById(*args, **kwargs):\n        \"\"\"FindById(self, int toolid) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_FindById(*args, **kwargs)\n\n    def IsVertical(*args, **kwargs):\n        \"\"\"IsVertical(self) -> bool\"\"\"\n        return _controls_.ToolBarBase_IsVertical(*args, **kwargs)\n\n    def GetToolsCount(*args, **kwargs):\n        \"\"\"GetToolsCount(self) -> size_t\"\"\"\n        return _controls_.ToolBarBase_GetToolsCount(*args, **kwargs)\n\n    def GetToolByPos(*args, **kwargs):\n        \"\"\"GetToolByPos(self, int pos) -> ToolBarToolBase\"\"\"\n        return _controls_.ToolBarBase_GetToolByPos(*args, **kwargs)\n\n    def SetDropdownMenu(*args, **kwargs):\n        \"\"\"SetDropdownMenu(self, int toolid, Menu menu) -> bool\"\"\"\n        return _controls_.ToolBarBase_SetDropdownMenu(*args, **kwargs)\n\n    Margins = property(GetMargins,SetMargins,doc=\"See `GetMargins` and `SetMargins`\") \n    MaxCols = property(GetMaxCols,doc=\"See `GetMaxCols`\") \n    MaxRows = property(GetMaxRows,doc=\"See `GetMaxRows`\") \n    ToolBitmapSize = property(GetToolBitmapSize,SetToolBitmapSize,doc=\"See `GetToolBitmapSize` and `SetToolBitmapSize`\") \n    ToolMargins = property(GetToolMargins,doc=\"See `GetToolMargins`\") \n    ToolPacking = property(GetToolPacking,SetToolPacking,doc=\"See `GetToolPacking` and `SetToolPacking`\") \n    ToolSeparation = property(GetToolSeparation,SetToolSeparation,doc=\"See `GetToolSeparation` and `SetToolSeparation`\") \n    ToolSize = property(GetToolSize,doc=\"See `GetToolSize`\") \n    ToolsCount = property(GetToolsCount,doc=\"See `GetToolsCount`\") \n_controls_.ToolBarBase_swigregister(ToolBarBase)\n\nclass ToolBar(ToolBarBase):\n    \"\"\"Proxy of C++ ToolBar class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=wxNO_BORDER|wxTB_HORIZONTAL, \n            String name=wxPyToolBarNameStr) -> ToolBar\n        \"\"\"\n        _controls_.ToolBar_swiginit(self,_controls_.new_ToolBar(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=wxNO_BORDER|wxTB_HORIZONTAL, \n            String name=wxPyToolBarNameStr) -> bool\n        \"\"\"\n        return _controls_.ToolBar_Create(*args, **kwargs)\n\n    def SetToolNormalBitmap(*args, **kwargs):\n        \"\"\"SetToolNormalBitmap(self, int id, Bitmap bitmap)\"\"\"\n        return _controls_.ToolBar_SetToolNormalBitmap(*args, **kwargs)\n\n    def SetToolDisabledBitmap(*args, **kwargs):\n        \"\"\"SetToolDisabledBitmap(self, int id, Bitmap bitmap)\"\"\"\n        return _controls_.ToolBar_SetToolDisabledBitmap(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.ToolBar_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n_controls_.ToolBar_swigregister(ToolBar)\n\ndef PreToolBar(*args, **kwargs):\n    \"\"\"PreToolBar() -> ToolBar\"\"\"\n    val = _controls_.new_PreToolBar(*args, **kwargs)\n    return val\n\ndef ToolBar_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    ToolBar_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.ToolBar_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nLC_VRULES = _controls_.LC_VRULES\nLC_HRULES = _controls_.LC_HRULES\nLC_ICON = _controls_.LC_ICON\nLC_SMALL_ICON = _controls_.LC_SMALL_ICON\nLC_LIST = _controls_.LC_LIST\nLC_REPORT = _controls_.LC_REPORT\nLC_ALIGN_TOP = _controls_.LC_ALIGN_TOP\nLC_ALIGN_LEFT = _controls_.LC_ALIGN_LEFT\nLC_AUTOARRANGE = _controls_.LC_AUTOARRANGE\nLC_VIRTUAL = _controls_.LC_VIRTUAL\nLC_EDIT_LABELS = _controls_.LC_EDIT_LABELS\nLC_NO_HEADER = _controls_.LC_NO_HEADER\nLC_NO_SORT_HEADER = _controls_.LC_NO_SORT_HEADER\nLC_SINGLE_SEL = _controls_.LC_SINGLE_SEL\nLC_SORT_ASCENDING = _controls_.LC_SORT_ASCENDING\nLC_SORT_DESCENDING = _controls_.LC_SORT_DESCENDING\nLC_MASK_TYPE = _controls_.LC_MASK_TYPE\nLC_MASK_ALIGN = _controls_.LC_MASK_ALIGN\nLC_MASK_SORT = _controls_.LC_MASK_SORT\nLIST_MASK_STATE = _controls_.LIST_MASK_STATE\nLIST_MASK_TEXT = _controls_.LIST_MASK_TEXT\nLIST_MASK_IMAGE = _controls_.LIST_MASK_IMAGE\nLIST_MASK_DATA = _controls_.LIST_MASK_DATA\nLIST_SET_ITEM = _controls_.LIST_SET_ITEM\nLIST_MASK_WIDTH = _controls_.LIST_MASK_WIDTH\nLIST_MASK_FORMAT = _controls_.LIST_MASK_FORMAT\nLIST_STATE_DONTCARE = _controls_.LIST_STATE_DONTCARE\nLIST_STATE_DROPHILITED = _controls_.LIST_STATE_DROPHILITED\nLIST_STATE_FOCUSED = _controls_.LIST_STATE_FOCUSED\nLIST_STATE_SELECTED = _controls_.LIST_STATE_SELECTED\nLIST_STATE_CUT = _controls_.LIST_STATE_CUT\nLIST_STATE_DISABLED = _controls_.LIST_STATE_DISABLED\nLIST_STATE_FILTERED = _controls_.LIST_STATE_FILTERED\nLIST_STATE_INUSE = _controls_.LIST_STATE_INUSE\nLIST_STATE_PICKED = _controls_.LIST_STATE_PICKED\nLIST_STATE_SOURCE = _controls_.LIST_STATE_SOURCE\nLIST_HITTEST_ABOVE = _controls_.LIST_HITTEST_ABOVE\nLIST_HITTEST_BELOW = _controls_.LIST_HITTEST_BELOW\nLIST_HITTEST_NOWHERE = _controls_.LIST_HITTEST_NOWHERE\nLIST_HITTEST_ONITEMICON = _controls_.LIST_HITTEST_ONITEMICON\nLIST_HITTEST_ONITEMLABEL = _controls_.LIST_HITTEST_ONITEMLABEL\nLIST_HITTEST_ONITEMRIGHT = _controls_.LIST_HITTEST_ONITEMRIGHT\nLIST_HITTEST_ONITEMSTATEICON = _controls_.LIST_HITTEST_ONITEMSTATEICON\nLIST_HITTEST_TOLEFT = _controls_.LIST_HITTEST_TOLEFT\nLIST_HITTEST_TORIGHT = _controls_.LIST_HITTEST_TORIGHT\nLIST_HITTEST_ONITEM = _controls_.LIST_HITTEST_ONITEM\nLIST_GETSUBITEMRECT_WHOLEITEM = _controls_.LIST_GETSUBITEMRECT_WHOLEITEM\nLIST_NEXT_ABOVE = _controls_.LIST_NEXT_ABOVE\nLIST_NEXT_ALL = _controls_.LIST_NEXT_ALL\nLIST_NEXT_BELOW = _controls_.LIST_NEXT_BELOW\nLIST_NEXT_LEFT = _controls_.LIST_NEXT_LEFT\nLIST_NEXT_RIGHT = _controls_.LIST_NEXT_RIGHT\nLIST_ALIGN_DEFAULT = _controls_.LIST_ALIGN_DEFAULT\nLIST_ALIGN_LEFT = _controls_.LIST_ALIGN_LEFT\nLIST_ALIGN_TOP = _controls_.LIST_ALIGN_TOP\nLIST_ALIGN_SNAP_TO_GRID = _controls_.LIST_ALIGN_SNAP_TO_GRID\nLIST_FORMAT_LEFT = _controls_.LIST_FORMAT_LEFT\nLIST_FORMAT_RIGHT = _controls_.LIST_FORMAT_RIGHT\nLIST_FORMAT_CENTRE = _controls_.LIST_FORMAT_CENTRE\nLIST_FORMAT_CENTER = _controls_.LIST_FORMAT_CENTER\nLIST_AUTOSIZE = _controls_.LIST_AUTOSIZE\nLIST_AUTOSIZE_USEHEADER = _controls_.LIST_AUTOSIZE_USEHEADER\nLIST_RECT_BOUNDS = _controls_.LIST_RECT_BOUNDS\nLIST_RECT_ICON = _controls_.LIST_RECT_ICON\nLIST_RECT_LABEL = _controls_.LIST_RECT_LABEL\nLIST_FIND_UP = _controls_.LIST_FIND_UP\nLIST_FIND_DOWN = _controls_.LIST_FIND_DOWN\nLIST_FIND_LEFT = _controls_.LIST_FIND_LEFT\nLIST_FIND_RIGHT = _controls_.LIST_FIND_RIGHT\n#---------------------------------------------------------------------------\n\nclass ListItemAttr(object):\n    \"\"\"Proxy of C++ ListItemAttr class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Colour colText=wxNullColour, Colour colBack=wxNullColour, \n            Font font=wxNullFont) -> ListItemAttr\n        \"\"\"\n        _controls_.ListItemAttr_swiginit(self,_controls_.new_ListItemAttr(*args, **kwargs))\n    __swig_destroy__ = _controls_.delete_ListItemAttr\n    __del__ = lambda self : None;\n    def SetTextColour(*args, **kwargs):\n        \"\"\"SetTextColour(self, Colour colText)\"\"\"\n        return _controls_.ListItemAttr_SetTextColour(*args, **kwargs)\n\n    def SetBackgroundColour(*args, **kwargs):\n        \"\"\"SetBackgroundColour(self, Colour colBack)\"\"\"\n        return _controls_.ListItemAttr_SetBackgroundColour(*args, **kwargs)\n\n    def SetFont(*args, **kwargs):\n        \"\"\"SetFont(self, Font font)\"\"\"\n        return _controls_.ListItemAttr_SetFont(*args, **kwargs)\n\n    def HasTextColour(*args, **kwargs):\n        \"\"\"HasTextColour(self) -> bool\"\"\"\n        return _controls_.ListItemAttr_HasTextColour(*args, **kwargs)\n\n    def HasBackgroundColour(*args, **kwargs):\n        \"\"\"HasBackgroundColour(self) -> bool\"\"\"\n        return _controls_.ListItemAttr_HasBackgroundColour(*args, **kwargs)\n\n    def HasFont(*args, **kwargs):\n        \"\"\"HasFont(self) -> bool\"\"\"\n        return _controls_.ListItemAttr_HasFont(*args, **kwargs)\n\n    def GetTextColour(*args, **kwargs):\n        \"\"\"GetTextColour(self) -> Colour\"\"\"\n        return _controls_.ListItemAttr_GetTextColour(*args, **kwargs)\n\n    def GetBackgroundColour(*args, **kwargs):\n        \"\"\"GetBackgroundColour(self) -> Colour\"\"\"\n        return _controls_.ListItemAttr_GetBackgroundColour(*args, **kwargs)\n\n    def GetFont(*args, **kwargs):\n        \"\"\"GetFont(self) -> Font\"\"\"\n        return _controls_.ListItemAttr_GetFont(*args, **kwargs)\n\n    def AssignFrom(*args, **kwargs):\n        \"\"\"AssignFrom(self, ListItemAttr source)\"\"\"\n        return _controls_.ListItemAttr_AssignFrom(*args, **kwargs)\n\n    def Destroy(*args, **kwargs):\n        \"\"\"Destroy(self)\"\"\"\n        args[0].this.own(False)\n        return _controls_.ListItemAttr_Destroy(*args, **kwargs)\n\n    BackgroundColour = property(GetBackgroundColour,SetBackgroundColour,doc=\"See `GetBackgroundColour` and `SetBackgroundColour`\") \n    Font = property(GetFont,SetFont,doc=\"See `GetFont` and `SetFont`\") \n    TextColour = property(GetTextColour,SetTextColour,doc=\"See `GetTextColour` and `SetTextColour`\") \n_controls_.ListItemAttr_swigregister(ListItemAttr)\nListCtrlNameStr = cvar.ListCtrlNameStr\n\n#---------------------------------------------------------------------------\n\nclass ListItem(_core.Object):\n    \"\"\"Proxy of C++ ListItem class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self) -> ListItem\"\"\"\n        _controls_.ListItem_swiginit(self,_controls_.new_ListItem(*args, **kwargs))\n    __swig_destroy__ = _controls_.delete_ListItem\n    __del__ = lambda self : None;\n    def Clear(*args, **kwargs):\n        \"\"\"Clear(self)\"\"\"\n        return _controls_.ListItem_Clear(*args, **kwargs)\n\n    def ClearAttributes(*args, **kwargs):\n        \"\"\"ClearAttributes(self)\"\"\"\n        return _controls_.ListItem_ClearAttributes(*args, **kwargs)\n\n    def SetMask(*args, **kwargs):\n        \"\"\"SetMask(self, long mask)\"\"\"\n        return _controls_.ListItem_SetMask(*args, **kwargs)\n\n    def SetId(*args, **kwargs):\n        \"\"\"SetId(self, long id)\"\"\"\n        return _controls_.ListItem_SetId(*args, **kwargs)\n\n    def SetColumn(*args, **kwargs):\n        \"\"\"SetColumn(self, int col)\"\"\"\n        return _controls_.ListItem_SetColumn(*args, **kwargs)\n\n    def SetState(*args, **kwargs):\n        \"\"\"SetState(self, long state)\"\"\"\n        return _controls_.ListItem_SetState(*args, **kwargs)\n\n    def SetStateMask(*args, **kwargs):\n        \"\"\"SetStateMask(self, long stateMask)\"\"\"\n        return _controls_.ListItem_SetStateMask(*args, **kwargs)\n\n    def SetText(*args, **kwargs):\n        \"\"\"SetText(self, String text)\"\"\"\n        return _controls_.ListItem_SetText(*args, **kwargs)\n\n    def SetImage(*args, **kwargs):\n        \"\"\"SetImage(self, int image)\"\"\"\n        return _controls_.ListItem_SetImage(*args, **kwargs)\n\n    def SetData(*args, **kwargs):\n        \"\"\"SetData(self, long data)\"\"\"\n        return _controls_.ListItem_SetData(*args, **kwargs)\n\n    def SetWidth(*args, **kwargs):\n        \"\"\"SetWidth(self, int width)\"\"\"\n        return _controls_.ListItem_SetWidth(*args, **kwargs)\n\n    def SetAlign(*args, **kwargs):\n        \"\"\"SetAlign(self, int align)\"\"\"\n        return _controls_.ListItem_SetAlign(*args, **kwargs)\n\n    def SetTextColour(*args, **kwargs):\n        \"\"\"SetTextColour(self, Colour colText)\"\"\"\n        return _controls_.ListItem_SetTextColour(*args, **kwargs)\n\n    def SetBackgroundColour(*args, **kwargs):\n        \"\"\"SetBackgroundColour(self, Colour colBack)\"\"\"\n        return _controls_.ListItem_SetBackgroundColour(*args, **kwargs)\n\n    def SetFont(*args, **kwargs):\n        \"\"\"SetFont(self, Font font)\"\"\"\n        return _controls_.ListItem_SetFont(*args, **kwargs)\n\n    def GetMask(*args, **kwargs):\n        \"\"\"GetMask(self) -> long\"\"\"\n        return _controls_.ListItem_GetMask(*args, **kwargs)\n\n    def GetId(*args, **kwargs):\n        \"\"\"GetId(self) -> long\"\"\"\n        return _controls_.ListItem_GetId(*args, **kwargs)\n\n    def GetColumn(*args, **kwargs):\n        \"\"\"GetColumn(self) -> int\"\"\"\n        return _controls_.ListItem_GetColumn(*args, **kwargs)\n\n    def GetState(*args, **kwargs):\n        \"\"\"GetState(self) -> long\"\"\"\n        return _controls_.ListItem_GetState(*args, **kwargs)\n\n    def GetText(*args, **kwargs):\n        \"\"\"GetText(self) -> String\"\"\"\n        return _controls_.ListItem_GetText(*args, **kwargs)\n\n    def GetImage(*args, **kwargs):\n        \"\"\"GetImage(self) -> int\"\"\"\n        return _controls_.ListItem_GetImage(*args, **kwargs)\n\n    def GetData(*args, **kwargs):\n        \"\"\"GetData(self) -> long\"\"\"\n        return _controls_.ListItem_GetData(*args, **kwargs)\n\n    def GetWidth(*args, **kwargs):\n        \"\"\"GetWidth(self) -> int\"\"\"\n        return _controls_.ListItem_GetWidth(*args, **kwargs)\n\n    def GetAlign(*args, **kwargs):\n        \"\"\"GetAlign(self) -> int\"\"\"\n        return _controls_.ListItem_GetAlign(*args, **kwargs)\n\n    def GetAttributes(*args, **kwargs):\n        \"\"\"GetAttributes(self) -> ListItemAttr\"\"\"\n        return _controls_.ListItem_GetAttributes(*args, **kwargs)\n\n    def HasAttributes(*args, **kwargs):\n        \"\"\"HasAttributes(self) -> bool\"\"\"\n        return _controls_.ListItem_HasAttributes(*args, **kwargs)\n\n    def GetTextColour(*args, **kwargs):\n        \"\"\"GetTextColour(self) -> Colour\"\"\"\n        return _controls_.ListItem_GetTextColour(*args, **kwargs)\n\n    def GetBackgroundColour(*args, **kwargs):\n        \"\"\"GetBackgroundColour(self) -> Colour\"\"\"\n        return _controls_.ListItem_GetBackgroundColour(*args, **kwargs)\n\n    def GetFont(*args, **kwargs):\n        \"\"\"GetFont(self) -> Font\"\"\"\n        return _controls_.ListItem_GetFont(*args, **kwargs)\n\n    m_mask = property(_controls_.ListItem_m_mask_get, _controls_.ListItem_m_mask_set)\n    m_itemId = property(_controls_.ListItem_m_itemId_get, _controls_.ListItem_m_itemId_set)\n    m_col = property(_controls_.ListItem_m_col_get, _controls_.ListItem_m_col_set)\n    m_state = property(_controls_.ListItem_m_state_get, _controls_.ListItem_m_state_set)\n    m_stateMask = property(_controls_.ListItem_m_stateMask_get, _controls_.ListItem_m_stateMask_set)\n    m_text = property(_controls_.ListItem_m_text_get, _controls_.ListItem_m_text_set)\n    m_image = property(_controls_.ListItem_m_image_get, _controls_.ListItem_m_image_set)\n    m_data = property(_controls_.ListItem_m_data_get, _controls_.ListItem_m_data_set)\n    m_format = property(_controls_.ListItem_m_format_get, _controls_.ListItem_m_format_set)\n    m_width = property(_controls_.ListItem_m_width_get, _controls_.ListItem_m_width_set)\n    Align = property(GetAlign,SetAlign,doc=\"See `GetAlign` and `SetAlign`\") \n    Attributes = property(GetAttributes,doc=\"See `GetAttributes`\") \n    BackgroundColour = property(GetBackgroundColour,SetBackgroundColour,doc=\"See `GetBackgroundColour` and `SetBackgroundColour`\") \n    Column = property(GetColumn,SetColumn,doc=\"See `GetColumn` and `SetColumn`\") \n    Data = property(GetData,SetData,doc=\"See `GetData` and `SetData`\") \n    Font = property(GetFont,SetFont,doc=\"See `GetFont` and `SetFont`\") \n    Id = property(GetId,SetId,doc=\"See `GetId` and `SetId`\") \n    Image = property(GetImage,SetImage,doc=\"See `GetImage` and `SetImage`\") \n    Mask = property(GetMask,SetMask,doc=\"See `GetMask` and `SetMask`\") \n    State = property(GetState,SetState,doc=\"See `GetState` and `SetState`\") \n    Text = property(GetText,SetText,doc=\"See `GetText` and `SetText`\") \n    TextColour = property(GetTextColour,SetTextColour,doc=\"See `GetTextColour` and `SetTextColour`\") \n    Width = property(GetWidth,SetWidth,doc=\"See `GetWidth` and `SetWidth`\") \n_controls_.ListItem_swigregister(ListItem)\n\n#---------------------------------------------------------------------------\n\nclass ListEvent(_core.NotifyEvent):\n    \"\"\"Proxy of C++ ListEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, EventType commandType=wxEVT_NULL, int id=0) -> ListEvent\"\"\"\n        _controls_.ListEvent_swiginit(self,_controls_.new_ListEvent(*args, **kwargs))\n    m_code = property(_controls_.ListEvent_m_code_get, _controls_.ListEvent_m_code_set)\n    m_oldItemIndex = property(_controls_.ListEvent_m_oldItemIndex_get, _controls_.ListEvent_m_oldItemIndex_set)\n    m_itemIndex = property(_controls_.ListEvent_m_itemIndex_get, _controls_.ListEvent_m_itemIndex_set)\n    m_col = property(_controls_.ListEvent_m_col_get, _controls_.ListEvent_m_col_set)\n    m_pointDrag = property(_controls_.ListEvent_m_pointDrag_get, _controls_.ListEvent_m_pointDrag_set)\n    m_item = property(_controls_.ListEvent_m_item_get)\n    def GetKeyCode(*args, **kwargs):\n        \"\"\"GetKeyCode(self) -> int\"\"\"\n        return _controls_.ListEvent_GetKeyCode(*args, **kwargs)\n\n    GetCode = GetKeyCode \n    def GetIndex(*args, **kwargs):\n        \"\"\"GetIndex(self) -> long\"\"\"\n        return _controls_.ListEvent_GetIndex(*args, **kwargs)\n\n    def GetColumn(*args, **kwargs):\n        \"\"\"GetColumn(self) -> int\"\"\"\n        return _controls_.ListEvent_GetColumn(*args, **kwargs)\n\n    def GetPoint(*args, **kwargs):\n        \"\"\"GetPoint(self) -> Point\"\"\"\n        return _controls_.ListEvent_GetPoint(*args, **kwargs)\n\n    GetPosition = GetPoint \n    def GetLabel(*args, **kwargs):\n        \"\"\"GetLabel(self) -> String\"\"\"\n        return _controls_.ListEvent_GetLabel(*args, **kwargs)\n\n    def GetText(*args, **kwargs):\n        \"\"\"GetText(self) -> String\"\"\"\n        return _controls_.ListEvent_GetText(*args, **kwargs)\n\n    def GetImage(*args, **kwargs):\n        \"\"\"GetImage(self) -> int\"\"\"\n        return _controls_.ListEvent_GetImage(*args, **kwargs)\n\n    def GetData(*args, **kwargs):\n        \"\"\"GetData(self) -> long\"\"\"\n        return _controls_.ListEvent_GetData(*args, **kwargs)\n\n    def GetMask(*args, **kwargs):\n        \"\"\"GetMask(self) -> long\"\"\"\n        return _controls_.ListEvent_GetMask(*args, **kwargs)\n\n    def GetItem(*args, **kwargs):\n        \"\"\"GetItem(self) -> ListItem\"\"\"\n        return _controls_.ListEvent_GetItem(*args, **kwargs)\n\n    def GetCacheFrom(*args, **kwargs):\n        \"\"\"GetCacheFrom(self) -> long\"\"\"\n        return _controls_.ListEvent_GetCacheFrom(*args, **kwargs)\n\n    def GetCacheTo(*args, **kwargs):\n        \"\"\"GetCacheTo(self) -> long\"\"\"\n        return _controls_.ListEvent_GetCacheTo(*args, **kwargs)\n\n    def IsEditCancelled(*args, **kwargs):\n        \"\"\"IsEditCancelled(self) -> bool\"\"\"\n        return _controls_.ListEvent_IsEditCancelled(*args, **kwargs)\n\n    def SetEditCanceled(*args, **kwargs):\n        \"\"\"SetEditCanceled(self, bool editCancelled)\"\"\"\n        return _controls_.ListEvent_SetEditCanceled(*args, **kwargs)\n\n    CacheFrom = property(GetCacheFrom,doc=\"See `GetCacheFrom`\") \n    CacheTo = property(GetCacheTo,doc=\"See `GetCacheTo`\") \n    Column = property(GetColumn,doc=\"See `GetColumn`\") \n    Data = property(GetData,doc=\"See `GetData`\") \n    Image = property(GetImage,doc=\"See `GetImage`\") \n    Index = property(GetIndex,doc=\"See `GetIndex`\") \n    Item = property(GetItem,doc=\"See `GetItem`\") \n    KeyCode = property(GetKeyCode,doc=\"See `GetKeyCode`\") \n    Label = property(GetLabel,doc=\"See `GetLabel`\") \n    Mask = property(GetMask,doc=\"See `GetMask`\") \n    Point = property(GetPoint,doc=\"See `GetPoint`\") \n    Text = property(GetText,doc=\"See `GetText`\") \n_controls_.ListEvent_swigregister(ListEvent)\n\nwxEVT_COMMAND_LIST_BEGIN_DRAG = _controls_.wxEVT_COMMAND_LIST_BEGIN_DRAG\nwxEVT_COMMAND_LIST_BEGIN_RDRAG = _controls_.wxEVT_COMMAND_LIST_BEGIN_RDRAG\nwxEVT_COMMAND_LIST_BEGIN_LABEL_EDIT = _controls_.wxEVT_COMMAND_LIST_BEGIN_LABEL_EDIT\nwxEVT_COMMAND_LIST_END_LABEL_EDIT = _controls_.wxEVT_COMMAND_LIST_END_LABEL_EDIT\nwxEVT_COMMAND_LIST_DELETE_ITEM = _controls_.wxEVT_COMMAND_LIST_DELETE_ITEM\nwxEVT_COMMAND_LIST_DELETE_ALL_ITEMS = _controls_.wxEVT_COMMAND_LIST_DELETE_ALL_ITEMS\nwxEVT_COMMAND_LIST_ITEM_SELECTED = _controls_.wxEVT_COMMAND_LIST_ITEM_SELECTED\nwxEVT_COMMAND_LIST_ITEM_DESELECTED = _controls_.wxEVT_COMMAND_LIST_ITEM_DESELECTED\nwxEVT_COMMAND_LIST_KEY_DOWN = _controls_.wxEVT_COMMAND_LIST_KEY_DOWN\nwxEVT_COMMAND_LIST_INSERT_ITEM = _controls_.wxEVT_COMMAND_LIST_INSERT_ITEM\nwxEVT_COMMAND_LIST_COL_CLICK = _controls_.wxEVT_COMMAND_LIST_COL_CLICK\nwxEVT_COMMAND_LIST_ITEM_RIGHT_CLICK = _controls_.wxEVT_COMMAND_LIST_ITEM_RIGHT_CLICK\nwxEVT_COMMAND_LIST_ITEM_MIDDLE_CLICK = _controls_.wxEVT_COMMAND_LIST_ITEM_MIDDLE_CLICK\nwxEVT_COMMAND_LIST_ITEM_ACTIVATED = _controls_.wxEVT_COMMAND_LIST_ITEM_ACTIVATED\nwxEVT_COMMAND_LIST_CACHE_HINT = _controls_.wxEVT_COMMAND_LIST_CACHE_HINT\nwxEVT_COMMAND_LIST_COL_RIGHT_CLICK = _controls_.wxEVT_COMMAND_LIST_COL_RIGHT_CLICK\nwxEVT_COMMAND_LIST_COL_BEGIN_DRAG = _controls_.wxEVT_COMMAND_LIST_COL_BEGIN_DRAG\nwxEVT_COMMAND_LIST_COL_DRAGGING = _controls_.wxEVT_COMMAND_LIST_COL_DRAGGING\nwxEVT_COMMAND_LIST_COL_END_DRAG = _controls_.wxEVT_COMMAND_LIST_COL_END_DRAG\nwxEVT_COMMAND_LIST_ITEM_FOCUSED = _controls_.wxEVT_COMMAND_LIST_ITEM_FOCUSED\nEVT_LIST_BEGIN_DRAG        = wx.PyEventBinder(wxEVT_COMMAND_LIST_BEGIN_DRAG       , 1)\nEVT_LIST_BEGIN_RDRAG       = wx.PyEventBinder(wxEVT_COMMAND_LIST_BEGIN_RDRAG      , 1)\nEVT_LIST_BEGIN_LABEL_EDIT  = wx.PyEventBinder(wxEVT_COMMAND_LIST_BEGIN_LABEL_EDIT , 1)\nEVT_LIST_END_LABEL_EDIT    = wx.PyEventBinder(wxEVT_COMMAND_LIST_END_LABEL_EDIT   , 1)\nEVT_LIST_DELETE_ITEM       = wx.PyEventBinder(wxEVT_COMMAND_LIST_DELETE_ITEM      , 1)\nEVT_LIST_DELETE_ALL_ITEMS  = wx.PyEventBinder(wxEVT_COMMAND_LIST_DELETE_ALL_ITEMS , 1)\n\n\n\n\nEVT_LIST_ITEM_SELECTED     = wx.PyEventBinder(wxEVT_COMMAND_LIST_ITEM_SELECTED    , 1)\nEVT_LIST_ITEM_DESELECTED   = wx.PyEventBinder(wxEVT_COMMAND_LIST_ITEM_DESELECTED  , 1)\nEVT_LIST_KEY_DOWN          = wx.PyEventBinder(wxEVT_COMMAND_LIST_KEY_DOWN         , 1)\nEVT_LIST_INSERT_ITEM       = wx.PyEventBinder(wxEVT_COMMAND_LIST_INSERT_ITEM      , 1)\nEVT_LIST_COL_CLICK         = wx.PyEventBinder(wxEVT_COMMAND_LIST_COL_CLICK        , 1)\nEVT_LIST_ITEM_RIGHT_CLICK  = wx.PyEventBinder(wxEVT_COMMAND_LIST_ITEM_RIGHT_CLICK , 1)\nEVT_LIST_ITEM_MIDDLE_CLICK = wx.PyEventBinder(wxEVT_COMMAND_LIST_ITEM_MIDDLE_CLICK, 1)\nEVT_LIST_ITEM_ACTIVATED    = wx.PyEventBinder(wxEVT_COMMAND_LIST_ITEM_ACTIVATED   , 1)\nEVT_LIST_CACHE_HINT        = wx.PyEventBinder(wxEVT_COMMAND_LIST_CACHE_HINT       , 1)\nEVT_LIST_COL_RIGHT_CLICK   = wx.PyEventBinder(wxEVT_COMMAND_LIST_COL_RIGHT_CLICK  , 1)\nEVT_LIST_COL_BEGIN_DRAG    = wx.PyEventBinder(wxEVT_COMMAND_LIST_COL_BEGIN_DRAG   , 1)\nEVT_LIST_COL_DRAGGING      = wx.PyEventBinder(wxEVT_COMMAND_LIST_COL_DRAGGING     , 1)\nEVT_LIST_COL_END_DRAG      = wx.PyEventBinder(wxEVT_COMMAND_LIST_COL_END_DRAG     , 1)\nEVT_LIST_ITEM_FOCUSED      = wx.PyEventBinder(wxEVT_COMMAND_LIST_ITEM_FOCUSED     , 1)\n\n\n\n\n\n#---------------------------------------------------------------------------\n\nclass ListCtrl(_core.Control):\n    \"\"\"Proxy of C++ ListCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=LC_ICON, \n            Validator validator=DefaultValidator, String name=ListCtrlNameStr) -> ListCtrl\n        \"\"\"\n        _controls_.ListCtrl_swiginit(self,_controls_.new_ListCtrl(*args, **kwargs))\n        self._setOORInfo(self);ListCtrl._setCallbackInfo(self, self, ListCtrl)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=LC_ICON, \n            Validator validator=DefaultValidator, String name=ListCtrlNameStr) -> bool\n\n        Do the 2nd phase and create the GUI control.\n        \"\"\"\n        return _controls_.ListCtrl_Create(*args, **kwargs)\n\n    def _setCallbackInfo(*args, **kwargs):\n        \"\"\"_setCallbackInfo(self, PyObject self, PyObject _class)\"\"\"\n        return _controls_.ListCtrl__setCallbackInfo(*args, **kwargs)\n\n    def GetColumn(*args, **kwargs):\n        \"\"\"GetColumn(self, int col) -> ListItem\"\"\"\n        val = _controls_.ListCtrl_GetColumn(*args, **kwargs)\n        if val is not None: val.thisown = 1\n        return val\n\n    def SetColumn(*args, **kwargs):\n        \"\"\"SetColumn(self, int col, ListItem item) -> bool\"\"\"\n        return _controls_.ListCtrl_SetColumn(*args, **kwargs)\n\n    def GetColumnWidth(*args, **kwargs):\n        \"\"\"GetColumnWidth(self, int col) -> int\"\"\"\n        return _controls_.ListCtrl_GetColumnWidth(*args, **kwargs)\n\n    def SetColumnWidth(*args, **kwargs):\n        \"\"\"SetColumnWidth(self, int col, int width) -> bool\"\"\"\n        return _controls_.ListCtrl_SetColumnWidth(*args, **kwargs)\n\n    def HasColumnOrderSupport(*args, **kwargs):\n        \"\"\"HasColumnOrderSupport() -> bool\"\"\"\n        return _controls_.ListCtrl_HasColumnOrderSupport(*args, **kwargs)\n\n    HasColumnOrderSupport = staticmethod(HasColumnOrderSupport)\n    def GetColumnOrder(*args, **kwargs):\n        \"\"\"GetColumnOrder(self, int col) -> int\"\"\"\n        return _controls_.ListCtrl_GetColumnOrder(*args, **kwargs)\n\n    def GetColumnIndexFromOrder(*args, **kwargs):\n        \"\"\"GetColumnIndexFromOrder(self, int order) -> int\"\"\"\n        return _controls_.ListCtrl_GetColumnIndexFromOrder(*args, **kwargs)\n\n    def GetColumnsOrder(*args, **kwargs):\n        \"\"\"GetColumnsOrder(self) -> wxArrayInt\"\"\"\n        return _controls_.ListCtrl_GetColumnsOrder(*args, **kwargs)\n\n    def SetColumnsOrder(*args, **kwargs):\n        \"\"\"SetColumnsOrder(self, wxArrayInt orders) -> bool\"\"\"\n        return _controls_.ListCtrl_SetColumnsOrder(*args, **kwargs)\n\n    def GetCountPerPage(*args, **kwargs):\n        \"\"\"GetCountPerPage(self) -> int\"\"\"\n        return _controls_.ListCtrl_GetCountPerPage(*args, **kwargs)\n\n    def GetViewRect(*args, **kwargs):\n        \"\"\"GetViewRect(self) -> Rect\"\"\"\n        return _controls_.ListCtrl_GetViewRect(*args, **kwargs)\n\n    def GetEditControl(*args, **kwargs):\n        \"\"\"GetEditControl(self) -> TextCtrl\"\"\"\n        return _controls_.ListCtrl_GetEditControl(*args, **kwargs)\n\n    def GetItem(*args, **kwargs):\n        \"\"\"GetItem(self, long itemId, int col=0) -> ListItem\"\"\"\n        val = _controls_.ListCtrl_GetItem(*args, **kwargs)\n        if val is not None: val.thisown = 1\n        return val\n\n    def SetItem(*args, **kwargs):\n        \"\"\"SetItem(self, ListItem info) -> bool\"\"\"\n        return _controls_.ListCtrl_SetItem(*args, **kwargs)\n\n    def SetStringItem(*args, **kwargs):\n        \"\"\"SetStringItem(self, long index, int col, String label, int imageId=-1) -> long\"\"\"\n        return _controls_.ListCtrl_SetStringItem(*args, **kwargs)\n\n    def GetItemState(*args, **kwargs):\n        \"\"\"GetItemState(self, long item, long stateMask) -> int\"\"\"\n        return _controls_.ListCtrl_GetItemState(*args, **kwargs)\n\n    def SetItemState(*args, **kwargs):\n        \"\"\"SetItemState(self, long item, long state, long stateMask) -> bool\"\"\"\n        return _controls_.ListCtrl_SetItemState(*args, **kwargs)\n\n    def SetItemImage(*args, **kwargs):\n        \"\"\"SetItemImage(self, long item, int image, int selImage=-1) -> bool\"\"\"\n        return _controls_.ListCtrl_SetItemImage(*args, **kwargs)\n\n    def SetItemColumnImage(*args, **kwargs):\n        \"\"\"SetItemColumnImage(self, long item, long column, int image) -> bool\"\"\"\n        return _controls_.ListCtrl_SetItemColumnImage(*args, **kwargs)\n\n    def GetItemText(*args, **kwargs):\n        \"\"\"GetItemText(self, long item, int col=0) -> String\"\"\"\n        return _controls_.ListCtrl_GetItemText(*args, **kwargs)\n\n    def SetItemText(*args, **kwargs):\n        \"\"\"SetItemText(self, long item, String str)\"\"\"\n        return _controls_.ListCtrl_SetItemText(*args, **kwargs)\n\n    def GetItemData(*args, **kwargs):\n        \"\"\"GetItemData(self, long item) -> long\"\"\"\n        return _controls_.ListCtrl_GetItemData(*args, **kwargs)\n\n    def SetItemData(*args, **kwargs):\n        \"\"\"SetItemData(self, long item, long data) -> bool\"\"\"\n        return _controls_.ListCtrl_SetItemData(*args, **kwargs)\n\n    def GetItemPosition(*args, **kwargs):\n        \"\"\"GetItemPosition(self, long item) -> Point\"\"\"\n        return _controls_.ListCtrl_GetItemPosition(*args, **kwargs)\n\n    def GetItemRect(*args, **kwargs):\n        \"\"\"GetItemRect(self, long item, int code=LIST_RECT_BOUNDS) -> Rect\"\"\"\n        return _controls_.ListCtrl_GetItemRect(*args, **kwargs)\n\n    def SetItemPosition(*args, **kwargs):\n        \"\"\"SetItemPosition(self, long item, Point pos) -> bool\"\"\"\n        return _controls_.ListCtrl_SetItemPosition(*args, **kwargs)\n\n    def GetItemCount(*args, **kwargs):\n        \"\"\"GetItemCount(self) -> int\"\"\"\n        return _controls_.ListCtrl_GetItemCount(*args, **kwargs)\n\n    def GetColumnCount(*args, **kwargs):\n        \"\"\"GetColumnCount(self) -> int\"\"\"\n        return _controls_.ListCtrl_GetColumnCount(*args, **kwargs)\n\n    def GetItemSpacing(*args, **kwargs):\n        \"\"\"GetItemSpacing(self) -> Size\"\"\"\n        return _controls_.ListCtrl_GetItemSpacing(*args, **kwargs)\n\n    GetItemSpacing = wx.deprecated(GetItemSpacing) \n    def SetItemSpacing(*args, **kwargs):\n        \"\"\"SetItemSpacing(self, int spacing, bool isSmall=False)\"\"\"\n        return _controls_.ListCtrl_SetItemSpacing(*args, **kwargs)\n\n    SetItemSpacing = wx.deprecated(SetItemSpacing) \n    def GetSelectedItemCount(*args, **kwargs):\n        \"\"\"GetSelectedItemCount(self) -> int\"\"\"\n        return _controls_.ListCtrl_GetSelectedItemCount(*args, **kwargs)\n\n    def GetTextColour(*args, **kwargs):\n        \"\"\"GetTextColour(self) -> Colour\"\"\"\n        return _controls_.ListCtrl_GetTextColour(*args, **kwargs)\n\n    def SetTextColour(*args, **kwargs):\n        \"\"\"SetTextColour(self, Colour col)\"\"\"\n        return _controls_.ListCtrl_SetTextColour(*args, **kwargs)\n\n    def GetTopItem(*args, **kwargs):\n        \"\"\"GetTopItem(self) -> long\"\"\"\n        return _controls_.ListCtrl_GetTopItem(*args, **kwargs)\n\n    def SetSingleStyle(*args, **kwargs):\n        \"\"\"SetSingleStyle(self, long style, bool add=True)\"\"\"\n        return _controls_.ListCtrl_SetSingleStyle(*args, **kwargs)\n\n    def GetNextItem(*args, **kwargs):\n        \"\"\"GetNextItem(self, long item, int geometry=LIST_NEXT_ALL, int state=LIST_STATE_DONTCARE) -> long\"\"\"\n        return _controls_.ListCtrl_GetNextItem(*args, **kwargs)\n\n    def GetImageList(*args, **kwargs):\n        \"\"\"GetImageList(self, int which) -> ImageList\"\"\"\n        return _controls_.ListCtrl_GetImageList(*args, **kwargs)\n\n    def SetImageList(*args, **kwargs):\n        \"\"\"SetImageList(self, ImageList imageList, int which)\"\"\"\n        return _controls_.ListCtrl_SetImageList(*args, **kwargs)\n\n    def AssignImageList(*args, **kwargs):\n        \"\"\"AssignImageList(self, ImageList imageList, int which)\"\"\"\n        return _controls_.ListCtrl_AssignImageList(*args, **kwargs)\n\n    def InReportView(*args, **kwargs):\n        \"\"\"InReportView(self) -> bool\"\"\"\n        return _controls_.ListCtrl_InReportView(*args, **kwargs)\n\n    def IsVirtual(*args, **kwargs):\n        \"\"\"IsVirtual(self) -> bool\"\"\"\n        return _controls_.ListCtrl_IsVirtual(*args, **kwargs)\n\n    def RefreshItem(*args, **kwargs):\n        \"\"\"RefreshItem(self, long item)\"\"\"\n        return _controls_.ListCtrl_RefreshItem(*args, **kwargs)\n\n    def RefreshItems(*args, **kwargs):\n        \"\"\"RefreshItems(self, long itemFrom, long itemTo)\"\"\"\n        return _controls_.ListCtrl_RefreshItems(*args, **kwargs)\n\n    def Arrange(*args, **kwargs):\n        \"\"\"Arrange(self, int flag=LIST_ALIGN_DEFAULT) -> bool\"\"\"\n        return _controls_.ListCtrl_Arrange(*args, **kwargs)\n\n    def DeleteItem(*args, **kwargs):\n        \"\"\"DeleteItem(self, long item) -> bool\"\"\"\n        return _controls_.ListCtrl_DeleteItem(*args, **kwargs)\n\n    def DeleteAllItems(*args, **kwargs):\n        \"\"\"DeleteAllItems(self) -> bool\"\"\"\n        return _controls_.ListCtrl_DeleteAllItems(*args, **kwargs)\n\n    def DeleteColumn(*args, **kwargs):\n        \"\"\"DeleteColumn(self, int col) -> bool\"\"\"\n        return _controls_.ListCtrl_DeleteColumn(*args, **kwargs)\n\n    def DeleteAllColumns(*args, **kwargs):\n        \"\"\"DeleteAllColumns(self) -> bool\"\"\"\n        return _controls_.ListCtrl_DeleteAllColumns(*args, **kwargs)\n\n    def ClearAll(*args, **kwargs):\n        \"\"\"ClearAll(self)\"\"\"\n        return _controls_.ListCtrl_ClearAll(*args, **kwargs)\n\n    def EditLabel(*args, **kwargs):\n        \"\"\"EditLabel(self, long item)\"\"\"\n        return _controls_.ListCtrl_EditLabel(*args, **kwargs)\n\n    def EnsureVisible(*args, **kwargs):\n        \"\"\"EnsureVisible(self, long item) -> bool\"\"\"\n        return _controls_.ListCtrl_EnsureVisible(*args, **kwargs)\n\n    def FindItem(*args, **kwargs):\n        \"\"\"FindItem(self, long start, String str, bool partial=False) -> long\"\"\"\n        return _controls_.ListCtrl_FindItem(*args, **kwargs)\n\n    def FindItemData(*args, **kwargs):\n        \"\"\"FindItemData(self, long start, long data) -> long\"\"\"\n        return _controls_.ListCtrl_FindItemData(*args, **kwargs)\n\n    def FindItemAtPos(*args, **kwargs):\n        \"\"\"FindItemAtPos(self, long start, Point pt, int direction) -> long\"\"\"\n        return _controls_.ListCtrl_FindItemAtPos(*args, **kwargs)\n\n    def HitTest(*args, **kwargs):\n        \"\"\"\n        HitTest(Point point) -> (item, where)\n\n        Determines which item (if any) is at the specified point, giving\n         in the second return value (see wx.LIST_HITTEST flags.)\n        \"\"\"\n        return _controls_.ListCtrl_HitTest(*args, **kwargs)\n\n    def HitTestSubItem(*args, **kwargs):\n        \"\"\"\n        HitTestSubItem(Point point) -> (item, where, subItem)\n\n        Determines which item (if any) is at the specified point, giving  in\n        the second return value (see wx.LIST_HITTEST flags) and also the subItem, if\n        any.\n        \"\"\"\n        return _controls_.ListCtrl_HitTestSubItem(*args, **kwargs)\n\n    def InsertItem(*args, **kwargs):\n        \"\"\"InsertItem(self, ListItem info) -> long\"\"\"\n        return _controls_.ListCtrl_InsertItem(*args, **kwargs)\n\n    def InsertStringItem(*args, **kwargs):\n        \"\"\"InsertStringItem(self, long index, String label, int imageIndex=-1) -> long\"\"\"\n        return _controls_.ListCtrl_InsertStringItem(*args, **kwargs)\n\n    def InsertImageItem(*args, **kwargs):\n        \"\"\"InsertImageItem(self, long index, int imageIndex) -> long\"\"\"\n        return _controls_.ListCtrl_InsertImageItem(*args, **kwargs)\n\n    def InsertImageStringItem(*args, **kwargs):\n        \"\"\"InsertImageStringItem(self, long index, String label, int imageIndex) -> long\"\"\"\n        return _controls_.ListCtrl_InsertImageStringItem(*args, **kwargs)\n\n    def InsertColumnItem(*args, **kwargs):\n        \"\"\"InsertColumnItem(self, long col, ListItem info) -> long\"\"\"\n        return _controls_.ListCtrl_InsertColumnItem(*args, **kwargs)\n\n    InsertColumnInfo = InsertColumnItem \n    def InsertColumn(*args, **kwargs):\n        \"\"\"\n        InsertColumn(self, long col, String heading, int format=LIST_FORMAT_LEFT, \n            int width=-1) -> long\n        \"\"\"\n        return _controls_.ListCtrl_InsertColumn(*args, **kwargs)\n\n    def SetItemCount(*args, **kwargs):\n        \"\"\"SetItemCount(self, long count)\"\"\"\n        return _controls_.ListCtrl_SetItemCount(*args, **kwargs)\n\n    def ScrollList(*args, **kwargs):\n        \"\"\"ScrollList(self, int dx, int dy) -> bool\"\"\"\n        return _controls_.ListCtrl_ScrollList(*args, **kwargs)\n\n    def SetItemTextColour(*args, **kwargs):\n        \"\"\"SetItemTextColour(self, long item, Colour col)\"\"\"\n        return _controls_.ListCtrl_SetItemTextColour(*args, **kwargs)\n\n    def GetItemTextColour(*args, **kwargs):\n        \"\"\"GetItemTextColour(self, long item) -> Colour\"\"\"\n        return _controls_.ListCtrl_GetItemTextColour(*args, **kwargs)\n\n    def SetItemBackgroundColour(*args, **kwargs):\n        \"\"\"SetItemBackgroundColour(self, long item, Colour col)\"\"\"\n        return _controls_.ListCtrl_SetItemBackgroundColour(*args, **kwargs)\n\n    def GetItemBackgroundColour(*args, **kwargs):\n        \"\"\"GetItemBackgroundColour(self, long item) -> Colour\"\"\"\n        return _controls_.ListCtrl_GetItemBackgroundColour(*args, **kwargs)\n\n    def SetItemFont(*args, **kwargs):\n        \"\"\"SetItemFont(self, long item, Font f)\"\"\"\n        return _controls_.ListCtrl_SetItemFont(*args, **kwargs)\n\n    def GetItemFont(*args, **kwargs):\n        \"\"\"GetItemFont(self, long item) -> Font\"\"\"\n        return _controls_.ListCtrl_GetItemFont(*args, **kwargs)\n\n    #\n    # Some helpers...\n    def Select(self, idx, on=1):\n        '''[de]select an item'''\n        if on: state = wx.LIST_STATE_SELECTED\n        else: state = 0\n        self.SetItemState(idx, state, wx.LIST_STATE_SELECTED)\n\n    def Focus(self, idx):\n        '''Focus and show the given item'''\n        self.SetItemState(idx, wx.LIST_STATE_FOCUSED, wx.LIST_STATE_FOCUSED)\n        self.EnsureVisible(idx)\n\n    def GetFocusedItem(self):\n        '''get the currently focused item or -1 if none'''\n        return self.GetNextItem(-1, wx.LIST_NEXT_ALL, wx.LIST_STATE_FOCUSED)\n\n    def GetFirstSelected(self, *args):\n        '''return first selected item, or -1 when none'''\n        return self.GetNextSelected(-1)\n\n    def GetNextSelected(self, item):\n        '''return subsequent selected items, or -1 when no more'''\n        return self.GetNextItem(item, wx.LIST_NEXT_ALL, wx.LIST_STATE_SELECTED)\n\n    def IsSelected(self, idx):\n        '''return True if the item is selected'''\n        return (self.GetItemState(idx, wx.LIST_STATE_SELECTED) & wx.LIST_STATE_SELECTED) != 0\n\n    def SetColumnImage(self, col, image):\n        item = self.GetColumn(col)\n        # preserve all other attributes too\n        item.SetMask( wx.LIST_MASK_STATE |\n                      wx.LIST_MASK_TEXT  |\n                      wx.LIST_MASK_IMAGE |\n                      wx.LIST_MASK_DATA  |\n                      wx.LIST_SET_ITEM   |\n                      wx.LIST_MASK_WIDTH |\n                      wx.LIST_MASK_FORMAT )\n        item.SetImage(image)\n        self.SetColumn(col, item)\n\n    def ClearColumnImage(self, col):\n        self.SetColumnImage(col, -1)\n\n    def Append(self, entry):\n        '''Append an item to the list control.  The entry parameter should be a\n           sequence with an item for each column'''\n        if len(entry):\n            if wx.USE_UNICODE:\n                cvtfunc = unicode\n            else:\n                cvtfunc = str\n            pos = self.GetItemCount()\n            self.InsertStringItem(pos, cvtfunc(entry[0]))\n            for i in range(1, len(entry)):\n                self.SetStringItem(pos, i, cvtfunc(entry[i]))\n            return pos\n\n    def SortItems(*args, **kwargs):\n        \"\"\"SortItems(self, PyObject func) -> bool\"\"\"\n        return _controls_.ListCtrl_SortItems(*args, **kwargs)\n\n    def GetMainWindow(*args, **kwargs):\n        \"\"\"GetMainWindow(self) -> Window\"\"\"\n        return _controls_.ListCtrl_GetMainWindow(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.ListCtrl_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    ColumnCount = property(GetColumnCount,doc=\"See `GetColumnCount`\") \n    CountPerPage = property(GetCountPerPage,doc=\"See `GetCountPerPage`\") \n    EditControl = property(GetEditControl,doc=\"See `GetEditControl`\") \n    FocusedItem = property(GetFocusedItem,doc=\"See `GetFocusedItem`\") \n    ItemCount = property(GetItemCount,SetItemCount,doc=\"See `GetItemCount` and `SetItemCount`\") \n    MainWindow = property(GetMainWindow,doc=\"See `GetMainWindow`\") \n    SelectedItemCount = property(GetSelectedItemCount,doc=\"See `GetSelectedItemCount`\") \n    TextColour = property(GetTextColour,SetTextColour,doc=\"See `GetTextColour` and `SetTextColour`\") \n    TopItem = property(GetTopItem,doc=\"See `GetTopItem`\") \n    ViewRect = property(GetViewRect,doc=\"See `GetViewRect`\") \n_controls_.ListCtrl_swigregister(ListCtrl)\n\ndef PreListCtrl(*args, **kwargs):\n    \"\"\"PreListCtrl() -> ListCtrl\"\"\"\n    val = _controls_.new_PreListCtrl(*args, **kwargs)\n    return val\n\ndef ListCtrl_HasColumnOrderSupport(*args):\n  \"\"\"ListCtrl_HasColumnOrderSupport() -> bool\"\"\"\n  return _controls_.ListCtrl_HasColumnOrderSupport(*args)\n\ndef ListCtrl_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    ListCtrl_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.ListCtrl_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nclass ListView(ListCtrl):\n    \"\"\"Proxy of C++ ListView class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=LC_REPORT, \n            Validator validator=DefaultValidator, String name=ListCtrlNameStr) -> ListView\n        \"\"\"\n        _controls_.ListView_swiginit(self,_controls_.new_ListView(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=LC_REPORT, \n            Validator validator=DefaultValidator, String name=ListCtrlNameStr) -> bool\n\n        Do the 2nd phase and create the GUI control.\n        \"\"\"\n        return _controls_.ListView_Create(*args, **kwargs)\n\n    def Select(*args, **kwargs):\n        \"\"\"Select(self, long n, bool on=True)\"\"\"\n        return _controls_.ListView_Select(*args, **kwargs)\n\n    def Focus(*args, **kwargs):\n        \"\"\"Focus(self, long index)\"\"\"\n        return _controls_.ListView_Focus(*args, **kwargs)\n\n    def GetFocusedItem(*args, **kwargs):\n        \"\"\"GetFocusedItem(self) -> long\"\"\"\n        return _controls_.ListView_GetFocusedItem(*args, **kwargs)\n\n    def GetNextSelected(*args, **kwargs):\n        \"\"\"GetNextSelected(self, long item) -> long\"\"\"\n        return _controls_.ListView_GetNextSelected(*args, **kwargs)\n\n    def GetFirstSelected(*args, **kwargs):\n        \"\"\"GetFirstSelected(self) -> long\"\"\"\n        return _controls_.ListView_GetFirstSelected(*args, **kwargs)\n\n    def IsSelected(*args, **kwargs):\n        \"\"\"IsSelected(self, long index) -> bool\"\"\"\n        return _controls_.ListView_IsSelected(*args, **kwargs)\n\n    def SetColumnImage(*args, **kwargs):\n        \"\"\"SetColumnImage(self, int col, int image)\"\"\"\n        return _controls_.ListView_SetColumnImage(*args, **kwargs)\n\n    def ClearColumnImage(*args, **kwargs):\n        \"\"\"ClearColumnImage(self, int col)\"\"\"\n        return _controls_.ListView_ClearColumnImage(*args, **kwargs)\n\n    FocusedItem = property(GetFocusedItem,doc=\"See `GetFocusedItem`\") \n_controls_.ListView_swigregister(ListView)\n\ndef PreListView(*args, **kwargs):\n    \"\"\"PreListView() -> ListView\"\"\"\n    val = _controls_.new_PreListView(*args, **kwargs)\n    return val\n\n#---------------------------------------------------------------------------\n\nTR_NO_BUTTONS = _controls_.TR_NO_BUTTONS\nTR_HAS_BUTTONS = _controls_.TR_HAS_BUTTONS\nTR_NO_LINES = _controls_.TR_NO_LINES\nTR_LINES_AT_ROOT = _controls_.TR_LINES_AT_ROOT\nTR_SINGLE = _controls_.TR_SINGLE\nTR_MULTIPLE = _controls_.TR_MULTIPLE\nTR_EXTENDED = _controls_.TR_EXTENDED\nTR_HAS_VARIABLE_ROW_HEIGHT = _controls_.TR_HAS_VARIABLE_ROW_HEIGHT\nTR_EDIT_LABELS = _controls_.TR_EDIT_LABELS\nTR_HIDE_ROOT = _controls_.TR_HIDE_ROOT\nTR_ROW_LINES = _controls_.TR_ROW_LINES\nTR_FULL_ROW_HIGHLIGHT = _controls_.TR_FULL_ROW_HIGHLIGHT\nTR_DEFAULT_STYLE = _controls_.TR_DEFAULT_STYLE\nTR_TWIST_BUTTONS = _controls_.TR_TWIST_BUTTONS\n# obsolete\nTR_MAC_BUTTONS = 0\nwxTR_AQUA_BUTTONS = 0\n\nTreeItemIcon_Normal = _controls_.TreeItemIcon_Normal\nTreeItemIcon_Selected = _controls_.TreeItemIcon_Selected\nTreeItemIcon_Expanded = _controls_.TreeItemIcon_Expanded\nTreeItemIcon_SelectedExpanded = _controls_.TreeItemIcon_SelectedExpanded\nTreeItemIcon_Max = _controls_.TreeItemIcon_Max\nTREE_ITEMSTATE_NONE = _controls_.TREE_ITEMSTATE_NONE\nTREE_ITEMSTATE_NEXT = _controls_.TREE_ITEMSTATE_NEXT\nTREE_ITEMSTATE_PREV = _controls_.TREE_ITEMSTATE_PREV\nTREE_HITTEST_ABOVE = _controls_.TREE_HITTEST_ABOVE\nTREE_HITTEST_BELOW = _controls_.TREE_HITTEST_BELOW\nTREE_HITTEST_NOWHERE = _controls_.TREE_HITTEST_NOWHERE\nTREE_HITTEST_ONITEMBUTTON = _controls_.TREE_HITTEST_ONITEMBUTTON\nTREE_HITTEST_ONITEMICON = _controls_.TREE_HITTEST_ONITEMICON\nTREE_HITTEST_ONITEMINDENT = _controls_.TREE_HITTEST_ONITEMINDENT\nTREE_HITTEST_ONITEMLABEL = _controls_.TREE_HITTEST_ONITEMLABEL\nTREE_HITTEST_ONITEMRIGHT = _controls_.TREE_HITTEST_ONITEMRIGHT\nTREE_HITTEST_ONITEMSTATEICON = _controls_.TREE_HITTEST_ONITEMSTATEICON\nTREE_HITTEST_TOLEFT = _controls_.TREE_HITTEST_TOLEFT\nTREE_HITTEST_TORIGHT = _controls_.TREE_HITTEST_TORIGHT\nTREE_HITTEST_ONITEMUPPERPART = _controls_.TREE_HITTEST_ONITEMUPPERPART\nTREE_HITTEST_ONITEMLOWERPART = _controls_.TREE_HITTEST_ONITEMLOWERPART\nTREE_HITTEST_ONITEM = _controls_.TREE_HITTEST_ONITEM\n#---------------------------------------------------------------------------\n\nclass TreeItemId(object):\n    \"\"\"Proxy of C++ TreeItemId class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self) -> TreeItemId\"\"\"\n        _controls_.TreeItemId_swiginit(self,_controls_.new_TreeItemId(*args, **kwargs))\n    __swig_destroy__ = _controls_.delete_TreeItemId\n    __del__ = lambda self : None;\n    def IsOk(*args, **kwargs):\n        \"\"\"IsOk(self) -> bool\"\"\"\n        return _controls_.TreeItemId_IsOk(*args, **kwargs)\n\n    def __eq__(*args, **kwargs):\n        \"\"\"__eq__(self, TreeItemId other) -> bool\"\"\"\n        return _controls_.TreeItemId___eq__(*args, **kwargs)\n\n    def __ne__(*args, **kwargs):\n        \"\"\"__ne__(self, TreeItemId other) -> bool\"\"\"\n        return _controls_.TreeItemId___ne__(*args, **kwargs)\n\n    m_pItem = property(_controls_.TreeItemId_m_pItem_get, _controls_.TreeItemId_m_pItem_set)\n    Ok = IsOk\n    def __nonzero__(self): return self.IsOk() \n_controls_.TreeItemId_swigregister(TreeItemId)\nTreeCtrlNameStr = cvar.TreeCtrlNameStr\n\nclass TreeItemData(object):\n    \"\"\"Proxy of C++ TreeItemData class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, PyObject obj=None) -> TreeItemData\"\"\"\n        _controls_.TreeItemData_swiginit(self,_controls_.new_TreeItemData(*args, **kwargs))\n    __swig_destroy__ = _controls_.delete_TreeItemData\n    __del__ = lambda self : None;\n    def GetData(*args, **kwargs):\n        \"\"\"GetData(self) -> PyObject\"\"\"\n        return _controls_.TreeItemData_GetData(*args, **kwargs)\n\n    def SetData(*args, **kwargs):\n        \"\"\"SetData(self, PyObject obj)\"\"\"\n        return _controls_.TreeItemData_SetData(*args, **kwargs)\n\n    def GetId(*args, **kwargs):\n        \"\"\"GetId(self) -> TreeItemId\"\"\"\n        return _controls_.TreeItemData_GetId(*args, **kwargs)\n\n    def SetId(*args, **kwargs):\n        \"\"\"SetId(self, TreeItemId id)\"\"\"\n        return _controls_.TreeItemData_SetId(*args, **kwargs)\n\n    def Destroy(*args, **kwargs):\n        \"\"\"Destroy(self)\"\"\"\n        args[0].this.own(False)\n        return _controls_.TreeItemData_Destroy(*args, **kwargs)\n\n    Data = property(GetData,SetData,doc=\"See `GetData` and `SetData`\") \n    Id = property(GetId,SetId,doc=\"See `GetId` and `SetId`\") \n_controls_.TreeItemData_swigregister(TreeItemData)\n\n#---------------------------------------------------------------------------\n\nwxEVT_COMMAND_TREE_BEGIN_DRAG = _controls_.wxEVT_COMMAND_TREE_BEGIN_DRAG\nwxEVT_COMMAND_TREE_BEGIN_RDRAG = _controls_.wxEVT_COMMAND_TREE_BEGIN_RDRAG\nwxEVT_COMMAND_TREE_BEGIN_LABEL_EDIT = _controls_.wxEVT_COMMAND_TREE_BEGIN_LABEL_EDIT\nwxEVT_COMMAND_TREE_END_LABEL_EDIT = _controls_.wxEVT_COMMAND_TREE_END_LABEL_EDIT\nwxEVT_COMMAND_TREE_DELETE_ITEM = _controls_.wxEVT_COMMAND_TREE_DELETE_ITEM\nwxEVT_COMMAND_TREE_GET_INFO = _controls_.wxEVT_COMMAND_TREE_GET_INFO\nwxEVT_COMMAND_TREE_SET_INFO = _controls_.wxEVT_COMMAND_TREE_SET_INFO\nwxEVT_COMMAND_TREE_ITEM_EXPANDED = _controls_.wxEVT_COMMAND_TREE_ITEM_EXPANDED\nwxEVT_COMMAND_TREE_ITEM_EXPANDING = _controls_.wxEVT_COMMAND_TREE_ITEM_EXPANDING\nwxEVT_COMMAND_TREE_ITEM_COLLAPSED = _controls_.wxEVT_COMMAND_TREE_ITEM_COLLAPSED\nwxEVT_COMMAND_TREE_ITEM_COLLAPSING = _controls_.wxEVT_COMMAND_TREE_ITEM_COLLAPSING\nwxEVT_COMMAND_TREE_SEL_CHANGED = _controls_.wxEVT_COMMAND_TREE_SEL_CHANGED\nwxEVT_COMMAND_TREE_SEL_CHANGING = _controls_.wxEVT_COMMAND_TREE_SEL_CHANGING\nwxEVT_COMMAND_TREE_KEY_DOWN = _controls_.wxEVT_COMMAND_TREE_KEY_DOWN\nwxEVT_COMMAND_TREE_ITEM_ACTIVATED = _controls_.wxEVT_COMMAND_TREE_ITEM_ACTIVATED\nwxEVT_COMMAND_TREE_ITEM_RIGHT_CLICK = _controls_.wxEVT_COMMAND_TREE_ITEM_RIGHT_CLICK\nwxEVT_COMMAND_TREE_ITEM_MIDDLE_CLICK = _controls_.wxEVT_COMMAND_TREE_ITEM_MIDDLE_CLICK\nwxEVT_COMMAND_TREE_END_DRAG = _controls_.wxEVT_COMMAND_TREE_END_DRAG\nwxEVT_COMMAND_TREE_STATE_IMAGE_CLICK = _controls_.wxEVT_COMMAND_TREE_STATE_IMAGE_CLICK\nwxEVT_COMMAND_TREE_ITEM_GETTOOLTIP = _controls_.wxEVT_COMMAND_TREE_ITEM_GETTOOLTIP\nwxEVT_COMMAND_TREE_ITEM_MENU = _controls_.wxEVT_COMMAND_TREE_ITEM_MENU\nEVT_TREE_BEGIN_DRAG        = wx.PyEventBinder(wxEVT_COMMAND_TREE_BEGIN_DRAG       , 1)\nEVT_TREE_BEGIN_RDRAG       = wx.PyEventBinder(wxEVT_COMMAND_TREE_BEGIN_RDRAG      , 1)\nEVT_TREE_BEGIN_LABEL_EDIT  = wx.PyEventBinder(wxEVT_COMMAND_TREE_BEGIN_LABEL_EDIT , 1)\nEVT_TREE_END_LABEL_EDIT    = wx.PyEventBinder(wxEVT_COMMAND_TREE_END_LABEL_EDIT   , 1)\nEVT_TREE_DELETE_ITEM       = wx.PyEventBinder(wxEVT_COMMAND_TREE_DELETE_ITEM      , 1)\nEVT_TREE_GET_INFO          = wx.PyEventBinder(wxEVT_COMMAND_TREE_GET_INFO         , 1)\nEVT_TREE_SET_INFO          = wx.PyEventBinder(wxEVT_COMMAND_TREE_SET_INFO         , 1)\nEVT_TREE_ITEM_EXPANDED     = wx.PyEventBinder(wxEVT_COMMAND_TREE_ITEM_EXPANDED    , 1)\nEVT_TREE_ITEM_EXPANDING    = wx.PyEventBinder(wxEVT_COMMAND_TREE_ITEM_EXPANDING   , 1)\nEVT_TREE_ITEM_COLLAPSED    = wx.PyEventBinder(wxEVT_COMMAND_TREE_ITEM_COLLAPSED   , 1)\nEVT_TREE_ITEM_COLLAPSING   = wx.PyEventBinder(wxEVT_COMMAND_TREE_ITEM_COLLAPSING  , 1)\nEVT_TREE_SEL_CHANGED       = wx.PyEventBinder(wxEVT_COMMAND_TREE_SEL_CHANGED      , 1)\nEVT_TREE_SEL_CHANGING      = wx.PyEventBinder(wxEVT_COMMAND_TREE_SEL_CHANGING     , 1)\nEVT_TREE_KEY_DOWN          = wx.PyEventBinder(wxEVT_COMMAND_TREE_KEY_DOWN         , 1)\nEVT_TREE_ITEM_ACTIVATED    = wx.PyEventBinder(wxEVT_COMMAND_TREE_ITEM_ACTIVATED   , 1)\nEVT_TREE_ITEM_RIGHT_CLICK  = wx.PyEventBinder(wxEVT_COMMAND_TREE_ITEM_RIGHT_CLICK , 1)\nEVT_TREE_ITEM_MIDDLE_CLICK = wx.PyEventBinder(wxEVT_COMMAND_TREE_ITEM_MIDDLE_CLICK, 1)\nEVT_TREE_END_DRAG          = wx.PyEventBinder(wxEVT_COMMAND_TREE_END_DRAG         , 1)\nEVT_TREE_STATE_IMAGE_CLICK = wx.PyEventBinder(wxEVT_COMMAND_TREE_STATE_IMAGE_CLICK, 1)\nEVT_TREE_ITEM_GETTOOLTIP   = wx.PyEventBinder(wxEVT_COMMAND_TREE_ITEM_GETTOOLTIP,   1)\nEVT_TREE_ITEM_MENU         = wx.PyEventBinder(wxEVT_COMMAND_TREE_ITEM_MENU,         1)\n\nclass TreeEvent(_core.NotifyEvent):\n    \"\"\"Proxy of C++ TreeEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args): \n        \"\"\"\n        __init__(self, EventType commandType=wxEVT_NULL, int id=0) -> TreeEvent\n        __init__(self, EventType commandType, TreeCtrl tree, TreeItemId item=NullTreeItemId) -> TreeEvent\n        \"\"\"\n        _controls_.TreeEvent_swiginit(self,_controls_.new_TreeEvent(*args))\n    def GetItem(*args, **kwargs):\n        \"\"\"GetItem(self) -> TreeItemId\"\"\"\n        return _controls_.TreeEvent_GetItem(*args, **kwargs)\n\n    def SetItem(*args, **kwargs):\n        \"\"\"SetItem(self, TreeItemId item)\"\"\"\n        return _controls_.TreeEvent_SetItem(*args, **kwargs)\n\n    def GetOldItem(*args, **kwargs):\n        \"\"\"GetOldItem(self) -> TreeItemId\"\"\"\n        return _controls_.TreeEvent_GetOldItem(*args, **kwargs)\n\n    def SetOldItem(*args, **kwargs):\n        \"\"\"SetOldItem(self, TreeItemId item)\"\"\"\n        return _controls_.TreeEvent_SetOldItem(*args, **kwargs)\n\n    def GetPoint(*args, **kwargs):\n        \"\"\"GetPoint(self) -> Point\"\"\"\n        return _controls_.TreeEvent_GetPoint(*args, **kwargs)\n\n    def SetPoint(*args, **kwargs):\n        \"\"\"SetPoint(self, Point pt)\"\"\"\n        return _controls_.TreeEvent_SetPoint(*args, **kwargs)\n\n    def GetKeyEvent(*args, **kwargs):\n        \"\"\"GetKeyEvent(self) -> KeyEvent\"\"\"\n        return _controls_.TreeEvent_GetKeyEvent(*args, **kwargs)\n\n    def GetKeyCode(*args, **kwargs):\n        \"\"\"GetKeyCode(self) -> int\"\"\"\n        return _controls_.TreeEvent_GetKeyCode(*args, **kwargs)\n\n    def SetKeyEvent(*args, **kwargs):\n        \"\"\"SetKeyEvent(self, KeyEvent evt)\"\"\"\n        return _controls_.TreeEvent_SetKeyEvent(*args, **kwargs)\n\n    def GetLabel(*args, **kwargs):\n        \"\"\"GetLabel(self) -> String\"\"\"\n        return _controls_.TreeEvent_GetLabel(*args, **kwargs)\n\n    def SetLabel(*args, **kwargs):\n        \"\"\"SetLabel(self, String label)\"\"\"\n        return _controls_.TreeEvent_SetLabel(*args, **kwargs)\n\n    def IsEditCancelled(*args, **kwargs):\n        \"\"\"IsEditCancelled(self) -> bool\"\"\"\n        return _controls_.TreeEvent_IsEditCancelled(*args, **kwargs)\n\n    def SetEditCanceled(*args, **kwargs):\n        \"\"\"SetEditCanceled(self, bool editCancelled)\"\"\"\n        return _controls_.TreeEvent_SetEditCanceled(*args, **kwargs)\n\n    def SetToolTip(*args, **kwargs):\n        \"\"\"SetToolTip(self, String toolTip)\"\"\"\n        return _controls_.TreeEvent_SetToolTip(*args, **kwargs)\n\n    def GetToolTip(*args, **kwargs):\n        \"\"\"GetToolTip(self) -> String\"\"\"\n        return _controls_.TreeEvent_GetToolTip(*args, **kwargs)\n\n    Item = property(GetItem,SetItem,doc=\"See `GetItem` and `SetItem`\") \n    KeyCode = property(GetKeyCode,doc=\"See `GetKeyCode`\") \n    KeyEvent = property(GetKeyEvent,SetKeyEvent,doc=\"See `GetKeyEvent` and `SetKeyEvent`\") \n    Label = property(GetLabel,SetLabel,doc=\"See `GetLabel` and `SetLabel`\") \n    OldItem = property(GetOldItem,SetOldItem,doc=\"See `GetOldItem` and `SetOldItem`\") \n    Point = property(GetPoint,SetPoint,doc=\"See `GetPoint` and `SetPoint`\") \n    ToolTip = property(GetToolTip,SetToolTip,doc=\"See `GetToolTip` and `SetToolTip`\") \n    EditCancelled = property(IsEditCancelled,SetEditCanceled,doc=\"See `IsEditCancelled` and `SetEditCanceled`\") \n_controls_.TreeEvent_swigregister(TreeEvent)\n\n#---------------------------------------------------------------------------\n\nclass TreeCtrl(_core.Control):\n    \"\"\"Proxy of C++ TreeCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=TR_DEFAULT_STYLE, \n            Validator validator=DefaultValidator, \n            String name=TreeCtrlNameStr) -> TreeCtrl\n        \"\"\"\n        _controls_.TreeCtrl_swiginit(self,_controls_.new_TreeCtrl(*args, **kwargs))\n        self._setOORInfo(self);TreeCtrl._setCallbackInfo(self, self, TreeCtrl)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=TR_DEFAULT_STYLE, \n            Validator validator=DefaultValidator, \n            String name=TreeCtrlNameStr) -> bool\n\n        Do the 2nd phase and create the GUI control.\n        \"\"\"\n        return _controls_.TreeCtrl_Create(*args, **kwargs)\n\n    def _setCallbackInfo(*args, **kwargs):\n        \"\"\"_setCallbackInfo(self, PyObject self, PyObject _class)\"\"\"\n        return _controls_.TreeCtrl__setCallbackInfo(*args, **kwargs)\n\n    def GetCount(*args, **kwargs):\n        \"\"\"GetCount(self) -> unsigned int\"\"\"\n        return _controls_.TreeCtrl_GetCount(*args, **kwargs)\n\n    def GetIndent(*args, **kwargs):\n        \"\"\"GetIndent(self) -> unsigned int\"\"\"\n        return _controls_.TreeCtrl_GetIndent(*args, **kwargs)\n\n    def SetIndent(*args, **kwargs):\n        \"\"\"SetIndent(self, unsigned int indent)\"\"\"\n        return _controls_.TreeCtrl_SetIndent(*args, **kwargs)\n\n    def GetSpacing(*args, **kwargs):\n        \"\"\"GetSpacing(self) -> unsigned int\"\"\"\n        return _controls_.TreeCtrl_GetSpacing(*args, **kwargs)\n\n    def SetSpacing(*args, **kwargs):\n        \"\"\"SetSpacing(self, unsigned int spacing)\"\"\"\n        return _controls_.TreeCtrl_SetSpacing(*args, **kwargs)\n\n    def GetImageList(*args, **kwargs):\n        \"\"\"GetImageList(self) -> ImageList\"\"\"\n        return _controls_.TreeCtrl_GetImageList(*args, **kwargs)\n\n    def GetStateImageList(*args, **kwargs):\n        \"\"\"GetStateImageList(self) -> ImageList\"\"\"\n        return _controls_.TreeCtrl_GetStateImageList(*args, **kwargs)\n\n    def SetImageList(*args, **kwargs):\n        \"\"\"SetImageList(self, ImageList imageList)\"\"\"\n        return _controls_.TreeCtrl_SetImageList(*args, **kwargs)\n\n    def SetStateImageList(*args, **kwargs):\n        \"\"\"SetStateImageList(self, ImageList imageList)\"\"\"\n        return _controls_.TreeCtrl_SetStateImageList(*args, **kwargs)\n\n    def AssignImageList(*args, **kwargs):\n        \"\"\"AssignImageList(self, ImageList imageList)\"\"\"\n        return _controls_.TreeCtrl_AssignImageList(*args, **kwargs)\n\n    def AssignStateImageList(*args, **kwargs):\n        \"\"\"AssignStateImageList(self, ImageList imageList)\"\"\"\n        return _controls_.TreeCtrl_AssignStateImageList(*args, **kwargs)\n\n    def GetItemText(*args, **kwargs):\n        \"\"\"GetItemText(self, TreeItemId item) -> String\"\"\"\n        return _controls_.TreeCtrl_GetItemText(*args, **kwargs)\n\n    def GetItemImage(*args, **kwargs):\n        \"\"\"GetItemImage(self, TreeItemId item, int which=TreeItemIcon_Normal) -> int\"\"\"\n        return _controls_.TreeCtrl_GetItemImage(*args, **kwargs)\n\n    def GetItemData(*args, **kwargs):\n        \"\"\"GetItemData(self, TreeItemId item) -> TreeItemData\"\"\"\n        return _controls_.TreeCtrl_GetItemData(*args, **kwargs)\n\n    def GetItemPyData(*args, **kwargs):\n        \"\"\"GetItemPyData(self, TreeItemId item) -> PyObject\"\"\"\n        return _controls_.TreeCtrl_GetItemPyData(*args, **kwargs)\n\n    GetPyData = GetItemPyData \n    def GetItemTextColour(*args, **kwargs):\n        \"\"\"GetItemTextColour(self, TreeItemId item) -> Colour\"\"\"\n        return _controls_.TreeCtrl_GetItemTextColour(*args, **kwargs)\n\n    def GetItemBackgroundColour(*args, **kwargs):\n        \"\"\"GetItemBackgroundColour(self, TreeItemId item) -> Colour\"\"\"\n        return _controls_.TreeCtrl_GetItemBackgroundColour(*args, **kwargs)\n\n    def GetItemFont(*args, **kwargs):\n        \"\"\"GetItemFont(self, TreeItemId item) -> Font\"\"\"\n        return _controls_.TreeCtrl_GetItemFont(*args, **kwargs)\n\n    def GetItemState(*args, **kwargs):\n        \"\"\"GetItemState(self, TreeItemId item) -> int\"\"\"\n        return _controls_.TreeCtrl_GetItemState(*args, **kwargs)\n\n    def SetItemText(*args, **kwargs):\n        \"\"\"SetItemText(self, TreeItemId item, String text)\"\"\"\n        return _controls_.TreeCtrl_SetItemText(*args, **kwargs)\n\n    def SetItemImage(*args, **kwargs):\n        \"\"\"SetItemImage(self, TreeItemId item, int image, int which=TreeItemIcon_Normal)\"\"\"\n        return _controls_.TreeCtrl_SetItemImage(*args, **kwargs)\n\n    def SetItemData(*args, **kwargs):\n        \"\"\"SetItemData(self, TreeItemId item, TreeItemData data)\"\"\"\n        return _controls_.TreeCtrl_SetItemData(*args, **kwargs)\n\n    def SetItemPyData(*args, **kwargs):\n        \"\"\"SetItemPyData(self, TreeItemId item, PyObject obj)\"\"\"\n        return _controls_.TreeCtrl_SetItemPyData(*args, **kwargs)\n\n    SetPyData = SetItemPyData \n    def SetItemHasChildren(*args, **kwargs):\n        \"\"\"SetItemHasChildren(self, TreeItemId item, bool has=True)\"\"\"\n        return _controls_.TreeCtrl_SetItemHasChildren(*args, **kwargs)\n\n    def SetItemBold(*args, **kwargs):\n        \"\"\"SetItemBold(self, TreeItemId item, bool bold=True)\"\"\"\n        return _controls_.TreeCtrl_SetItemBold(*args, **kwargs)\n\n    def SetItemDropHighlight(*args, **kwargs):\n        \"\"\"SetItemDropHighlight(self, TreeItemId item, bool highlight=True)\"\"\"\n        return _controls_.TreeCtrl_SetItemDropHighlight(*args, **kwargs)\n\n    def SetItemTextColour(*args, **kwargs):\n        \"\"\"SetItemTextColour(self, TreeItemId item, Colour col)\"\"\"\n        return _controls_.TreeCtrl_SetItemTextColour(*args, **kwargs)\n\n    def SetItemBackgroundColour(*args, **kwargs):\n        \"\"\"SetItemBackgroundColour(self, TreeItemId item, Colour col)\"\"\"\n        return _controls_.TreeCtrl_SetItemBackgroundColour(*args, **kwargs)\n\n    def SetItemFont(*args, **kwargs):\n        \"\"\"SetItemFont(self, TreeItemId item, Font font)\"\"\"\n        return _controls_.TreeCtrl_SetItemFont(*args, **kwargs)\n\n    def SetItemState(*args, **kwargs):\n        \"\"\"SetItemState(self, TreeItemId item, int state)\"\"\"\n        return _controls_.TreeCtrl_SetItemState(*args, **kwargs)\n\n    def IsVisible(*args, **kwargs):\n        \"\"\"IsVisible(self, TreeItemId item) -> bool\"\"\"\n        return _controls_.TreeCtrl_IsVisible(*args, **kwargs)\n\n    def ItemHasChildren(*args, **kwargs):\n        \"\"\"ItemHasChildren(self, TreeItemId item) -> bool\"\"\"\n        return _controls_.TreeCtrl_ItemHasChildren(*args, **kwargs)\n\n    def IsExpanded(*args, **kwargs):\n        \"\"\"IsExpanded(self, TreeItemId item) -> bool\"\"\"\n        return _controls_.TreeCtrl_IsExpanded(*args, **kwargs)\n\n    def IsSelected(*args, **kwargs):\n        \"\"\"IsSelected(self, TreeItemId item) -> bool\"\"\"\n        return _controls_.TreeCtrl_IsSelected(*args, **kwargs)\n\n    def IsBold(*args, **kwargs):\n        \"\"\"IsBold(self, TreeItemId item) -> bool\"\"\"\n        return _controls_.TreeCtrl_IsBold(*args, **kwargs)\n\n    def IsEmpty(*args, **kwargs):\n        \"\"\"IsEmpty(self) -> bool\"\"\"\n        return _controls_.TreeCtrl_IsEmpty(*args, **kwargs)\n\n    def GetChildrenCount(*args, **kwargs):\n        \"\"\"GetChildrenCount(self, TreeItemId item, bool recursively=True) -> size_t\"\"\"\n        return _controls_.TreeCtrl_GetChildrenCount(*args, **kwargs)\n\n    def GetRootItem(*args, **kwargs):\n        \"\"\"GetRootItem(self) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetRootItem(*args, **kwargs)\n\n    def GetSelection(*args, **kwargs):\n        \"\"\"GetSelection(self) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetSelection(*args, **kwargs)\n\n    def GetSelections(*args, **kwargs):\n        \"\"\"GetSelections(self) -> PyObject\"\"\"\n        return _controls_.TreeCtrl_GetSelections(*args, **kwargs)\n\n    def GetFocusedItem(*args, **kwargs):\n        \"\"\"GetFocusedItem(self) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetFocusedItem(*args, **kwargs)\n\n    def ClearFocusedItem(*args, **kwargs):\n        \"\"\"ClearFocusedItem(self)\"\"\"\n        return _controls_.TreeCtrl_ClearFocusedItem(*args, **kwargs)\n\n    def SetFocusedItem(*args, **kwargs):\n        \"\"\"SetFocusedItem(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_SetFocusedItem(*args, **kwargs)\n\n    def GetItemParent(*args, **kwargs):\n        \"\"\"GetItemParent(self, TreeItemId item) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetItemParent(*args, **kwargs)\n\n    def GetFirstChild(*args, **kwargs):\n        \"\"\"GetFirstChild(self, TreeItemId item) -> PyObject\"\"\"\n        return _controls_.TreeCtrl_GetFirstChild(*args, **kwargs)\n\n    def GetNextChild(*args, **kwargs):\n        \"\"\"GetNextChild(self, TreeItemId item, void cookie) -> PyObject\"\"\"\n        return _controls_.TreeCtrl_GetNextChild(*args, **kwargs)\n\n    def GetLastChild(*args, **kwargs):\n        \"\"\"GetLastChild(self, TreeItemId item) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetLastChild(*args, **kwargs)\n\n    def GetNextSibling(*args, **kwargs):\n        \"\"\"GetNextSibling(self, TreeItemId item) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetNextSibling(*args, **kwargs)\n\n    def GetPrevSibling(*args, **kwargs):\n        \"\"\"GetPrevSibling(self, TreeItemId item) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetPrevSibling(*args, **kwargs)\n\n    def GetFirstVisibleItem(*args, **kwargs):\n        \"\"\"GetFirstVisibleItem(self) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetFirstVisibleItem(*args, **kwargs)\n\n    def GetNextVisible(*args, **kwargs):\n        \"\"\"GetNextVisible(self, TreeItemId item) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetNextVisible(*args, **kwargs)\n\n    def GetPrevVisible(*args, **kwargs):\n        \"\"\"GetPrevVisible(self, TreeItemId item) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_GetPrevVisible(*args, **kwargs)\n\n    def AddRoot(*args, **kwargs):\n        \"\"\"AddRoot(self, String text, int image=-1, int selectedImage=-1, TreeItemData data=None) -> TreeItemId\"\"\"\n        return _controls_.TreeCtrl_AddRoot(*args, **kwargs)\n\n    def PrependItem(*args, **kwargs):\n        \"\"\"\n        PrependItem(self, TreeItemId parent, String text, int image=-1, int selectedImage=-1, \n            TreeItemData data=None) -> TreeItemId\n        \"\"\"\n        return _controls_.TreeCtrl_PrependItem(*args, **kwargs)\n\n    def InsertItem(*args, **kwargs):\n        \"\"\"\n        InsertItem(self, TreeItemId parent, TreeItemId idPrevious, String text, \n            int image=-1, int selectedImage=-1, TreeItemData data=None) -> TreeItemId\n        \"\"\"\n        return _controls_.TreeCtrl_InsertItem(*args, **kwargs)\n\n    def InsertItemBefore(*args, **kwargs):\n        \"\"\"\n        InsertItemBefore(self, TreeItemId parent, size_t index, String text, int image=-1, \n            int selectedImage=-1, TreeItemData data=None) -> TreeItemId\n        \"\"\"\n        return _controls_.TreeCtrl_InsertItemBefore(*args, **kwargs)\n\n    def AppendItem(*args, **kwargs):\n        \"\"\"\n        AppendItem(self, TreeItemId parent, String text, int image=-1, int selectedImage=-1, \n            TreeItemData data=None) -> TreeItemId\n        \"\"\"\n        return _controls_.TreeCtrl_AppendItem(*args, **kwargs)\n\n    def Delete(*args, **kwargs):\n        \"\"\"Delete(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_Delete(*args, **kwargs)\n\n    def DeleteChildren(*args, **kwargs):\n        \"\"\"DeleteChildren(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_DeleteChildren(*args, **kwargs)\n\n    def DeleteAllItems(*args, **kwargs):\n        \"\"\"DeleteAllItems(self)\"\"\"\n        return _controls_.TreeCtrl_DeleteAllItems(*args, **kwargs)\n\n    def Expand(*args, **kwargs):\n        \"\"\"Expand(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_Expand(*args, **kwargs)\n\n    def ExpandAllChildren(*args, **kwargs):\n        \"\"\"ExpandAllChildren(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_ExpandAllChildren(*args, **kwargs)\n\n    def ExpandAll(*args, **kwargs):\n        \"\"\"ExpandAll(self)\"\"\"\n        return _controls_.TreeCtrl_ExpandAll(*args, **kwargs)\n\n    def Collapse(*args, **kwargs):\n        \"\"\"Collapse(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_Collapse(*args, **kwargs)\n\n    def CollapseAllChildren(*args, **kwargs):\n        \"\"\"CollapseAllChildren(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_CollapseAllChildren(*args, **kwargs)\n\n    def CollapseAll(*args, **kwargs):\n        \"\"\"CollapseAll(self)\"\"\"\n        return _controls_.TreeCtrl_CollapseAll(*args, **kwargs)\n\n    def CollapseAndReset(*args, **kwargs):\n        \"\"\"CollapseAndReset(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_CollapseAndReset(*args, **kwargs)\n\n    def Toggle(*args, **kwargs):\n        \"\"\"Toggle(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_Toggle(*args, **kwargs)\n\n    def Unselect(*args, **kwargs):\n        \"\"\"Unselect(self)\"\"\"\n        return _controls_.TreeCtrl_Unselect(*args, **kwargs)\n\n    def UnselectItem(*args, **kwargs):\n        \"\"\"UnselectItem(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_UnselectItem(*args, **kwargs)\n\n    def UnselectAll(*args, **kwargs):\n        \"\"\"UnselectAll(self)\"\"\"\n        return _controls_.TreeCtrl_UnselectAll(*args, **kwargs)\n\n    def SelectItem(*args, **kwargs):\n        \"\"\"SelectItem(self, TreeItemId item, bool select=True)\"\"\"\n        return _controls_.TreeCtrl_SelectItem(*args, **kwargs)\n\n    def SelectChildren(*args, **kwargs):\n        \"\"\"SelectChildren(self, TreeItemId parent)\"\"\"\n        return _controls_.TreeCtrl_SelectChildren(*args, **kwargs)\n\n    def ToggleItemSelection(*args, **kwargs):\n        \"\"\"ToggleItemSelection(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_ToggleItemSelection(*args, **kwargs)\n\n    def EnsureVisible(*args, **kwargs):\n        \"\"\"EnsureVisible(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_EnsureVisible(*args, **kwargs)\n\n    def ScrollTo(*args, **kwargs):\n        \"\"\"ScrollTo(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_ScrollTo(*args, **kwargs)\n\n    def EditLabel(*args, **kwargs):\n        \"\"\"EditLabel(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_EditLabel(*args, **kwargs)\n\n    def GetEditControl(*args, **kwargs):\n        \"\"\"GetEditControl(self) -> TextCtrl\"\"\"\n        return _controls_.TreeCtrl_GetEditControl(*args, **kwargs)\n\n    def SortChildren(*args, **kwargs):\n        \"\"\"SortChildren(self, TreeItemId item)\"\"\"\n        return _controls_.TreeCtrl_SortChildren(*args, **kwargs)\n\n    def HitTest(*args, **kwargs):\n        \"\"\"\n        HitTest(Point point) -> (item, where)\n\n        Determine which item (if any) belongs the given point.  The coordinates\n        specified are relative to the client area of tree ctrl and the where return\n        value is set to a bitmask of wxTREE_HITTEST_xxx constants.\n\n        \"\"\"\n        return _controls_.TreeCtrl_HitTest(*args, **kwargs)\n\n    def GetBoundingRect(*args, **kwargs):\n        \"\"\"GetBoundingRect(self, TreeItemId item, bool textOnly=False) -> PyObject\"\"\"\n        return _controls_.TreeCtrl_GetBoundingRect(*args, **kwargs)\n\n    def GetClassDefaultAttributes(*args, **kwargs):\n        \"\"\"\n        GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n        Get the default attributes for this class.  This is useful if you want\n        to use the same font or colour in your own control as in a standard\n        control -- which is a much better idea than hard coding specific\n        colours or fonts which might look completely out of place on the\n        user's system, especially if it uses themes.\n\n        The variant parameter is only relevant under Mac currently and is\n        ignore under other platforms. Under Mac, it will change the size of\n        the returned font. See `wx.Window.SetWindowVariant` for more about\n        this.\n        \"\"\"\n        return _controls_.TreeCtrl_GetClassDefaultAttributes(*args, **kwargs)\n\n    GetClassDefaultAttributes = staticmethod(GetClassDefaultAttributes)\n    def SetQuickBestSize(*args, **kwargs):\n        \"\"\"SetQuickBestSize(self, bool q)\"\"\"\n        return _controls_.TreeCtrl_SetQuickBestSize(*args, **kwargs)\n\n    def GetQuickBestSize(*args, **kwargs):\n        \"\"\"GetQuickBestSize(self) -> bool\"\"\"\n        return _controls_.TreeCtrl_GetQuickBestSize(*args, **kwargs)\n\n    Count = property(GetCount,doc=\"See `GetCount`\") \n    EditControl = property(GetEditControl,doc=\"See `GetEditControl`\") \n    FirstVisibleItem = property(GetFirstVisibleItem,doc=\"See `GetFirstVisibleItem`\") \n    ImageList = property(GetImageList,SetImageList,doc=\"See `GetImageList` and `SetImageList`\") \n    Indent = property(GetIndent,SetIndent,doc=\"See `GetIndent` and `SetIndent`\") \n    QuickBestSize = property(GetQuickBestSize,SetQuickBestSize,doc=\"See `GetQuickBestSize` and `SetQuickBestSize`\") \n    RootItem = property(GetRootItem,doc=\"See `GetRootItem`\") \n    Selection = property(GetSelection,doc=\"See `GetSelection`\") \n    Selections = property(GetSelections,doc=\"See `GetSelections`\") \n    Spacing = property(GetSpacing,SetSpacing,doc=\"See `GetSpacing` and `SetSpacing`\") \n    StateImageList = property(GetStateImageList,SetStateImageList,doc=\"See `GetStateImageList` and `SetStateImageList`\") \n_controls_.TreeCtrl_swigregister(TreeCtrl)\n\ndef PreTreeCtrl(*args, **kwargs):\n    \"\"\"PreTreeCtrl() -> TreeCtrl\"\"\"\n    val = _controls_.new_PreTreeCtrl(*args, **kwargs)\n    return val\n\ndef TreeCtrl_GetClassDefaultAttributes(*args, **kwargs):\n  \"\"\"\n    TreeCtrl_GetClassDefaultAttributes(int variant=WINDOW_VARIANT_NORMAL) -> VisualAttributes\n\n    Get the default attributes for this class.  This is useful if you want\n    to use the same font or colour in your own control as in a standard\n    control -- which is a much better idea than hard coding specific\n    colours or fonts which might look completely out of place on the\n    user's system, especially if it uses themes.\n\n    The variant parameter is only relevant under Mac currently and is\n    ignore under other platforms. Under Mac, it will change the size of\n    the returned font. See `wx.Window.SetWindowVariant` for more about\n    this.\n    \"\"\"\n  return _controls_.TreeCtrl_GetClassDefaultAttributes(*args, **kwargs)\n\n#---------------------------------------------------------------------------\n\nDIRCTRL_DIR_ONLY = _controls_.DIRCTRL_DIR_ONLY\nDIRCTRL_SELECT_FIRST = _controls_.DIRCTRL_SELECT_FIRST\nDIRCTRL_SHOW_FILTERS = _controls_.DIRCTRL_SHOW_FILTERS\nDIRCTRL_3D_INTERNAL = _controls_.DIRCTRL_3D_INTERNAL\nDIRCTRL_EDIT_LABELS = _controls_.DIRCTRL_EDIT_LABELS\nDIRCTRL_MULTIPLE = _controls_.DIRCTRL_MULTIPLE\nclass DirItemData(_core.Object):\n    \"\"\"Proxy of C++ DirItemData class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    def __init__(self): raise AttributeError, \"No constructor defined\"\n    __repr__ = _swig_repr\n    def SetNewDirName(*args, **kwargs):\n        \"\"\"SetNewDirName(self, String path)\"\"\"\n        return _controls_.DirItemData_SetNewDirName(*args, **kwargs)\n\n    m_path = property(_controls_.DirItemData_m_path_get, _controls_.DirItemData_m_path_set)\n    m_name = property(_controls_.DirItemData_m_name_get, _controls_.DirItemData_m_name_set)\n    m_isHidden = property(_controls_.DirItemData_m_isHidden_get, _controls_.DirItemData_m_isHidden_set)\n    m_isExpanded = property(_controls_.DirItemData_m_isExpanded_get, _controls_.DirItemData_m_isExpanded_set)\n    m_isDir = property(_controls_.DirItemData_m_isDir_get, _controls_.DirItemData_m_isDir_set)\n_controls_.DirItemData_swigregister(DirItemData)\nDirDialogDefaultFolderStr = cvar.DirDialogDefaultFolderStr\n\nclass GenericDirCtrl(_core.Control):\n    \"\"\"Proxy of C++ GenericDirCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String dir=DirDialogDefaultFolderStr, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=DIRCTRL_3D_INTERNAL, \n            String filter=EmptyString, int defaultFilter=0, \n            String name=TreeCtrlNameStr) -> GenericDirCtrl\n        \"\"\"\n        _controls_.GenericDirCtrl_swiginit(self,_controls_.new_GenericDirCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String dir=DirDialogDefaultFolderStr, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=DIRCTRL_3D_INTERNAL, \n            String filter=EmptyString, int defaultFilter=0, \n            String name=TreeCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.GenericDirCtrl_Create(*args, **kwargs)\n\n    def ExpandPath(*args, **kwargs):\n        \"\"\"ExpandPath(self, String path) -> bool\"\"\"\n        return _controls_.GenericDirCtrl_ExpandPath(*args, **kwargs)\n\n    def CollapsePath(*args, **kwargs):\n        \"\"\"CollapsePath(self, String path) -> bool\"\"\"\n        return _controls_.GenericDirCtrl_CollapsePath(*args, **kwargs)\n\n    def GetDefaultPath(*args, **kwargs):\n        \"\"\"GetDefaultPath(self) -> String\"\"\"\n        return _controls_.GenericDirCtrl_GetDefaultPath(*args, **kwargs)\n\n    def SetDefaultPath(*args, **kwargs):\n        \"\"\"SetDefaultPath(self, String path)\"\"\"\n        return _controls_.GenericDirCtrl_SetDefaultPath(*args, **kwargs)\n\n    def GetPath(*args, **kwargs):\n        \"\"\"GetPath(self) -> String\"\"\"\n        return _controls_.GenericDirCtrl_GetPath(*args, **kwargs)\n\n    def GetPaths(*args, **kwargs):\n        \"\"\"GetPaths(self) -> wxArrayString\"\"\"\n        return _controls_.GenericDirCtrl_GetPaths(*args, **kwargs)\n\n    def GetFilePath(*args, **kwargs):\n        \"\"\"GetFilePath(self) -> String\"\"\"\n        return _controls_.GenericDirCtrl_GetFilePath(*args, **kwargs)\n\n    def SetPath(*args, **kwargs):\n        \"\"\"SetPath(self, String path)\"\"\"\n        return _controls_.GenericDirCtrl_SetPath(*args, **kwargs)\n\n    def GetFilePaths(*args, **kwargs):\n        \"\"\"GetFilePaths(self) -> wxArrayString\"\"\"\n        return _controls_.GenericDirCtrl_GetFilePaths(*args, **kwargs)\n\n    def SelectPath(*args, **kwargs):\n        \"\"\"SelectPath(self, String path, bool select=True)\"\"\"\n        return _controls_.GenericDirCtrl_SelectPath(*args, **kwargs)\n\n    def SelectPaths(*args, **kwargs):\n        \"\"\"SelectPaths(self, wxArrayString paths)\"\"\"\n        return _controls_.GenericDirCtrl_SelectPaths(*args, **kwargs)\n\n    def ShowHidden(*args, **kwargs):\n        \"\"\"ShowHidden(self, bool show)\"\"\"\n        return _controls_.GenericDirCtrl_ShowHidden(*args, **kwargs)\n\n    def GetShowHidden(*args, **kwargs):\n        \"\"\"GetShowHidden(self) -> bool\"\"\"\n        return _controls_.GenericDirCtrl_GetShowHidden(*args, **kwargs)\n\n    def GetFilter(*args, **kwargs):\n        \"\"\"GetFilter(self) -> String\"\"\"\n        return _controls_.GenericDirCtrl_GetFilter(*args, **kwargs)\n\n    def SetFilter(*args, **kwargs):\n        \"\"\"SetFilter(self, String filter)\"\"\"\n        return _controls_.GenericDirCtrl_SetFilter(*args, **kwargs)\n\n    def GetFilterIndex(*args, **kwargs):\n        \"\"\"GetFilterIndex(self) -> int\"\"\"\n        return _controls_.GenericDirCtrl_GetFilterIndex(*args, **kwargs)\n\n    def SetFilterIndex(*args, **kwargs):\n        \"\"\"SetFilterIndex(self, int n)\"\"\"\n        return _controls_.GenericDirCtrl_SetFilterIndex(*args, **kwargs)\n\n    def GetRootId(*args, **kwargs):\n        \"\"\"GetRootId(self) -> TreeItemId\"\"\"\n        return _controls_.GenericDirCtrl_GetRootId(*args, **kwargs)\n\n    def GetTreeCtrl(*args, **kwargs):\n        \"\"\"GetTreeCtrl(self) -> TreeCtrl\"\"\"\n        return _controls_.GenericDirCtrl_GetTreeCtrl(*args, **kwargs)\n\n    def GetFilterListCtrl(*args, **kwargs):\n        \"\"\"GetFilterListCtrl(self) -> DirFilterListCtrl\"\"\"\n        return _controls_.GenericDirCtrl_GetFilterListCtrl(*args, **kwargs)\n\n    def UnselectAll(*args, **kwargs):\n        \"\"\"UnselectAll(self)\"\"\"\n        return _controls_.GenericDirCtrl_UnselectAll(*args, **kwargs)\n\n    def GetDirItemData(*args, **kwargs):\n        \"\"\"GetDirItemData(self, TreeItemId id) -> DirItemData\"\"\"\n        return _controls_.GenericDirCtrl_GetDirItemData(*args, **kwargs)\n\n    def FindChild(*args, **kwargs):\n        \"\"\"\n        FindChild(wxTreeItemId parentId, wxString path) -> (item, done)\n\n        Find the child that matches the first part of 'path'.  E.g. if a child\n        path is \"\/usr\" and 'path' is \"\/usr\/include\" then the child for\n        \/usr is returned.  If the path string has been used (we're at the\n        leaf), done is set to True.\n\n        \"\"\"\n        return _controls_.GenericDirCtrl_FindChild(*args, **kwargs)\n\n    def DoResize(*args, **kwargs):\n        \"\"\"DoResize(self)\"\"\"\n        return _controls_.GenericDirCtrl_DoResize(*args, **kwargs)\n\n    def ReCreateTree(*args, **kwargs):\n        \"\"\"ReCreateTree(self)\"\"\"\n        return _controls_.GenericDirCtrl_ReCreateTree(*args, **kwargs)\n\n    DefaultPath = property(GetDefaultPath,SetDefaultPath,doc=\"See `GetDefaultPath` and `SetDefaultPath`\") \n    FilePath = property(GetFilePath,doc=\"See `GetFilePath`\") \n    Filter = property(GetFilter,SetFilter,doc=\"See `GetFilter` and `SetFilter`\") \n    FilterIndex = property(GetFilterIndex,SetFilterIndex,doc=\"See `GetFilterIndex` and `SetFilterIndex`\") \n    FilterListCtrl = property(GetFilterListCtrl,doc=\"See `GetFilterListCtrl`\") \n    Path = property(GetPath,SetPath,doc=\"See `GetPath` and `SetPath`\") \n    RootId = property(GetRootId,doc=\"See `GetRootId`\") \n    TreeCtrl = property(GetTreeCtrl,doc=\"See `GetTreeCtrl`\") \n_controls_.GenericDirCtrl_swigregister(GenericDirCtrl)\n\ndef PreGenericDirCtrl(*args, **kwargs):\n    \"\"\"PreGenericDirCtrl() -> GenericDirCtrl\"\"\"\n    val = _controls_.new_PreGenericDirCtrl(*args, **kwargs)\n    return val\n\nclass DirFilterListCtrl(Choice):\n    \"\"\"Proxy of C++ DirFilterListCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, GenericDirCtrl parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=0) -> DirFilterListCtrl\n        \"\"\"\n        _controls_.DirFilterListCtrl_swiginit(self,_controls_.new_DirFilterListCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, GenericDirCtrl parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=0) -> bool\n        \"\"\"\n        return _controls_.DirFilterListCtrl_Create(*args, **kwargs)\n\n    def FillFilterList(*args, **kwargs):\n        \"\"\"FillFilterList(self, String filter, int defaultFilter)\"\"\"\n        return _controls_.DirFilterListCtrl_FillFilterList(*args, **kwargs)\n\n_controls_.DirFilterListCtrl_swigregister(DirFilterListCtrl)\n\ndef PreDirFilterListCtrl(*args, **kwargs):\n    \"\"\"PreDirFilterListCtrl() -> DirFilterListCtrl\"\"\"\n    val = _controls_.new_PreDirFilterListCtrl(*args, **kwargs)\n    return val\n\n#---------------------------------------------------------------------------\n\nclass PyControl(_core.Control):\n    \"\"\"Proxy of C++ PyControl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=0, Validator validator=DefaultValidator, \n            String name=ControlNameStr) -> PyControl\n        \"\"\"\n        _controls_.PyControl_swiginit(self,_controls_.new_PyControl(*args, **kwargs))\n        self._setOORInfo(self);PyControl._setCallbackInfo(self, self, PyControl)\n\n    def _setCallbackInfo(*args, **kwargs):\n        \"\"\"_setCallbackInfo(self, PyObject self, PyObject _class)\"\"\"\n        return _controls_.PyControl__setCallbackInfo(*args, **kwargs)\n\n    SetBestSize = wx.Window.SetInitialSize \n    def DoEraseBackground(*args, **kwargs):\n        \"\"\"DoEraseBackground(self, DC dc) -> bool\"\"\"\n        return _controls_.PyControl_DoEraseBackground(*args, **kwargs)\n\n    def DoMoveWindow(*args, **kwargs):\n        \"\"\"DoMoveWindow(self, int x, int y, int width, int height)\"\"\"\n        return _controls_.PyControl_DoMoveWindow(*args, **kwargs)\n\n    def DoSetSize(*args, **kwargs):\n        \"\"\"DoSetSize(self, int x, int y, int width, int height, int sizeFlags=SIZE_AUTO)\"\"\"\n        return _controls_.PyControl_DoSetSize(*args, **kwargs)\n\n    def DoSetClientSize(*args, **kwargs):\n        \"\"\"DoSetClientSize(self, int width, int height)\"\"\"\n        return _controls_.PyControl_DoSetClientSize(*args, **kwargs)\n\n    def DoSetVirtualSize(*args, **kwargs):\n        \"\"\"DoSetVirtualSize(self, int x, int y)\"\"\"\n        return _controls_.PyControl_DoSetVirtualSize(*args, **kwargs)\n\n    def DoGetSize(*args, **kwargs):\n        \"\"\"DoGetSize() -> (width, height)\"\"\"\n        return _controls_.PyControl_DoGetSize(*args, **kwargs)\n\n    def DoGetClientSize(*args, **kwargs):\n        \"\"\"DoGetClientSize() -> (width, height)\"\"\"\n        return _controls_.PyControl_DoGetClientSize(*args, **kwargs)\n\n    def DoGetPosition(*args, **kwargs):\n        \"\"\"DoGetPosition() -> (x,y)\"\"\"\n        return _controls_.PyControl_DoGetPosition(*args, **kwargs)\n\n    def DoGetVirtualSize(*args, **kwargs):\n        \"\"\"DoGetVirtualSize(self) -> Size\"\"\"\n        return _controls_.PyControl_DoGetVirtualSize(*args, **kwargs)\n\n    def DoGetBestSize(*args, **kwargs):\n        \"\"\"DoGetBestSize(self) -> Size\"\"\"\n        return _controls_.PyControl_DoGetBestSize(*args, **kwargs)\n\n    def GetDefaultAttributes(*args, **kwargs):\n        \"\"\"GetDefaultAttributes(self) -> VisualAttributes\"\"\"\n        return _controls_.PyControl_GetDefaultAttributes(*args, **kwargs)\n\n    def OnInternalIdle(*args, **kwargs):\n        \"\"\"OnInternalIdle(self)\"\"\"\n        return _controls_.PyControl_OnInternalIdle(*args, **kwargs)\n\n    def base_DoMoveWindow(*args, **kw):\n        return PyControl.DoMoveWindow(*args, **kw)\n    base_DoMoveWindow = wx.deprecated(base_DoMoveWindow,\n                                   \"Please use PyControl.DoMoveWindow instead.\")\n\n    def base_DoSetSize(*args, **kw):\n        return PyControl.DoSetSize(*args, **kw)\n    base_DoSetSize = wx.deprecated(base_DoSetSize,\n                                   \"Please use PyControl.DoSetSize instead.\")\n\n    def base_DoSetClientSize(*args, **kw):\n        return PyControl.DoSetClientSize(*args, **kw)\n    base_DoSetClientSize = wx.deprecated(base_DoSetClientSize,\n                                   \"Please use PyControl.DoSetClientSize instead.\")\n\n    def base_DoSetVirtualSize(*args, **kw):\n        return PyControl.DoSetVirtualSize(*args, **kw)\n    base_DoSetVirtualSize = wx.deprecated(base_DoSetVirtualSize,\n                                   \"Please use PyControl.DoSetVirtualSize instead.\")\n\n    def base_DoGetSize(*args, **kw):\n        return PyControl.DoGetSize(*args, **kw)\n    base_DoGetSize = wx.deprecated(base_DoGetSize,\n                                   \"Please use PyControl.DoGetSize instead.\")\n\n    def base_DoGetClientSize(*args, **kw):\n        return PyControl.DoGetClientSize(*args, **kw)\n    base_DoGetClientSize = wx.deprecated(base_DoGetClientSize,\n                                   \"Please use PyControl.DoGetClientSize instead.\")\n\n    def base_DoGetPosition(*args, **kw):\n        return PyControl.DoGetPosition(*args, **kw)\n    base_DoGetPosition = wx.deprecated(base_DoGetPosition,\n                                   \"Please use PyControl.DoGetPosition instead.\")\n\n    def base_DoGetVirtualSize(*args, **kw):\n        return PyControl.DoGetVirtualSize(*args, **kw)\n    base_DoGetVirtualSize = wx.deprecated(base_DoGetVirtualSize,\n                                   \"Please use PyControl.DoGetVirtualSize instead.\")\n\n    def base_DoGetBestSize(*args, **kw):\n        return PyControl.DoGetBestSize(*args, **kw)\n    base_DoGetBestSize = wx.deprecated(base_DoGetBestSize,\n                                   \"Please use PyControl.DoGetBestSize instead.\")\n\n    def base_InitDialog(*args, **kw):\n        return PyControl.InitDialog(*args, **kw)\n    base_InitDialog = wx.deprecated(base_InitDialog,\n                                   \"Please use PyControl.InitDialog instead.\")\n\n    def base_TransferDataToWindow(*args, **kw):\n        return PyControl.TransferDataToWindow(*args, **kw)\n    base_TransferDataToWindow = wx.deprecated(base_TransferDataToWindow,\n                                   \"Please use PyControl.TransferDataToWindow instead.\")\n\n    def base_TransferDataFromWindow(*args, **kw):\n        return PyControl.TransferDataFromWindow(*args, **kw)\n    base_TransferDataFromWindow = wx.deprecated(base_TransferDataFromWindow,\n                                   \"Please use PyControl.TransferDataFromWindow instead.\")\n\n    def base_Validate(*args, **kw):\n        return PyControl.Validate(*args, **kw)\n    base_Validate = wx.deprecated(base_Validate,\n                                   \"Please use PyControl.Validate instead.\")\n\n    def base_AcceptsFocus(*args, **kw):\n        return PyControl.AcceptsFocus(*args, **kw)\n    base_AcceptsFocus = wx.deprecated(base_AcceptsFocus,\n                                   \"Please use PyControl.AcceptsFocus instead.\")\n\n    def base_AcceptsFocusFromKeyboard(*args, **kw):\n        return PyControl.AcceptsFocusFromKeyboard(*args, **kw)\n    base_AcceptsFocusFromKeyboard = wx.deprecated(base_AcceptsFocusFromKeyboard,\n                                   \"Please use PyControl.AcceptsFocusFromKeyboard instead.\")\n\n    def base_GetMaxSize(*args, **kw):\n        return PyControl.GetMaxSize(*args, **kw)\n    base_GetMaxSize = wx.deprecated(base_GetMaxSize,\n                                   \"Please use PyControl.GetMaxSize instead.\")\n\n    def base_Enable(*args, **kw):\n        return PyControl.Enable(*args, **kw)\n    base_Enable = wx.deprecated(base_Enable,\n                                   \"Please use PyControl.Enable instead.\")\n\n    def base_AddChild(*args, **kw):\n        return PyControl.AddChild(*args, **kw)\n    base_AddChild = wx.deprecated(base_AddChild,\n                                   \"Please use PyControl.AddChild instead.\")\n\n    def base_RemoveChild(*args, **kw):\n        return PyControl.RemoveChild(*args, **kw)\n    base_RemoveChild = wx.deprecated(base_RemoveChild,\n                                   \"Please use PyControl.RemoveChild instead.\")\n\n    def base_ShouldInheritColours(*args, **kw):\n        return PyControl.ShouldInheritColours(*args, **kw)\n    base_ShouldInheritColours = wx.deprecated(base_ShouldInheritColours,\n                                   \"Please use PyControl.ShouldInheritColours instead.\")\n\n    def base_GetDefaultAttributes(*args, **kw):\n        return PyControl.GetDefaultAttributes(*args, **kw)\n    base_GetDefaultAttributes = wx.deprecated(base_GetDefaultAttributes,\n                                   \"Please use PyControl.GetDefaultAttributes instead.\")\n\n    def base_OnInternalIdle(*args, **kw):\n        return PyControl.OnInternalIdle(*args, **kw)\n    base_OnInternalIdle = wx.deprecated(base_OnInternalIdle,\n                                   \"Please use PyControl.OnInternalIdle instead.\")\n\n_controls_.PyControl_swigregister(PyControl)\n\ndef PrePyControl(*args, **kwargs):\n    \"\"\"PrePyControl() -> PyControl\"\"\"\n    val = _controls_.new_PrePyControl(*args, **kwargs)\n    return val\n\n#---------------------------------------------------------------------------\n\nwxEVT_HELP = _controls_.wxEVT_HELP\nwxEVT_DETAILED_HELP = _controls_.wxEVT_DETAILED_HELP\nEVT_HELP = wx.PyEventBinder( wxEVT_HELP, 1)\nEVT_HELP_RANGE = wx.PyEventBinder(  wxEVT_HELP, 2)\nEVT_DETAILED_HELP = wx.PyEventBinder( wxEVT_DETAILED_HELP, 1)\nEVT_DETAILED_HELP_RANGE = wx.PyEventBinder( wxEVT_DETAILED_HELP, 2)\n\nclass HelpEvent(_core.CommandEvent):\n    \"\"\"\n    A help event is sent when the user has requested context-sensitive\n    help. This can either be caused by the application requesting\n    context-sensitive help mode via wx.ContextHelp, or (on MS Windows) by\n    the system generating a WM_HELP message when the user pressed F1 or\n    clicked on the query button in a dialog caption.\n\n    A help event is sent to the window that the user clicked on, and is\n    propagated up the window hierarchy until the event is processed or\n    there are no more event handlers. The application should call\n    event.GetId to check the identity of the clicked-on window, and then\n    either show some suitable help or call event.Skip if the identifier is\n    unrecognised. Calling Skip is important because it allows wxWindows to\n    generate further events for ancestors of the clicked-on\n    window. Otherwise it would be impossible to show help for container\n    windows, since processing would stop after the first window found.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    Origin_Unknown = _controls_.HelpEvent_Origin_Unknown\n    Origin_Keyboard = _controls_.HelpEvent_Origin_Keyboard\n    Origin_HelpButton = _controls_.HelpEvent_Origin_HelpButton\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, EventType type=wxEVT_NULL, int winid=0, Point pt=DefaultPosition, \n            int origin=Origin_Unknown) -> HelpEvent\n        \"\"\"\n        _controls_.HelpEvent_swiginit(self,_controls_.new_HelpEvent(*args, **kwargs))\n    def GetPosition(*args, **kwargs):\n        \"\"\"\n        GetPosition(self) -> Point\n\n        Returns the left-click position of the mouse, in screen\n        coordinates. This allows the application to position the help\n        appropriately.\n        \"\"\"\n        return _controls_.HelpEvent_GetPosition(*args, **kwargs)\n\n    def SetPosition(*args, **kwargs):\n        \"\"\"\n        SetPosition(self, Point pos)\n\n        Sets the left-click position of the mouse, in screen coordinates.\n        \"\"\"\n        return _controls_.HelpEvent_SetPosition(*args, **kwargs)\n\n    def GetLink(*args, **kwargs):\n        \"\"\"\n        GetLink(self) -> String\n\n        Get an optional link to further help\n        \"\"\"\n        return _controls_.HelpEvent_GetLink(*args, **kwargs)\n\n    def SetLink(*args, **kwargs):\n        \"\"\"\n        SetLink(self, String link)\n\n        Set an optional link to further help\n        \"\"\"\n        return _controls_.HelpEvent_SetLink(*args, **kwargs)\n\n    def GetTarget(*args, **kwargs):\n        \"\"\"\n        GetTarget(self) -> String\n\n        Get an optional target to display help in. E.g. a window specification\n        \"\"\"\n        return _controls_.HelpEvent_GetTarget(*args, **kwargs)\n\n    def SetTarget(*args, **kwargs):\n        \"\"\"\n        SetTarget(self, String target)\n\n        Set an optional target to display help in. E.g. a window specification\n        \"\"\"\n        return _controls_.HelpEvent_SetTarget(*args, **kwargs)\n\n    def GetOrigin(*args, **kwargs):\n        \"\"\"\n        GetOrigin(self) -> int\n\n        Optiononal indication of the source of the event.\n        \"\"\"\n        return _controls_.HelpEvent_GetOrigin(*args, **kwargs)\n\n    def SetOrigin(*args, **kwargs):\n        \"\"\"SetOrigin(self, int origin)\"\"\"\n        return _controls_.HelpEvent_SetOrigin(*args, **kwargs)\n\n    Link = property(GetLink,SetLink,doc=\"See `GetLink` and `SetLink`\") \n    Origin = property(GetOrigin,SetOrigin,doc=\"See `GetOrigin` and `SetOrigin`\") \n    Position = property(GetPosition,SetPosition,doc=\"See `GetPosition` and `SetPosition`\") \n    Target = property(GetTarget,SetTarget,doc=\"See `GetTarget` and `SetTarget`\") \n_controls_.HelpEvent_swigregister(HelpEvent)\n\nclass ContextHelp(_core.Object):\n    \"\"\"\n    This class changes the cursor to a query and puts the application into\n    a 'context-sensitive help mode'. When the user left-clicks on a window\n    within the specified window, a ``EVT_HELP`` event is sent to that\n    control, and the application may respond to it by popping up some\n    help.\n\n    There are a couple of ways to invoke this behaviour implicitly:\n\n        * Use the wx.WS_EX_CONTEXTHELP extended style for a dialog or frame\n          (Windows only). This will put a question mark in the titlebar,\n          and Windows will put the application into context-sensitive help\n          mode automatically, with further programming.\n\n        * Create a `wx.ContextHelpButton`, whose predefined behaviour is\n          to create a context help object. Normally you will write your\n          application so that this button is only added to a dialog for\n          non-Windows platforms (use ``wx.WS_EX_CONTEXTHELP`` on\n          Windows).\n\n    :see: `wx.ContextHelpButton`\n\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window window=None, bool doNow=True) -> ContextHelp\n\n        Constructs a context help object, calling BeginContextHelp if doNow is\n        true (the default).\n\n        If window is None, the top window is used.\n        \"\"\"\n        _controls_.ContextHelp_swiginit(self,_controls_.new_ContextHelp(*args, **kwargs))\n    __swig_destroy__ = _controls_.delete_ContextHelp\n    __del__ = lambda self : None;\n    def BeginContextHelp(*args, **kwargs):\n        \"\"\"\n        BeginContextHelp(self, Window window=None) -> bool\n\n        Puts the application into context-sensitive help mode. window is the\n        window which will be used to catch events; if NULL, the top window\n        will be used.\n\n        Returns true if the application was successfully put into\n        context-sensitive help mode. This function only returns when the event\n        loop has finished.\n        \"\"\"\n        return _controls_.ContextHelp_BeginContextHelp(*args, **kwargs)\n\n    def EndContextHelp(*args, **kwargs):\n        \"\"\"\n        EndContextHelp(self) -> bool\n\n        Ends context-sensitive help mode. Not normally called by the\n        application.\n        \"\"\"\n        return _controls_.ContextHelp_EndContextHelp(*args, **kwargs)\n\n_controls_.ContextHelp_swigregister(ContextHelp)\n\nclass ContextHelpButton(BitmapButton):\n    \"\"\"\n    Instances of this class may be used to add a question mark button that\n    when pressed, puts the application into context-help mode. It does\n    this by creating a wx.ContextHelp object which itself generates a\n    ``EVT_HELP`` event when the user clicks on a window.\n\n    On Windows, you may add a question-mark icon to a dialog by use of the\n    ``wx.DIALOG_EX_CONTEXTHELP`` extra style, but on other platforms you\n    will have to add a button explicitly, usually next to OK, Cancel or\n    similar buttons.\n\n    :see: `wx.ContextHelp`, `wx.ContextHelpButton`\n\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=ID_CONTEXT_HELP, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=BU_AUTODRAW) -> ContextHelpButton\n\n        Constructor, creating and showing a context help button.\n        \"\"\"\n        _controls_.ContextHelpButton_swiginit(self,_controls_.new_ContextHelpButton(*args, **kwargs))\n        self._setOORInfo(self)\n\n_controls_.ContextHelpButton_swigregister(ContextHelpButton)\n\nclass HelpProvider(object):\n    \"\"\"\n    wx.HelpProvider is an abstract class used by a program\n    implementing context-sensitive help to show the help text for the\n    given window.\n\n    The current help provider must be explicitly set by the\n    application using wx.HelpProvider.Set().\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    def __init__(self): raise AttributeError, \"No constructor defined\"\n    __repr__ = _swig_repr\n    __swig_destroy__ = _controls_.delete_HelpProvider\n    __del__ = lambda self : None;\n    def Set(*args, **kwargs):\n        \"\"\"\n        Set(HelpProvider helpProvider) -> HelpProvider\n\n        Sset the current, application-wide help provider. Returns the previous\n        one.  Unlike some other classes, the help provider is not created on\n        demand. This must be explicitly done by the application.\n        \"\"\"\n        return _controls_.HelpProvider_Set(*args, **kwargs)\n\n    Set = staticmethod(Set)\n    def Get(*args, **kwargs):\n        \"\"\"\n        Get() -> HelpProvider\n\n        Return the current application-wide help provider.\n        \"\"\"\n        return _controls_.HelpProvider_Get(*args, **kwargs)\n\n    Get = staticmethod(Get)\n    def GetHelp(*args, **kwargs):\n        \"\"\"\n        GetHelp(self, Window window) -> String\n\n        Gets the help string for this window. Its interpretation is dependent\n        on the help provider except that empty string always means that no\n        help is associated with the window.\n        \"\"\"\n        return _controls_.HelpProvider_GetHelp(*args, **kwargs)\n\n    def ShowHelp(*args, **kwargs):\n        \"\"\"\n        ShowHelp(self, Window window) -> bool\n\n        Shows help for the given window. Uses GetHelp internally if\n        applicable. Returns True if it was done, or False if no help was\n        available for this window.\n        \"\"\"\n        return _controls_.HelpProvider_ShowHelp(*args, **kwargs)\n\n    def ShowHelpAtPoint(*args, **kwargs):\n        \"\"\"\n        ShowHelpAtPoint(self, wxWindowBase window, Point pt, int origin) -> bool\n\n        Show help for the given window (uses window.GetHelpAtPoint()\n        internally if applicable), return true if it was done or false if no\n        help available for this window.\n        \"\"\"\n        return _controls_.HelpProvider_ShowHelpAtPoint(*args, **kwargs)\n\n    def AddHelp(*args, **kwargs):\n        \"\"\"\n        AddHelp(self, Window window, String text)\n\n        Associates the text with the given window.\n        \"\"\"\n        return _controls_.HelpProvider_AddHelp(*args, **kwargs)\n\n    def AddHelpById(*args, **kwargs):\n        \"\"\"\n        AddHelpById(self, int id, String text)\n\n        This version associates the given text with all windows with this\n        id. May be used to set the same help string for all Cancel buttons in\n        the application, for example.\n        \"\"\"\n        return _controls_.HelpProvider_AddHelpById(*args, **kwargs)\n\n    def RemoveHelp(*args, **kwargs):\n        \"\"\"\n        RemoveHelp(self, Window window)\n\n        Removes the association between the window pointer and the help\n        text. This is called by the wx.Window destructor. Without this, the\n        table of help strings will fill up and when window pointers are\n        reused, the wrong help string will be found.\n        \"\"\"\n        return _controls_.HelpProvider_RemoveHelp(*args, **kwargs)\n\n    def Destroy(*args, **kwargs):\n        \"\"\"Destroy(self)\"\"\"\n        args[0].this.own(False)\n        return _controls_.HelpProvider_Destroy(*args, **kwargs)\n\n_controls_.HelpProvider_swigregister(HelpProvider)\n\ndef HelpProvider_Set(*args, **kwargs):\n  \"\"\"\n    HelpProvider_Set(HelpProvider helpProvider) -> HelpProvider\n\n    Sset the current, application-wide help provider. Returns the previous\n    one.  Unlike some other classes, the help provider is not created on\n    demand. This must be explicitly done by the application.\n    \"\"\"\n  return _controls_.HelpProvider_Set(*args, **kwargs)\n\ndef HelpProvider_Get(*args):\n  \"\"\"\n    HelpProvider_Get() -> HelpProvider\n\n    Return the current application-wide help provider.\n    \"\"\"\n  return _controls_.HelpProvider_Get(*args)\n\nclass SimpleHelpProvider(HelpProvider):\n    \"\"\"\n    wx.SimpleHelpProvider is an implementation of `wx.HelpProvider` which\n    supports only plain text help strings, and shows the string associated\n    with the control (if any) in a tooltip.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self) -> SimpleHelpProvider\n\n        wx.SimpleHelpProvider is an implementation of `wx.HelpProvider` which\n        supports only plain text help strings, and shows the string associated\n        with the control (if any) in a tooltip.\n        \"\"\"\n        _controls_.SimpleHelpProvider_swiginit(self,_controls_.new_SimpleHelpProvider(*args, **kwargs))\n_controls_.SimpleHelpProvider_swigregister(SimpleHelpProvider)\n\n#---------------------------------------------------------------------------\n\nclass DragImage(_core.Object):\n    \"\"\"Proxy of C++ DragImage class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, Bitmap image, Cursor cursor=wxNullCursor) -> DragImage\"\"\"\n        _controls_.DragImage_swiginit(self,_controls_.new_DragImage(*args, **kwargs))\n    __swig_destroy__ = _controls_.delete_DragImage\n    __del__ = lambda self : None;\n    def SetBackingBitmap(*args, **kwargs):\n        \"\"\"SetBackingBitmap(self, Bitmap bitmap)\"\"\"\n        return _controls_.DragImage_SetBackingBitmap(*args, **kwargs)\n\n    def BeginDrag(*args, **kwargs):\n        \"\"\"\n        BeginDrag(self, Point hotspot, Window window, bool fullScreen=False, \n            Rect rect=None) -> bool\n        \"\"\"\n        return _controls_.DragImage_BeginDrag(*args, **kwargs)\n\n    def BeginDragBounded(*args, **kwargs):\n        \"\"\"BeginDragBounded(self, Point hotspot, Window window, Window boundingWindow) -> bool\"\"\"\n        return _controls_.DragImage_BeginDragBounded(*args, **kwargs)\n\n    def EndDrag(*args, **kwargs):\n        \"\"\"EndDrag(self) -> bool\"\"\"\n        return _controls_.DragImage_EndDrag(*args, **kwargs)\n\n    def Move(*args, **kwargs):\n        \"\"\"Move(self, Point pt) -> bool\"\"\"\n        return _controls_.DragImage_Move(*args, **kwargs)\n\n    def Show(*args, **kwargs):\n        \"\"\"Show(self) -> bool\"\"\"\n        return _controls_.DragImage_Show(*args, **kwargs)\n\n    def Hide(*args, **kwargs):\n        \"\"\"Hide(self) -> bool\"\"\"\n        return _controls_.DragImage_Hide(*args, **kwargs)\n\n    def GetImageRect(*args, **kwargs):\n        \"\"\"GetImageRect(self, Point pos) -> Rect\"\"\"\n        return _controls_.DragImage_GetImageRect(*args, **kwargs)\n\n    def DoDrawImage(*args, **kwargs):\n        \"\"\"DoDrawImage(self, DC dc, Point pos) -> bool\"\"\"\n        return _controls_.DragImage_DoDrawImage(*args, **kwargs)\n\n    def UpdateBackingFromWindow(*args, **kwargs):\n        \"\"\"UpdateBackingFromWindow(self, DC windowDC, MemoryDC destDC, Rect sourceRect, Rect destRect) -> bool\"\"\"\n        return _controls_.DragImage_UpdateBackingFromWindow(*args, **kwargs)\n\n    def RedrawImage(*args, **kwargs):\n        \"\"\"RedrawImage(self, Point oldPos, Point newPos, bool eraseOld, bool drawNew) -> bool\"\"\"\n        return _controls_.DragImage_RedrawImage(*args, **kwargs)\n\n    ImageRect = property(GetImageRect,doc=\"See `GetImageRect`\") \n_controls_.DragImage_swigregister(DragImage)\n\ndef DragIcon(*args, **kwargs):\n    \"\"\"DragIcon(Icon image, Cursor cursor=wxNullCursor) -> DragImage\"\"\"\n    val = _controls_.new_DragIcon(*args, **kwargs)\n    return val\n\ndef DragString(*args, **kwargs):\n    \"\"\"DragString(String str, Cursor cursor=wxNullCursor) -> DragImage\"\"\"\n    val = _controls_.new_DragString(*args, **kwargs)\n    return val\n\ndef DragTreeItem(*args, **kwargs):\n    \"\"\"DragTreeItem(TreeCtrl treeCtrl, TreeItemId id) -> DragImage\"\"\"\n    val = _controls_.new_DragTreeItem(*args, **kwargs)\n    return val\n\ndef DragListItem(*args, **kwargs):\n    \"\"\"DragListItem(ListCtrl listCtrl, long id) -> DragImage\"\"\"\n    val = _controls_.new_DragListItem(*args, **kwargs)\n    return val\n\n#---------------------------------------------------------------------------\n\nDP_DEFAULT = _controls_.DP_DEFAULT\nDP_SPIN = _controls_.DP_SPIN\nDP_DROPDOWN = _controls_.DP_DROPDOWN\nDP_SHOWCENTURY = _controls_.DP_SHOWCENTURY\nDP_ALLOWNONE = _controls_.DP_ALLOWNONE\nclass DatePickerCtrlBase(_core.Control):\n    \"\"\"Proxy of C++ DatePickerCtrlBase class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    def __init__(self): raise AttributeError, \"No constructor defined\"\n    __repr__ = _swig_repr\n    def SetValue(*args, **kwargs):\n        \"\"\"\n        SetValue(self, DateTime dt)\n\n        Changes the current value of the control. The date should be valid and\n        included in the currently selected range, if any.\n\n        Calling this method does not result in a date change event.\n        \"\"\"\n        return _controls_.DatePickerCtrlBase_SetValue(*args, **kwargs)\n\n    def GetValue(*args, **kwargs):\n        \"\"\"\n        GetValue(self) -> DateTime\n\n        Returns the currently selected date. If there is no selection or the\n        selection is outside of the current range, an invalid `wx.DateTime`\n        object is returned.\n        \"\"\"\n        return _controls_.DatePickerCtrlBase_GetValue(*args, **kwargs)\n\n    def SetRange(*args, **kwargs):\n        \"\"\"\n        SetRange(self, DateTime dt1, DateTime dt2)\n\n        Sets the valid range for the date selection. If dt1 is valid, it\n        becomes the earliest date (inclusive) accepted by the control. If dt2\n        is valid, it becomes the latest possible date.\n\n        If the current value of the control is outside of the newly set range\n        bounds, the behaviour is undefined.\n        \"\"\"\n        return _controls_.DatePickerCtrlBase_SetRange(*args, **kwargs)\n\n    def GetLowerLimit(*args, **kwargs):\n        \"\"\"\n        GetLowerLimit(self) -> DateTime\n\n        Get the lower limit of the valid range for the date selection, if any.\n        If there is no range or there is no lower limit, then the\n        `wx.DateTime` value returned will be invalid.\n        \"\"\"\n        return _controls_.DatePickerCtrlBase_GetLowerLimit(*args, **kwargs)\n\n    def GetUpperLimit(*args, **kwargs):\n        \"\"\"\n        GetUpperLimit(self) -> DateTime\n\n        Get the upper limit of the valid range for the date selection, if any.\n        If there is no range or there is no upper limit, then the\n        `wx.DateTime` value returned will be invalid.\n        \"\"\"\n        return _controls_.DatePickerCtrlBase_GetUpperLimit(*args, **kwargs)\n\n    LowerLimit = property(GetLowerLimit,doc=\"See `GetLowerLimit`\") \n    UpperLimit = property(GetUpperLimit,doc=\"See `GetUpperLimit`\") \n    Value = property(GetValue,SetValue,doc=\"See `GetValue` and `SetValue`\") \n_controls_.DatePickerCtrlBase_swigregister(DatePickerCtrlBase)\nDatePickerCtrlNameStr = cvar.DatePickerCtrlNameStr\n\nclass DatePickerCtrl(DatePickerCtrlBase):\n    \"\"\"\n    This control allows the user to select a date. Unlike\n    `wx.calendar.CalendarCtrl`, which is a relatively big control,\n    `wx.DatePickerCtrl` is implemented as a small window showing the\n    currently selected date. The control can be edited using the keyboard,\n    and can also display a popup window for more user-friendly date\n    selection, depending on the styles used and the platform.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, DateTime dt=wxDefaultDateTime, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=wxDP_DEFAULT|wxDP_SHOWCENTURY, \n            Validator validator=DefaultValidator, \n            String name=DatePickerCtrlNameStr) -> DatePickerCtrl\n\n        Create a new DatePickerCtrl.\n        \"\"\"\n        _controls_.DatePickerCtrl_swiginit(self,_controls_.new_DatePickerCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, DateTime dt=wxDefaultDateTime, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=wxDP_DEFAULT|wxDP_SHOWCENTURY, \n            Validator validator=DefaultValidator, \n            String name=DatePickerCtrlNameStr) -> bool\n\n        Create the GUI parts of the DatePickerCtrl, for use in 2-phase\n        creation.\n        \"\"\"\n        return _controls_.DatePickerCtrl_Create(*args, **kwargs)\n\n_controls_.DatePickerCtrl_swigregister(DatePickerCtrl)\n\ndef PreDatePickerCtrl(*args, **kwargs):\n    \"\"\"\n    PreDatePickerCtrl() -> DatePickerCtrl\n\n    Precreate a DatePickerCtrl for use in 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreDatePickerCtrl(*args, **kwargs)\n    return val\n\nclass GenericDatePickerCtrl(DatePickerCtrlBase):\n    \"\"\"Proxy of C++ GenericDatePickerCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, DateTime dt=wxDefaultDateTime, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=wxDP_DEFAULT|wxDP_SHOWCENTURY, \n            Validator validator=DefaultValidator, \n            String name=DatePickerCtrlNameStr) -> GenericDatePickerCtrl\n\n        Create a new GenericDatePickerCtrl.\n        \"\"\"\n        _controls_.GenericDatePickerCtrl_swiginit(self,_controls_.new_GenericDatePickerCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, DateTime dt=wxDefaultDateTime, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=wxDP_DEFAULT|wxDP_SHOWCENTURY, \n            Validator validator=DefaultValidator, \n            String name=DatePickerCtrlNameStr) -> bool\n\n        Create the GUI parts of the GenericDatePickerCtrl, for use in 2-phase\n        creation.\n        \"\"\"\n        return _controls_.GenericDatePickerCtrl_Create(*args, **kwargs)\n\n_controls_.GenericDatePickerCtrl_swigregister(GenericDatePickerCtrl)\n\ndef PreGenericDatePickerCtrl(*args, **kwargs):\n    \"\"\"\n    PreGenericDatePickerCtrl() -> GenericDatePickerCtrl\n\n    Precreate a GenericDatePickerCtrl for use in 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreGenericDatePickerCtrl(*args, **kwargs)\n    return val\n\nHL_CONTEXTMENU = _controls_.HL_CONTEXTMENU\nHL_ALIGN_LEFT = _controls_.HL_ALIGN_LEFT\nHL_ALIGN_RIGHT = _controls_.HL_ALIGN_RIGHT\nHL_ALIGN_CENTRE = _controls_.HL_ALIGN_CENTRE\nHL_DEFAULT_STYLE = _controls_.HL_DEFAULT_STYLE\n#---------------------------------------------------------------------------\n\nclass HyperlinkCtrl(_core.Control):\n    \"\"\"\n    A static text control that emulates a hyperlink. The link is displayed\n    in an appropriate text style, derived from the control's normal font.\n    When the mouse rolls over the link, the cursor changes to a hand and\n    the link's color changes to the active color.\n\n    Clicking on the link does not launch a web browser; instead, a\n    wx.HyperlinkEvent is fired. Use the wx.EVT_HYPERLINK to catch link\n    events.\n\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String label=wxEmptyString, \n            String url=wxEmptyString, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=HL_DEFAULT_STYLE, \n            String name=HyperlinkCtrlNameStr) -> HyperlinkCtrl\n\n        A static text control that emulates a hyperlink. The link is displayed\n        in an appropriate text style, derived from the control's normal font.\n        When the mouse rolls over the link, the cursor changes to a hand and\n        the link's color changes to the active color.\n\n        Clicking on the link does not launch a web browser; instead, a\n        wx.HyperlinkEvent is fired. Use the wx.EVT_HYPERLINK to catch link\n        events.\n\n        \"\"\"\n        _controls_.HyperlinkCtrl_swiginit(self,_controls_.new_HyperlinkCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String label=wxEmptyString, \n            String url=wxEmptyString, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=HL_DEFAULT_STYLE, \n            String name=HyperlinkCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.HyperlinkCtrl_Create(*args, **kwargs)\n\n    def GetHoverColour(*args, **kwargs):\n        \"\"\"GetHoverColour(self) -> Colour\"\"\"\n        return _controls_.HyperlinkCtrl_GetHoverColour(*args, **kwargs)\n\n    def SetHoverColour(*args, **kwargs):\n        \"\"\"SetHoverColour(self, Colour colour)\"\"\"\n        return _controls_.HyperlinkCtrl_SetHoverColour(*args, **kwargs)\n\n    def GetNormalColour(*args, **kwargs):\n        \"\"\"GetNormalColour(self) -> Colour\"\"\"\n        return _controls_.HyperlinkCtrl_GetNormalColour(*args, **kwargs)\n\n    def SetNormalColour(*args, **kwargs):\n        \"\"\"SetNormalColour(self, Colour colour)\"\"\"\n        return _controls_.HyperlinkCtrl_SetNormalColour(*args, **kwargs)\n\n    def GetVisitedColour(*args, **kwargs):\n        \"\"\"GetVisitedColour(self) -> Colour\"\"\"\n        return _controls_.HyperlinkCtrl_GetVisitedColour(*args, **kwargs)\n\n    def SetVisitedColour(*args, **kwargs):\n        \"\"\"SetVisitedColour(self, Colour colour)\"\"\"\n        return _controls_.HyperlinkCtrl_SetVisitedColour(*args, **kwargs)\n\n    def GetURL(*args, **kwargs):\n        \"\"\"GetURL(self) -> String\"\"\"\n        return _controls_.HyperlinkCtrl_GetURL(*args, **kwargs)\n\n    def SetURL(*args, **kwargs):\n        \"\"\"SetURL(self, String url)\"\"\"\n        return _controls_.HyperlinkCtrl_SetURL(*args, **kwargs)\n\n    def SetVisited(*args, **kwargs):\n        \"\"\"SetVisited(self, bool visited=True)\"\"\"\n        return _controls_.HyperlinkCtrl_SetVisited(*args, **kwargs)\n\n    def GetVisited(*args, **kwargs):\n        \"\"\"GetVisited(self) -> bool\"\"\"\n        return _controls_.HyperlinkCtrl_GetVisited(*args, **kwargs)\n\n    HoverColour = property(GetHoverColour,SetHoverColour,doc=\"See `GetHoverColour` and `SetHoverColour`\") \n    NormalColour = property(GetNormalColour,SetNormalColour,doc=\"See `GetNormalColour` and `SetNormalColour`\") \n    URL = property(GetURL,SetURL,doc=\"See `GetURL` and `SetURL`\") \n    Visited = property(GetVisited,SetVisited,doc=\"See `GetVisited` and `SetVisited`\") \n    VisitedColour = property(GetVisitedColour,SetVisitedColour,doc=\"See `GetVisitedColour` and `SetVisitedColour`\") \n_controls_.HyperlinkCtrl_swigregister(HyperlinkCtrl)\nHyperlinkCtrlNameStr = cvar.HyperlinkCtrlNameStr\n\ndef PreHyperlinkCtrl(*args, **kwargs):\n    \"\"\"\n    PreHyperlinkCtrl() -> HyperlinkCtrl\n\n    A static text control that emulates a hyperlink. The link is displayed\n    in an appropriate text style, derived from the control's normal font.\n    When the mouse rolls over the link, the cursor changes to a hand and\n    the link's color changes to the active color.\n\n    Clicking on the link does not launch a web browser; instead, a\n    wx.HyperlinkEvent is fired. Use the wx.EVT_HYPERLINK to catch link\n    events.\n\n    \"\"\"\n    val = _controls_.new_PreHyperlinkCtrl(*args, **kwargs)\n    return val\n\nwxEVT_COMMAND_HYPERLINK = _controls_.wxEVT_COMMAND_HYPERLINK\nclass HyperlinkEvent(_core.CommandEvent):\n    \"\"\"Proxy of C++ HyperlinkEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, Object generator, int id, String url) -> HyperlinkEvent\"\"\"\n        _controls_.HyperlinkEvent_swiginit(self,_controls_.new_HyperlinkEvent(*args, **kwargs))\n    def GetURL(*args, **kwargs):\n        \"\"\"GetURL(self) -> String\"\"\"\n        return _controls_.HyperlinkEvent_GetURL(*args, **kwargs)\n\n    def SetURL(*args, **kwargs):\n        \"\"\"SetURL(self, String url)\"\"\"\n        return _controls_.HyperlinkEvent_SetURL(*args, **kwargs)\n\n    URL = property(GetURL,SetURL,doc=\"See `GetURL` and `SetURL`\") \n_controls_.HyperlinkEvent_swigregister(HyperlinkEvent)\n\nEVT_HYPERLINK = wx.PyEventBinder( wxEVT_COMMAND_HYPERLINK, 1 )\n\n#---------------------------------------------------------------------------\n\nPB_USE_TEXTCTRL = _controls_.PB_USE_TEXTCTRL\nclass PickerBase(_core.Control):\n    \"\"\"\n    Base abstract class for all pickers which support an auxiliary text\n    control. This class handles all positioning and sizing of the text\n    control like a horizontal `wx.BoxSizer` would do, with the text\n    control on the left of the picker button and the proportion of the\n    picker fixed to value 1.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    def __init__(self): raise AttributeError, \"No constructor defined\"\n    __repr__ = _swig_repr\n    def CreateBase(*args, **kwargs):\n        \"\"\"\n        CreateBase(self, Window parent, int id, String text=wxEmptyString, Point pos=DefaultPosition, \n            Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=wxButtonNameStr) -> bool\n        \"\"\"\n        return _controls_.PickerBase_CreateBase(*args, **kwargs)\n\n    def SetInternalMargin(*args, **kwargs):\n        \"\"\"\n        SetInternalMargin(self, int newmargin)\n\n        Sets the margin (in pixels) between the picker and the text control.\n        \"\"\"\n        return _controls_.PickerBase_SetInternalMargin(*args, **kwargs)\n\n    def GetInternalMargin(*args, **kwargs):\n        \"\"\"\n        GetInternalMargin(self) -> int\n\n        Returns the margin (in pixels) between the picker and the text\n        control.\n        \"\"\"\n        return _controls_.PickerBase_GetInternalMargin(*args, **kwargs)\n\n    def SetTextCtrlProportion(*args, **kwargs):\n        \"\"\"\n        SetTextCtrlProportion(self, int prop)\n\n        Sets the proportion between the text control and the picker button.\n        This is used to set relative sizes of the text contorl and the picker.\n        The value passed to this function must be >= 1.\n        \"\"\"\n        return _controls_.PickerBase_SetTextCtrlProportion(*args, **kwargs)\n\n    def GetTextCtrlProportion(*args, **kwargs):\n        \"\"\"\n        GetTextCtrlProportion(self) -> int\n\n        Returns the proportion between the text control and the picker.\n        \"\"\"\n        return _controls_.PickerBase_GetTextCtrlProportion(*args, **kwargs)\n\n    def SetPickerCtrlProportion(*args, **kwargs):\n        \"\"\"\n        SetPickerCtrlProportion(self, int prop)\n\n        Sets the proportion value of the picker.\n        \"\"\"\n        return _controls_.PickerBase_SetPickerCtrlProportion(*args, **kwargs)\n\n    def GetPickerCtrlProportion(*args, **kwargs):\n        \"\"\"\n        GetPickerCtrlProportion(self) -> int\n\n        Gets the proportion value of the picker.\n        \"\"\"\n        return _controls_.PickerBase_GetPickerCtrlProportion(*args, **kwargs)\n\n    def IsTextCtrlGrowable(*args, **kwargs):\n        \"\"\"IsTextCtrlGrowable(self) -> bool\"\"\"\n        return _controls_.PickerBase_IsTextCtrlGrowable(*args, **kwargs)\n\n    def SetTextCtrlGrowable(*args, **kwargs):\n        \"\"\"SetTextCtrlGrowable(self, bool grow=True)\"\"\"\n        return _controls_.PickerBase_SetTextCtrlGrowable(*args, **kwargs)\n\n    def IsPickerCtrlGrowable(*args, **kwargs):\n        \"\"\"IsPickerCtrlGrowable(self) -> bool\"\"\"\n        return _controls_.PickerBase_IsPickerCtrlGrowable(*args, **kwargs)\n\n    def SetPickerCtrlGrowable(*args, **kwargs):\n        \"\"\"SetPickerCtrlGrowable(self, bool grow=True)\"\"\"\n        return _controls_.PickerBase_SetPickerCtrlGrowable(*args, **kwargs)\n\n    def HasTextCtrl(*args, **kwargs):\n        \"\"\"\n        HasTextCtrl(self) -> bool\n\n        Returns true if this class has a valid text control (i.e. if the\n        wx.PB_USE_TEXTCTRL style was given when creating this control).\n        \"\"\"\n        return _controls_.PickerBase_HasTextCtrl(*args, **kwargs)\n\n    def GetTextCtrl(*args, **kwargs):\n        \"\"\"\n        GetTextCtrl(self) -> TextCtrl\n\n        Returns a pointer to the text control handled by this class or None if\n        the wx.PB_USE_TEXTCTRL style was not specified when this control was\n        created.\n\n        Very important: the contents of the text control could be containing\n        an invalid representation of the entity which can be chosen through\n        the picker (e.g. the user entered an invalid colour syntax because of\n        a typo). Thus you should never parse the content of the textctrl to\n        get the user's input; rather use the derived-class getter\n        (e.g. `wx.ColourPickerCtrl.GetColour`, `wx.FilePickerCtrl.GetPath`,\n        etc).\n        \"\"\"\n        return _controls_.PickerBase_GetTextCtrl(*args, **kwargs)\n\n    def GetPickerCtrl(*args, **kwargs):\n        \"\"\"GetPickerCtrl(self) -> Control\"\"\"\n        return _controls_.PickerBase_GetPickerCtrl(*args, **kwargs)\n\n    InternalMargin = property(GetInternalMargin,SetInternalMargin,doc=\"See `GetInternalMargin` and `SetInternalMargin`\") \n    PickerCtrl = property(GetPickerCtrl,doc=\"See `GetPickerCtrl`\") \n    PickerCtrlProportion = property(GetPickerCtrlProportion,SetPickerCtrlProportion,doc=\"See `GetPickerCtrlProportion` and `SetPickerCtrlProportion`\") \n    TextCtrl = property(GetTextCtrl,doc=\"See `GetTextCtrl`\") \n    TextCtrlProportion = property(GetTextCtrlProportion,SetTextCtrlProportion,doc=\"See `GetTextCtrlProportion` and `SetTextCtrlProportion`\") \n    TextCtrlGrowable = property(IsTextCtrlGrowable,SetTextCtrlGrowable,doc=\"See `IsTextCtrlGrowable` and `SetTextCtrlGrowable`\") \n    PickerCtrlGrowable = property(IsPickerCtrlGrowable,SetPickerCtrlGrowable,doc=\"See `IsPickerCtrlGrowable` and `SetPickerCtrlGrowable`\") \n_controls_.PickerBase_swigregister(PickerBase)\n\nclass PyPickerBase(PickerBase):\n    \"\"\"Proxy of C++ PyPickerBase class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String text=wxEmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=wxButtonNameStr) -> PyPickerBase\n        \"\"\"\n        _controls_.PyPickerBase_swiginit(self,_controls_.new_PyPickerBase(*args, **kwargs))\n        self._setOORInfo(self);PyPickerBase._setCallbackInfo(self, self, PyPickerBase)\n\n    def _setCallbackInfo(*args, **kwargs):\n        \"\"\"_setCallbackInfo(self, PyObject self, PyObject _class)\"\"\"\n        return _controls_.PyPickerBase__setCallbackInfo(*args, **kwargs)\n\n    def UpdatePickerFromTextCtrl(*args, **kwargs):\n        \"\"\"UpdatePickerFromTextCtrl(self)\"\"\"\n        return _controls_.PyPickerBase_UpdatePickerFromTextCtrl(*args, **kwargs)\n\n    def UpdateTextCtrlFromPicker(*args, **kwargs):\n        \"\"\"UpdateTextCtrlFromPicker(self)\"\"\"\n        return _controls_.PyPickerBase_UpdateTextCtrlFromPicker(*args, **kwargs)\n\n    def GetTextCtrlStyle(*args, **kwargs):\n        \"\"\"GetTextCtrlStyle(self, long style) -> long\"\"\"\n        return _controls_.PyPickerBase_GetTextCtrlStyle(*args, **kwargs)\n\n    def GetPickerStyle(*args, **kwargs):\n        \"\"\"GetPickerStyle(self, long style) -> long\"\"\"\n        return _controls_.PyPickerBase_GetPickerStyle(*args, **kwargs)\n\n    def SetTextCtrl(*args, **kwargs):\n        \"\"\"SetTextCtrl(self, TextCtrl text)\"\"\"\n        return _controls_.PyPickerBase_SetTextCtrl(*args, **kwargs)\n\n    def SetPickerCtrl(*args, **kwargs):\n        \"\"\"SetPickerCtrl(self, Control picker)\"\"\"\n        return _controls_.PyPickerBase_SetPickerCtrl(*args, **kwargs)\n\n    def PostCreation(*args, **kwargs):\n        \"\"\"PostCreation(self)\"\"\"\n        return _controls_.PyPickerBase_PostCreation(*args, **kwargs)\n\n_controls_.PyPickerBase_swigregister(PyPickerBase)\n\ndef PrePyPickerBase(*args, **kwargs):\n    \"\"\"PrePyPickerBase() -> PyPickerBase\"\"\"\n    val = _controls_.new_PrePyPickerBase(*args, **kwargs)\n    return val\n\n#---------------------------------------------------------------------------\n\nCLRP_SHOW_LABEL = _controls_.CLRP_SHOW_LABEL\nCLRP_USE_TEXTCTRL = _controls_.CLRP_USE_TEXTCTRL\nCLRP_DEFAULT_STYLE = _controls_.CLRP_DEFAULT_STYLE\nclass ColourPickerCtrl(PickerBase):\n    \"\"\"\n    This control allows the user to select a colour. The implementation\n    varies by platform but is usually a button which brings up a\n    `wx.ColourDialog` when clicked.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Colour col=*wxBLACK, Point pos=DefaultPosition, \n            Size size=DefaultSize, \n            long style=CLRP_DEFAULT_STYLE, Validator validator=DefaultValidator, \n            String name=ColourPickerCtrlNameStr) -> ColourPickerCtrl\n\n        This control allows the user to select a colour. The implementation\n        varies by platform but is usually a button which brings up a\n        `wx.ColourDialog` when clicked.\n        \"\"\"\n        _controls_.ColourPickerCtrl_swiginit(self,_controls_.new_ColourPickerCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id, Colour col=*wxBLACK, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=CLRP_DEFAULT_STYLE, \n            Validator validator=DefaultValidator, \n            String name=ColourPickerCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.ColourPickerCtrl_Create(*args, **kwargs)\n\n    def GetColour(*args, **kwargs):\n        \"\"\"\n        GetColour(self) -> Colour\n\n        Returns the currently selected colour.\n        \"\"\"\n        return _controls_.ColourPickerCtrl_GetColour(*args, **kwargs)\n\n    def SetColour(*args, **kwargs):\n        \"\"\"\n        SetColour(self, Colour col)\n\n        Set the displayed colour.\n        \"\"\"\n        return _controls_.ColourPickerCtrl_SetColour(*args, **kwargs)\n\n    Colour = property(GetColour,SetColour,doc=\"See `GetColour` and `SetColour`\") \n_controls_.ColourPickerCtrl_swigregister(ColourPickerCtrl)\nColourPickerCtrlNameStr = cvar.ColourPickerCtrlNameStr\n\ndef PreColourPickerCtrl(*args, **kwargs):\n    \"\"\"\n    PreColourPickerCtrl() -> ColourPickerCtrl\n\n    This control allows the user to select a colour. The implementation\n    varies by platform but is usually a button which brings up a\n    `wx.ColourDialog` when clicked.\n    \"\"\"\n    val = _controls_.new_PreColourPickerCtrl(*args, **kwargs)\n    return val\n\nwxEVT_COMMAND_COLOURPICKER_CHANGED = _controls_.wxEVT_COMMAND_COLOURPICKER_CHANGED\nEVT_COLOURPICKER_CHANGED = wx.PyEventBinder( wxEVT_COMMAND_COLOURPICKER_CHANGED, 1 )\n\nclass ColourPickerEvent(_core.CommandEvent):\n    \"\"\"Proxy of C++ ColourPickerEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, Object generator, int id, Colour col) -> ColourPickerEvent\"\"\"\n        _controls_.ColourPickerEvent_swiginit(self,_controls_.new_ColourPickerEvent(*args, **kwargs))\n    def GetColour(*args, **kwargs):\n        \"\"\"GetColour(self) -> Colour\"\"\"\n        return _controls_.ColourPickerEvent_GetColour(*args, **kwargs)\n\n    def SetColour(*args, **kwargs):\n        \"\"\"SetColour(self, Colour c)\"\"\"\n        return _controls_.ColourPickerEvent_SetColour(*args, **kwargs)\n\n    Colour = property(GetColour,SetColour,doc=\"See `GetColour` and `SetColour`\") \n_controls_.ColourPickerEvent_swigregister(ColourPickerEvent)\n\n#---------------------------------------------------------------------------\n\nFLP_OPEN = _controls_.FLP_OPEN\nFLP_SAVE = _controls_.FLP_SAVE\nFLP_OVERWRITE_PROMPT = _controls_.FLP_OVERWRITE_PROMPT\nFLP_FILE_MUST_EXIST = _controls_.FLP_FILE_MUST_EXIST\nFLP_CHANGE_DIR = _controls_.FLP_CHANGE_DIR\nFLP_SMALL = _controls_.FLP_SMALL\nDIRP_DIR_MUST_EXIST = _controls_.DIRP_DIR_MUST_EXIST\nDIRP_CHANGE_DIR = _controls_.DIRP_CHANGE_DIR\nDIRP_SMALL = _controls_.DIRP_SMALL\nFLP_USE_TEXTCTRL = _controls_.FLP_USE_TEXTCTRL\nFLP_DEFAULT_STYLE = _controls_.FLP_DEFAULT_STYLE\nDIRP_USE_TEXTCTRL = _controls_.DIRP_USE_TEXTCTRL\nDIRP_DEFAULT_STYLE = _controls_.DIRP_DEFAULT_STYLE\nclass FilePickerCtrl(PickerBase):\n    \"\"\"Proxy of C++ FilePickerCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String path=EmptyString, \n            String message=FileSelectorPromptStr, String wildcard=FileSelectorDefaultWildcardStr, \n            Point pos=DefaultPosition, \n            Size size=DefaultSize, \n            long style=FLP_DEFAULT_STYLE, Validator validator=DefaultValidator, \n            String name=FilePickerCtrlNameStr) -> FilePickerCtrl\n        \"\"\"\n        _controls_.FilePickerCtrl_swiginit(self,_controls_.new_FilePickerCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String path=EmptyString, \n            String message=FileSelectorPromptStr, String wildcard=FileSelectorDefaultWildcardStr, \n            Point pos=DefaultPosition, \n            Size size=DefaultSize, \n            long style=FLP_DEFAULT_STYLE, Validator validator=DefaultValidator, \n            String name=FilePickerCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.FilePickerCtrl_Create(*args, **kwargs)\n\n    def GetPath(*args, **kwargs):\n        \"\"\"GetPath(self) -> String\"\"\"\n        return _controls_.FilePickerCtrl_GetPath(*args, **kwargs)\n\n    def SetPath(*args, **kwargs):\n        \"\"\"SetPath(self, String str)\"\"\"\n        return _controls_.FilePickerCtrl_SetPath(*args, **kwargs)\n\n    def GetTextCtrlValue(*args, **kwargs):\n        \"\"\"GetTextCtrlValue(self) -> String\"\"\"\n        return _controls_.FilePickerCtrl_GetTextCtrlValue(*args, **kwargs)\n\n    def SetInitialDirectory(*args, **kwargs):\n        \"\"\"SetInitialDirectory(self, String dir)\"\"\"\n        return _controls_.FilePickerCtrl_SetInitialDirectory(*args, **kwargs)\n\n    Path = property(GetPath,SetPath,doc=\"See `GetPath` and `SetPath`\") \n    TextCtrlValue = property(GetTextCtrlValue,doc=\"See `GetTextCtrlValue`\") \n_controls_.FilePickerCtrl_swigregister(FilePickerCtrl)\nFilePickerCtrlNameStr = cvar.FilePickerCtrlNameStr\nFileSelectorPromptStr = cvar.FileSelectorPromptStr\nDirPickerCtrlNameStr = cvar.DirPickerCtrlNameStr\nDirSelectorPromptStr = cvar.DirSelectorPromptStr\nFileSelectorDefaultWildcardStr = cvar.FileSelectorDefaultWildcardStr\n\ndef PreFilePickerCtrl(*args, **kwargs):\n    \"\"\"PreFilePickerCtrl() -> FilePickerCtrl\"\"\"\n    val = _controls_.new_PreFilePickerCtrl(*args, **kwargs)\n    return val\n\nclass DirPickerCtrl(PickerBase):\n    \"\"\"Proxy of C++ DirPickerCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String path=EmptyString, \n            String message=DirSelectorPromptStr, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=DIRP_DEFAULT_STYLE, \n            Validator validator=DefaultValidator, \n            String name=DirPickerCtrlNameStr) -> DirPickerCtrl\n        \"\"\"\n        _controls_.DirPickerCtrl_swiginit(self,_controls_.new_DirPickerCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String path=EmptyString, \n            String message=DirSelectorPromptStr, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=DIRP_DEFAULT_STYLE, \n            Validator validator=DefaultValidator, \n            String name=DirPickerCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.DirPickerCtrl_Create(*args, **kwargs)\n\n    def GetPath(*args, **kwargs):\n        \"\"\"GetPath(self) -> String\"\"\"\n        return _controls_.DirPickerCtrl_GetPath(*args, **kwargs)\n\n    def SetPath(*args, **kwargs):\n        \"\"\"SetPath(self, String str)\"\"\"\n        return _controls_.DirPickerCtrl_SetPath(*args, **kwargs)\n\n    def GetTextCtrlValue(*args, **kwargs):\n        \"\"\"GetTextCtrlValue(self) -> String\"\"\"\n        return _controls_.DirPickerCtrl_GetTextCtrlValue(*args, **kwargs)\n\n    Path = property(GetPath,SetPath,doc=\"See `GetPath` and `SetPath`\") \n    TextCtrlValue = property(GetTextCtrlValue,doc=\"See `GetTextCtrlValue`\") \n_controls_.DirPickerCtrl_swigregister(DirPickerCtrl)\n\ndef PreDirPickerCtrl(*args, **kwargs):\n    \"\"\"PreDirPickerCtrl() -> DirPickerCtrl\"\"\"\n    val = _controls_.new_PreDirPickerCtrl(*args, **kwargs)\n    return val\n\nwxEVT_COMMAND_FILEPICKER_CHANGED = _controls_.wxEVT_COMMAND_FILEPICKER_CHANGED\nwxEVT_COMMAND_DIRPICKER_CHANGED = _controls_.wxEVT_COMMAND_DIRPICKER_CHANGED\nEVT_FILEPICKER_CHANGED = wx.PyEventBinder( wxEVT_COMMAND_FILEPICKER_CHANGED, 1 )\nEVT_DIRPICKER_CHANGED  = wx.PyEventBinder( wxEVT_COMMAND_DIRPICKER_CHANGED,  1 )\n\nclass FileDirPickerEvent(_core.CommandEvent):\n    \"\"\"Proxy of C++ FileDirPickerEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, EventType type, Object generator, int id, String path) -> FileDirPickerEvent\"\"\"\n        _controls_.FileDirPickerEvent_swiginit(self,_controls_.new_FileDirPickerEvent(*args, **kwargs))\n    def GetPath(*args, **kwargs):\n        \"\"\"GetPath(self) -> String\"\"\"\n        return _controls_.FileDirPickerEvent_GetPath(*args, **kwargs)\n\n    def SetPath(*args, **kwargs):\n        \"\"\"SetPath(self, String p)\"\"\"\n        return _controls_.FileDirPickerEvent_SetPath(*args, **kwargs)\n\n    Path = property(GetPath,SetPath,doc=\"See `GetPath` and `SetPath`\") \n_controls_.FileDirPickerEvent_swigregister(FileDirPickerEvent)\n\n#---------------------------------------------------------------------------\n\nFNTP_FONTDESC_AS_LABEL = _controls_.FNTP_FONTDESC_AS_LABEL\nFNTP_USEFONT_FOR_LABEL = _controls_.FNTP_USEFONT_FOR_LABEL\nFNTP_USE_TEXTCTRL = _controls_.FNTP_USE_TEXTCTRL\nFNTP_DEFAULT_STYLE = _controls_.FNTP_DEFAULT_STYLE\nclass FontPickerCtrl(PickerBase):\n    \"\"\"Proxy of C++ FontPickerCtrl class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, Font initial=wxNullFont, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=FNTP_DEFAULT_STYLE, Validator validator=DefaultValidator, \n            String name=FontPickerCtrlNameStr) -> FontPickerCtrl\n        \"\"\"\n        _controls_.FontPickerCtrl_swiginit(self,_controls_.new_FontPickerCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, Font initial=wxNullFont, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=FNTP_DEFAULT_STYLE, Validator validator=DefaultValidator, \n            String name=FontPickerCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.FontPickerCtrl_Create(*args, **kwargs)\n\n    def GetSelectedFont(*args, **kwargs):\n        \"\"\"GetSelectedFont(self) -> Font\"\"\"\n        return _controls_.FontPickerCtrl_GetSelectedFont(*args, **kwargs)\n\n    def SetSelectedFont(*args, **kwargs):\n        \"\"\"SetSelectedFont(self, Font f)\"\"\"\n        return _controls_.FontPickerCtrl_SetSelectedFont(*args, **kwargs)\n\n    def SetMaxPointSize(*args, **kwargs):\n        \"\"\"SetMaxPointSize(self, unsigned int max)\"\"\"\n        return _controls_.FontPickerCtrl_SetMaxPointSize(*args, **kwargs)\n\n    def GetMaxPointSize(*args, **kwargs):\n        \"\"\"GetMaxPointSize(self) -> unsigned int\"\"\"\n        return _controls_.FontPickerCtrl_GetMaxPointSize(*args, **kwargs)\n\n    MaxPointSize = property(GetMaxPointSize,SetMaxPointSize,doc=\"See `GetMaxPointSize` and `SetMaxPointSize`\") \n    SelectedFont = property(GetSelectedFont,SetSelectedFont,doc=\"See `GetSelectedFont` and `SetSelectedFont`\") \n_controls_.FontPickerCtrl_swigregister(FontPickerCtrl)\nFontPickerCtrlNameStr = cvar.FontPickerCtrlNameStr\n\ndef PreFontPickerCtrl(*args, **kwargs):\n    \"\"\"PreFontPickerCtrl() -> FontPickerCtrl\"\"\"\n    val = _controls_.new_PreFontPickerCtrl(*args, **kwargs)\n    return val\n\nwxEVT_COMMAND_FONTPICKER_CHANGED = _controls_.wxEVT_COMMAND_FONTPICKER_CHANGED\nEVT_FONTPICKER_CHANGED = wx.PyEventBinder( wxEVT_COMMAND_FONTPICKER_CHANGED, 1 )\n\nclass FontPickerEvent(_core.CommandEvent):\n    \"\"\"Proxy of C++ FontPickerEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, Object generator, int id, Font f) -> FontPickerEvent\"\"\"\n        _controls_.FontPickerEvent_swiginit(self,_controls_.new_FontPickerEvent(*args, **kwargs))\n    def GetFont(*args, **kwargs):\n        \"\"\"GetFont(self) -> Font\"\"\"\n        return _controls_.FontPickerEvent_GetFont(*args, **kwargs)\n\n    def SetFont(*args, **kwargs):\n        \"\"\"SetFont(self, Font c)\"\"\"\n        return _controls_.FontPickerEvent_SetFont(*args, **kwargs)\n\n    Font = property(GetFont,SetFont,doc=\"See `GetFont` and `SetFont`\") \n_controls_.FontPickerEvent_swigregister(FontPickerEvent)\n\n#---------------------------------------------------------------------------\n\nCP_DEFAULT_STYLE = _controls_.CP_DEFAULT_STYLE\nCP_NO_TLW_RESIZE = _controls_.CP_NO_TLW_RESIZE\nclass CollapsiblePane(_core.Control):\n    \"\"\"\n    A collapsable pane is a container with an embedded button-like\n    control which can be used by the user to collapse or expand the pane's\n    contents.\n\n    Once constructed you should use the `GetPane` function to access the\n    pane and add your controls inside it (i.e. use the window returned\n    from `GetPane` as the parent for the controls which must go in the\n    pane, NOT the wx.CollapsiblePane itself!).\n\n    Note that because of its nature of control which can dynamically (and\n    drastically) change its size at run-time under user-input, when\n    putting a wx.CollapsiblePane inside a `wx.Sizer` you should be careful\n    to add it with a proportion value of zero; this is because otherwise\n    all other windows with non-zero proportion values would automatically\n    get resized each time the user expands or collapses the pane window,\n    usually resulting a weird, flickering effect.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int winid=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=CP_DEFAULT_STYLE, Validator val=DefaultValidator, \n            String name=CollapsiblePaneNameStr) -> CollapsiblePane\n\n        Create and show a wx.CollapsiblePane\n        \"\"\"\n        _controls_.CollapsiblePane_swiginit(self,_controls_.new_CollapsiblePane(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int winid=-1, String label=EmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=CP_DEFAULT_STYLE, Validator val=DefaultValidator, \n            String name=CollapsiblePaneNameStr) -> bool\n        \"\"\"\n        return _controls_.CollapsiblePane_Create(*args, **kwargs)\n\n    def Collapse(*args, **kwargs):\n        \"\"\"\n        Collapse(self, bool collapse=True)\n\n        Collapses or expands the pane window.\n        \"\"\"\n        return _controls_.CollapsiblePane_Collapse(*args, **kwargs)\n\n    def Expand(*args, **kwargs):\n        \"\"\"\n        Expand(self)\n\n        Same as Collapse(False).\n        \"\"\"\n        return _controls_.CollapsiblePane_Expand(*args, **kwargs)\n\n    def IsCollapsed(*args, **kwargs):\n        \"\"\"\n        IsCollapsed(self) -> bool\n\n        Returns ``True`` if the pane window is currently hidden.\n        \"\"\"\n        return _controls_.CollapsiblePane_IsCollapsed(*args, **kwargs)\n\n    def IsExpanded(*args, **kwargs):\n        \"\"\"\n        IsExpanded(self) -> bool\n\n        Returns ``True`` if the pane window is currently shown.\n        \"\"\"\n        return _controls_.CollapsiblePane_IsExpanded(*args, **kwargs)\n\n    def GetPane(*args, **kwargs):\n        \"\"\"\n        GetPane(self) -> Window\n\n        Returns a reference to the pane window.  Use the returned `wx.Window`\n        as the parent of widgets to make them part of the collapsible area.\n        \"\"\"\n        return _controls_.CollapsiblePane_GetPane(*args, **kwargs)\n\n    Expanded = property(IsExpanded) \n    Collapsed = property(IsCollapsed) \n_controls_.CollapsiblePane_swigregister(CollapsiblePane)\nCollapsiblePaneNameStr = cvar.CollapsiblePaneNameStr\n\ndef PreCollapsiblePane(*args, **kwargs):\n    \"\"\"\n    PreCollapsiblePane() -> CollapsiblePane\n\n    Precreate a wx.CollapsiblePane for 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreCollapsiblePane(*args, **kwargs)\n    return val\n\nwxEVT_COMMAND_COLLPANE_CHANGED = _controls_.wxEVT_COMMAND_COLLPANE_CHANGED\nEVT_COLLAPSIBLEPANE_CHANGED = wx.PyEventBinder( wxEVT_COMMAND_COLLPANE_CHANGED, 1 )\n\nclass CollapsiblePaneEvent(_core.CommandEvent):\n    \"\"\"Proxy of C++ CollapsiblePaneEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, Object generator, int id, bool collapsed) -> CollapsiblePaneEvent\"\"\"\n        _controls_.CollapsiblePaneEvent_swiginit(self,_controls_.new_CollapsiblePaneEvent(*args, **kwargs))\n    def GetCollapsed(*args, **kwargs):\n        \"\"\"GetCollapsed(self) -> bool\"\"\"\n        return _controls_.CollapsiblePaneEvent_GetCollapsed(*args, **kwargs)\n\n    def SetCollapsed(*args, **kwargs):\n        \"\"\"SetCollapsed(self, bool c)\"\"\"\n        return _controls_.CollapsiblePaneEvent_SetCollapsed(*args, **kwargs)\n\n    Collapsed = property(GetCollapsed,SetCollapsed) \n_controls_.CollapsiblePaneEvent_swigregister(CollapsiblePaneEvent)\n\n#---------------------------------------------------------------------------\n\nclass SearchCtrlBase(_core.Control,_core.TextCtrlIface):\n    \"\"\"Proxy of C++ SearchCtrlBase class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    def __init__(self): raise AttributeError, \"No constructor defined\"\n    __repr__ = _swig_repr\n_controls_.SearchCtrlBase_swigregister(SearchCtrlBase)\nSearchCtrlNameStr = cvar.SearchCtrlNameStr\n\nclass SearchCtrl(SearchCtrlBase):\n    \"\"\"\n    A search control is a composite of a `wx.TextCtrl` with optional\n    bitmap buttons and a drop-down menu.  Controls like this can typically\n    be found on a toolbar of applications that support some form of search\n    functionality.  On the Mac this control is implemented using the\n    native HISearchField control, on the other platforms a generic control\n    is used, although that may change in the future as more platforms\n    introduce native search widgets.\n\n    If you wish to use a drop-down menu with your wx.SearchCtrl then you\n    will need to manage its content and handle the menu events yourself,\n    but this is an easy thing to do.  Simply build the menu, pass it to\n    `SetMenu`, and also bind a handler for a range of EVT_MENU events.\n    This gives you the flexibility to use the drop-down menu however you\n    wish, such as for a history of searches, or as a way to select\n    different kinds of searches.  The ToolBar.py sample in the demo shows\n    one way to do this.\n\n    Since the control derives from `wx.TextCtrl` it is convenient to use\n    the styles and events designed for `wx.TextCtrl`.  For example you can\n    use the ``wx.TE_PROCESS_ENTER`` style and catch the\n    ``wx.EVT_TEXT_ENTER`` event to know when the user has pressed the\n    Enter key in the control and wishes to start a search.\n\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String value=wxEmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=SearchCtrlNameStr) -> SearchCtrl\n\n        A search control is a composite of a `wx.TextCtrl` with optional\n        bitmap buttons and a drop-down menu.  Controls like this can typically\n        be found on a toolbar of applications that support some form of search\n        functionality.  On the Mac this control is implemented using the\n        native HISearchField control, on the other platforms a generic control\n        is used, although that may change in the future as more platforms\n        introduce native search widgets.\n\n        If you wish to use a drop-down menu with your wx.SearchCtrl then you\n        will need to manage its content and handle the menu events yourself,\n        but this is an easy thing to do.  Simply build the menu, pass it to\n        `SetMenu`, and also bind a handler for a range of EVT_MENU events.\n        This gives you the flexibility to use the drop-down menu however you\n        wish, such as for a history of searches, or as a way to select\n        different kinds of searches.  The ToolBar.py sample in the demo shows\n        one way to do this.\n\n        Since the control derives from `wx.TextCtrl` it is convenient to use\n        the styles and events designed for `wx.TextCtrl`.  For example you can\n        use the ``wx.TE_PROCESS_ENTER`` style and catch the\n        ``wx.EVT_TEXT_ENTER`` event to know when the user has pressed the\n        Enter key in the control and wishes to start a search.\n\n        \"\"\"\n        _controls_.SearchCtrl_swiginit(self,_controls_.new_SearchCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String value=wxEmptyString, \n            Point pos=DefaultPosition, Size size=DefaultSize, \n            long style=0, Validator validator=DefaultValidator, \n            String name=SearchCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.SearchCtrl_Create(*args, **kwargs)\n\n    def SetMenu(*args, **kwargs):\n        \"\"\"\n        SetMenu(self, Menu menu)\n\n        Sets the search control's menu object. If there is already a menu\n        associated with the search control it is deleted.\n        \"\"\"\n        return _controls_.SearchCtrl_SetMenu(*args, **kwargs)\n\n    def GetMenu(*args, **kwargs):\n        \"\"\"\n        GetMenu(self) -> Menu\n\n        Returns a pointer to the search control's menu object or None if there\n        is no menu attached.\n        \"\"\"\n        return _controls_.SearchCtrl_GetMenu(*args, **kwargs)\n\n    def ShowSearchButton(*args, **kwargs):\n        \"\"\"\n        ShowSearchButton(self, bool show)\n\n        Sets the search button visibility value on the search control. If\n        there is a menu attached, the search button will be visible regardless\n        of the search button visibility value.  This has no effect in Mac OS X\n        v10.3\n        \"\"\"\n        return _controls_.SearchCtrl_ShowSearchButton(*args, **kwargs)\n\n    def IsSearchButtonVisible(*args, **kwargs):\n        \"\"\"\n        IsSearchButtonVisible(self) -> bool\n\n        Returns the search button visibility value. If there is a menu\n        attached, the search button will be visible regardless of the search\n        button visibility value.  This always returns false in Mac OS X v10.3\n        \"\"\"\n        return _controls_.SearchCtrl_IsSearchButtonVisible(*args, **kwargs)\n\n    def ShowCancelButton(*args, **kwargs):\n        \"\"\"\n        ShowCancelButton(self, bool show)\n\n        Shows or hides the cancel button.\n        \"\"\"\n        return _controls_.SearchCtrl_ShowCancelButton(*args, **kwargs)\n\n    def IsCancelButtonVisible(*args, **kwargs):\n        \"\"\"\n        IsCancelButtonVisible(self) -> bool\n\n        Indicates whether the cancel button is visible. \n        \"\"\"\n        return _controls_.SearchCtrl_IsCancelButtonVisible(*args, **kwargs)\n\n    def SetDescriptiveText(*args, **kwargs):\n        \"\"\"\n        SetDescriptiveText(self, String text)\n\n        Set the text to be displayed when the user has not yet typed anything\n        in the control.\n        \"\"\"\n        return _controls_.SearchCtrl_SetDescriptiveText(*args, **kwargs)\n\n    def GetDescriptiveText(*args, **kwargs):\n        \"\"\"\n        GetDescriptiveText(self) -> String\n\n        Get the text to be displayed when the user has not yet typed anything\n        in the control.\n        \"\"\"\n        return _controls_.SearchCtrl_GetDescriptiveText(*args, **kwargs)\n\n    def SetSearchBitmap(*args, **kwargs):\n        \"\"\"\n        SetSearchBitmap(self, Bitmap bitmap)\n\n        Sets the bitmap to use for the search button.  This currently does not\n        work on the Mac.\n        \"\"\"\n        return _controls_.SearchCtrl_SetSearchBitmap(*args, **kwargs)\n\n    def SetSearchMenuBitmap(*args, **kwargs):\n        \"\"\"\n        SetSearchMenuBitmap(self, Bitmap bitmap)\n\n        Sets the bitmap to use for the search button when there is a drop-down\n        menu associated with the search control.  This currently does not work\n        on the Mac.\n        \"\"\"\n        return _controls_.SearchCtrl_SetSearchMenuBitmap(*args, **kwargs)\n\n    def SetCancelBitmap(*args, **kwargs):\n        \"\"\"\n        SetCancelBitmap(self, Bitmap bitmap)\n\n        Sets the bitmap to use for the cancel button.  This currently does not\n        work on the Mac.\n        \"\"\"\n        return _controls_.SearchCtrl_SetCancelBitmap(*args, **kwargs)\n\n    Menu = property(GetMenu,SetMenu) \n    SearchButtonVisible = property(IsSearchButtonVisible,ShowSearchButton) \n    CancelButtonVisible = property(IsCancelButtonVisible,ShowCancelButton) \n    DescriptiveText = property(GetDescriptiveText,SetDescriptiveText) \n_controls_.SearchCtrl_swigregister(SearchCtrl)\n\ndef PreSearchCtrl(*args, **kwargs):\n    \"\"\"\n    PreSearchCtrl() -> SearchCtrl\n\n    Precreate a wx.SearchCtrl for 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreSearchCtrl(*args, **kwargs)\n    return val\n\nwxEVT_COMMAND_SEARCHCTRL_CANCEL_BTN = _controls_.wxEVT_COMMAND_SEARCHCTRL_CANCEL_BTN\nwxEVT_COMMAND_SEARCHCTRL_SEARCH_BTN = _controls_.wxEVT_COMMAND_SEARCHCTRL_SEARCH_BTN\nEVT_SEARCHCTRL_CANCEL_BTN = wx.PyEventBinder( wxEVT_COMMAND_SEARCHCTRL_CANCEL_BTN, 1)\nEVT_SEARCHCTRL_SEARCH_BTN = wx.PyEventBinder( wxEVT_COMMAND_SEARCHCTRL_SEARCH_BTN, 1)\n\n#---------------------------------------------------------------------------\n\nFC_OPEN = _controls_.FC_OPEN\nFC_SAVE = _controls_.FC_SAVE\nFC_MULTIPLE = _controls_.FC_MULTIPLE\nFC_NOSHOWHIDDEN = _controls_.FC_NOSHOWHIDDEN\nFC_DEFAULT_STYLE = _controls_.FC_DEFAULT_STYLE\nclass FileCtrl(_core.Window):\n    \"\"\"\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String defaultDirectory=wxEmptyString, \n            String defaultFilename=wxEmptyString, \n            String wildCard=wxFileSelectorDefaultWildcardStr, \n            long style=FC_DEFAULT_STYLE, Point pos=DefaultPosition, \n            Size size=DefaultSize, \n            String name=FileCtrlNameStr) -> FileCtrl\n\n        \"\"\"\n        _controls_.FileCtrl_swiginit(self,_controls_.new_FileCtrl(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String defaultDirectory=wxEmptyString, \n            String defaultFilename=wxEmptyString, \n            String wildCard=wxFileSelectorDefaultWildcardStr, \n            long style=FC_DEFAULT_STYLE, Point pos=DefaultPosition, \n            Size size=DefaultSize, \n            String name=FileCtrlNameStr) -> bool\n        \"\"\"\n        return _controls_.FileCtrl_Create(*args, **kwargs)\n\n    def SetWildcard(*args, **kwargs):\n        \"\"\"SetWildcard(self, String wildCard)\"\"\"\n        return _controls_.FileCtrl_SetWildcard(*args, **kwargs)\n\n    def SetFilterIndex(*args, **kwargs):\n        \"\"\"SetFilterIndex(self, int filterindex)\"\"\"\n        return _controls_.FileCtrl_SetFilterIndex(*args, **kwargs)\n\n    def SetDirectory(*args, **kwargs):\n        \"\"\"SetDirectory(self, String dir) -> bool\"\"\"\n        return _controls_.FileCtrl_SetDirectory(*args, **kwargs)\n\n    def SetFilename(*args, **kwargs):\n        \"\"\"SetFilename(self, String name) -> bool\"\"\"\n        return _controls_.FileCtrl_SetFilename(*args, **kwargs)\n\n    def SetPath(*args, **kwargs):\n        \"\"\"SetPath(self, String path) -> bool\"\"\"\n        return _controls_.FileCtrl_SetPath(*args, **kwargs)\n\n    def GetFilename(*args, **kwargs):\n        \"\"\"GetFilename(self) -> String\"\"\"\n        return _controls_.FileCtrl_GetFilename(*args, **kwargs)\n\n    def GetDirectory(*args, **kwargs):\n        \"\"\"GetDirectory(self) -> String\"\"\"\n        return _controls_.FileCtrl_GetDirectory(*args, **kwargs)\n\n    def GetWildcard(*args, **kwargs):\n        \"\"\"GetWildcard(self) -> String\"\"\"\n        return _controls_.FileCtrl_GetWildcard(*args, **kwargs)\n\n    def GetPath(*args, **kwargs):\n        \"\"\"GetPath(self) -> String\"\"\"\n        return _controls_.FileCtrl_GetPath(*args, **kwargs)\n\n    def GetFilterIndex(*args, **kwargs):\n        \"\"\"GetFilterIndex(self) -> int\"\"\"\n        return _controls_.FileCtrl_GetFilterIndex(*args, **kwargs)\n\n    def GetPaths(*args, **kwargs):\n        \"\"\"GetPaths(self) -> wxArrayString\"\"\"\n        return _controls_.FileCtrl_GetPaths(*args, **kwargs)\n\n    def GetFilenames(*args, **kwargs):\n        \"\"\"GetFilenames(self) -> wxArrayString\"\"\"\n        return _controls_.FileCtrl_GetFilenames(*args, **kwargs)\n\n    def HasMultipleFileSelection(*args, **kwargs):\n        \"\"\"HasMultipleFileSelection(self) -> bool\"\"\"\n        return _controls_.FileCtrl_HasMultipleFileSelection(*args, **kwargs)\n\n    def ShowHidden(*args, **kwargs):\n        \"\"\"ShowHidden(self, bool show)\"\"\"\n        return _controls_.FileCtrl_ShowHidden(*args, **kwargs)\n\n    Filename = property(GetFilename,SetFilename) \n    Directory = property(GetDirectory,SetDirectory) \n    Wildcard = property(GetWildcard,SetWildcard) \n    Path = property(GetPath,SetPath) \n    FilterIndex = property(GetFilterIndex,SetFilterIndex) \n    Paths = property(GetPaths) \n    Filenames = property(GetFilenames) \n_controls_.FileCtrl_swigregister(FileCtrl)\nFileCtrlNameStr = cvar.FileCtrlNameStr\n\ndef PreFileCtrl(*args, **kwargs):\n    \"\"\"\n    PreFileCtrl() -> FileCtrl\n\n    Precreate a wx.FileCtrl for 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreFileCtrl(*args, **kwargs)\n    return val\n\nclass FileCtrlEvent(_core.CommandEvent):\n    \"\"\"Proxy of C++ FileCtrlEvent class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"__init__(self, EventType type, Object evtObject, int id) -> FileCtrlEvent\"\"\"\n        _controls_.FileCtrlEvent_swiginit(self,_controls_.new_FileCtrlEvent(*args, **kwargs))\n    def SetFiles(*args, **kwargs):\n        \"\"\"SetFiles(self, wxArrayString files)\"\"\"\n        return _controls_.FileCtrlEvent_SetFiles(*args, **kwargs)\n\n    def SetDirectory(*args, **kwargs):\n        \"\"\"SetDirectory(self, String directory)\"\"\"\n        return _controls_.FileCtrlEvent_SetDirectory(*args, **kwargs)\n\n    def SetFilterIndex(*args, **kwargs):\n        \"\"\"SetFilterIndex(self, int filterIndex)\"\"\"\n        return _controls_.FileCtrlEvent_SetFilterIndex(*args, **kwargs)\n\n    def GetFiles(*args, **kwargs):\n        \"\"\"GetFiles(self) -> wxArrayString\"\"\"\n        return _controls_.FileCtrlEvent_GetFiles(*args, **kwargs)\n\n    def GetDirectory(*args, **kwargs):\n        \"\"\"GetDirectory(self) -> String\"\"\"\n        return _controls_.FileCtrlEvent_GetDirectory(*args, **kwargs)\n\n    def GetFilterIndex(*args, **kwargs):\n        \"\"\"GetFilterIndex(self) -> int\"\"\"\n        return _controls_.FileCtrlEvent_GetFilterIndex(*args, **kwargs)\n\n    def GetFile(*args, **kwargs):\n        \"\"\"GetFile(self) -> String\"\"\"\n        return _controls_.FileCtrlEvent_GetFile(*args, **kwargs)\n\n    Files = property(GetFiles,SetFiles) \n    Directory = property(GetDirectory,SetDirectory) \n    FilterIndex = property(GetFilterIndex,SetFilterIndex) \n_controls_.FileCtrlEvent_swigregister(FileCtrlEvent)\n\nwxEVT_FILECTRL_SELECTIONCHANGED = _controls_.wxEVT_FILECTRL_SELECTIONCHANGED\nwxEVT_FILECTRL_FILEACTIVATED = _controls_.wxEVT_FILECTRL_FILEACTIVATED\nwxEVT_FILECTRL_FOLDERCHANGED = _controls_.wxEVT_FILECTRL_FOLDERCHANGED\nwxEVT_FILECTRL_FILTERCHANGED = _controls_.wxEVT_FILECTRL_FILTERCHANGED\nEVT_FILECTRL_SELECTIONCHANGED = wx.PyEventBinder( wxEVT_FILECTRL_SELECTIONCHANGED, 1)\nEVT_FILECTRL_FILEACTIVATED = wx.PyEventBinder( wxEVT_FILECTRL_FILEACTIVATED, 1)\nEVT_FILECTRL_FOLDERCHANGED = wx.PyEventBinder( wxEVT_FILECTRL_FOLDERCHANGED, 1)\nEVT_FILECTRL_FILTERCHANGED = wx.PyEventBinder( wxEVT_FILECTRL_FILTERCHANGED, 1)\n\n#---------------------------------------------------------------------------\n\nclass InfoBar(_core.Control):\n    \"\"\"\n    An info bar is a transient window shown at top or bottom of its parent\n    window to display non-critical information to the user.  It works\n    similarly to message bars in current web browsers.\n    \"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int winid=ID_ANY) -> InfoBar\n\n        An info bar is a transient window shown at top or bottom of its parent\n        window to display non-critical information to the user.  It works\n        similarly to message bars in current web browsers.\n        \"\"\"\n        _controls_.InfoBar_swiginit(self,_controls_.new_InfoBar(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int winid=ID_ANY) -> bool\n\n        Do the 2nd phase and create the GUI control.\n        \"\"\"\n        return _controls_.InfoBar_Create(*args, **kwargs)\n\n    def ShowMessage(*args, **kwargs):\n        \"\"\"ShowMessage(self, String msg, int flags=ICON_INFORMATION)\"\"\"\n        return _controls_.InfoBar_ShowMessage(*args, **kwargs)\n\n    def Dismiss(*args, **kwargs):\n        \"\"\"Dismiss(self)\"\"\"\n        return _controls_.InfoBar_Dismiss(*args, **kwargs)\n\n    def AddButton(*args, **kwargs):\n        \"\"\"AddButton(self, int btnid, String label=wxEmptyString)\"\"\"\n        return _controls_.InfoBar_AddButton(*args, **kwargs)\n\n    def RemoveButton(*args, **kwargs):\n        \"\"\"RemoveButton(self, int btnid)\"\"\"\n        return _controls_.InfoBar_RemoveButton(*args, **kwargs)\n\n    def SetShowHideEffects(*args, **kwargs):\n        \"\"\"SetShowHideEffects(self, int showEffect, int hideEffect)\"\"\"\n        return _controls_.InfoBar_SetShowHideEffects(*args, **kwargs)\n\n    def GetShowEffect(*args, **kwargs):\n        \"\"\"GetShowEffect(self) -> int\"\"\"\n        return _controls_.InfoBar_GetShowEffect(*args, **kwargs)\n\n    def GetHideEffect(*args, **kwargs):\n        \"\"\"GetHideEffect(self) -> int\"\"\"\n        return _controls_.InfoBar_GetHideEffect(*args, **kwargs)\n\n    def SetEffectDuration(*args, **kwargs):\n        \"\"\"SetEffectDuration(self, int duration)\"\"\"\n        return _controls_.InfoBar_SetEffectDuration(*args, **kwargs)\n\n    def GetEffectDuration(*args, **kwargs):\n        \"\"\"GetEffectDuration(self) -> int\"\"\"\n        return _controls_.InfoBar_GetEffectDuration(*args, **kwargs)\n\n_controls_.InfoBar_swigregister(InfoBar)\n\ndef PreInfoBar(*args, **kwargs):\n    \"\"\"\n    PreInfoBar() -> InfoBar\n\n    An info bar is a transient window shown at top or bottom of its parent\n    window to display non-critical information to the user.  It works\n    similarly to message bars in current web browsers.\n    \"\"\"\n    val = _controls_.new_PreInfoBar(*args, **kwargs)\n    return val\n\n#---------------------------------------------------------------------------\n\nclass CommandLinkButton(Button):\n    \"\"\"Proxy of C++ CommandLinkButton class\"\"\"\n    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc='The membership flag')\n    __repr__ = _swig_repr\n    def __init__(self, *args, **kwargs): \n        \"\"\"\n        __init__(self, Window parent, int id=-1, String mainLabel=wxEmptyString, \n            String note=wxEmptyString, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=0, \n            Validator validator=DefaultValidator, \n            String name=wxButtonNameStr) -> CommandLinkButton\n        \"\"\"\n        _controls_.CommandLinkButton_swiginit(self,_controls_.new_CommandLinkButton(*args, **kwargs))\n        self._setOORInfo(self)\n\n    def Create(*args, **kwargs):\n        \"\"\"\n        Create(self, Window parent, int id=-1, String mainLabel=wxEmptyString, \n            String note=wxEmptyString, Point pos=DefaultPosition, \n            Size size=DefaultSize, long style=0, \n            Validator validator=DefaultValidator, \n            String name=wxButtonNameStr) -> bool\n        \"\"\"\n        return _controls_.CommandLinkButton_Create(*args, **kwargs)\n\n    def SetMainLabelAndNote(*args, **kwargs):\n        \"\"\"SetMainLabelAndNote(self, String mainLabel, String note)\"\"\"\n        return _controls_.CommandLinkButton_SetMainLabelAndNote(*args, **kwargs)\n\n    def SetMainLabel(*args, **kwargs):\n        \"\"\"SetMainLabel(self, String mainLabel)\"\"\"\n        return _controls_.CommandLinkButton_SetMainLabel(*args, **kwargs)\n\n    def SetNote(*args, **kwargs):\n        \"\"\"SetNote(self, String note)\"\"\"\n        return _controls_.CommandLinkButton_SetNote(*args, **kwargs)\n\n    def GetMainLabel(*args, **kwargs):\n        \"\"\"GetMainLabel(self) -> String\"\"\"\n        return _controls_.CommandLinkButton_GetMainLabel(*args, **kwargs)\n\n    def GetNote(*args, **kwargs):\n        \"\"\"GetNote(self) -> String\"\"\"\n        return _controls_.CommandLinkButton_GetNote(*args, **kwargs)\n\n    MainLabel = property(GetMainLabel,SetMainLabel) \n    Note = property(GetNote,SetNote) \n_controls_.CommandLinkButton_swigregister(CommandLinkButton)\n\ndef PreCommandLinkButton(*args, **kwargs):\n    \"\"\"\n    PreCommandLinkButton() -> CommandLinkButton\n\n    Precreate a Button for 2-phase creation.\n    \"\"\"\n    val = _controls_.new_PreCommandLinkButton(*args, **kwargs)\n    return val\n\n\n\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2017 F5 Networks Inc.\n# GNU General Public License v3.0 (see COPYING or https:\/\/www.gnu.org\/licenses\/gpl-3.0.txt)\n\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nimport os\nimport json\nimport sys\n\nfrom nose.plugins.skip import SkipTest\nif sys.version_info < (2, 7):\n    raise SkipTest(\"F5 Ansible modules require Python >= 2.7\")\n\nfrom ansible.compat.tests import unittest\nfrom ansible.compat.tests.mock import Mock\nfrom ansible.compat.tests.mock import patch\nfrom ansible.module_utils.basic import AnsibleModule\n\ntry:\n    from library.modules.bigip_gtm_datacenter import ApiParameters\n    from library.modules.bigip_gtm_datacenter import ModuleParameters\n    from library.modules.bigip_gtm_datacenter import ModuleManager\n    from library.modules.bigip_gtm_datacenter import ArgumentSpec\n    from library.module_utils.network.f5.common import F5ModuleError\n    from library.module_utils.network.f5.common import iControlUnexpectedHTTPError\n    from test.unit.modules.utils import set_module_args\nexcept ImportError:\n    try:\n        from ansible.modules.network.f5.bigip_gtm_datacenter import ApiParameters\n        from ansible.modules.network.f5.bigip_gtm_datacenter import ModuleParameters\n        from ansible.modules.network.f5.bigip_gtm_datacenter import ModuleManager\n        from ansible.modules.network.f5.bigip_gtm_datacenter import ArgumentSpec\n        from ansible.module_utils.network.f5.common import F5ModuleError\n        from ansible.module_utils.network.f5.common import iControlUnexpectedHTTPError\n        from units.modules.utils import set_module_args\n    except ImportError:\n        raise SkipTest(\"F5 Ansible modules require the f5-sdk Python library\")\n\nfixture_path = os.path.join(os.path.dirname(__file__), 'fixtures')\nfixture_data = {}\n\n\ndef load_fixture(name):\n    path = os.path.join(fixture_path, name)\n\n    if path in fixture_data:\n        return fixture_data[path]\n\n    with open(path) as f:\n        data = f.read()\n\n    try:\n        data = json.loads(data)\n    except Exception:\n        pass\n\n    fixture_data[path] = data\n    return data\n\n\nclass TestParameters(unittest.TestCase):\n    def test_module_parameters(self):\n        args = dict(\n            state='present',\n            contact='foo',\n            description='bar',\n            location='baz',\n            name='datacenter'\n        )\n        p = ModuleParameters(params=args)\n        assert p.state == 'present'\n\n    def test_api_parameters(self):\n        args = load_fixture('load_gtm_datacenter_default.json')\n        p = ApiParameters(params=args)\n        assert p.name == 'asd'\n\n    def test_module_parameters_state_present(self):\n        args = dict(\n            state='present'\n        )\n        p = ModuleParameters(params=args)\n        assert p.state == 'present'\n        assert p.enabled is True\n\n    def test_module_parameters_state_absent(self):\n        args = dict(\n            state='absent'\n        )\n        p = ModuleParameters(params=args)\n        assert p.state == 'absent'\n\n    def test_module_parameters_state_enabled(self):\n        args = dict(\n            state='enabled'\n        )\n        p = ModuleParameters(params=args)\n        assert p.state == 'enabled'\n        assert p.enabled is True\n        assert p.disabled is None\n\n    def test_module_parameters_state_disabled(self):\n        args = dict(\n            state='disabled'\n        )\n        p = ModuleParameters(params=args)\n        assert p.state == 'disabled'\n        assert p.enabled is None\n        assert p.disabled is True\n\n\nclass TestManager(unittest.TestCase):\n\n    def setUp(self):\n        self.spec = ArgumentSpec()\n\n    def test_create_datacenter(self, *args):\n        set_module_args(dict(\n            state='present',\n            password='admin',\n            server='localhost',\n            user='admin',\n            name='foo'\n        ))\n\n        module = AnsibleModule(\n            argument_spec=self.spec.argument_spec,\n            supports_check_mode=self.spec.supports_check_mode\n        )\n        mm = ModuleManager(module=module)\n\n        # Override methods to force specific logic in the module to happen\n        mm.exists = Mock(side_effect=[False, True])\n        mm.create_on_device = Mock(return_value=True)\n\n        results = mm.exec_module()\n        assert results['changed'] is True\n        assert results['state'] == 'present'\n\n    def test_create_disabled_datacenter(self, *args):\n        set_module_args(dict(\n            state='disabled',\n            password='admin',\n            server='localhost',\n            user='admin',\n            name='foo'\n        ))\n\n        module = AnsibleModule(\n            argument_spec=self.spec.argument_spec,\n            supports_check_mode=self.spec.supports_check_mode\n        )\n        mm = ModuleManager(module=module)\n\n        # Override methods to force specific logic in the module to happen\n        mm.exists = Mock(side_effect=[False, True])\n        mm.create_on_device = Mock(return_value=True)\n\n        results = mm.exec_module()\n        assert results['changed'] is True\n        assert results['enabled'] is False\n        assert results['disabled'] is True\n\n    def test_create_enabled_datacenter(self, *args):\n        set_module_args(dict(\n            state='enabled',\n            password='admin',\n            server='localhost',\n            user='admin',\n            name='foo'\n        ))\n\n        module = AnsibleModule(\n            argument_spec=self.spec.argument_spec,\n            supports_check_mode=self.spec.supports_check_mode\n        )\n        mm = ModuleManager(module=module)\n\n        # Override methods to force specific logic in the module to happen\n        mm.exists = Mock(side_effect=[False, True])\n        mm.create_on_device = Mock(return_value=True)\n\n        results = mm.exec_module()\n        assert results['changed'] is True\n        assert results['enabled'] is True\n        assert results['disabled'] is False\n\n    def test_idempotent_disable_datacenter(self, *args):\n        set_module_args(dict(\n            state='disabled',\n            password='admin',\n            server='localhost',\n            user='admin',\n            name='foo'\n        ))\n\n        module = AnsibleModule(\n            argument_spec=self.spec.argument_spec,\n            supports_check_mode=self.spec.supports_check_mode\n        )\n\n        current = ApiParameters(params=load_fixture('load_gtm_datacenter_disabled.json'))\n\n        mm = ModuleManager(module=module)\n\n        # Override methods to force specific logic in the module to happen\n        mm.exists = Mock(return_value=True)\n        mm.update_on_device = Mock(return_value=True)\n        mm.read_current_from_device = Mock(return_value=current)\n\n        results = mm.exec_module()\n        assert results['changed'] is False\n","label":0}
{"content":"import unittest, os, errno\nfrom ctypes import *\nfrom ctypes.util import find_library\nfrom test import test_support\ntry:\n    import threading\nexcept ImportError:\n    threading = None\n\nclass Test(unittest.TestCase):\n    def test_open(self):\n        libc_name = find_library(\"c\")\n        if libc_name is None:\n            raise unittest.SkipTest(\"Unable to find C library\")\n        libc = CDLL(libc_name, use_errno=True)\n        if os.name == \"nt\":\n            libc_open = libc._open\n        else:\n            libc_open = libc.open\n\n        libc_open.argtypes = c_char_p, c_int\n\n        self.assertEqual(libc_open(\"\", 0), -1)\n        self.assertEqual(get_errno(), errno.ENOENT)\n\n        self.assertEqual(set_errno(32), errno.ENOENT)\n        self.assertEqual(get_errno(), 32)\n\n        if threading:\n            def _worker():\n                set_errno(0)\n\n                libc = CDLL(libc_name, use_errno=False)\n                if os.name == \"nt\":\n                    libc_open = libc._open\n                else:\n                    libc_open = libc.open\n                libc_open.argtypes = c_char_p, c_int\n                self.assertEqual(libc_open(\"\", 0), -1)\n                self.assertEqual(get_errno(), 0)\n\n            t = threading.Thread(target=_worker)\n            t.start()\n            t.join()\n\n            self.assertEqual(get_errno(), 32)\n            set_errno(0)\n\n    @unittest.skipUnless(os.name == \"nt\", 'Test specific to Windows')\n    def test_GetLastError(self):\n        dll = WinDLL(\"kernel32\", use_last_error=True)\n        GetModuleHandle = dll.GetModuleHandleA\n        GetModuleHandle.argtypes = [c_wchar_p]\n\n        self.assertEqual(0, GetModuleHandle(\"foo\"))\n        self.assertEqual(get_last_error(), 126)\n\n        self.assertEqual(set_last_error(32), 126)\n        self.assertEqual(get_last_error(), 32)\n\n        def _worker():\n            set_last_error(0)\n\n            dll = WinDLL(\"kernel32\", use_last_error=False)\n            GetModuleHandle = dll.GetModuleHandleW\n            GetModuleHandle.argtypes = [c_wchar_p]\n            GetModuleHandle(\"bar\")\n\n            self.assertEqual(get_last_error(), 0)\n\n        t = threading.Thread(target=_worker)\n        t.start()\n        t.join()\n\n        self.assertEqual(get_last_error(), 32)\n\n        set_last_error(0)\n\nif __name__ == \"__main__\":\n    unittest.main()\n","label":0}
{"content":"\"\"\"Provide access to Python's configuration information.\r\n\r\n\"\"\"\r\nimport sys\r\nimport os\r\nfrom os.path import pardir, realpath\r\n\r\n_INSTALL_SCHEMES = {\r\n    'posix_prefix': {\r\n        'stdlib': '{base}\/lib\/python{py_version_short}',\r\n        'platstdlib': '{platbase}\/lib\/python{py_version_short}',\r\n        'purelib': '{base}\/lib\/python{py_version_short}\/site-packages',\r\n        'platlib': '{platbase}\/lib\/python{py_version_short}\/site-packages',\r\n        'include': '{base}\/include\/python{py_version_short}',\r\n        'platinclude': '{platbase}\/include\/python{py_version_short}',\r\n        'scripts': '{base}\/bin',\r\n        'data': '{base}',\r\n        },\r\n    'posix_home': {\r\n        'stdlib': '{base}\/lib\/python',\r\n        'platstdlib': '{base}\/lib\/python',\r\n        'purelib': '{base}\/lib\/python',\r\n        'platlib': '{base}\/lib\/python',\r\n        'include': '{base}\/include\/python',\r\n        'platinclude': '{base}\/include\/python',\r\n        'scripts': '{base}\/bin',\r\n        'data'   : '{base}',\r\n        },\r\n    'nt': {\r\n        'stdlib': '{base}\/Lib',\r\n        'platstdlib': '{base}\/Lib',\r\n        'purelib': '{base}\/Lib\/site-packages',\r\n        'platlib': '{base}\/Lib\/site-packages',\r\n        'include': '{base}\/Include',\r\n        'platinclude': '{base}\/Include',\r\n        'scripts': '{base}\/Scripts',\r\n        'data'   : '{base}',\r\n        },\r\n    'os2': {\r\n        'stdlib': '{base}\/Lib',\r\n        'platstdlib': '{base}\/Lib',\r\n        'purelib': '{base}\/Lib\/site-packages',\r\n        'platlib': '{base}\/Lib\/site-packages',\r\n        'include': '{base}\/Include',\r\n        'platinclude': '{base}\/Include',\r\n        'scripts': '{base}\/Scripts',\r\n        'data'   : '{base}',\r\n        },\r\n    'os2_home': {\r\n        'stdlib': '{userbase}\/lib\/python{py_version_short}',\r\n        'platstdlib': '{userbase}\/lib\/python{py_version_short}',\r\n        'purelib': '{userbase}\/lib\/python{py_version_short}\/site-packages',\r\n        'platlib': '{userbase}\/lib\/python{py_version_short}\/site-packages',\r\n        'include': '{userbase}\/include\/python{py_version_short}',\r\n        'scripts': '{userbase}\/bin',\r\n        'data'   : '{userbase}',\r\n        },\r\n    'nt_user': {\r\n        'stdlib': '{userbase}\/IronPython{py_version_nodot}',\r\n        'platstdlib': '{userbase}\/IronPython{py_version_nodot}',\r\n        'purelib': '{userbase}\/IronPython{py_version_nodot}\/site-packages',\r\n        'platlib': '{userbase}\/IronPython{py_version_nodot}\/site-packages',\r\n        'include': '{userbase}\/IronPython{py_version_nodot}\/Include',\r\n        'scripts': '{userbase}\/Scripts',\r\n        'data'   : '{userbase}',\r\n        },\r\n    'posix_user': {\r\n        'stdlib': '{userbase}\/lib\/python{py_version_short}',\r\n        'platstdlib': '{userbase}\/lib\/python{py_version_short}',\r\n        'purelib': '{userbase}\/lib\/python{py_version_short}\/site-packages',\r\n        'platlib': '{userbase}\/lib\/python{py_version_short}\/site-packages',\r\n        'include': '{userbase}\/include\/python{py_version_short}',\r\n        'scripts': '{userbase}\/bin',\r\n        'data'   : '{userbase}',\r\n        },\r\n    'osx_framework_user': {\r\n        'stdlib': '{userbase}\/lib\/python',\r\n        'platstdlib': '{userbase}\/lib\/python',\r\n        'purelib': '{userbase}\/lib\/python\/site-packages',\r\n        'platlib': '{userbase}\/lib\/python\/site-packages',\r\n        'include': '{userbase}\/include',\r\n        'scripts': '{userbase}\/bin',\r\n        'data'   : '{userbase}',\r\n        },\r\n    }\r\n\r\n_SCHEME_KEYS = ('stdlib', 'platstdlib', 'purelib', 'platlib', 'include',\r\n                'scripts', 'data')\r\n_PY_VERSION = sys.version.split()[0]\r\n_PY_VERSION_SHORT = sys.version[:3]\r\n_PY_VERSION_SHORT_NO_DOT = _PY_VERSION[0] + _PY_VERSION[2]\r\n_PREFIX = os.path.normpath(sys.prefix)\r\n_EXEC_PREFIX = os.path.normpath(sys.exec_prefix)\r\n_CONFIG_VARS = None\r\n_USER_BASE = None\r\n\r\ndef _safe_realpath(path):\r\n    try:\r\n        return realpath(path)\r\n    except OSError:\r\n        return path\r\n\r\nif sys.executable:\r\n    _PROJECT_BASE = os.path.dirname(_safe_realpath(sys.executable))\r\nelse:\r\n    # sys.executable can be empty if argv[0] has been changed and Python is\r\n    # unable to retrieve the real program name\r\n    _PROJECT_BASE = _safe_realpath(os.getcwd())\r\n\r\nif os.name == \"nt\" and \"pcbuild\" in _PROJECT_BASE[-8:].lower():\r\n    _PROJECT_BASE = _safe_realpath(os.path.join(_PROJECT_BASE, pardir))\r\n# PC\/VS7.1\r\nif os.name == \"nt\" and \"\\\\pc\\\\v\" in _PROJECT_BASE[-10:].lower():\r\n    _PROJECT_BASE = _safe_realpath(os.path.join(_PROJECT_BASE, pardir, pardir))\r\n# PC\/AMD64\r\nif os.name == \"nt\" and \"\\\\pcbuild\\\\amd64\" in _PROJECT_BASE[-14:].lower():\r\n    _PROJECT_BASE = _safe_realpath(os.path.join(_PROJECT_BASE, pardir, pardir))\r\n\r\ndef is_python_build():\r\n    for fn in (\"Setup.dist\", \"Setup.local\"):\r\n        if os.path.isfile(os.path.join(_PROJECT_BASE, \"Modules\", fn)):\r\n            return True\r\n    return False\r\n\r\n_PYTHON_BUILD = is_python_build()\r\n\r\nif _PYTHON_BUILD:\r\n    for scheme in ('posix_prefix', 'posix_home'):\r\n        _INSTALL_SCHEMES[scheme]['include'] = '{projectbase}\/Include'\r\n        _INSTALL_SCHEMES[scheme]['platinclude'] = '{srcdir}'\r\n\r\ndef _subst_vars(s, local_vars):\r\n    try:\r\n        return s.format(**local_vars)\r\n    except KeyError:\r\n        try:\r\n            return s.format(**os.environ)\r\n        except KeyError, var:\r\n            raise AttributeError('{%s}' % var)\r\n\r\ndef _extend_dict(target_dict, other_dict):\r\n    target_keys = target_dict.keys()\r\n    for key, value in other_dict.items():\r\n        if key in target_keys:\r\n            continue\r\n        target_dict[key] = value\r\n\r\ndef _expand_vars(scheme, vars):\r\n    res = {}\r\n    if vars is None:\r\n        vars = {}\r\n    _extend_dict(vars, get_config_vars())\r\n\r\n    for key, value in _INSTALL_SCHEMES[scheme].items():\r\n        if os.name in ('posix', 'nt'):\r\n            value = os.path.expanduser(value)\r\n        res[key] = os.path.normpath(_subst_vars(value, vars))\r\n    return res\r\n\r\ndef _get_default_scheme():\r\n    if sys.platform == 'cli':\r\n        return 'nt'\r\n    if os.name == 'posix':\r\n        # the default scheme for posix is posix_prefix\r\n        return 'posix_prefix'\r\n    return os.name\r\n\r\ndef _getuserbase():\r\n    env_base = os.environ.get(\"IRONPYTHONUSERBASE\", None)\r\n    def joinuser(*args):\r\n        return os.path.expanduser(os.path.join(*args))\r\n\r\n    # what about 'os2emx', 'riscos' ?\r\n    if os.name == \"nt\":\r\n        base = os.environ.get(\"APPDATA\") or \"~\"\r\n        return env_base if env_base else joinuser(base, \"Python\")\r\n\r\n    if sys.platform == \"darwin\":\r\n        framework = get_config_var(\"PYTHONFRAMEWORK\")\r\n        if framework:\r\n            return joinuser(\"~\", \"Library\", framework, \"%d.%d\"%(\r\n                sys.version_info[:2]))\r\n\r\n    return env_base if env_base else joinuser(\"~\", \".local\")\r\n\r\n\r\ndef _parse_makefile(filename, vars=None):\r\n    \"\"\"Parse a Makefile-style file.\r\n\r\n    A dictionary containing name\/value pairs is returned.  If an\r\n    optional dictionary is passed in as the second argument, it is\r\n    used instead of a new dictionary.\r\n    \"\"\"\r\n    import re\r\n    # Regexes needed for parsing Makefile (and similar syntaxes,\r\n    # like old-style Setup files).\r\n    _variable_rx = re.compile(\"([a-zA-Z][a-zA-Z0-9_]+)\\s*=\\s*(.*)\")\r\n    _findvar1_rx = re.compile(r\"\\$\\(([A-Za-z][A-Za-z0-9_]*)\\)\")\r\n    _findvar2_rx = re.compile(r\"\\${([A-Za-z][A-Za-z0-9_]*)}\")\r\n\r\n    if vars is None:\r\n        vars = {}\r\n    done = {}\r\n    notdone = {}\r\n\r\n    with open(filename) as f:\r\n        lines = f.readlines()\r\n\r\n    for line in lines:\r\n        if line.startswith('#') or line.strip() == '':\r\n            continue\r\n        m = _variable_rx.match(line)\r\n        if m:\r\n            n, v = m.group(1, 2)\r\n            v = v.strip()\r\n            # `$$' is a literal `$' in make\r\n            tmpv = v.replace('$$', '')\r\n\r\n            if \"$\" in tmpv:\r\n                notdone[n] = v\r\n            else:\r\n                try:\r\n                    v = int(v)\r\n                except ValueError:\r\n                    # insert literal `$'\r\n                    done[n] = v.replace('$$', '$')\r\n                else:\r\n                    done[n] = v\r\n\r\n    # do variable interpolation here\r\n    while notdone:\r\n        for name in notdone.keys():\r\n            value = notdone[name]\r\n            m = _findvar1_rx.search(value) or _findvar2_rx.search(value)\r\n            if m:\r\n                n = m.group(1)\r\n                found = True\r\n                if n in done:\r\n                    item = str(done[n])\r\n                elif n in notdone:\r\n                    # get it on a subsequent round\r\n                    found = False\r\n                elif n in os.environ:\r\n                    # do it like make: fall back to environment\r\n                    item = os.environ[n]\r\n                else:\r\n                    done[n] = item = \"\"\r\n                if found:\r\n                    after = value[m.end():]\r\n                    value = value[:m.start()] + item + after\r\n                    if \"$\" in after:\r\n                        notdone[name] = value\r\n                    else:\r\n                        try: value = int(value)\r\n                        except ValueError:\r\n                            done[name] = value.strip()\r\n                        else:\r\n                            done[name] = value\r\n                        del notdone[name]\r\n            else:\r\n                # bogus variable reference; just drop it since we can't deal\r\n                del notdone[name]\r\n    # strip spurious spaces\r\n    for k, v in done.items():\r\n        if isinstance(v, str):\r\n            done[k] = v.strip()\r\n\r\n    # save the results in the global dictionary\r\n    vars.update(done)\r\n    return vars\r\n\r\n\r\ndef _get_makefile_filename():\r\n    if _PYTHON_BUILD:\r\n        return os.path.join(_PROJECT_BASE, \"Makefile\")\r\n    return os.path.join(get_path('platstdlib'), \"config\", \"Makefile\")\r\n\r\n\r\ndef _init_posix(vars):\r\n    \"\"\"Initialize the module as appropriate for POSIX systems.\"\"\"\r\n    # load the installed Makefile:\r\n    makefile = _get_makefile_filename()\r\n    try:\r\n        _parse_makefile(makefile, vars)\r\n    except IOError, e:\r\n        msg = \"invalid Python installation: unable to open %s\" % makefile\r\n        if hasattr(e, \"strerror\"):\r\n            msg = msg + \" (%s)\" % e.strerror\r\n        raise IOError(msg)\r\n\r\n    # load the installed pyconfig.h:\r\n    config_h = get_config_h_filename()\r\n    try:\r\n        with open(config_h) as f:\r\n            parse_config_h(f, vars)\r\n    except IOError, e:\r\n        msg = \"invalid Python installation: unable to open %s\" % config_h\r\n        if hasattr(e, \"strerror\"):\r\n            msg = msg + \" (%s)\" % e.strerror\r\n        raise IOError(msg)\r\n\r\n    # On AIX, there are wrong paths to the linker scripts in the Makefile\r\n    # -- these paths are relative to the Python source, but when installed\r\n    # the scripts are in another directory.\r\n    if _PYTHON_BUILD:\r\n        vars['LDSHARED'] = vars['BLDSHARED']\r\n\r\ndef _init_non_posix(vars):\r\n    \"\"\"Initialize the module as appropriate for NT\"\"\"\r\n    # set basic install directories\r\n    vars['LIBDEST'] = get_path('stdlib')\r\n    vars['BINLIBDEST'] = get_path('platstdlib')\r\n    vars['INCLUDEPY'] = get_path('include')\r\n    vars['SO'] = '.pyd'\r\n    vars['EXE'] = '.exe'\r\n    vars['VERSION'] = _PY_VERSION_SHORT_NO_DOT\r\n    vars['BINDIR'] = os.path.dirname(_safe_realpath(sys.executable))\r\n\r\n#\r\n# public APIs\r\n#\r\n\r\n\r\ndef parse_config_h(fp, vars=None):\r\n    \"\"\"Parse a config.h-style file.\r\n\r\n    A dictionary containing name\/value pairs is returned.  If an\r\n    optional dictionary is passed in as the second argument, it is\r\n    used instead of a new dictionary.\r\n    \"\"\"\r\n    import re\r\n    if vars is None:\r\n        vars = {}\r\n    define_rx = re.compile(\"#define ([A-Z][A-Za-z0-9_]+) (.*)\\n\")\r\n    undef_rx = re.compile(\"\/[*] #undef ([A-Z][A-Za-z0-9_]+) [*]\/\\n\")\r\n\r\n    while True:\r\n        line = fp.readline()\r\n        if not line:\r\n            break\r\n        m = define_rx.match(line)\r\n        if m:\r\n            n, v = m.group(1, 2)\r\n            try: v = int(v)\r\n            except ValueError: pass\r\n            vars[n] = v\r\n        else:\r\n            m = undef_rx.match(line)\r\n            if m:\r\n                vars[m.group(1)] = 0\r\n    return vars\r\n\r\ndef get_config_h_filename():\r\n    \"\"\"Returns the path of pyconfig.h.\"\"\"\r\n    if _PYTHON_BUILD:\r\n        if os.name == \"nt\":\r\n            inc_dir = os.path.join(_PROJECT_BASE, \"PC\")\r\n        else:\r\n            inc_dir = _PROJECT_BASE\r\n    else:\r\n        inc_dir = get_path('platinclude')\r\n    return os.path.join(inc_dir, 'pyconfig.h')\r\n\r\ndef get_scheme_names():\r\n    \"\"\"Returns a tuple containing the schemes names.\"\"\"\r\n    schemes = _INSTALL_SCHEMES.keys()\r\n    schemes.sort()\r\n    return tuple(schemes)\r\n\r\ndef get_path_names():\r\n    \"\"\"Returns a tuple containing the paths names.\"\"\"\r\n    return _SCHEME_KEYS\r\n\r\ndef get_paths(scheme=_get_default_scheme(), vars=None, expand=True):\r\n    \"\"\"Returns a mapping containing an install scheme.\r\n\r\n    ``scheme`` is the install scheme name. If not provided, it will\r\n    return the default scheme for the current platform.\r\n    \"\"\"\r\n    if expand:\r\n        return _expand_vars(scheme, vars)\r\n    else:\r\n        return _INSTALL_SCHEMES[scheme]\r\n\r\ndef get_path(name, scheme=_get_default_scheme(), vars=None, expand=True):\r\n    \"\"\"Returns a path corresponding to the scheme.\r\n\r\n    ``scheme`` is the install scheme name.\r\n    \"\"\"\r\n    return get_paths(scheme, vars, expand)[name]\r\n\r\ndef get_config_vars(*args):\r\n    \"\"\"With no arguments, return a dictionary of all configuration\r\n    variables relevant for the current platform.\r\n\r\n    On Unix, this means every variable defined in Python's installed Makefile;\r\n    On Windows and Mac OS it's a much smaller set.\r\n\r\n    With arguments, return a list of values that result from looking up\r\n    each argument in the configuration variable dictionary.\r\n    \"\"\"\r\n    import re\r\n    global _CONFIG_VARS\r\n    if _CONFIG_VARS is None:\r\n        _CONFIG_VARS = {}\r\n        # Normalized versions of prefix and exec_prefix are handy to have;\r\n        # in fact, these are the standard versions used most places in the\r\n        # Distutils.\r\n        _CONFIG_VARS['prefix'] = _PREFIX\r\n        _CONFIG_VARS['exec_prefix'] = _EXEC_PREFIX\r\n        _CONFIG_VARS['py_version'] = _PY_VERSION\r\n        _CONFIG_VARS['py_version_short'] = _PY_VERSION_SHORT\r\n        _CONFIG_VARS['py_version_nodot'] = _PY_VERSION[0] + _PY_VERSION[2]\r\n        _CONFIG_VARS['base'] = _PREFIX\r\n        _CONFIG_VARS['platbase'] = _EXEC_PREFIX\r\n        _CONFIG_VARS['projectbase'] = _PROJECT_BASE\r\n\r\n        if os.name in ('nt', 'os2') or sys.platform == 'cli':\r\n            _init_non_posix(_CONFIG_VARS)\r\n        elif os.name == 'posix':\r\n            _init_posix(_CONFIG_VARS)\r\n\r\n        # Setting 'userbase' is done below the call to the\r\n        # init function to enable using 'get_config_var' in\r\n        # the init-function.\r\n        _CONFIG_VARS['userbase'] = _getuserbase()\r\n\r\n        if 'srcdir' not in _CONFIG_VARS:\r\n            _CONFIG_VARS['srcdir'] = _PROJECT_BASE\r\n\r\n        # Convert srcdir into an absolute path if it appears necessary.\r\n        # Normally it is relative to the build directory.  However, during\r\n        # testing, for example, we might be running a non-installed python\r\n        # from a different directory.\r\n        if _PYTHON_BUILD and os.name == \"posix\":\r\n            base = _PROJECT_BASE\r\n            try:\r\n                cwd = os.getcwd()\r\n            except OSError:\r\n                cwd = None\r\n            if (not os.path.isabs(_CONFIG_VARS['srcdir']) and\r\n                base != cwd):\r\n                # srcdir is relative and we are not in the same directory\r\n                # as the executable. Assume executable is in the build\r\n                # directory and make srcdir absolute.\r\n                srcdir = os.path.join(base, _CONFIG_VARS['srcdir'])\r\n                _CONFIG_VARS['srcdir'] = os.path.normpath(srcdir)\r\n\r\n        if sys.platform == 'darwin':\r\n            kernel_version = os.uname()[2] # Kernel version (8.4.3)\r\n            major_version = int(kernel_version.split('.')[0])\r\n\r\n            if major_version < 8:\r\n                # On Mac OS X before 10.4, check if -arch and -isysroot\r\n                # are in CFLAGS or LDFLAGS and remove them if they are.\r\n                # This is needed when building extensions on a 10.3 system\r\n                # using a universal build of python.\r\n                for key in ('LDFLAGS', 'BASECFLAGS',\r\n                        # a number of derived variables. These need to be\r\n                        # patched up as well.\r\n                        'CFLAGS', 'PY_CFLAGS', 'BLDSHARED'):\r\n                    flags = _CONFIG_VARS[key]\r\n                    flags = re.sub('-arch\\s+\\w+\\s', ' ', flags)\r\n                    flags = re.sub('-isysroot [^ \\t]*', ' ', flags)\r\n                    _CONFIG_VARS[key] = flags\r\n            else:\r\n                # Allow the user to override the architecture flags using\r\n                # an environment variable.\r\n                # NOTE: This name was introduced by Apple in OSX 10.5 and\r\n                # is used by several scripting languages distributed with\r\n                # that OS release.\r\n                if 'ARCHFLAGS' in os.environ:\r\n                    arch = os.environ['ARCHFLAGS']\r\n                    for key in ('LDFLAGS', 'BASECFLAGS',\r\n                        # a number of derived variables. These need to be\r\n                        # patched up as well.\r\n                        'CFLAGS', 'PY_CFLAGS', 'BLDSHARED'):\r\n\r\n                        flags = _CONFIG_VARS[key]\r\n                        flags = re.sub('-arch\\s+\\w+\\s', ' ', flags)\r\n                        flags = flags + ' ' + arch\r\n                        _CONFIG_VARS[key] = flags\r\n\r\n                # If we're on OSX 10.5 or later and the user tries to\r\n                # compiles an extension using an SDK that is not present\r\n                # on the current machine it is better to not use an SDK\r\n                # than to fail.\r\n                #\r\n                # The major usecase for this is users using a Python.org\r\n                # binary installer  on OSX 10.6: that installer uses\r\n                # the 10.4u SDK, but that SDK is not installed by default\r\n                # when you install Xcode.\r\n                #\r\n                CFLAGS = _CONFIG_VARS.get('CFLAGS', '')\r\n                m = re.search('-isysroot\\s+(\\S+)', CFLAGS)\r\n                if m is not None:\r\n                    sdk = m.group(1)\r\n                    if not os.path.exists(sdk):\r\n                        for key in ('LDFLAGS', 'BASECFLAGS',\r\n                             # a number of derived variables. These need to be\r\n                             # patched up as well.\r\n                            'CFLAGS', 'PY_CFLAGS', 'BLDSHARED'):\r\n\r\n                            flags = _CONFIG_VARS[key]\r\n                            flags = re.sub('-isysroot\\s+\\S+(\\s|$)', ' ', flags)\r\n                            _CONFIG_VARS[key] = flags\r\n\r\n    if args:\r\n        vals = []\r\n        for name in args:\r\n            vals.append(_CONFIG_VARS.get(name))\r\n        return vals\r\n    else:\r\n        return _CONFIG_VARS\r\n\r\ndef get_config_var(name):\r\n    \"\"\"Return the value of a single variable using the dictionary returned by\r\n    'get_config_vars()'.\r\n\r\n    Equivalent to get_config_vars().get(name)\r\n    \"\"\"\r\n    return get_config_vars().get(name)\r\n\r\ndef get_platform():\r\n    \"\"\"Return a string that identifies the current platform.\r\n\r\n    This is used mainly to distinguish platform-specific build directories and\r\n    platform-specific built distributions.  Typically includes the OS name\r\n    and version and the architecture (as supplied by 'os.uname()'),\r\n    although the exact information included depends on the OS; eg. for IRIX\r\n    the architecture isn't particularly important (IRIX only runs on SGI\r\n    hardware), but for Linux the kernel version isn't particularly\r\n    important.\r\n\r\n    Examples of returned values:\r\n       linux-i586\r\n       linux-alpha (?)\r\n       solaris-2.6-sun4u\r\n       irix-5.3\r\n       irix64-6.2\r\n\r\n    Windows will return one of:\r\n       win-amd64 (64bit Windows on AMD64 (aka x86_64, Intel64, EM64T, etc)\r\n       win-ia64 (64bit Windows on Itanium)\r\n       win32 (all others - specifically, sys.platform is returned)\r\n\r\n    For other non-POSIX platforms, currently just returns 'sys.platform'.\r\n    \"\"\"\r\n    import re\r\n    if os.name == 'nt':\r\n        # sniff sys.version for architecture.\r\n        prefix = \" bit (\"\r\n        i = sys.version.find(prefix)\r\n        if i == -1:\r\n            return sys.platform\r\n        j = sys.version.find(\")\", i)\r\n        look = sys.version[i+len(prefix):j].lower()\r\n        if look == 'amd64':\r\n            return 'win-amd64'\r\n        if look == 'itanium':\r\n            return 'win-ia64'\r\n        return sys.platform\r\n\r\n    if os.name != \"posix\" or not hasattr(os, 'uname'):\r\n        # XXX what about the architecture? NT is Intel or Alpha,\r\n        # Mac OS is M68k or PPC, etc.\r\n        return sys.platform\r\n\r\n    # Try to distinguish various flavours of Unix\r\n    osname, host, release, version, machine = os.uname()\r\n\r\n    # Convert the OS name to lowercase, remove '\/' characters\r\n    # (to accommodate BSD\/OS), and translate spaces (for \"Power Macintosh\")\r\n    osname = osname.lower().replace('\/', '')\r\n    machine = machine.replace(' ', '_')\r\n    machine = machine.replace('\/', '-')\r\n\r\n    if osname[:5] == \"linux\":\r\n        # At least on Linux\/Intel, 'machine' is the processor --\r\n        # i386, etc.\r\n        # XXX what about Alpha, SPARC, etc?\r\n        return  \"%s-%s\" % (osname, machine)\r\n    elif osname[:5] == \"sunos\":\r\n        if release[0] >= \"5\":           # SunOS 5 == Solaris 2\r\n            osname = \"solaris\"\r\n            release = \"%d.%s\" % (int(release[0]) - 3, release[2:])\r\n        # fall through to standard osname-release-machine representation\r\n    elif osname[:4] == \"irix\":              # could be \"irix64\"!\r\n        return \"%s-%s\" % (osname, release)\r\n    elif osname[:3] == \"aix\":\r\n        return \"%s-%s.%s\" % (osname, version, release)\r\n    elif osname[:6] == \"cygwin\":\r\n        osname = \"cygwin\"\r\n        rel_re = re.compile (r'[\\d.]+')\r\n        m = rel_re.match(release)\r\n        if m:\r\n            release = m.group()\r\n    elif osname[:6] == \"darwin\":\r\n        #\r\n        # For our purposes, we'll assume that the system version from\r\n        # distutils' perspective is what MACOSX_DEPLOYMENT_TARGET is set\r\n        # to. This makes the compatibility story a bit more sane because the\r\n        # machine is going to compile and link as if it were\r\n        # MACOSX_DEPLOYMENT_TARGET.\r\n        cfgvars = get_config_vars()\r\n        macver = cfgvars.get('MACOSX_DEPLOYMENT_TARGET')\r\n\r\n        if 1:\r\n            # Always calculate the release of the running machine,\r\n            # needed to determine if we can build fat binaries or not.\r\n\r\n            macrelease = macver\r\n            # Get the system version. Reading this plist is a documented\r\n            # way to get the system version (see the documentation for\r\n            # the Gestalt Manager)\r\n            try:\r\n                f = open('\/System\/Library\/CoreServices\/SystemVersion.plist')\r\n            except IOError:\r\n                # We're on a plain darwin box, fall back to the default\r\n                # behaviour.\r\n                pass\r\n            else:\r\n                try:\r\n                    m = re.search(\r\n                            r'<key>ProductUserVisibleVersion<\/key>\\s*' +\r\n                            r'<string>(.*?)<\/string>', f.read())\r\n                    if m is not None:\r\n                        macrelease = '.'.join(m.group(1).split('.')[:2])\r\n                    # else: fall back to the default behaviour\r\n                finally:\r\n                    f.close()\r\n\r\n        if not macver:\r\n            macver = macrelease\r\n\r\n        if macver:\r\n            release = macver\r\n            osname = \"macosx\"\r\n\r\n            if (macrelease + '.') >= '10.4.' and \\\r\n                    '-arch' in get_config_vars().get('CFLAGS', '').strip():\r\n                # The universal build will build fat binaries, but not on\r\n                # systems before 10.4\r\n                #\r\n                # Try to detect 4-way universal builds, those have machine-type\r\n                # 'universal' instead of 'fat'.\r\n\r\n                machine = 'fat'\r\n                cflags = get_config_vars().get('CFLAGS')\r\n\r\n                archs = re.findall('-arch\\s+(\\S+)', cflags)\r\n                archs = tuple(sorted(set(archs)))\r\n\r\n                if len(archs) == 1:\r\n                    machine = archs[0]\r\n                elif archs == ('i386', 'ppc'):\r\n                    machine = 'fat'\r\n                elif archs == ('i386', 'x86_64'):\r\n                    machine = 'intel'\r\n                elif archs == ('i386', 'ppc', 'x86_64'):\r\n                    machine = 'fat3'\r\n                elif archs == ('ppc64', 'x86_64'):\r\n                    machine = 'fat64'\r\n                elif archs == ('i386', 'ppc', 'ppc64', 'x86_64'):\r\n                    machine = 'universal'\r\n                else:\r\n                    raise ValueError(\r\n                       \"Don't know machine value for archs=%r\"%(archs,))\r\n\r\n            elif machine == 'i386':\r\n                # On OSX the machine type returned by uname is always the\r\n                # 32-bit variant, even if the executable architecture is\r\n                # the 64-bit variant\r\n                if sys.maxint >= 2**32:\r\n                    machine = 'x86_64'\r\n\r\n            elif machine in ('PowerPC', 'Power_Macintosh'):\r\n                # Pick a sane name for the PPC architecture.\r\n                # See 'i386' case\r\n                if sys.maxint >= 2**32:\r\n                    machine = 'ppc64'\r\n                else:\r\n                    machine = 'ppc'\r\n\r\n    return \"%s-%s-%s\" % (osname, release, machine)\r\n\r\n\r\ndef get_python_version():\r\n    return _PY_VERSION_SHORT\r\n","label":0}
{"content":"import os, sys, requests, pprint, re, json\nfrom uritemplate import URITemplate, expand\nfrom subprocess import call\n\nchangelog_file = '..\/..\/changelog.txt'\ntoken_file = '..\/..\/..\/TelegramPrivate\/github-releases-token.txt'\n\nversion = ''\ncommit = ''\nfor arg in sys.argv:\n  if re.match(r'\\d+\\.\\d+', arg):\n    version = arg\n  elif re.match(r'^[a-f0-9]{40}$', arg):\n    commit = arg\n\n# thanks http:\/\/stackoverflow.com\/questions\/13909900\/progress-of-python-requests-post\nclass upload_in_chunks(object):\n  def __init__(self, filename, chunksize=1 << 13):\n    self.filename = filename\n    self.chunksize = chunksize\n    self.totalsize = os.path.getsize(filename)\n    self.readsofar = 0\n\n  def __iter__(self):\n    with open(self.filename, 'rb') as file:\n      while True:\n        data = file.read(self.chunksize)\n        if not data:\n          sys.stderr.write(\"\\n\")\n          break\n        self.readsofar += len(data)\n        percent = self.readsofar * 1e2 \/ self.totalsize\n        sys.stderr.write(\"\\r{percent:3.0f}%\".format(percent=percent))\n        yield data\n\n  def __len__(self):\n    return self.totalsize\n\nclass IterableToFileAdapter(object):\n  def __init__(self, iterable):\n    self.iterator = iter(iterable)\n    self.length = len(iterable)\n\n  def read(self, size=-1): # TBD: add buffer for `len(data) > size` case\n    return next(self.iterator, b'')\n\n  def __len__(self):\n    return self.length\n\ndef checkResponseCode(result, right_code):\n  if (result.status_code != right_code):\n    print('Wrong result code: ' + str(result.status_code) + ', should be ' + str(right_code))\n    sys.exit(1)\n\npp = pprint.PrettyPrinter(indent=2)\nurl = 'https:\/\/api.github.com\/'\n\nversion_parts = version.split('.')\n\nstable = 1\nalpha = 0\ndev = 0\n\nif len(version_parts) < 2:\n  print('Error: expected at least major version ' + version)\n  sys.exit(1)\nif len(version_parts) > 4:\n  print('Error: bad version passed ' + version)\n  sys.exit(1)\nversion_major = version_parts[0] + '.' + version_parts[1]\nif len(version_parts) == 2:\n  version = version_major + '.0'\n  version_full = version\nelse:\n  version = version_major + '.' + version_parts[2]\n  version_full = version\n  if len(version_parts) == 4:\n    if version_parts[3] == 'dev':\n      dev = 1\n      stable = 0\n      version_full = version + '.dev'\n    elif version_parts[3] == 'alpha':\n      alpha = 1\n      stable = 0\n      version_full = version + '.alpha'\n    else:\n      print('Error: unexpected version part ' + version_parts[3])\n      sys.exit(1)\n\naccess_token = ''\nif os.path.isfile(token_file):\n  with open(token_file) as f:\n    for line in f:\n      access_token = line.replace('\\n', '')\n\nif access_token == '':\n  print('Access token not found!')\n  sys.exit(1)\n\nprint('Version: ' + version_full);\nlocal_folder = '\/Volumes\/Storage\/backup\/' + version_major + '\/' + version_full\n\nif stable == 1:\n  if os.path.isdir(local_folder + '.dev'):\n    dev = 1\n    stable = 0\n    version_full = version + '.dev'\n    local_folder = local_folder + '.dev'\n  elif os.path.isdir(local_folder + '.alpha'):\n    alpha = 1\n    stable = 0\n    version_full = version + '.alpha'\n    local_folder = local_folder + '.alpha'\n\nif not os.path.isdir(local_folder):\n  print('Storage path not found!')\n  sys.exit(1)\n\nlocal_folder = local_folder + '\/'\n\nfiles = []\nfiles.append({\n  'local': 'tsetup.' + version_full + '.exe',\n  'remote': 'tsetup.' + version_full + '.exe',\n  'backup_folder': 'tsetup',\n  'mime': 'application\/octet-stream',\n  'label': 'Windows: Installer',\n})\nfiles.append({\n  'local': 'tportable.' + version_full + '.zip',\n  'remote': 'tportable.' + version_full + '.zip',\n  'backup_folder': 'tsetup',\n  'mime': 'application\/zip',\n  'label': 'Windows: Portable',\n})\nfiles.append({\n  'local': 'tsetup.' + version_full + '.dmg',\n  'remote': 'tsetup.' + version_full + '.dmg',\n  'backup_folder': 'tmac',\n  'mime': 'application\/octet-stream',\n  'label': 'macOS and OS X 10.8+: Installer',\n})\nfiles.append({\n  'local': 'tsetup32.' + version_full + '.dmg',\n  'remote': 'tsetup32.' + version_full + '.dmg',\n  'backup_folder': 'tmac32',\n  'mime': 'application\/octet-stream',\n  'label': 'OS X 10.6 and 10.7: Installer',\n})\nfiles.append({\n  'local': 'tsetup.' + version_full + '.tar.xz',\n  'remote': 'tsetup.' + version_full + '.tar.xz',\n  'backup_folder': 'tlinux',\n  'mime': 'application\/octet-stream',\n  'label': 'Linux 64 bit: Binary',\n})\nfiles.append({\n  'local': 'tsetup32.' + version_full + '.tar.xz',\n  'remote': 'tsetup32.' + version_full + '.tar.xz',\n  'backup_folder': 'tlinux32',\n  'mime': 'application\/octet-stream',\n  'label': 'Linux 32 bit: Binary',\n})\n\nr = requests.get(url + 'repos\/telegramdesktop\/tdesktop\/releases\/tags\/v' + version)\nif r.status_code == 404:\n  print('Release not found, creating.')\n  if commit == '':\n    print('Error: specify the commit.')\n    sys.exit(1)\n  if not os.path.isfile(changelog_file):\n    print('Error: Changelog file not found.')\n    sys.exit(1)\n  changelog = ''\n  started = 0\n  with open(changelog_file) as f:\n    for line in f:\n      if started == 1:\n        if re.match(r'^\\d+\\.\\d+', line):\n          break;\n        if re.match(r'^\\s+$', line):\n          continue\n        changelog += line\n      else:\n        if re.match(r'^\\d+\\.\\d+', line):\n          if line[0:len(version) + 1] == version + ' ':\n            started = 1\n          elif line[0:len(version_major) + 1] == version_major + ' ':\n            if version_major + '.0' == version:\n              started = 1\n  if started != 1:\n    print('Error: Changelog not found.')\n    sys.exit(1)\n\n  changelog = changelog.strip()\n  print('Changelog: ');\n  print(changelog);\n\n  r = requests.post(url + 'repos\/telegramdesktop\/tdesktop\/releases', headers={'Authorization': 'token ' + access_token}, data=json.dumps({\n    'tag_name': 'v' + version,\n    'target_commitish': commit,\n    'name': 'v ' + version,\n    'body': changelog,\n    'prerelease': (dev == 1 or alpha == 1),\n  }))\n  checkResponseCode(r, 201)\n\nr = requests.get(url + 'repos\/telegramdesktop\/tdesktop\/releases\/tags\/v' + version)\ncheckResponseCode(r, 200);\n\nrelease_data = r.json()\n#pp.pprint(release_data)\n\nrelease_id = release_data['id']\nprint('Release ID: ' + str(release_id))\n\nr = requests.get(url + 'repos\/telegramdesktop\/tdesktop\/releases\/' + str(release_id) + '\/assets');\ncheckResponseCode(r, 200);\n\nassets = release_data['assets']\nfor asset in assets:\n  name = asset['name']\n  found = 0\n  for file in files:\n    if file['remote'] == name:\n      print('Already uploaded: ' + name)\n      file['already'] = 1\n      found = 1\n      break\n  if found == 0:\n    print('Warning: strange asset: ' + name)\n\nfor file in files:\n  if 'already' in file:\n    continue\n  file_path = local_folder + file['backup_folder'] + '\/' + file['local']\n  if not os.path.isfile(file_path):\n    print('Warning: file not found ' + file['local'])\n    continue\n\n  upload_url = expand(release_data['upload_url'], {'name': file['remote'], 'label': file['label']}) + '&access_token=' + access_token;\n\n  content = upload_in_chunks(file_path, 10)\n\n  print('Uploading: ' + file['remote'] + ' (' + str(round(len(content) \/ 10000) \/ 100.) + ' MB)')\n  r = requests.post(upload_url, headers={\"Content-Type\": file['mime']}, data=IterableToFileAdapter(content))\n\n  checkResponseCode(r, 201)\n\n  print('Success! Removing.')\n  return_code = call([\"rm\", file_path])\n  if return_code != 0:\n    print('Bad rm code: ' + str(return_code))\n    sys.exit(1)\n\nsys.exit()\n","label":0}
{"content":"#\n# Copyright 2010 Free Software Foundation, Inc.\n#\n# This file was generated by gr_modtool, a tool from the GNU Radio framework\n# This file is a part of gr-howto\n#\n# SPDX-License-Identifier: GPL-3.0-or-later\n#\n#\n\"\"\"\nUtilities for extracting text from generated classes.\n\"\"\"\nfrom __future__ import unicode_literals\n\ndef is_string(txt):\n    if isinstance(txt, str):\n        return True\n    try:\n        if isinstance(txt, str):\n            return True\n    except NameError:\n        pass\n    return False\n\ndef description(obj):\n    if obj is None:\n        return None\n    return description_bit(obj).strip()\n\ndef description_bit(obj):\n    if hasattr(obj, 'content'):\n        contents = [description_bit(item) for item in obj.content]\n        result = ''.join(contents)\n    elif hasattr(obj, 'content_'):\n        contents = [description_bit(item) for item in obj.content_]\n        result = ''.join(contents)\n    elif hasattr(obj, 'value'):\n        result = description_bit(obj.value)\n    elif is_string(obj):\n        return obj\n    else:\n        raise Exception('Expecting a string or something with content, content_ or value attribute')\n    # If this bit is a paragraph then add one some line breaks.\n    if hasattr(obj, 'name') and obj.name == 'para':\n        result += \"\\n\\n\"\n    return result\n","label":0}
{"content":"# test_hhfit.py --- \n# \n# Filename: test_hhfit.py\n# Description: \n# Author: \n# Maintainer: \n# Created: Tue May 21 16:34:45 2013 (+0530)\n# Version: \n# Last-Updated: Tue May 21 16:37:28 2013 (+0530)\n#           By: subha\n#     Update #: 9\n# URL: \n# Keywords: \n# Compatibility: \n# \n# \n\n# Commentary: \n# \n# \n# \n# \n\n# Change log:\n# \n# Tue May 21 16:34:53 IST 2013 - Subha moved code from\n# test_converter.py to test_hhfit.py.\n\n# \n# \n# This program is free software; you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 3, or\n# (at your option) any later version.\n# \n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with this program; see the file COPYING.  If not, write to\n# the Free Software Foundation, Inc., 51 Franklin Street, Fifth\n# Floor, Boston, MA 02110-1301, USA.\n# \n# \n\n# Code:\n\nimport os\nimport numpy as np\nimport uuid\nimport unittest\nimport pylab\nimport hhfit\n\nclass TestFindRateFn(unittest.TestCase):\n    def setUp(self):\n        self.vmin = -120e-3\n        self.vmax = 40e-3\n        self.vdivs = 640\n        self.v_array = np.linspace(self.vmin, self.vmax, self.vdivs+1)\n        # Parameters for sigmoid function - from traub2005, NaF->m_inf\n        p_sigmoid = (1.0, 1\/-10e-3, -38e-3, 0.0)\n        self.sigmoid = p_sigmoid[0] \/ (1.0 + np.exp(p_sigmoid[1] * (self.v_array - p_sigmoid[2]))) + p_sigmoid[3]\n        self.p_sigmoid = p_sigmoid\n        # Parameters for exponential function - from traub2005, KC->n_inf\n        p_exp = (2e3, 1\/-27e-3, -53.5e-3, 0.0)\n        self.exp = p_exp[0] * np.exp(p_exp[1] * (self.v_array - p_exp[2])) + p_exp[3]\n        self.p_exp = p_exp\n        # Parameters for linoid function: alpha_n from original Hodgkin-Huxley K channel.\n        p_linoid = (-0.01*1e3, -1\/10e-3, 10e-3, 0.0)\n        self.linoid = p_linoid[3] + p_linoid[0] * (self.v_array - p_linoid[2]) \/ (np.exp(p_linoid[1] * (self.v_array - p_linoid[2])) - 1)\n        self.p_linoid = p_linoid\n        # This is tau_m of transient Ca2+ current (eq. 7) from\n        # Huguenard and McCormick, J Neurophysiol, 68:1373-1383,\n        # 1992.;\n        #1e-3 * (0.612 + 1 \/ (np.exp((self.v_array*1e3 + 132)\/-16.7) + np.exp((self.v_array*1e3 + 16.8)\/18.2)))\n        p_dblexp = (1e-3, -1\/16.7e-3, -132e-3, 1\/18.2e-3, -16.8e-3, 0.612e-3)\n        self.dblexp = p_dblexp[5] + p_dblexp[0] \/ (np.exp(p_dblexp[1] * (self.v_array - p_dblexp[2])) + \n                                                        np.exp(p_dblexp[3] * (self.v_array - p_dblexp[4])))\n        self.p_dblexp = p_dblexp\n\n    def test_sigmoid(self):\n        print 'Testing sigmoid'\n        fn, params = hhfit.find_ratefn(self.v_array, self.sigmoid)\n        print 'Sigmoid params original:', self.p_sigmoid, 'detected:', params\n        pylab.plot(self.v_array, self.sigmoid, 'y-', \n                   self.v_array, hhfit.sigmoid(self.v_array, *self.p_sigmoid), 'b--', \n                   self.v_array, fn(self.v_array, *params), 'r-.')\n        pylab.legend(('original sigmoid', 'computed', 'fitted %s' % (fn)))\n        pylab.show()\n        self.assertEqual(hhfit.sigmoid, fn)\n        rms_error = np.sqrt(np.mean((self.sigmoid - fn(self.v_array, *params))**2))\n        self.assertAlmostEqual(rms_error\/max(abs(self.sigmoid)), 0.0, places=3)\n\n    def test_exponential(self):\n        print 'Testing exponential'\n        fn, params = hhfit.find_ratefn(self.v_array, self.exp)\n        print 'Exponential params original:', self.p_exp, 'detected:', params\n        fnval = hhfit.exponential(self.v_array, *params)\n        pylab.plot(self.v_array, self.exp, 'y-',\n                   self.v_array, hhfit.exponential(self.v_array, *self.p_exp), 'b--',\n                   self.v_array, fnval, 'r-.')\n        pylab.legend(('original exp', 'computed', 'fitted %s' % (fn)))\n        pylab.show()\n        self.assertEqual(hhfit.exponential, fn)\n        # The same exponential can be satisfied by an infinite number\n        # of parameter values. Hence we cannot compare the parameters,\n        # but only the fit\n        rms_error = np.sqrt(np.sum((self.exp - fnval)**2))\n        # pylab.plot(self.v_array, self.exp, 'b-')\n        # pylab.plot(self.v_array, fnval, 'r-.') \n        # pylab.show()\n        print rms_error, rms_error\/max(self.exp)\n        self.assertAlmostEqual(rms_error\/max(self.exp), 0.0, places=3)\n\n    def test_linoid(self):\n        print 'Testing linoid'\n        fn, params = hhfit.find_ratefn(self.v_array, self.linoid)\n        print 'Linoid params original:', self.p_linoid, 'detected:', params\n        pylab.plot(self.v_array, self.linoid, 'y-', \n                   self.v_array, hhfit.linoid(self.v_array, *self.p_linoid), 'b--',\n                   self.v_array, fn(self.v_array, *params), 'r-.')\n        pylab.legend(('original linoid', 'computed', 'fitted %s' % (fn)))\n        pylab.show()\n        self.assertEqual(hhfit.linoid, fn)\n        fnval = fn(self.v_array, *params)\n        rms_error = np.sqrt(np.mean((self.linoid - fnval)**2))\n        self.assertAlmostEqual(rms_error\/max(self.linoid), 0.0, places=3)\n        # errors = params - np.array(self.p_linoid)\n        # for orig, err in zip(self.p_linoid, errors):\n        #     self.assertAlmostEqual(abs(err\/orig), 0.0, places=2)\n\n    def test_dblexponential(self):\n        print 'Testing double exponential'\n        fn, params = hhfit.find_ratefn(self.v_array, self.dblexp)\n        fnval = fn(self.v_array, *params)\n        pylab.plot(self.v_array, self.dblexp, 'y-', \n                   self.v_array, hhfit.double_exp(self.v_array, *self.p_dblexp), 'b--',\n                   self.v_array, fnval, 'r-.')\n        pylab.legend(('original dblexp', 'computed', 'fitted %s' % (fn)))\n        pylab.show()\n        self.assertEqual(hhfit.double_exp, fn)\n        rms_error = np.sqrt(np.mean((self.dblexp - fnval)**2))\n        print params, rms_error\n        self.assertAlmostEqual(rms_error\/max(self.dblexp), 0.0, places=3)\n\n\nif __name__ == '__main__':\n    unittest.main()\n        \n\n# \n# test_hhfit.py ends here\n","label":0}
{"content":"\"\"\"Schur decomposition functions.\"\"\"\n\nimport numpy\nfrom numpy import asarray_chkfinite, single\n\n# Local imports.\nimport misc\nfrom misc import LinAlgError, _datacopied\nfrom lapack import get_lapack_funcs\nfrom decomp import eigvals\n\n\n__all__ = ['schur', 'rsf2csf']\n\n_double_precision = ['i','l','d']\n\ndef schur(a, output='real', lwork=None, overwrite_a=False):\n    \"\"\"Compute Schur decomposition of a matrix.\n\n    The Schur decomposition is\n\n        A = Z T Z^H\n\n    where Z is unitary and T is either upper-triangular, or for real\n    Schur decomposition (output='real'), quasi-upper triangular.  In\n    the quasi-triangular form, 2x2 blocks describing complex-valued\n    eigenvalue pairs may extrude from the diagonal.\n\n    Parameters\n    ----------\n    a : array, shape (M, M)\n        Matrix to decompose\n    output : {'real', 'complex'}\n        Construct the real or complex Schur decomposition (for real matrices).\n    lwork : integer\n        Work array size. If None or -1, it is automatically computed.\n    overwrite_a : boolean\n        Whether to overwrite data in a (may improve performance)\n\n    Returns\n    -------\n    T : array, shape (M, M)\n        Schur form of A. It is real-valued for the real Schur decomposition.\n    Z : array, shape (M, M)\n        An unitary Schur transformation matrix for A.\n        It is real-valued for the real Schur decomposition.\n\n    See also\n    --------\n    rsf2csf : Convert real Schur form to complex Schur form\n\n    \"\"\"\n    if not output in ['real','complex','r','c']:\n        raise ValueError(\"argument must be 'real', or 'complex'\")\n    a1 = asarray_chkfinite(a)\n    if len(a1.shape) != 2 or (a1.shape[0] != a1.shape[1]):\n        raise ValueError('expected square matrix')\n    typ = a1.dtype.char\n    if output in ['complex','c'] and typ not in ['F','D']:\n        if typ in _double_precision:\n            a1 = a1.astype('D')\n            typ = 'D'\n        else:\n            a1 = a1.astype('F')\n            typ = 'F'\n    overwrite_a = overwrite_a or (_datacopied(a1, a))\n    gees, = get_lapack_funcs(('gees',), (a1,))\n    if lwork is None or lwork == -1:\n        # get optimal work array\n        result = gees(lambda x: None, a1, lwork=-1)\n        lwork = result[-2][0].real.astype(numpy.int)\n    result = gees(lambda x: None, a1, lwork=lwork, overwrite_a=overwrite_a)\n    info = result[-1]\n    if info < 0:\n        raise ValueError('illegal value in %d-th argument of internal gees'\n                                                                    % -info)\n    elif info > 0:\n        raise LinAlgError(\"Schur form not found.  Possibly ill-conditioned.\")\n    return result[0], result[-3]\n\n\neps = numpy.finfo(float).eps\nfeps = numpy.finfo(single).eps\n\n_array_kind = {'b':0, 'h':0, 'B': 0, 'i':0, 'l': 0, 'f': 0, 'd': 0, 'F': 1, 'D': 1}\n_array_precision = {'i': 1, 'l': 1, 'f': 0, 'd': 1, 'F': 0, 'D': 1}\n_array_type = [['f', 'd'], ['F', 'D']]\n\ndef _commonType(*arrays):\n    kind = 0\n    precision = 0\n    for a in arrays:\n        t = a.dtype.char\n        kind = max(kind, _array_kind[t])\n        precision = max(precision, _array_precision[t])\n    return _array_type[kind][precision]\n\ndef _castCopy(type, *arrays):\n    cast_arrays = ()\n    for a in arrays:\n        if a.dtype.char == type:\n            cast_arrays = cast_arrays + (a.copy(),)\n        else:\n            cast_arrays = cast_arrays + (a.astype(type),)\n    if len(cast_arrays) == 1:\n        return cast_arrays[0]\n    else:\n        return cast_arrays\n\n\ndef rsf2csf(T, Z):\n    \"\"\"Convert real Schur form to complex Schur form.\n\n    Convert a quasi-diagonal real-valued Schur form to the upper triangular\n    complex-valued Schur form.\n\n    Parameters\n    ----------\n    T : array, shape (M, M)\n        Real Schur form of the original matrix\n    Z : array, shape (M, M)\n        Schur transformation matrix\n\n    Returns\n    -------\n    T : array, shape (M, M)\n        Complex Schur form of the original matrix\n    Z : array, shape (M, M)\n        Schur transformation matrix corresponding to the complex form\n\n    See also\n    --------\n    schur : Schur decompose a matrix\n\n    \"\"\"\n    Z, T = map(asarray_chkfinite, (Z, T))\n    if len(Z.shape) != 2 or Z.shape[0] != Z.shape[1]:\n        raise ValueError(\"matrix must be square.\")\n    if len(T.shape) != 2 or T.shape[0] != T.shape[1]:\n        raise ValueError(\"matrix must be square.\")\n    if T.shape[0] != Z.shape[0]:\n        raise ValueError(\"matrices must be same dimension.\")\n    N = T.shape[0]\n    arr = numpy.array\n    t = _commonType(Z, T, arr([3.0],'F'))\n    Z, T = _castCopy(t, Z, T)\n    conj = numpy.conj\n    dot = numpy.dot\n    r_ = numpy.r_\n    transp = numpy.transpose\n    for m in range(N-1, 0, -1):\n        if abs(T[m,m-1]) > eps*(abs(T[m-1,m-1]) + abs(T[m,m])):\n            k = slice(m-1, m+1)\n            mu = eigvals(T[k,k]) - T[m,m]\n            r = misc.norm([mu[0], T[m,m-1]])\n            c = mu[0] \/ r\n            s = T[m,m-1] \/ r\n            G = r_[arr([[conj(c), s]], dtype=t), arr([[-s, c]], dtype=t)]\n            Gc = conj(transp(G))\n            j = slice(m-1, N)\n            T[k,j] = dot(G, T[k,j])\n            i = slice(0, m+1)\n            T[i,k] = dot(T[i,k], Gc)\n            i = slice(0, N)\n            Z[i,k] = dot(Z[i,k], Gc)\n        T[m,m-1] = 0.0;\n    return T, Z\n","label":0}
{"content":"# encoding:UTF-8\n\nimport sys\nimport os\nimport time\n\nimport wlstModule as wlst\nimport ConfigParser\n\nfrom java.util import Properties\nfrom java.lang import System\nfrom java.io import FileInputStream\nfrom java.io import FileOutputStream\n\nfrom weblogic.security.internal import SerializedSystemIni\nfrom weblogic.security.internal.encryption import ClearOrEncryptedService\n\n\nconfig = None\n\n\nclass crtwls:\n\n    def log(cls, message):\n        print(\"\\n*** %s\" % message)\n\n    log = classmethod(log)\n\n    def connectToAdminServer(cls):\n        adminAddress = config.get('crtwls', 'admin-address')\n        cls.log(\"Conectando ao AdminServer %s\" % adminAddress)\n        wlst.connect(url='t3:\/\/' + adminAddress)\n\n    connectToAdminServer = classmethod(connectToAdminServer)\n\n    def edit(cls, waitTime=0, timeout=0, start=True):\n        cls.log(\"Indo para arvore de edicao\")\n        wlst.edit()\n\n        if start:\n            cls.log(\"Obtendo Lock da Console\")\n            wlst.startEdit(waitTime, timeout)\n\n    edit = classmethod(edit)\n\n    def save(cls):\n        cls.log(\"Salvando a modificacao\")\n        wlst.save()\n\n        cls.log(\"Ativando as mudancas\")\n        wlst.activate(block='true')\n\n    save = classmethod(save)\n\n    def getDomainName(cls):\n        return wlst.cmo.getName()\n\n    getDomainName = classmethod(getDomainName)\n\n    def getAdminAddress(cls):\n        adminAddress = config.get('crtwls', 'admin-address')\n        return adminAddress\n\n    getAdminAddress = classmethod(getAdminAddress)\n\n    def getEnvSuffix(cls):\n        envSuffix = config.get('crtwls', 'env-suffix')\n        return envSuffix\n\n    getEnvSuffix = classmethod(getEnvSuffix)\n\n\n# ==================================\ndef _wait(progress):\n    crtwls.log(\"Aguardando a operacao %s\" % progress.getCommandType())\n    while progress.isRunning() == 1:\n        progress.printStatus()\n        print '##############################.',\n        time.sleep(1)\n\n    print '.'\n    progress.printStatus()\n\n    completed = progress.isCompleted() == 1\n    crtwls.log(\"Operacao completada? %s\" % completed)\n\n    return completed\n\n\nclass Application:\n\n    def __init__(self, name):\n        if not name:\n            raise ValueError(\"name required\")\n\n        name = name.strip()\n        if len(name) == 0:\n            raise ValueError(\"name required\")\n\n        self.name = name\n\n        if not config.has_section(self.name):\n            config.add_section(self.name)\n\n    def group(self, group=None):\n\n        if group:\n            config.set(self.name, 'group', group)\n\n        else:\n            if config.has_option(self.name, 'group'):\n                group = config.get(self.name, 'group')\n            else:\n                group = self.name\n\n        return group\n\n    def redeploy(self, path):\n\n        crtwls.connectToAdminServer()\n\n        reinstall = False\n        remove = False\n\n        applications = wlst.cmo.getAppDeployments()\n        for application in applications:\n            if application.getName() == self.name:\n                reinstall = True\n                break\n\n#\t\tif reinstall and application.getSourcePath() != path:\n        if True:\n            remove = True\n            reinstall = False\n\n        target = Cluster.resolveClusterName(self)\n\n        try:\n            if reinstall:\n                crtwls.edit(10 * 60 * 1000, 5 * 60 * 1000)\n\n                crtwls.log(\"Fazendo redeploy da Aplicacao '%s'\" % self.name)\n                progress = wlst.redeploy(self.name, block='true')\n                wlst.activate()\n\n            else:\n\n                if remove:\n                    crtwls.edit(10 * 60 * 1000, 5 * 60 * 1000)\n\n                    crtwls.log(\n                        \"Fazendo undeploy da Aplicacao '%s'\" %\n                        self.name)\n                    progress = wlst.undeploy(self.name, block='true')\n                    wlst.activate()\n\n                crtwls.edit(10 * 60 * 1000, 5 * 60 * 1000)\n\n                crtwls.log(\"Fazendo deploy da Aplicacao '%s'\" % self.name)\n                progress = wlst.deploy(self.name, path, target, block='true')\n                wlst.activate()\n        except wlst.WLSTException as e:\n            e.printStackTrace()\n            raise e\n\n    def __findSid__(self, url):\n        idx = url.find('SERVICE_NAME')\n        if idx < 0:\n            idx = url.find('SID')\n\n        if not idx < 0:\n            sta = url.find('=', idx) + 1\n            end = url.find(')', sta)\n            return url[sta:end].strip()\n\n        idx = url.find('@')\n        sta = url.rfind('\/', idx)\n        if sta < 0:\n            sta = url.rfind(':', idx)\n\n        sta = sta + 1\n        return url[sta:].strip()\n\n    def newDatasource(self, name, url, username, password, isXA):\n        # Reduz espa\u00e7os repetidos\n        url = ' '.join(url.split())\n\n        sid = self.__findSid__(url)\n        dsName = \"%s \/ %s\" % (username, sid)\n        dsName = dsName.lower()\n\n        if isXA:\n            dsName = dsName + ' - XA'\n\n        #config.set(self.name, 'ds.'+name+'.url', url)\n        #config.set(self.name, 'ds.'+name+'.username', username)\n        #config.set(self.name, 'ds.'+name+'.password', password)\n\n        crtwls.connectToAdminServer()\n        crtwls.edit()\n\n        cluster = Cluster.findCluster(self)\n\n        if not cluster:\n            raise Exception(\n                \"Cluster da aplicacao %s nao encontrado\" %\n                self.name)\n\n        crtwls.log(\"Criando o DataSource\")\n        datasource = wlst.cmo.createJDBCSystemResource(dsName)\n\n        jdbcResource = datasource.getJDBCResource()\n        jdbcResource.setName(dsName)\n\n        jndiName = '%s.ds.%s' % (self.name, name)\n        jdbcResource.getJDBCDataSourceParams().setJNDINames([jndiName])\n\n        jdbcResource.getJDBCConnectionPoolParams().setInitialCapacity(0)\n        jdbcResource.getJDBCConnectionPoolParams().setMaxCapacity(20)\n        jdbcResource.getJDBCConnectionPoolParams().setShrinkFrequencySeconds(900)\n        jdbcResource.getJDBCConnectionPoolParams().setTestConnectionsOnReserve(True)\n        jdbcResource.getJDBCConnectionPoolParams().setStatementCacheSize(30)\n        jdbcResource.getJDBCConnectionPoolParams().setStatementCacheType('LRU')\n\n        if isXA:\n            jdbcResource.getJDBCDriverParams().setDriverName(\n                'oracle.jdbc.xa.client.OracleXADataSource')\n        else:\n            jdbcResource.getJDBCDriverParams().setDriverName('oracle.jdbc.OracleDriver')\n        jdbcResource.getJDBCDriverParams().setPassword(password)\n        jdbcResource.getJDBCDriverParams().setUrl(url)\n\n        props = jdbcResource.getJDBCDriverParams().getProperties()\n        props.createProperty('user')\n        props.lookupProperty('user').setValue(username)\n\n        crtwls.log(\"Ajustando Target\")\n        datasource.addTarget(cluster)\n\n        crtwls.save()\n\n    def newMultiDatasource(self, name, dsList):\n        dsName = \"%s.ds.%s\" % (self.name, name)\n        dsName = dsName.lower()\n\n        crtwls.connectToAdminServer()\n        crtwls.edit()\n\n        cluster = Cluster.findCluster(self)\n\n        if not cluster:\n            raise Exception(\n                \"Cluster da aplicacao %s nao encontrado\" %\n                self.name)\n\n        crtwls.log(\"Criando o MultiDataSource\")\n        datasource = wlst.cmo.createJDBCSystemResource(dsName)\n\n        jdbcResource = datasource.getJDBCResource()\n        jdbcResource.setName(dsName)\n\n        jndiName = '%s.ds.%s' % (self.name, name)\n        jdbcResource.getJDBCDataSourceParams().setJNDINames([jndiName])\n        jdbcResource.getJDBCDataSourceParams().setAlgorithmType('Load-Balancing')\n        jdbcResource.getJDBCDataSourceParams().setDataSourceList(dsList)\n\n        crtwls.log(\"Ajustando Target\")\n        datasource.addTarget(cluster)\n\n        crtwls.save()\n\n    def createEnv(self, group=None):\n        crtwls.connectToAdminServer()\n\n        domainApp = System.getenv(\"DOMAIN_APP\")\n        usrRoot = System.getenv(\"USR_ROOT\")\n\n        if os.path.exists('%s\/install\/%s' % (domainApp, self.name)):\n            raise Exception(\"Ambiente de %s j\u00e1 existe\" % self.name)\n\n        self.group(group)\n\n        site = self.name\n        cfgvars = {'APP_NAME': self.name, 'SITE': site,\n                   'ENV': crtwls.getEnvSuffix(), 'DOMAIN_APP': domainApp,\n                   'DOMAIN_NAME': crtwls.getDomainName(),\n                   'CLUSTER': '${WLS_CLUSTER_%s}' % self.group()}\n\n        crtwls.log(\"Criando diret\u00f3rios\")\n        DIRS = ['appfiles', 'applogs', 'config', 'deployments',\n                'docroot', 'install']\n\n        for d in DIRS:\n            os.makedirs('%s\/%s\/%s' % (domainApp, d, self.name))\n\n        crtwls.log(\"Criando arquivo config.properties\")\n        template = open(\n            '%s\/tools\/config-properties.tmpl' %\n            usrRoot, 'r').read()\n\n        cfgname = '%s\/config\/%s\/config.properties' % (domainApp, self.name)\n        cfgfile = open(cfgname, 'w')\n        cfgfile.write(template % cfgvars)\n        cfgfile.close()\n\n        cfgname = '%s\/httpconf\/%s.cfg' % (domainApp, site)\n\n        if not os.path.exists(cfgname):\n            crtwls.log(\"Criando Apache VirtualHost '%s'\" % site)\n            template = open('%s\/tools\/virtualhost.tmpl' % usrRoot, 'r').read()\n\n            cfgfile = open(cfgname, 'w')\n            cfgfile.write(template % cfgvars)\n            cfgfile.close()\n\n            os.makedirs('%s\/httplogs\/%s' % (domainApp, site))\n        else:\n            crtwls.log(\"Apache VirtualHost '%s' j\u00e1 existe\" % site)\n\n\nclass JMSModule:\n\n    def resolveJMSModuleName(cls, application):\n        group = application.group()\n        jmsModuleName = '%s-jms' % group\n        return jmsModuleName\n\n    resolveJMSModuleName = classmethod(resolveJMSModuleName)\n\n    def findJMSModule(cls, application):\n        jmsName = cls.resolveJMSModuleName(application)\n        crtwls.log(\"Buscando o JMS Module %s\" % jmsName)\n        jmsModule = wlst.cmo.lookupJMSSystemResource(jmsName)\n        return jmsModule\n\n    findJMSModule = classmethod(findJMSModule)\n\n    def ensureJMSServers(cls, cluster):\n\n        servers = cluster.getServers()\n\n        for server in servers:\n            serverName = server.getName()\n            jmsServerName = serverName + '-jms'\n\n            jmsserver = wlst.cmo.lookupJMSServer(jmsServerName)\n\n            if not jmsserver:\n                crtwls.log(\"Criando o JMSServer '%s'\" % jmsServerName)\n                jmsserver = wlst.cmo.createJMSServer(jmsServerName)\n                jmsserver.addTarget(server)\n\n                crtwls.log(\"Configurando o JMSServer Log\")\n                jmsserver.getJMSMessageLogFile().setFileName('logs\/%s-jms.log' % serverName)\n                jmsserver.getJMSMessageLogFile().setFileMinSize(40000)\n                jmsserver.getJMSMessageLogFile().setNumberOfFilesLimited(True)\n                jmsserver.getJMSMessageLogFile().setFileCount(5)\n\n    ensureJMSServers = classmethod(ensureJMSServers)\n\n    def __createJMSModule(cls, application, cluster):\n        jmsName = cls.resolveJMSModuleName(application)\n\n        crtwls.log(\"Criando o JmsModule\")\n        cls.ensureJMSServers(cluster)\n        jmsmodule = wlst.cmo.createJMSSystemResource(jmsName)\n\n        crtwls.log(\"Ajustando Targets\")\n        jmsmodule.addTarget(cluster)\n\n        crtwls.log(\"Criando Default Connection Factory\")\n        connection = jmsmodule.getJMSResource().createConnectionFactory(\n            'jms.ConnectionFactory.default')\n        connection.setJNDIName('jms.ConnectionFactory.default')\n        connection.setDefaultTargetingEnabled(True)\n\n        return jmsmodule\n\n    __createJMSModule = classmethod(__createJMSModule)\n\n    def createJMSQueue(cls, application, name):\n        crtwls.connectToAdminServer()\n\n        cluster = Cluster.findCluster(application)\n\n        if not cluster:\n            raise Exception(\n                \"Cluster da aplicacao %s nao encontrado\" %\n                application.name)\n\n        crtwls.edit()\n\n        jmsmodule = cls.findJMSModule(application)\n\n        if not jmsmodule:\n            jmsmodule = cls.__createJMSModule(application, cluster)\n#\t\t\traise Exception(\"JMS Module da aplicacao %s nao encontrado\" % application.name)\n\n        crtwls.log(\"Criando o JmsQueue\")\n        jmsQueueName = '%s.jms.%s' % (application.name, name)\n        jmsQueue = jmsmodule.getJMSResource().createUniformDistributedQueue(jmsQueueName)\n        jmsQueue.setJNDIName(jmsQueueName)\n        jmsQueue.setDefaultTargetingEnabled(True)\n\n        crtwls.save()\n\n    createJMSQueue = classmethod(createJMSQueue)\n\n\nclass Cluster:\n\n    def createCluster(cls, application):\n        clusterName = cls.resolveClusterName(application)\n\n        crtwls.connectToAdminServer()\n        crtwls.edit()\n\n        crtwls.log(\"Criando o Cluster\")\n        cluster = wlst.cmo.createCluster(clusterName)\n\n        crtwls.log(\"Configurando o Cluster %s\" % clusterName)\n        cluster.setWeblogicPluginEnabled(True)\n        cluster.setClusterMessagingMode('unicast')\n\n        crtwls.log(\"Ajustando Targets dos MailSession\")\n        mailsessions = wlst.cmo.getMailSessions()\n        for mailsession in mailsessions:\n            mailsession.addTarget(cluster)\n            crtwls.log(\".. %s\" % mailsession.getName())\n\n        crtwls.save()\n\n    createCluster = classmethod(createCluster)\n\n    def resolveClusterName(cls, application):\n        mask = '%s-cluster'\n\n        if config.has_option('crtwls', 'cluster-name-mask'):\n            mask = config.get('crtwls', 'cluster-name-mask')\n\n        group = application.group()\n        clusterName = mask % group\n        return clusterName\n\n    resolveClusterName = classmethod(resolveClusterName)\n\n    def findCluster(cls, application):\n        clusterName = cls.resolveClusterName(application)\n        crtwls.log(\"Buscando o Cluster %s\" % clusterName)\n        cluster = wlst.cmo.lookupCluster(clusterName)\n        return cluster\n\n    findCluster = classmethod(findCluster)\n\n    __JROCKIT = '-jrockit -Xms%s -Xmx%s -Xgc:genpar \\\n-Xmanagement:ssl=false,port=%d -Dweblogic.wsee.useRequestHost=true \\\n-Djava.awt.headless=true -Dconfig.applogssuffix=${weblogic.Name} \\\n-Dconfig.applogspath=%s\/applogs'\n\n    __HOTSPOT = '-server -Xms%s -Xmx%s -XX:MaxPermSize=256M \\\n-Dcom.sun.management.jmxremote.port=%d -Dcom.sun.management.jmxremote.ssl=false \\\n-Djavax.management.builder.initial=weblogic.management.jmx.mbeanserver.WLSMBeanServerBuilder \\\n-Dweblogic.wsee.useRequestHost=true \\\n-Djava.awt.headless=true -Dconfig.applogssuffix=${weblogic.Name} \\\n-Dconfig.applogspath=%s\/applogs'\n\n    def createManagedServer(\n            cls,\n            application,\n            hostname,\n            port,\n            serial,\n            memory='1G'):\n        vendor = System.getenv(\"JAVA_VENDOR\")\n        CMDLINE = vendor == 'SUN' and cls.__HOTSPOT or cls.__JROCKIT\n\n        if not memory:\n            memory = '1G'\n\n        crtwls.connectToAdminServer()\n        crtwls.edit()\n\n        cluster = cls.findCluster(application)\n        if not cluster:\n            raise Exception(\n                \"Cluster da aplicacao %s nao encontrado\" %\n                application.name)\n\n        mcn = Domain.findMachine(hostname)\n        if not mcn:\n            raise Exception(\"Machine do hostname %s nao encontrado\" % hostname)\n\n        domainName = crtwls.getDomainName()\n        shortname = hostname.split('.')[0]\n        group = application.group()\n\n        serverName = '%s-%s-%s-%s' % (domainName, group, shortname, serial)\n\n        crtwls.log(\"Buscando o Server\")\n        server = wlst.cmo.lookupServer(serverName)\n\n        if not server:\n            crtwls.log(\"Criando o Server\")\n            server = wlst.cmo.createServer(serverName)\n\n        server.setCluster(cluster)\n        server.setMachine(mcn)\n\n        crtwls.log(\"Configurando o Server '%s'\" % serverName)\n        server.setListenAddress(hostname)\n        server.setListenPort(port)\n        server.setWeblogicPluginEnabled(True)\n\n        server.getSSL().setEnabled(True)\n        server.getSSL().setListenPort(int(port) + 1)\n\n        crtwls.log(\"Ajustando Deployment Options\")\n        server.setUploadDirectoryName('\/nonexistent')\n        server.setStagingMode('nostage')\n\n        crtwls.log(\"Ajustando Server StartUp Options\")\n        domainApp = System.getenv(\"DOMAIN_APP\")\n        cmdLine = CMDLINE % (memory, memory, port + 2, domainApp)\n        server.getServerStart().setArguments(cmdLine)\n\n        crtwls.log(\"Configurando o Server Log\")\n        server.getLog().setFileName('logs\/%s-server.log' % serverName)\n        server.getLog().setFileMinSize(40000)\n        server.getLog().setNumberOfFilesLimited(True)\n        server.getLog().setFileCount(5)\n\n        crtwls.log(\"Configurando o WebServer\")\n        server.getWebServer().setMaxPostSize(23068672)\n\n        crtwls.log(\"Configurando o WebServer Log\")\n        server.getWebServer().getWebServerLog().setFileName(\n            'logs\/%s-access.log' % serverName)\n        server.getWebServer().getWebServerLog().setFileMinSize(40000)\n        server.getWebServer().getWebServerLog().setNumberOfFilesLimited(True)\n        server.getWebServer().getWebServerLog().setFileCount(5)\n\n        crtwls.log(\"Criando link simbolico em serverlogs\")\n        relativeLogPath = \"..\/..\/..\/domains\/\" + \\\n            domainName + \"\/servers\/\" + serverName + \"\/logs\"\n        linkName = domainApp + \"\/serverlogs\/\" + serverName\n        os.system('ln -s ' + relativeLogPath + ' ' + linkName)\n\n        jmsModule = JMSModule.findJMSModule(application)\n        if jmsModule:\n            JMSModule.ensureJMSServers(cluster)\n\n        crtwls.save()\n\n    createManagedServer = classmethod(createManagedServer)\n\n\nclass Domain:\n\n    def create(cls, domainName, envSuffix, adminAddress):\n        wlHome = System.getenv('WL_HOME')\n        usrRoot = System.getenv(\"USR_ROOT\")\n        appRoot = System.getenv('APP_ROOT')\n        domainRoot = System.getenv('DOMAIN_ROOT')\n        apacheRoot = System.getenv('APACHE_ROOT')\n\n        hostname, port = adminAddress.split(':')\n        port = int(port)\n        adminName = '%s-adminserver' % domainName\n\n        domainHome = '%s\/%s' % (domainRoot, domainName)\n        domainApp = \"%s\/%s\" % (appRoot, domainName)\n\n        cfgvars = {'DOMAIN_NAME': domainName,\n                   'CLUSTER': adminAddress, 'ENV': envSuffix}\n\n        wlst.readTemplate('%s\/..\/basedomain.jar' % wlHome)\n        wlst.cmo.setName(domainName)\n\n        wlst.cmo.getServers()[0].setName(adminName)\n        wlst.cmo.getServers()[0].setListenAddress(hostname)\n        wlst.cmo.getServers()[0].setListenPort(port)\n        wlst.cmo.setAdminServerName(adminName)\n\n        wlst.writeDomain(domainHome)\n\n        crtwls.log(\"Criando diret\u00f3rios\")\n        os.makedirs('%s\/jmsstores' % (domainHome))\n\n        DIRS = ['appfiles', 'applogs', 'config', 'deployments',\n                'docroot', 'install', 'httplogs', 'httpconf', 'serverlogs']\n\n        for d in DIRS:\n            os.makedirs('%s\/%s' % (domainApp, d))\n\n        crtwls.log(\"Criando common.properties\")\n        cfgname = '%s\/config\/common.properties' % (domainApp)\n        cfgfile = open(cfgname, 'w')\n        cfgfile.write('allowjobfrom=\\n')\n        cfgfile.close()\n\n        crtwls.log(\"Criando Apache VirtualHost\")\n        template = open('%s\/tools\/domainhost.tmpl' % usrRoot, 'r').read()\n\n        cfgname = '%s\/httpconf\/default.conf' % (domainApp)\n        cfgfile = open(cfgname, 'w')\n        cfgfile.write(template % cfgvars)\n        cfgfile.close()\n\n        open('%s\/httpconf\/manutencao.txt' % (domainApp), 'w').close()\n        os.makedirs('%s\/httplogs\/default' % (domainApp))\n\n        crtwls.log(\"Incluindo o VirtualHost no Apache Conf\")\n        template = 'Include ${APP_ROOT}\/%(DOMAIN_NAME)s\/httpconf\/default.conf\\n'\n\n        cfgname = '%s\/conf.d\/%s.cfg' % (apacheRoot, domainName)\n        cfgfile = open(cfgname, 'w')\n        cfgfile.write(template % cfgvars)\n        cfgfile.close()\n\n        crtwls.log(\"Criando crtwls.cfg\")\n        template = '[crtwls]\\nadmin-address = %(CLUSTER)s\\nenv-suffix = %(ENV)s\\n'\n\n        cfgname = '%s\/crtwls.cfg' % (domainHome)\n        cfgfile = open(cfgname, 'w')\n        cfgfile.write(template % cfgvars)\n        cfgfile.close()\n\n        crtwls.log(\"Criando startEnv.sh\")\n        template = 'ADMIN_NAME=%s\\nNM_PORT=%d\\n'\n\n        cfgname = '%s\/startEnv.sh' % (domainHome)\n        cfgfile = open(cfgname, 'w')\n        cfgfile.write(template % (adminName, port + 4))\n        cfgfile.close()\n\n        crtwls.log(\"Copiando boot.properties\")\n        template = open('%s\/servers\/%s\/security\/boot.properties'\n                        % (domainHome, adminName), 'r').read()\n\n        cfgname = '%s\/boot.properties' % (domainHome)\n        cfgfile = open(cfgname, 'w')\n        cfgfile.write(template)\n        cfgfile.close()\n\n    create = classmethod(create)\n\n    def authenticator(cls):\n        crtwls.connectToAdminServer()\n        crtwls.edit()\n\n        crtwls.log(\"Identificando o REALM\")\n        realm = wlst.cmo.getSecurityConfiguration().getDefaultRealm()\n\n        crtwls.log(\"Buscando o autenticador 'Petrobras AD Authenticator'\")\n        auth = realm.lookupAuthenticationProvider('Petrobras AD Authenticator')\n\n        if not auth:\n            crtwls.log(\"Criando o autenticador 'Petrobras AD Authenticator'\")\n            auth = realm.createAuthenticationProvider(\n                'Petrobras AD Authenticator',\n                'weblogic.security.providers.authentication.ActiveDirectoryAuthenticator')\n\n        crtwls.log(\"Configurando o autenticador 'Petrobras AD Authenticator'\")\n        auth.setGroupBaseDN('DC=biz')\n        auth.setUserNameAttribute('sAMAccountName')\n        auth.setConnectionRetryLimit(3)\n        auth.setConnectTimeout(10)\n        auth.setParallelConnectDelay(5)\n        auth.setResultsTimeLimit(1000)\n        auth.setAllUsersFilter('objectClass=user')\n        auth.setPropagateCauseForLoginException(False)\n        auth.setHost(\n            'sptbrdc04.petrobras.biz sptbrdc14.petrobras.biz sptbrdc08.petrobras.biz sptbrdc02.petrobras.biz')\n        auth.setAllGroupsFilter('objectClass=group')\n        auth.setUseTokenGroupsForGroupMembershipLookup(True)\n        auth.setUserFromNameFilter('(&(samAccountName=%u)(objectclass=user))')\n        auth.setGroupFromNameFilter(\n            '(&(sAMAccountName=%g)(objectclass=group))')\n        auth.setPort(3268)\n        auth.setUserBaseDN('DC=biz')\n        auth.setStaticGroupNameAttribute('sAMAccountName')\n        auth.setPrincipal('sacduxba@petrobras.biz')\n        auth.setCredential('--------')\n        auth.setControlFlag('SUFFICIENT')\n        auth.setEnableSIDtoGroupLookupCaching(True)\n\n        crtwls.log(\"Configurando outros autenticadores\")\n\n        from weblogic.management.security.authentication import AuthenticatorMBean\n        for tmp in realm.getAuthenticationProviders():\n            if isinstance(tmp, AuthenticatorMBean):\n                crtwls.log(\n                    \".. Ajustando ControlFlag de '%s' para SUFFICIENT\" %\n                    tmp.getName())\n                tmp.setControlFlag('SUFFICIENT')\n\n        crtwls.save()\n\n        crtwls.log(\"Configurando grupo Administrador\")\n        wlst.serverConfig()\n        realm = wlst.cmo.getSecurityConfiguration().getDefaultRealm()\n\n        mapper = realm.lookupRoleMapper('XACMLRoleMapper')\n\n        expr = '{Grp(Administrators)|Grp(GG_BA_TICBA_UNIX_WEB_ADMINS)}'\n        mapper.setRoleExpression(None, 'Admin', expr)\n\n        expr = '{Grp(AppTesters)|Usr(sawjciba)}'\n        mapper.setRoleExpression(None, 'AppTester', expr)\n\n    authenticator = classmethod(authenticator)\n\n    def configure(cls):\n        crtwls.connectToAdminServer()\n        crtwls.edit()\n\n        domainName = wlst.cmo.getName()\n\n        crtwls.log(\"Configurando o Domain Log\")\n        wlst.cmo.getLog().setFileMinSize(40000)\n        wlst.cmo.getLog().setNumberOfFilesLimited(True)\n        wlst.cmo.getLog().setFileCount(5)\n\n        crtwls.log(\"AdminServer - Configurando\")\n        server = wlst.cmo.lookupServer(domainName + '-adminserver')\n\n        crtwls.log(\"AdminServer - Ajustando WeblogicPluginEnabled\")\n        server.setWeblogicPluginEnabled(True)\n\n        crtwls.log(\"AdminServer - Ajustando UploadDirectoryName\")\n        server.setUploadDirectoryName('\/nonexistent')\n\n        crtwls.log(\"AdminServer - Configurando o Server Log\")\n        server.getLog().setFileMinSize(40000)\n        server.getLog().setNumberOfFilesLimited(True)\n        server.getLog().setFileCount(5)\n\n        crtwls.log(\"AdminServer - Configurando o WebServer\")\n        server.getWebServer().setMaxPostSize(15728640)\n        server.getWebServer().setFrontendHost('%s.petrobras.com.br' % domainName)\n        server.getWebServer().setFrontendHTTPPort(80)\n\n        crtwls.log(\"AdminServer - Configurando o WebServer Log\")\n        server.getWebServer().getWebServerLog().setFileMinSize(40000)\n        server.getWebServer().getWebServerLog().setNumberOfFilesLimited(True)\n        server.getWebServer().getWebServerLog().setFileCount(5)\n\n        crtwls.save()\n\n    configure = classmethod(configure)\n\n    def listDatasource(cls):\n        crtwls.connectToAdminServer()\n        crtwls.edit(False)\n\n        datasources = wlst.cmo.getJDBCSystemResources()\n\n        for datasource in datasources:\n            jdbcResource = datasource.getJDBCResource()\n\n            jndiName = jdbcResource.getJDBCDataSourceParams().getJNDINames()[0]\n            jndiName = jndiName.split('.')\n            appName = jndiName[0]\n            name = jndiName[2]\n\n            dsList = jdbcResource.getJDBCDataSourceParams().getDataSourceList()\n\n            if dsList:\n                print '%s new-multidatasource %s \"%s\"' % (appName, name, dsList)\n\n            else:\n                drivername = jdbcResource.getJDBCDriverParams().getDriverName()\n                password = jdbcResource.getJDBCDriverParams().getPassword()\n                url = jdbcResource.getJDBCDriverParams().getUrl()\n\n                props = jdbcResource.getJDBCDriverParams().getProperties()\n                username = props.lookupProperty('user').getValue()\n\n                if drivername == 'oracle.jdbc.xa.client.OracleXADataSource':\n                    cmd = 'new-xadatasource'\n                else:\n                    cmd = 'new-datasource'\n\n                print '%s %s %s \"%s\" %s %s' % (appName, cmd, name, url, username, password)\n\n    listDatasource = classmethod(listDatasource)\n\n    def findMachine(cls, hostname):\n        machines = wlst.cmo.getMachines()\n        for machine in machines:\n            if machine.getNodeManager().getListenAddress() == hostname:\n                return machine\n\n    findMachine = classmethod(findMachine)\n\n    def createMachine(cls, hostname):\n        adminAddress = crtwls.getAdminAddress()\n\n        port = int(adminAddress.split(':')[1]) + 4\n        name = hostname.split('.')[0]\n\n        crtwls.connectToAdminServer()\n        crtwls.edit()\n\n        crtwls.log(\"Criando a Machine\")\n        nmgr = wlst.cmo.createMachine(name)\n\n        crtwls.log(\"Configurando a Machine %s\" % name)\n        nmgr.getNodeManager().setListenAddress(hostname)\n        nmgr.getNodeManager().setListenPort(port)\n        nmgr.getNodeManager().setDebugEnabled(True)\n\n        crtwls.save()\n\n    createMachine = classmethod(createMachine)\n\n    def mailSession(cls):\n        crtwls.connectToAdminServer()\n        crtwls.edit()\n\n        crtwls.log(\"Buscando o MailSession\")\n        mailsession = wlst.cmo.lookupMailSession('mail.default')\n\n        if not mailsession:\n            crtwls.log(\"Criando o MailSession\")\n            mailsession = wlst.cmo.createMailSession('mail.default')\n\n        mailsession.setJNDIName('mail.default')\n\n        crtwls.log(\"Ajustando Targets\")\n        clusters = wlst.cmo.getClusters()\n        for cluster in clusters:\n            mailsession.addTarget(cluster)\n            crtwls.log(\".. %s\" % cluster.getName())\n\n        crtwls.log(\"Ajustando as configura\u00e7\u00f5es de SMTP\")\n        props = Properties()\n        props.setProperty('mail.transport.protocol', 'smtp')\n        props.setProperty('mail.smtp.host', 'smtp.petrobras.com.br')\n        props.setProperty('mail.smtp.port', '25')\n        props.setProperty('mail.smtp.connectiontimeout', '5000')\n        props.setProperty('mail.smtp.timeout', '10000')\n        mailsession.setProperties(props)\n\n        crtwls.save()\n\n    mailSession = classmethod(mailSession)\n\n    def undeployApps():\n        crtwls.connectToAdminServer()\n\n        crtwls.log(\"Obtendo lista de Aplicacoes\")\n        appList = wlst.cmo.getAppDeployments()\n\n        for app in appList:\n            if not app.getName().startswith('crtwls-'):\n                crtwls.log(\"Desinstalando Aplicacao: \" + app.getName())\n                wlst.undeploy(app.getName())\n\n    undeployApps = classmethod(undeployApps)\n\n    def decrypt(cls, encryptedText):\n        domainHome = System.getenv(\"DOMAIN_HOME\")\n        encryptionService = SerializedSystemIni.getEncryptionService(\n            domainHome)\n        ceService = ClearOrEncryptedService(encryptionService)\n\n        clearText = ceService.decrypt(encryptedText)\n        print '>>', clearText\n\n    decrypt = classmethod(decrypt)\n\n    def decryptProperties(cls, propertiesFile):\n        domainApp = System.getenv(\"DOMAIN_APP\")\n        domainHome = System.getenv(\"DOMAIN_HOME\")\n\n        encryptionService = SerializedSystemIni.getEncryptionService(\n            domainHome)\n        ceService = ClearOrEncryptedService(encryptionService)\n\n        propertiesFile = '%s\/%s' % (domainApp, propertiesFile)\n        fis = FileInputStream(propertiesFile)\n        props = Properties()\n        props.load(fis)\n        fis.close()\n\n        changed = False\n\n        for entry in props.entrySet():\n            value = entry.getValue()\n\n            if ceService.isEncrypted(value):\n                clearText = ceService.decrypt(value)\n                props.setProperty(entry.getKey(), clearText)\n                changed = True\n\n        if changed:\n            fos = FileOutputStream(propertiesFile)\n            props.store(fos, None)\n            fos.close()\n\n    decryptProperties = classmethod(decryptProperties)\n\n    def restartRunningManagedServers(cls):\n        crtwls.connectToAdminServer()\n        wlst.domainRuntime()\n        server_lifecycles = wlst.cmo.getServerLifeCycleRuntimes()\n\n        for server_lifecycle in server_lifecycles:\n            if (server_lifecycle.getState() ==\n                    'RUNNING' and server_lifecycle.getName() != wlst.serverName):\n                wlst.shutdown(\n                    server_lifecycle.getName(),\n                    'Server',\n                    'true',\n                    1000,\n                    block='true')\n                print \"Waiting process to shutdown...\"\n                while (server_lifecycle.getState() != \"SHUTDOWN\"):\n                    time.sleep(1)\n                    print \".\"\n                print \"OK\"\n                wlst.start(server_lifecycle.getName())\n            else:\n                print 'Doing nothing: ' + server_lifecycle.getName() + ' state: ' + server_lifecycle.getState()\n\n    restartRunningManagedServers = classmethod(restartRunningManagedServers)\n\n\ndef usage():\n    print \"Usage: %s\" % sys.argv[0]\n\n    print \"\"\"\n\tdomain create <domainName> <envSuffix> <adminAddress>\n\tdomain configure\n\tdomain configure-authenticator\n\tdomain configure-mailsession\n\tdomain create-machine <hostname>\n\tdomain list-datasource\n\tdomain undeploy-apps\n\tdomain decrypt <text>\n\tdomain decrypt-properties <file> #inline decrypt\n\tdomain restart-running-servers\n\tapplication <appname> create-env [group]\n\tapplication <appname> create-cluster\n\tapplication <appname> create-server <hostname> <port> <serial> [memory]\n\tapplication <appname> new-datasource <name> <url> <username> <password>\n\tapplication <appname> new-xadatasource <name> <url> <username> <password>\n\tapplication <appname> new-multidatasource <name> <dslist>\n\tapplication <appname> new-jmsqueue <name>\n\tapplication <appname> redeploy <path>\n\t\"\"\"\n    sys.exit(2)\n\n\ndef openConfig():\n    global config\n\n    config = ConfigParser.ConfigParser()\n\n    try:\n        cfgfile = open('crtwls.cfg')\n        config.readfp(cfgfile)\n    except IOError as e:\n        pass\n\n\ndef closeConfig():\n    global config\n\n    cfgfile = open('crtwls.cfg', 'wb')\n    config.write(cfgfile)\n\n\ndef argv(idx):\n    if len(sys.argv) <= idx:\n        usage()\n\n    return sys.argv[idx]\n\n\nif __name__ == \"__main__\":\n    try:\n        openConfig()\n\n        cmd = argv(1)\n\n        if cmd == 'application':\n            appName = argv(2)\n            subcmd = argv(3)\n\n            application = Application(appName)\n\n            if subcmd == 'create-env':\n                group = None\n                if len(sys.argv) > 4:\n                    group = argv(4)\n\n                application.createEnv(group)\n\n            elif subcmd == 'create-cluster':\n                Cluster.createCluster(application)\n\n            elif subcmd == 'create-server':\n                hostname = argv(4)\n                port = int(argv(5))\n                serial = argv(6)\n\n                memory = None\n                if len(sys.argv) > 7:\n                    memory = argv(7)\n\n                Cluster.createManagedServer(\n                    application, hostname, port, serial, memory)\n\n            elif subcmd == 'new-datasource':\n                name = argv(4)\n                url = argv(5)\n                username = argv(6)\n                password = argv(7)\n                application.newDatasource(name, url, username, password, False)\n\n            elif subcmd == 'new-xadatasource':\n                name = argv(4)\n                url = argv(5)\n                username = argv(6)\n                password = argv(7)\n                application.newDatasource(name, url, username, password, True)\n\n            elif subcmd == 'new-multidatasource':\n                name = argv(4)\n                dsList = argv(5)\n                application.newMultiDatasource(name, dsList)\n\n            elif subcmd == 'new-jmsqueue':\n                name = argv(4)\n                JMSModule.createJMSQueue(application, name)\n\n            elif subcmd == 'redeploy':\n                path = argv(4)\n                application.redeploy(path)\n\n        elif cmd == 'domain':\n            subcmd = argv(2)\n\n            if subcmd == 'create':\n                domainName = argv(3)\n                envSuffix = argv(4)\n                adminAddress = argv(5)\n                Domain.create(domainName, envSuffix, adminAddress)\n\n            elif subcmd == 'configure':\n                Domain.configure()\n\n            elif subcmd == 'configure-authenticator':\n                Domain.authenticator()\n\n            elif subcmd == 'list-datasource':\n                Domain.listDatasource()\n\n            elif subcmd == 'configure-mailsession':\n                Domain.mailSession()\n\n            elif subcmd == 'create-machine':\n                hostname = argv(3)\n                Domain.createMachine(hostname)\n\n            elif subcmd == 'undeploy-apps':\n                Domain.undeployApps()\n\n            elif subcmd == 'decrypt':\n                text = argv(3)\n                Domain.decrypt(text)\n\n            elif subcmd == 'decrypt-properties':\n                propertiesFile = argv(3)\n                Domain.decryptProperties(propertiesFile)\n\n            elif subcmd == 'restart-running-servers':\n                Domain.restartRunningManagedServers()\n\n        else:\n            usage()\n\n    finally:\n        closeConfig()\n","label":0}
{"content":"# coding: utf-8\nfrom __future__ import unicode_literals\n\nimport re\n\nfrom .common import InfoExtractor\nfrom ..utils import unescapeHTML\n\n\nclass BaiduVideoIE(InfoExtractor):\n    IE_DESC = '\u767e\u5ea6\u89c6\u9891'\n    _VALID_URL = r'https?:\/\/v\\.baidu\\.com\/(?P<type>[a-z]+)\/(?P<id>\\d+)\\.htm'\n    _TESTS = [{\n        'url': 'http:\/\/v.baidu.com\/comic\/1069.htm?frp=bdbrand&q=%E4%B8%AD%E5%8D%8E%E5%B0%8F%E5%BD%93%E5%AE%B6',\n        'info_dict': {\n            'id': '1069',\n            'title': '\u4e2d\u534e\u5c0f\u5f53\u5bb6 TV\u7248\u56fd\u8bed',\n            'description': 'md5:51be07afe461cf99fa61231421b5397c',\n        },\n        'playlist_count': 52,\n    }, {\n        'url': 'http:\/\/v.baidu.com\/show\/11595.htm?frp=bdbrand',\n        'info_dict': {\n            'id': '11595',\n            'title': 're:^\u5954\u8dd1\u5427\u5144\u5f1f',\n            'description': 'md5:1bf88bad6d850930f542d51547c089b8',\n        },\n        'playlist_mincount': 12,\n    }]\n\n    def _call_api(self, path, category, playlist_id, note):\n        return self._download_json('http:\/\/app.video.baidu.com\/%s\/?worktype=adnative%s&id=%s' % (\n            path, category, playlist_id), playlist_id, note)\n\n    def _real_extract(self, url):\n        category, playlist_id = re.match(self._VALID_URL, url).groups()\n        if category == 'show':\n            category = 'tvshow'\n        if category == 'tv':\n            category = 'tvplay'\n\n        playlist_detail = self._call_api(\n            'xqinfo', category, playlist_id, 'Download playlist JSON metadata')\n\n        playlist_title = playlist_detail['title']\n        playlist_description = unescapeHTML(playlist_detail.get('intro'))\n\n        episodes_detail = self._call_api(\n            'xqsingle', category, playlist_id, 'Download episodes JSON metadata')\n\n        entries = [self.url_result(\n            episode['url'], video_title=episode['title']\n        ) for episode in episodes_detail['videos']]\n\n        return self.playlist_result(\n            entries, playlist_id, playlist_title, playlist_description)\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http:\/\/tiny.be>).\n#\n#    This program is free software: you can redistribute it and\/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#\n##############################################################################\n\nfrom openerp.osv import fields, osv\nfrom openerp.tools.translate import _\n\nclass account_move_line_reconcile_select(osv.osv_memory):\n    _name = \"account.move.line.reconcile.select\"\n    _description = \"Move line reconcile select\"\n    _columns = {\n       'account_id': fields.many2one('account.account', 'Account', \\\n                            domain = [('reconcile', '=', 1)], required=True),\n    }\n\n    def action_open_window(self, cr, uid, ids, context=None):\n        \"\"\"\n        This function Open  account move line window for reconcile on given account id\n        @param cr: the current row, from the database cursor,\n        @param uid: the current user\u2019s ID for security checks,\n        @param ids: account move line reconcile select\u2019s ID or list of IDs\n        @return: dictionary of  Open  account move line window for reconcile on given account id\n\n         \"\"\"\n        data = self.read(cr, uid, ids, context=context)[0]\n        return {\n            'domain': \"[('account_id','=',%d),('reconcile_id','=',False),('state','<>','draft')]\" % data['account_id'],\n            'name': _('Reconciliation'),\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'view_id': False,\n            'res_model': 'account.move.line',\n            'type': 'ir.actions.act_window'\n        }\n\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n","label":0}
{"content":"# Copyright (c) 2013, Web Notes Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nfrom frappe.utils import cint, validate_email_add\nfrom frappe import throw, msgprint, _\n\nfrom frappe.model.document import Document\n\nclass Warehouse(Document):\n\n\tdef autoname(self):\n\t\tsuffix = \" - \" + frappe.db.get_value(\"Company\", self.company, \"abbr\")\n\t\tif not self.warehouse_name.endswith(suffix):\n\t\t\tself.name = self.warehouse_name + suffix\n\n\tdef validate(self):\n\t\tif self.email_id and not validate_email_add(self.email_id):\n\t\t\t\tthrow(_(\"Please enter valid Email Id\"))\n\n\t\tself.update_parent_account()\n\n\tdef update_parent_account(self):\n\t\t\n\t\tif not getattr(self, \"__islocal\", None) \\\n\t\t\tand (self.create_account_under != frappe.db.get_value(\"Warehouse\", self.name, \"create_account_under\")):\n\t\t\t\t\n\t\t\t\tself.validate_parent_account()\n\t\t\t\t\n\t\t\t\twarehouse_account = frappe.db.get_value(\"Account\", \n\t\t\t\t\t{\"account_type\": \"Warehouse\", \"company\": self.company, \"master_name\": self.name}, \n\t\t\t\t\t[\"name\", \"parent_account\"])\n\t\t\t\t\t\n\t\t\t\tif warehouse_account and warehouse_account[1] != self.create_account_under:\n\t\t\t\t\tacc_doc = frappe.get_doc(\"Account\", warehouse_account[0])\n\t\t\t\t\tacc_doc.parent_account = self.create_account_under\n\t\t\t\t\tacc_doc.save()\n\n\tdef on_update(self):\n\t\tself.create_account_head()\n\n\tdef create_account_head(self):\n\t\tif cint(frappe.defaults.get_global_default(\"auto_accounting_for_stock\")):\n\t\t\tif not frappe.db.get_value(\"Account\", {\"account_type\": \"Warehouse\",\n\t\t\t\t\t\"master_name\": self.name}):\n\t\t\t\tif self.get(\"__islocal\") or not frappe.db.get_value(\n\t\t\t\t\t\t\"Stock Ledger Entry\", {\"warehouse\": self.name}):\n\t\t\t\t\tself.validate_parent_account()\n\t\t\t\t\tac_doc = frappe.get_doc({\n\t\t\t\t\t\t\"doctype\": \"Account\",\n\t\t\t\t\t\t'account_name': self.warehouse_name,\n\t\t\t\t\t\t'parent_account': self.create_account_under,\n\t\t\t\t\t\t'group_or_ledger':'Ledger',\n\t\t\t\t\t\t'company':self.company,\n\t\t\t\t\t\t\"account_type\": \"Warehouse\",\n\t\t\t\t\t\t\"master_name\": self.name,\n\t\t\t\t\t\t\"freeze_account\": \"No\"\n\t\t\t\t\t})\n\t\t\t\t\tac_doc.ignore_permissions = True\n\t\t\t\t\tac_doc.insert()\n\t\t\t\t\tmsgprint(_(\"Account head {0} created\").format(ac_doc.name))\n\n\tdef validate_parent_account(self):\n\t\tif not self.company:\n\t\t\tfrappe.throw(_(\"Warehouse {0}: Company is mandatory\").format(self.name))\n\t\t\n\t\tif not self.create_account_under:\n\t\t\tparent_account = frappe.db.get_value(\"Account\",\n\t\t\t\t{\"account_name\": \"Stock Assets\", \"company\": self.company})\n\t\t\t\n\t\t\tif parent_account:\n\t\t\t\tself.create_account_under = parent_account\n\t\t\telse:\n\t\t\t\tfrappe.throw(_(\"Please enter parent account group for warehouse account\"))\n\t\telif frappe.db.get_value(\"Account\", self.create_account_under, \"company\") != self.company:\n\t\t\tfrappe.throw(_(\"Warehouse {0}: Parent account {1} does not bolong to the company {2}\")\n\t\t\t\t.format(self.name, self.create_account_under, self.company))\n\t\t\t\n\n\tdef on_trash(self):\n\t\t# delete bin\n\t\tbins = frappe.db.sql(\"select * from `tabBin` where warehouse = %s\",\n\t\t\tself.name, as_dict=1)\n\t\tfor d in bins:\n\t\t\tif d['actual_qty'] or d['reserved_qty'] or d['ordered_qty'] or \\\n\t\t\t\t\td['indented_qty'] or d['projected_qty'] or d['planned_qty']:\n\t\t\t\tthrow(_(\"Warehouse {0} can not be deleted as quantity exists for Item {1}\").format(self.name, d['item_code']))\n\t\t\telse:\n\t\t\t\tfrappe.db.sql(\"delete from `tabBin` where name = %s\", d['name'])\n\n\t\twarehouse_account = frappe.db.get_value(\"Account\",\n\t\t\t{\"account_type\": \"Warehouse\", \"master_name\": self.name})\n\t\tif warehouse_account:\n\t\t\tfrappe.delete_doc(\"Account\", warehouse_account)\n\n\t\tif frappe.db.sql(\"\"\"select name from `tabStock Ledger Entry`\n\t\t\t\twhere warehouse = %s\"\"\", self.name):\n\t\t\tthrow(_(\"Warehouse can not be deleted as stock ledger entry exists for this warehouse.\"))\n\n\tdef before_rename(self, olddn, newdn, merge=False):\n\t\t# Add company abbr if not provided\n\t\tfrom erpnext.setup.doctype.company.company import get_name_with_abbr\n\t\tnew_warehouse = get_name_with_abbr(newdn, self.company)\n\n\t\tif merge:\n\t\t\tif not frappe.db.exists(\"Warehouse\", new_warehouse):\n\t\t\t\tfrappe.throw(_(\"Warehouse {0} does not exist\").format(new_warehouse))\n\n\t\t\tif self.company != frappe.db.get_value(\"Warehouse\", new_warehouse, \"company\"):\n\t\t\t\tfrappe.throw(_(\"Both Warehouse must belong to same Company\"))\n\n\t\t\tfrappe.db.sql(\"delete from `tabBin` where warehouse=%s\", olddn)\n\n\t\tfrom erpnext.accounts.utils import rename_account_for\n\t\trename_account_for(\"Warehouse\", olddn, newdn, merge, self.company)\n\n\t\treturn new_warehouse\n\n\tdef after_rename(self, olddn, newdn, merge=False):\n\t\tif merge:\n\t\t\tself.recalculate_bin_qty(newdn)\n\n\tdef recalculate_bin_qty(self, newdn):\n\t\tfrom erpnext.utilities.repost_stock import repost_stock\n\t\tfrappe.db.auto_commit_on_many_writes = 1\n\t\tfrappe.db.set_default(\"allow_negative_stock\", 1)\n\n\t\tfor item in frappe.db.sql(\"\"\"select distinct item_code from (\n\t\t\tselect name as item_code from `tabItem` where ifnull(is_stock_item, 'Yes')='Yes'\n\t\t\tunion\n\t\t\tselect distinct item_code from tabBin) a\"\"\"):\n\t\t\t\trepost_stock(item[0], newdn)\n\n\t\tfrappe.db.set_default(\"allow_negative_stock\",\n\t\t\tfrappe.db.get_value(\"Stock Settings\", None, \"allow_negative_stock\"))\n\t\tfrappe.db.auto_commit_on_many_writes = 0\n","label":0}
{"content":"#!\/usr\/bin\/env python\n# Copyright 2010-2012 RethinkDB, all rights reserved.\nfrom vcoptparse import *\nimport vm_build\nimport sys\nfrom threading import Thread, Semaphore\n\nclass Builder(Thread):\n    def __init__(self, name, branch, target, semaphore):\n        Thread.__init__(self)\n        self.name = name\n        self.branch = branch\n        self.target = target\n        self.semaphore = semaphore\n    def run(self):\n        self.success = False\n        try:\n            semaphore.acquire()\n            self.target.run(self.branch, self.name)\n            self.success = True\n        except vm_build.RunError, err:\n            self.exception = err\n        finally:\n            semaphore.release()\n\ntarget_names = [\"suse\", \"redhat5_1\", \"ubuntu\", \"debian\", \"centos5_5\", \"centos6\"]\n\ndef help():\n    print >>sys.stderr, \"Virtual builder:\"\n    print >>sys.stderr, \"     --help      Print this help.\"\n    print >>sys.stderr, \"     --target target1 [target2, target3]\"\n    print >>sys.stderr, \"                 Build just one target, options are:\"\n    print >>sys.stderr, \"                 \", target_names\n    print >>sys.stderr, \"                 defaults to all of them.\"\n    print >>sys.stderr, \"     --branch branch_name\"\n    print >>sys.stderr, \"                 Build from a branch mutually exclusive with --tag.\"\n    print >>sys.stderr, \"     --tag tag-name\"\n    print >>sys.stderr, \"                 Build from a tag mutually exclusive with --branch.\"\n    print >>sys.stderr, \"     --threads number\"\n    print >>sys.stderr, \"                 The number of parallel threads to run.\"\n    print >>sys.stderr, \"     --debug\"\n    print >>sys.stderr, \"                 Whether to build the packages with debugging enabled.\"\n    print >>sys.stderr, \"     --interact\"\n    print >>sys.stderr, \"                 This starts a target so that you can interact with it.\"\n    print >>sys.stderr, \"                 Requires a target.\"\n    print >>sys.stderr, \"     --clean-up\"\n    print >>sys.stderr, \"                 Shutdown all running vms\"\n    print >>sys.stderr, \"     --username\"\n    print >>sys.stderr, \"                 Starts the Virtual Machine using VirtualBox from the specified username.\"\n    print >>sys.stderr, \"     --hostname\"\n    print >>sys.stderr, \"                 Starts the Virtual Machine using VirtualBox from the specified host machine.\"\n\no = OptParser()\no[\"help\"] = BoolFlag(\"--help\")\no[\"target\"] = StringFlag(\"--target\", None)\no[\"branch\"] = StringFlag(\"--branch\", None)\no[\"tag\"] = StringFlag(\"--tag\", None)\no[\"threads\"] = IntFlag(\"--threads\", 3)\no[\"clean-up\"] = BoolFlag(\"--clean-up\")\no[\"interact\"] = BoolFlag(\"--interact\")\no[\"debug\"] = BoolFlag(\"--debug\");\no[\"username\"] = StringFlag(\"--username\", \"rethinkdb\") # For now, these default values should always be the ones you should use\no[\"hostname\"] = StringFlag(\"--hostname\", \"deadshot\") # because the UUID values below are hard-coded to correspond with rethinkdb@deadshot\n\ntry:\n    opts = o.parse(sys.argv)\nexcept OptError:\n    print >>sys.stderr, \"Argument parsing error\"\n    help()\n    exit(-1)\n\nif opts[\"help\"]:\n    help()\n    sys.exit(0)\n\nif opts[\"branch\"] and opts[\"tag\"]:\n    print >>sys.stderr, \"Error cannot use --tag and --branch together.\"\n    help()\n    sys.exit(1)\n\nif opts[\"branch\"]:\n    rspec = vm_build.Branch(opts[\"branch\"])\nelif opts[\"tag\"]:\n    rspec = vm_build.Tag(opts[\"tag\"])\nelse:\n    rspec = vm_build.Branch(\"master\")\n\n# Prepare the build flags\nflags = \"\" # this will be given to the makefile\nif opts[\"debug\"]:\n    flags += \" DEBUG=1 UNIT_TESTS=0\"\nelse:\n    flags += \" DEBUG=0\"\n\nsuse = vm_build.target('765127b8-2007-43ff-8668-fe4c60176a2b', '192.168.0.173', 'rethinkdb', 'make LEGACY_LINUX=1 LEGACY_GCC=1 NO_EVENTFD=1 rpm-suse10 ' + flags, 'rpm', vm_build.rpm_install, vm_build.rpm_uninstall, vm_build.rpm_get_binary, opts[\"username\"], opts[\"hostname\"])\nredhat5_1 = vm_build.target('32340f79-cea9-42ca-94d5-2da13d408d02', '192.168.0.159', 'rethinkdb', 'make rpm LEGACY_GCC=1 LEGACY_LINUX=1 NO_EVENTFD=1' + flags, 'rpm', vm_build.rpm_install, vm_build.rpm_uninstall, vm_build.rpm_get_binary, opts[\"username\"], opts[\"hostname\"])\nubuntu = vm_build.target('1f4521a0-6e74-4d20-b4b9-9ffd8e231423', '192.168.0.172', 'rethinkdb', 'make deb' + flags, 'deb', vm_build.deb_install, vm_build.deb_uninstall, vm_build.deb_get_binary, opts[\"username\"], opts[\"hostname\"])\ndebian = vm_build.target('cc76e2a5-92c0-4208-be08-5c02429c2c50', '192.168.0.176', 'root', 'make deb NO_EVENTFD=1 LEGACY_LINUX=1 ' + flags, 'deb', vm_build.deb_install, vm_build.deb_uninstall, vm_build.deb_get_binary, opts[\"username\"], opts[\"hostname\"])\ncentos5_5 = vm_build.target('25710682-666f-4449-bd28-68b25abd8bea', '192.168.0.153', 'root', 'make rpm LEGACY_GCC=1 LEGACY_LINUX=1 ' + flags, 'rpm', vm_build.rpm_install, vm_build.rpm_uninstall, vm_build.rpm_get_binary, opts[\"username\"], opts[\"hostname\"])\ncentos6 = vm_build.target('d9058650-a45a-44a5-953f-c2402253a614', '192.168.0.178', 'rethinkdb', 'make rpm LEGACY_GCC=1 LEGACY_LINUX=1 ' + flags, 'rpm', vm_build.rpm_install, vm_build.rpm_uninstall, vm_build.rpm_get_binary, opts[\"username\"], opts[\"hostname\"])\n\ntargets = {\"suse\": suse, \"redhat5_1\": redhat5_1, \"ubuntu\": ubuntu, \"debian\": debian, \"centos5_5\": centos5_5, \"centos6\": centos6}\n\nif (opts[\"target\"]):\n    targets = {opts[\"target\"]: targets[opts[\"target\"]]}\n\nif opts[\"clean-up\"]:\n    map(lambda x: x[1].clean_up(), targets.iteritems())\n    exit(0)\n\nif opts[\"interact\"]:\n    if not opts[\"target\"]:\n        print >>sys.stderr, \"Error must specify a --target for --interact mode.\"\n        exit(1)\n    for name, target in targets.iteritems():\n        target.interact(name)\nelse:\n    success = {}\n    exception = {}\n    semaphore = Semaphore(opts[\"threads\"])\n\n    builders = map(lambda x: Builder(x[0], rspec, x[1], semaphore), targets.iteritems())\n    map(lambda x: x.start(), builders)\n    map(lambda x: x.join(), builders)\n\n    for b in builders:\n        success[b.name] = b.success\n        if not b.success:\n            exception[b.name] = b.exception\n\n    print \"Build summary:\"\n    from termcolor import colored\n    for name, val in success.iteritems():\n        print name, \".\" * (20 - len(name)), colored(\"[Pass]\", \"green\") if val else colored(\"[Fail]\", \"red\")\n        if (not val):\n            print \"Failed on: \", exception[name]\n            raise exception[name]\n\nprint \"Done.\"\n","label":0}
{"content":"\"\"\"\nVarious tests for synchronization primitives.\n\"\"\"\n\nimport sys\nimport time\nfrom thread import start_new_thread, get_ident\nimport threading\nimport unittest\n\nfrom test import test_support as support\n\n\ndef _wait():\n    # A crude wait\/yield function not relying on synchronization primitives.\n    time.sleep(0.01)\n\nclass Bunch(object):\n    \"\"\"\n    A bunch of threads.\n    \"\"\"\n    def __init__(self, f, n, wait_before_exit=False):\n        \"\"\"\n        Construct a bunch of `n` threads running the same function `f`.\n        If `wait_before_exit` is True, the threads won't terminate until\n        do_finish() is called.\n        \"\"\"\n        self.f = f\n        self.n = n\n        self.started = []\n        self.finished = []\n        self._can_exit = not wait_before_exit\n        def task():\n            tid = get_ident()\n            self.started.append(tid)\n            try:\n                f()\n            finally:\n                self.finished.append(tid)\n                while not self._can_exit:\n                    _wait()\n        for i in range(n):\n            start_new_thread(task, ())\n\n    def wait_for_started(self):\n        while len(self.started) < self.n:\n            _wait()\n\n    def wait_for_finished(self):\n        while len(self.finished) < self.n:\n            _wait()\n\n    def do_finish(self):\n        self._can_exit = True\n\n\nclass BaseTestCase(unittest.TestCase):\n    def setUp(self):\n        self._threads = support.threading_setup()\n\n    def tearDown(self):\n        support.threading_cleanup(*self._threads)\n        support.reap_children()\n\n\nclass BaseLockTests(BaseTestCase):\n    \"\"\"\n    Tests for both recursive and non-recursive locks.\n    \"\"\"\n\n    def test_constructor(self):\n        lock = self.locktype()\n        del lock\n\n    def test_acquire_destroy(self):\n        lock = self.locktype()\n        lock.acquire()\n        del lock\n\n    def test_acquire_release(self):\n        lock = self.locktype()\n        lock.acquire()\n        lock.release()\n        del lock\n\n    def test_try_acquire(self):\n        lock = self.locktype()\n        self.assertTrue(lock.acquire(False))\n        lock.release()\n\n    def test_try_acquire_contended(self):\n        lock = self.locktype()\n        lock.acquire()\n        result = []\n        def f():\n            result.append(lock.acquire(False))\n        Bunch(f, 1).wait_for_finished()\n        self.assertFalse(result[0])\n        lock.release()\n\n    def test_acquire_contended(self):\n        lock = self.locktype()\n        lock.acquire()\n        N = 5\n        def f():\n            lock.acquire()\n            lock.release()\n\n        b = Bunch(f, N)\n        b.wait_for_started()\n        _wait()\n        self.assertEqual(len(b.finished), 0)\n        lock.release()\n        b.wait_for_finished()\n        self.assertEqual(len(b.finished), N)\n\n    def test_with(self):\n        lock = self.locktype()\n        def f():\n            lock.acquire()\n            lock.release()\n        def _with(err=None):\n            with lock:\n                if err is not None:\n                    raise err\n        _with()\n        # Check the lock is unacquired\n        Bunch(f, 1).wait_for_finished()\n        self.assertRaises(TypeError, _with, TypeError)\n        # Check the lock is unacquired\n        Bunch(f, 1).wait_for_finished()\n\n    def test_thread_leak(self):\n        # The lock shouldn't leak a Thread instance when used from a foreign\n        # (non-threading) thread.\n        lock = self.locktype()\n        def f():\n            lock.acquire()\n            lock.release()\n        n = len(threading.enumerate())\n        # We run many threads in the hope that existing threads ids won't\n        # be recycled.\n        Bunch(f, 15).wait_for_finished()\n        self.assertEqual(n, len(threading.enumerate()))\n\n\nclass LockTests(BaseLockTests):\n    \"\"\"\n    Tests for non-recursive, weak locks\n    (which can be acquired and released from different threads).\n    \"\"\"\n    def test_reacquire(self):\n        # Lock needs to be released before re-acquiring.\n        lock = self.locktype()\n        phase = []\n        def f():\n            lock.acquire()\n            phase.append(None)\n            lock.acquire()\n            phase.append(None)\n        start_new_thread(f, ())\n        while len(phase) == 0:\n            _wait()\n        _wait()\n        self.assertEqual(len(phase), 1)\n        lock.release()\n        while len(phase) == 1:\n            _wait()\n        self.assertEqual(len(phase), 2)\n\n    def test_different_thread(self):\n        # Lock can be released from a different thread.\n        lock = self.locktype()\n        lock.acquire()\n        def f():\n            lock.release()\n        b = Bunch(f, 1)\n        b.wait_for_finished()\n        lock.acquire()\n        lock.release()\n\n\nclass RLockTests(BaseLockTests):\n    \"\"\"\n    Tests for recursive locks.\n    \"\"\"\n    def test_reacquire(self):\n        lock = self.locktype()\n        lock.acquire()\n        lock.acquire()\n        lock.release()\n        lock.acquire()\n        lock.release()\n        lock.release()\n\n    def test_release_unacquired(self):\n        # Cannot release an unacquired lock\n        lock = self.locktype()\n        self.assertRaises(RuntimeError, lock.release)\n        lock.acquire()\n        lock.acquire()\n        lock.release()\n        lock.acquire()\n        lock.release()\n        lock.release()\n        self.assertRaises(RuntimeError, lock.release)\n\n    def test_different_thread(self):\n        # Cannot release from a different thread\n        lock = self.locktype()\n        def f():\n            lock.acquire()\n        b = Bunch(f, 1, True)\n        try:\n            self.assertRaises(RuntimeError, lock.release)\n        finally:\n            b.do_finish()\n\n    def test__is_owned(self):\n        lock = self.locktype()\n        self.assertFalse(lock._is_owned())\n        lock.acquire()\n        self.assertTrue(lock._is_owned())\n        lock.acquire()\n        self.assertTrue(lock._is_owned())\n        result = []\n        def f():\n            result.append(lock._is_owned())\n        Bunch(f, 1).wait_for_finished()\n        self.assertFalse(result[0])\n        lock.release()\n        self.assertTrue(lock._is_owned())\n        lock.release()\n        self.assertFalse(lock._is_owned())\n\n\nclass EventTests(BaseTestCase):\n    \"\"\"\n    Tests for Event objects.\n    \"\"\"\n\n    def test_is_set(self):\n        evt = self.eventtype()\n        self.assertFalse(evt.is_set())\n        evt.set()\n        self.assertTrue(evt.is_set())\n        evt.set()\n        self.assertTrue(evt.is_set())\n        evt.clear()\n        self.assertFalse(evt.is_set())\n        evt.clear()\n        self.assertFalse(evt.is_set())\n\n    def _check_notify(self, evt):\n        # All threads get notified\n        N = 5\n        results1 = []\n        results2 = []\n        def f():\n            results1.append(evt.wait())\n            results2.append(evt.wait())\n        b = Bunch(f, N)\n        b.wait_for_started()\n        _wait()\n        self.assertEqual(len(results1), 0)\n        evt.set()\n        b.wait_for_finished()\n        self.assertEqual(results1, [True] * N)\n        self.assertEqual(results2, [True] * N)\n\n    def test_notify(self):\n        evt = self.eventtype()\n        self._check_notify(evt)\n        # Another time, after an explicit clear()\n        evt.set()\n        evt.clear()\n        self._check_notify(evt)\n\n    def test_timeout(self):\n        evt = self.eventtype()\n        results1 = []\n        results2 = []\n        N = 5\n        def f():\n            results1.append(evt.wait(0.0))\n            t1 = time.time()\n            r = evt.wait(0.2)\n            t2 = time.time()\n            results2.append((r, t2 - t1))\n        Bunch(f, N).wait_for_finished()\n        self.assertEqual(results1, [False] * N)\n        for r, dt in results2:\n            self.assertFalse(r)\n            self.assertTrue(dt >= 0.2, dt)\n        # The event is set\n        results1 = []\n        results2 = []\n        evt.set()\n        Bunch(f, N).wait_for_finished()\n        self.assertEqual(results1, [True] * N)\n        for r, dt in results2:\n            self.assertTrue(r)\n\n\nclass ConditionTests(BaseTestCase):\n    \"\"\"\n    Tests for condition variables.\n    \"\"\"\n\n    def test_acquire(self):\n        cond = self.condtype()\n        # Be default we have an RLock: the condition can be acquired multiple\n        # times.\n        cond.acquire()\n        cond.acquire()\n        cond.release()\n        cond.release()\n        lock = threading.Lock()\n        cond = self.condtype(lock)\n        cond.acquire()\n        self.assertFalse(lock.acquire(False))\n        cond.release()\n        self.assertTrue(lock.acquire(False))\n        self.assertFalse(cond.acquire(False))\n        lock.release()\n        with cond:\n            self.assertFalse(lock.acquire(False))\n\n    def test_unacquired_wait(self):\n        cond = self.condtype()\n        self.assertRaises(RuntimeError, cond.wait)\n\n    def test_unacquired_notify(self):\n        cond = self.condtype()\n        self.assertRaises(RuntimeError, cond.notify)\n\n    def _check_notify(self, cond):\n        N = 5\n        results1 = []\n        results2 = []\n        phase_num = 0\n        def f():\n            cond.acquire()\n            cond.wait()\n            cond.release()\n            results1.append(phase_num)\n            cond.acquire()\n            cond.wait()\n            cond.release()\n            results2.append(phase_num)\n        b = Bunch(f, N)\n        b.wait_for_started()\n        _wait()\n        self.assertEqual(results1, [])\n        # Notify 3 threads at first\n        cond.acquire()\n        cond.notify(3)\n        _wait()\n        phase_num = 1\n        cond.release()\n        while len(results1) < 3:\n            _wait()\n        self.assertEqual(results1, [1] * 3)\n        self.assertEqual(results2, [])\n        # Notify 5 threads: they might be in their first or second wait\n        cond.acquire()\n        cond.notify(5)\n        _wait()\n        phase_num = 2\n        cond.release()\n        while len(results1) + len(results2) < 8:\n            _wait()\n        self.assertEqual(results1, [1] * 3 + [2] * 2)\n        self.assertEqual(results2, [2] * 3)\n        # Notify all threads: they are all in their second wait\n        cond.acquire()\n        cond.notify_all()\n        _wait()\n        phase_num = 3\n        cond.release()\n        while len(results2) < 5:\n            _wait()\n        self.assertEqual(results1, [1] * 3 + [2] * 2)\n        self.assertEqual(results2, [2] * 3 + [3] * 2)\n        b.wait_for_finished()\n\n    def test_notify(self):\n        cond = self.condtype()\n        self._check_notify(cond)\n        # A second time, to check internal state is still ok.\n        self._check_notify(cond)\n\n    def test_timeout(self):\n        cond = self.condtype()\n        results = []\n        N = 5\n        def f():\n            cond.acquire()\n            t1 = time.time()\n            cond.wait(0.2)\n            t2 = time.time()\n            cond.release()\n            results.append(t2 - t1)\n        Bunch(f, N).wait_for_finished()\n        self.assertEqual(len(results), 5)\n        for dt in results:\n            self.assertTrue(dt >= 0.2, dt)\n\n\nclass BaseSemaphoreTests(BaseTestCase):\n    \"\"\"\n    Common tests for {bounded, unbounded} semaphore objects.\n    \"\"\"\n\n    def test_constructor(self):\n        self.assertRaises(ValueError, self.semtype, value = -1)\n        self.assertRaises(ValueError, self.semtype, value = -sys.maxint)\n\n    def test_acquire(self):\n        sem = self.semtype(1)\n        sem.acquire()\n        sem.release()\n        sem = self.semtype(2)\n        sem.acquire()\n        sem.acquire()\n        sem.release()\n        sem.release()\n\n    def test_acquire_destroy(self):\n        sem = self.semtype()\n        sem.acquire()\n        del sem\n\n    def test_acquire_contended(self):\n        sem = self.semtype(7)\n        sem.acquire()\n        N = 10\n        results1 = []\n        results2 = []\n        phase_num = 0\n        def f():\n            sem.acquire()\n            results1.append(phase_num)\n            sem.acquire()\n            results2.append(phase_num)\n        b = Bunch(f, 10)\n        b.wait_for_started()\n        while len(results1) + len(results2) < 6:\n            _wait()\n        self.assertEqual(results1 + results2, [0] * 6)\n        phase_num = 1\n        for i in range(7):\n            sem.release()\n        while len(results1) + len(results2) < 13:\n            _wait()\n        self.assertEqual(sorted(results1 + results2), [0] * 6 + [1] * 7)\n        phase_num = 2\n        for i in range(6):\n            sem.release()\n        while len(results1) + len(results2) < 19:\n            _wait()\n        self.assertEqual(sorted(results1 + results2), [0] * 6 + [1] * 7 + [2] * 6)\n        # The semaphore is still locked\n        self.assertFalse(sem.acquire(False))\n        # Final release, to let the last thread finish\n        sem.release()\n        b.wait_for_finished()\n\n    def test_try_acquire(self):\n        sem = self.semtype(2)\n        self.assertTrue(sem.acquire(False))\n        self.assertTrue(sem.acquire(False))\n        self.assertFalse(sem.acquire(False))\n        sem.release()\n        self.assertTrue(sem.acquire(False))\n\n    def test_try_acquire_contended(self):\n        sem = self.semtype(4)\n        sem.acquire()\n        results = []\n        def f():\n            results.append(sem.acquire(False))\n            results.append(sem.acquire(False))\n        Bunch(f, 5).wait_for_finished()\n        # There can be a thread switch between acquiring the semaphore and\n        # appending the result, therefore results will not necessarily be\n        # ordered.\n        self.assertEqual(sorted(results), [False] * 7 + [True] *  3 )\n\n    def test_default_value(self):\n        # The default initial value is 1.\n        sem = self.semtype()\n        sem.acquire()\n        def f():\n            sem.acquire()\n            sem.release()\n        b = Bunch(f, 1)\n        b.wait_for_started()\n        _wait()\n        self.assertFalse(b.finished)\n        sem.release()\n        b.wait_for_finished()\n\n    def test_with(self):\n        sem = self.semtype(2)\n        def _with(err=None):\n            with sem:\n                self.assertTrue(sem.acquire(False))\n                sem.release()\n                with sem:\n                    self.assertFalse(sem.acquire(False))\n                    if err:\n                        raise err\n        _with()\n        self.assertTrue(sem.acquire(False))\n        sem.release()\n        self.assertRaises(TypeError, _with, TypeError)\n        self.assertTrue(sem.acquire(False))\n        sem.release()\n\nclass SemaphoreTests(BaseSemaphoreTests):\n    \"\"\"\n    Tests for unbounded semaphores.\n    \"\"\"\n\n    def test_release_unacquired(self):\n        # Unbounded releases are allowed and increment the semaphore's value\n        sem = self.semtype(1)\n        sem.release()\n        sem.acquire()\n        sem.acquire()\n        sem.release()\n\n\nclass BoundedSemaphoreTests(BaseSemaphoreTests):\n    \"\"\"\n    Tests for bounded semaphores.\n    \"\"\"\n\n    def test_release_unacquired(self):\n        # Cannot go past the initial value\n        sem = self.semtype()\n        self.assertRaises(ValueError, sem.release)\n        sem.acquire()\n        sem.release()\n        self.assertRaises(ValueError, sem.release)\n","label":0}
{"content":"from javascript import JSObject\nfrom browser import window\nimport urllib.request\n\nclass TempMod:\n  def __init__(self, name):\n      self.name=name\n\n#define my custom import hook (just to see if it get called etc).\nclass BaseHook:\n  def __init__(self, fullname=None, path=None):\n      self._fullname=fullname\n      self._path=path    # we don't are about this...\n      self._modpath=''\n      self._module=''\n\n  def find_module(self, name=None, path=None):\n      if name is None:\n         name=self._fullname\n\n      for _i in ('libs\/%s.js' % name, 'Lib\/%s.py' % name, \n                 'Lib\/%s\/__init__.py' % name):\n          _path=\"%s%s\" % (__BRYTHON__.brython_path, _i)\n          try:\n            _fp,_,_headers=urllib.request.urlopen(_path)\n            if _headers['status'] != 200:\n               continue \n            self._module=_fp.read()\n            self._modpath=_path\n            return self\n          except urllib.error.HTTPError as e:\n            print(str(e))\n            self._modpath=''\n            self._module=''\n            \n      raise ImportError\n\n  def is_package(self):\n      return '.' in self._fullname\n\n  def load_module(self, name):\n      if name is None:\n         name=self._fullname\n      window.eval('__BRYTHON__.imported[\"%s\"]={}' % name)\n      return JSObject(__BRYTHON__.run_py)(TempMod(name),\n                                          self._modpath, self._module)\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http:\/\/tiny.be>).\n#\n#    This program is free software: you can redistribute it and\/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#\n##############################################################################\n\n\n{\n    'name': 'Invoice on Timesheets',\n    'version': '1.0',\n    'category': 'Sales Management',\n    'description': \"\"\"\nGenerate your Invoices from Expenses, Timesheet Entries.\n========================================================\n\nModule to generate invoices based on costs (human resources, expenses, ...).\n\nYou can define price lists in analytic account, make some theoretical revenue\nreports.\"\"\",\n    'author': 'OpenERP SA',\n    'website': 'https:\/\/www.odoo.com\/page\/employees',\n    'depends': ['account', 'hr_timesheet', 'report'],\n    'data': [\n        'security\/ir.model.access.csv',\n        'hr_timesheet_invoice_data.xml',\n        'hr_timesheet_invoice_view.xml',\n        'hr_timesheet_invoice_wizard.xml',\n        'hr_timesheet_invoice_report.xml',\n        'report\/report_analytic_view.xml',\n        'report\/hr_timesheet_invoice_report_view.xml',\n        'wizard\/hr_timesheet_analytic_profit_view.xml',\n        'wizard\/hr_timesheet_invoice_create_view.xml',\n        'wizard\/hr_timesheet_invoice_create_final_view.xml',\n        'views\/report_analyticprofit.xml',\n    ],\n    'demo': ['hr_timesheet_invoice_demo.xml'],\n    'test': ['test\/test_hr_timesheet_invoice.yml',\n             'test\/test_hr_timesheet_invoice_no_prod_tax.yml',\n             'test\/hr_timesheet_invoice_report.yml',\n    ],\n    'installable': True,\n    'auto_install': False,\n}\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n","label":0}
{"content":"#\n# gdb helper commands and functions for Linux kernel debugging\n#\n#  load kernel and module symbols\n#\n# Copyright (c) Siemens AG, 2011-2013\n#\n# Authors:\n#  Jan Kiszka <jan.kiszka@siemens.com>\n#\n# This work is licensed under the terms of the GNU GPL version 2.\n#\n\nimport gdb\nimport os\nimport re\n\nfrom linux import modules\n\n\nif hasattr(gdb, 'Breakpoint'):\n    class LoadModuleBreakpoint(gdb.Breakpoint):\n        def __init__(self, spec, gdb_command):\n            super(LoadModuleBreakpoint, self).__init__(spec, internal=True)\n            self.silent = True\n            self.gdb_command = gdb_command\n\n        def stop(self):\n            module = gdb.parse_and_eval(\"mod\")\n            module_name = module['name'].string()\n            cmd = self.gdb_command\n\n            # enforce update if object file is not found\n            cmd.module_files_updated = False\n\n            # Disable pagination while reporting symbol (re-)loading.\n            # The console input is blocked in this context so that we would\n            # get stuck waiting for the user to acknowledge paged output.\n            show_pagination = gdb.execute(\"show pagination\", to_string=True)\n            pagination = show_pagination.endswith(\"on.\\n\")\n            gdb.execute(\"set pagination off\")\n\n            if module_name in cmd.loaded_modules:\n                gdb.write(\"refreshing all symbols to reload module \"\n                          \"'{0}'\\n\".format(module_name))\n                cmd.load_all_symbols()\n            else:\n                cmd.load_module_symbols(module)\n\n            # restore pagination state\n            gdb.execute(\"set pagination %s\" % (\"on\" if pagination else \"off\"))\n\n            return False\n\n\nclass LxSymbols(gdb.Command):\n    \"\"\"(Re-)load symbols of Linux kernel and currently loaded modules.\n\nThe kernel (vmlinux) is taken from the current working directly. Modules (.ko)\nare scanned recursively, starting in the same directory. Optionally, the module\nsearch path can be extended by a space separated list of paths passed to the\nlx-symbols command.\"\"\"\n\n    module_paths = []\n    module_files = []\n    module_files_updated = False\n    loaded_modules = []\n    breakpoint = None\n\n    def __init__(self):\n        super(LxSymbols, self).__init__(\"lx-symbols\", gdb.COMMAND_FILES,\n                                        gdb.COMPLETE_FILENAME)\n\n    def _update_module_files(self):\n        self.module_files = []\n        for path in self.module_paths:\n            gdb.write(\"scanning for modules in {0}\\n\".format(path))\n            for root, dirs, files in os.walk(path):\n                for name in files:\n                    if name.endswith(\".ko\"):\n                        self.module_files.append(root + \"\/\" + name)\n        self.module_files_updated = True\n\n    def _get_module_file(self, module_name):\n        module_pattern = \".*\/{0}\\.ko$\".format(\n            module_name.replace(\"_\", r\"[_\\-]\"))\n        for name in self.module_files:\n            if re.match(module_pattern, name) and os.path.exists(name):\n                return name\n        return None\n\n    def _section_arguments(self, module):\n        try:\n            sect_attrs = module['sect_attrs'].dereference()\n        except gdb.error:\n            return \"\"\n        attrs = sect_attrs['attrs']\n        section_name_to_address = {\n            attrs[n]['name'].string(): attrs[n]['address']\n            for n in range(int(sect_attrs['nsections']))}\n        args = []\n        for section_name in [\".data\", \".data..read_mostly\", \".rodata\", \".bss\"]:\n            address = section_name_to_address.get(section_name)\n            if address:\n                args.append(\" -s {name} {addr}\".format(\n                    name=section_name, addr=str(address)))\n        return \"\".join(args)\n\n    def load_module_symbols(self, module):\n        module_name = module['name'].string()\n        module_addr = str(module['core_layout']['base']).split()[0]\n\n        module_file = self._get_module_file(module_name)\n        if not module_file and not self.module_files_updated:\n            self._update_module_files()\n            module_file = self._get_module_file(module_name)\n\n        if module_file:\n            gdb.write(\"loading @{addr}: {filename}\\n\".format(\n                addr=module_addr, filename=module_file))\n            cmdline = \"add-symbol-file {filename} {addr}{sections}\".format(\n                filename=module_file,\n                addr=module_addr,\n                sections=self._section_arguments(module))\n            gdb.execute(cmdline, to_string=True)\n            if module_name not in self.loaded_modules:\n                self.loaded_modules.append(module_name)\n        else:\n            gdb.write(\"no module object found for '{0}'\\n\".format(module_name))\n\n    def load_all_symbols(self):\n        gdb.write(\"loading vmlinux\\n\")\n\n        # Dropping symbols will disable all breakpoints. So save their states\n        # and restore them afterward.\n        saved_states = []\n        if hasattr(gdb, 'breakpoints') and not gdb.breakpoints() is None:\n            for bp in gdb.breakpoints():\n                saved_states.append({'breakpoint': bp, 'enabled': bp.enabled})\n\n        # drop all current symbols and reload vmlinux\n        gdb.execute(\"symbol-file\", to_string=True)\n        gdb.execute(\"symbol-file vmlinux\")\n\n        self.loaded_modules = []\n        module_list = modules.module_list()\n        if not module_list:\n            gdb.write(\"no modules found\\n\")\n        else:\n            [self.load_module_symbols(module) for module in module_list]\n\n        for saved_state in saved_states:\n            saved_state['breakpoint'].enabled = saved_state['enabled']\n\n    def invoke(self, arg, from_tty):\n        self.module_paths = arg.split()\n        self.module_paths.append(os.getcwd())\n\n        # enforce update\n        self.module_files = []\n        self.module_files_updated = False\n\n        self.load_all_symbols()\n\n        if hasattr(gdb, 'Breakpoint'):\n            if self.breakpoint is not None:\n                self.breakpoint.delete()\n                self.breakpoint = None\n            self.breakpoint = LoadModuleBreakpoint(\n                \"kernel\/module.c:do_init_module\", self)\n        else:\n            gdb.write(\"Note: symbol update on module loading not supported \"\n                      \"with this gdb version\\n\")\n\n\nLxSymbols()\n","label":0}
{"content":"# Copyright: (c) 2017, Ansible Project\n# GNU General Public License v3.0+ (see COPYING or https:\/\/www.gnu.org\/licenses\/gpl-3.0.txt)\n\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible import constants as C\nfrom ansible.plugins.action import ActionBase\nfrom ansible.utils.vars import merge_hash\n\n\nclass ActionModule(ActionBase):\n\n    def run(self, tmp=None, task_vars=None):\n        self._supports_async = True\n        results = super(ActionModule, self).run(tmp, task_vars)\n        del tmp  # tmp no longer has any effect\n\n        # Command module has a special config option to turn off the command nanny warnings\n        if 'warn' not in self._task.args:\n            self._task.args['warn'] = C.COMMAND_WARNINGS\n\n        wrap_async = self._task.async_val and not self._connection.has_native_async\n        results = merge_hash(results, self._execute_module(task_vars=task_vars, wrap_async=wrap_async))\n\n        if not wrap_async:\n            # remove a temporary path we created\n            self._remove_tmp_path(self._connection._shell.tmpdir)\n\n        return results\n","label":0}
{"content":"# HTTM: A transformation library for RAW and Electron Flux TESS Images\n# Copyright (C) 2016, 2017 John Doty and Matthew Wampler-Doty of Noqsi Aerospace, Ltd.\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n\n\"\"\"\n``httm.transformations.metadata``\n=================================\n\nThis module contains metadata related to transformation functions.\n\n  - ``electron_flux_transformations`` is metadata describing transformation functions from images in\n    electron counts to simulated raw images in *Analogue to Digital Converter Units* (ADU).\n  - ``raw_transformations`` is metadata describing transformation functions from raw images in\n    *Analogue to Digital Converter Units* (ADU) to calibrated images in electron counts.\n\"\"\"\n\nfrom collections import OrderedDict\n\nfrom .raw_converters_to_calibrated import remove_pattern_noise, convert_adu_to_electrons, remove_baseline, \\\n    remove_start_of_line_ringing, remove_undershoot, remove_smear\nfrom .electron_flux_converters_to_raw import introduce_smear_rows, add_shot_noise, simulate_blooming, \\\n    add_readout_noise, simulate_undershoot, simulate_start_of_line_ringing, add_baseline, convert_electrons_to_adu, \\\n    add_pattern_noise\n\nelectron_flux_transformations = OrderedDict([\n    ('introduce_smear_rows', {\n        'default': True,\n        'documentation': 'Introduce *smear rows* to each slice of the image.',\n        'function': introduce_smear_rows,\n    }),\n    ('add_shot_noise', {\n        'default': True,\n        'documentation': 'Add *shot noise* to each pixel in each slice of the image.',\n        'function': add_shot_noise,\n    }),\n    ('simulate_blooming', {\n        'default': True,\n        'documentation': 'Simulate *blooming* on for each column for each slice of the image.',\n        'function': simulate_blooming,\n    }),\n    ('add_readout_noise', {\n        'default': True,\n        'documentation': 'Add *readout noise* to each pixel in each slice of the image.',\n        'function': add_readout_noise,\n    }),\n    ('simulate_undershoot', {\n        'default': True,\n        'documentation': 'Simulate *undershoot* on each row of each slice in the image.',\n        'function': simulate_undershoot,\n    }),\n    ('simulate_start_of_line_ringing', {\n        'default': True,\n        'documentation': 'Simulate *start of line ringing* on each row of each slice in the image.',\n        'function': simulate_start_of_line_ringing,\n    }),\n    ('add_baseline', {\n        'default': True,\n        'documentation': 'Add a *baseline electron count* to each slice in the image.',\n        'function': add_baseline,\n    }),\n    ('convert_electrons_to_adu', {\n        'default': True,\n        'documentation': 'Convert the image from having pixel units in electron counts to '\n                         '*Analogue to Digital Converter Units* (ADU).',\n        'function': convert_electrons_to_adu,\n    }),\n    ('add_pattern_noise', {\n        'default': True,\n        'documentation': 'Add a fixed *pattern noise* to each slice in the image.',\n        'function': add_pattern_noise,\n    }),\n])\n\nraw_transformations = OrderedDict([\n    ('remove_pattern_noise', {\n        'default': True,\n        'documentation': 'Compensate for a fixed *pattern noise* on each slice of the image.',\n        'function': remove_pattern_noise,\n    }),\n    ('convert_adu_to_electrons', {\n        'default': True,\n        'documentation': 'Convert the image from having units in '\n                         '*Analogue to Digital Converter Units* (ADU) '\n                         'to electron counts.',\n        'function': convert_adu_to_electrons,\n    }),\n    ('remove_baseline', {\n        'default': True,\n        'documentation': 'Average the pixels in the dark columns and subtract '\n                         'the result from each pixel in the image.',\n        'function': remove_baseline,\n    }),\n    ('remove_start_of_line_ringing', {\n        'default': True,\n        'documentation': 'Compensate for *start of line ringing* on each row of each slice of the image.',\n        'function': remove_start_of_line_ringing,\n    }),\n    ('remove_undershoot', {\n        'default': True,\n        'documentation': 'Compensate for *undershoot* for each row of each slice of the image.',\n        'function': remove_undershoot,\n    }),\n    ('remove_smear', {\n        'default': True,\n        'documentation': 'Compensate for *smear* in the image by reading it from the '\n                         '*smear rows* each slice and removing it from the rest of the slice.',\n        'function': remove_smear,\n    }),\n])\n","label":0}
{"content":"\"\"\" Copyright (C) 2010-2011 ST-Ericsson SA \"\"\"\n\n\"\"\" Author: Szymon Janc <szymon.janc@tieto.com> for ST-Ericsson. \"\"\"\n\n\"\"\" This program is free software; you can redistribute it and\/or modify \"\"\"\n\"\"\" it under the terms of the GNU General Public License as published by \"\"\"\n\"\"\" the Free Software Foundation; either version 2 of the License, or \"\"\"\n\"\"\" (at your option) any later version. \"\"\"\n\n\"\"\" This program is distributed in the hope that it will be useful, \"\"\"\n\"\"\" but WITHOUT ANY WARRANTY; without even the implied warranty of \"\"\"\n\"\"\" MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the \"\"\"\n\"\"\" GNU General Public License for more details. \"\"\"\n\n\"\"\" You should have received a copy of the GNU General Public License \"\"\"\n\"\"\" along with this program; if not, write to the Free Software \"\"\"\n\"\"\" Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA \"\"\"\n\nfrom array import array\nfrom bluetooth import *\nimport time\nimport re\n\nclass SAPParam:\n    \"\"\" SAP Parameter Class \"\"\"\n\n    MaxMsgSize = 0x00\n    ConnectionStatus = 0x01\n    ResultCode = 0x02\n    DisconnectionType = 0x03\n    CommandAPDU = 0x04\n    ResponseAPDU = 0x05\n    ATR = 0x06\n    CardReaderStatus = 0x07\n    StatusChange = 0x08\n    TransportProtocol = 0x09\n    CommandAPDU7816 = 0x10\n\n    def __init__(self, name, id, value = None):\n        self.name = name\n        self.id = id\n        self.value = value\n\n    def _padding(self,  buf):\n        pad = array('B')\n        while ( (len(buf) + len(pad)) % 4 ) != 0:\n            pad.append(0)\n        return pad\n\n    def _basicCheck(self,  buf):\n        if len(buf) < 4 or (len(buf) % 4) != 0 or buf[1] != 0:\n                return (-1,  -1)\n        if buf[0] != self.id:\n            return (-1,  -1)\n        plen = buf[2] * 256 + buf[3] + 4\n        if plen > len(buf):\n            return (-1,  -1)\n        pad = plen\n        while (pad % 4) != 0:\n            if buf[pad] != 0:\n                return (-1,  -1)\n            pad+=1\n        return (plen,  pad)\n\n    def getID(self):\n        return self.id\n\n    def getValue(self):\n        return self.value\n\n    def getContent(self):\n        return \"%s(id=0x%.2X), value=%s \\n\" %  (self.name,  self.id, self.value)\n\n    def serialize(self):\n        a = array('B', '\\00\\00\\00\\00')\n        a[0] = self.id\n        a[1] = 0\t# reserved\n        a[2] = 0\t# length\n        a[3] = 1\t# length\n        a.append(self.value)\n        a.extend(self._padding(a))\n        return a\n\n    def deserialize(self,  buf):\n        p = self._basicCheck(buf)\n        if p[0] == -1:\n            return -1\n        self.id = buf[0]\n        self.value = buf[4]\n        return p[1]\n\n\nclass SAPParam_MaxMsgSize(SAPParam):\n    \"\"\"MaxMsgSize Param \"\"\"\n\n    def __init__(self,  value = None):\n        SAPParam.__init__(self,\"MaxMsgSize\",  SAPParam.MaxMsgSize, value)\n        self.__validate()\n\n    def __validate(self):\n        if self.value > 0xFFFF:\n             self.value = 0xFFFF\n\n    def serialize(self):\n        a = array('B', '\\00\\00\\00\\00')\n        a[0] = self.id\n        a[3] = 2\n        a.append(self.value \/ 256)\n        a.append(self.value % 256)\n        a.extend(self._padding(a))\n        return a\n\n    def deserialize(self,  buf):\n        p = self._basicCheck(buf)\n        if p[0] == -1 :\n            return -1\n        self.value = buf[4] * 256 + buf[5]\n        return p[1]\n\nclass SAPParam_CommandAPDU(SAPParam):\n    def __init__(self,  value = None):\n        if value is None:\n            SAPParam.__init__(self, \"CommandAPDU\",  SAPParam.CommandAPDU, array('B'))\n        else:\n            SAPParam.__init__(self, \"CommandAPDU\",  SAPParam.CommandAPDU, array('B', value))\n\n    def serialize(self):\n        a = array('B', '\\00\\00\\00\\00')\n        a[0] = self.id\n        plen = len(self.value)\n        a[2] = plen \/ 256\n        a[3] = plen % 256\n        a.extend(self.value)\n        a.extend(self._padding(a))\n        return a\n\n    def deserialize(self,  buf):\n        p = self._basicCheck(buf)\n        if p[0] == -1:\n            return -1\n        self.value = buf[4:p[0]]\n        return p[1]\n\nclass SAPParam_ResponseAPDU(SAPParam_CommandAPDU):\n    \"\"\"ResponseAPDU Param \"\"\"\n\n    def __init__(self,  value = None):\n        if value is None:\n            SAPParam.__init__(self, \"ResponseAPDU\",  SAPParam.ResponseAPDU, array('B'))\n        else:\n            SAPParam.__init__(self, \"ResponseAPDU\",  SAPParam.ResponseAPDU, array('B', value))\n\nclass SAPParam_ATR(SAPParam_CommandAPDU):\n    \"\"\"ATR Param \"\"\"\n\n    def __init__(self,  value = None):\n        if value is None:\n            SAPParam.__init__(self, \"ATR\",  SAPParam.ATR, array('B'))\n        else:\n            SAPParam.__init__(self, \"ATR\",  SAPParam.ATR, array('B', value))\n\nclass SAPParam_CommandAPDU7816(SAPParam_CommandAPDU):\n    \"\"\"Command APDU7816 Param.\"\"\"\n\n    def __init__(self,  value = None):\n        if value is None:\n            SAPParam.__init__(self, \"CommandAPDU7816\",  SAPParam.CommandAPDU7816, array('B'))\n        else:\n            SAPParam.__init__(self, \"CommandAPDU7816\",  SAPParam.CommandAPDU7816, array('B', value))\n\n\nclass SAPParam_ConnectionStatus(SAPParam):\n    \"\"\"Connection status Param.\"\"\"\n\n    def __init__(self,  value = None):\n        SAPParam.__init__(self,\"ConnectionStatus\",  SAPParam.ConnectionStatus, value)\n        self.__validate()\n\n    def __validate(self):\n        if self.value is not None and self.value not in (0x00,  0x01,  0x02,  0x03,  0x04):\n            print \"Warning. ConnectionStatus value in reserved range (0x%x)\" % self.value\n\n    def deserialize(self,  buf):\n        ret = SAPParam.deserialize(self, buf)\n        if ret == -1:\n            return -1\n        self.__validate()\n        return ret\n\nclass SAPParam_ResultCode(SAPParam):\n    \"\"\" Result Code Param \"\"\"\n\n    def __init__(self,  value = None):\n        SAPParam.__init__(self,\"ResultCode\",  SAPParam.ResultCode, value)\n        self.__validate()\n\n    def __validate(self):\n        if self.value is not None and self.value not in (0x00,  0x01,  0x02,  0x03,  0x04,  0x05,  0x06,  0x07):\n            print \"Warning. ResultCode value in reserved range (0x%x)\" % self.value\n\n    def deserialize(self,  buf):\n        ret = SAPParam.deserialize(self, buf)\n        if ret == -1:\n            return -1\n        self.__validate()\n        return ret\n\nclass SAPParam_DisconnectionType(SAPParam):\n    \"\"\"Disconnection Type Param.\"\"\"\n\n    def __init__(self,  value = None):\n        SAPParam.__init__(self,\"DisconnectionType\",  SAPParam.DisconnectionType, value)\n        self.__validate()\n\n    def __validate(self):\n        if self.value is not None and self.value not in (0x00,  0x01):\n            print \"Warning. DisconnectionType value in reserved range (0x%x)\" % self.value\n\n    def deserialize(self,  buf):\n        ret = SAPParam.deserialize(self, buf)\n        if ret == -1:\n            return -1\n        self.__validate()\n        return ret\n\nclass SAPParam_CardReaderStatus(SAPParam_CommandAPDU):\n    \"\"\"Card reader Status Param.\"\"\"\n\n    def __init__(self,  value = None):\n        if value is None:\n            SAPParam.__init__(self, \"CardReaderStatus\",  SAPParam.CardReaderStatus, array('B'))\n        else:\n            SAPParam.__init__(self, \"CardReaderStatus\",  SAPParam.CardReaderStatus, array('B', value))\n\nclass SAPParam_StatusChange(SAPParam):\n    \"\"\"Status Change Param \"\"\"\n\n    def __init__(self,  value = None):\n        SAPParam.__init__(self,\"StatusChange\",  SAPParam.StatusChange, value)\n\n    def __validate(self):\n        if self.value is not None and self.value not in (0x00,  0x01,  0x02,  0x03,  0x04,  0x05):\n            print \"Warning. StatusChange value in reserved range (0x%x)\" % self.value\n\n    def deserialize(self,  buf):\n        ret = SAPParam.deserialize(self, buf)\n        if ret == -1:\n            return -1\n        self.__validate()\n        return ret\n\nclass SAPParam_TransportProtocol(SAPParam):\n    \"\"\"Transport Protocol Param \"\"\"\n\n    def __init__(self,  value = None):\n        SAPParam.__init__(self,\"TransportProtocol\",  SAPParam.TransportProtocol, value)\n        self.__validate()\n\n    def __validate(self):\n        if self.value is not None and self.value not in (0x00,  0x01):\n            print \"Warning. TransportProtoco value in reserved range (0x%x)\" % self.value\n\n    def deserialize(self,  buf):\n        ret = SAPParam.deserialize(self, buf)\n        if ret == -1:\n            return -1\n        self.__validate()\n        return ret\n\nclass SAPMessage:\n\n    CONNECT_REQ = 0x00\n    CONNECT_RESP = 0x01\n    DISCONNECT_REQ = 0x02\n    DISCONNECT_RESP =0x03\n    DISCONNECT_IND = 0x04\n    TRANSFER_APDU_REQ = 0x05\n    TRANSFER_APDU_RESP = 0x06\n    TRANSFER_ATR_REQ = 0x07\n    TRANSFER_ATR_RESP = 0x08\n    POWER_SIM_OFF_REQ = 0x09\n    POWER_SIM_OFF_RESP = 0x0A\n    POWER_SIM_ON_REQ = 0x0B\n    POWER_SIM_ON_RESP = 0x0C\n    RESET_SIM_REQ = 0x0D\n    RESET_SIM_RESP = 0x0E\n    TRANSFER_CARD_READER_STATUS_REQ = 0x0F\n    TRANSFER_CARD_READER_STATUS_RESP = 0x10\n    STATUS_IND = 0x11\n    ERROR_RESP = 0x12\n    SET_TRANSPORT_PROTOCOL_REQ = 0x13\n    SET_TRANSPORT_PROTOCOL_RESP = 0x14\n\n    def __init__(self,  name,  id):\n        self.name = name\n        self.id = id\n        self.params = []\n        self.buf = array('B')\n\n    def _basicCheck(self,  buf):\n        if len(buf) < 4 or (len(buf) % 4) != 0 :\n            return False\n\n        if buf[0] != self.id:\n            return False\n\n        return True\n\n    def getID(self):\n        return self.id\n\n    def getContent(self):\n        s = \"%s(id=0x%.2X) \" % (self.name,  self.id)\n        if len( self.buf): s = s + \"[%s]\" % re.sub(\"(.{2})\", \"0x\\\\1 \" , self.buf.tostring().encode(\"hex\").upper(), re.DOTALL)\n        s = s + \"\\n\\t\"\n        for p in self.params:\n            s = s + \"\\t\" + p.getContent()\n        return s\n\n    def getParams(self):\n        return self.params\n\n    def addParam(self,  param):\n        self.params.append(param)\n\n    def serialize(self):\n        ret = array('B', '\\00\\00\\00\\00')\n        ret[0] = self.id\n        ret[1] = len(self.params)\n        ret[2] = 0\t# reserved\n        ret[3] = 0\t# reserved\n        for p in self.params:\n            ret.extend(p.serialize())\n\n        self.buf = ret\n        return ret\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        return len(buf) == 4 and buf[1] == 0 and self._basicCheck(buf)\n\n\nclass SAPMessage_CONNECT_REQ(SAPMessage):\n    def __init__(self,  MaxMsgSize = None):\n        SAPMessage.__init__(self,\"CONNECT_REQ\",  SAPMessage.CONNECT_REQ)\n        if MaxMsgSize is not None:\n            self.addParam(SAPParam_MaxMsgSize(MaxMsgSize))\n\n    def _validate(self):\n        if len(self.params) == 1:\n            if self.params[0].getID() == SAPParam.MaxMsgSize:\n                return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n        if SAPMessage._basicCheck(self,  buf):\n            p = SAPParam_MaxMsgSize()\n            if p.deserialize(buf[4:]) == len(buf[4:]):\n                self.addParam(p)\n                return self._validate()\n\n        return False\n\nclass SAPMessage_CONNECT_RESP(SAPMessage):\n    def __init__(self,  ConnectionStatus = None,  MaxMsgSize = None):\n        SAPMessage.__init__(self,\"CONNECT_RESP\",  SAPMessage.CONNECT_RESP)\n        if ConnectionStatus is not None:\n            self.addParam(SAPParam_ConnectionStatus(ConnectionStatus))\n            if MaxMsgSize is not None:\n                self.addParam(SAPParam_MaxMsgSize(MaxMsgSize))\n\n    def _validate(self):\n        if len(self.params) > 0:\n            if self.params[0] .getID() == SAPParam.ConnectionStatus:\n                if self.params[0].getValue() ==  0x02:\n                    if len(self.params) == 2:\n                        return True\n                else:\n                    if len(self.params) == 1:\n                        return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n\n        if SAPMessage._basicCheck(self,  buf):\n            p = SAPParam_ConnectionStatus()\n            r = p.deserialize(buf[4:])\n            if  r != -1:\n                self.addParam(p)\n                if buf[1] == 2:\n                    p = SAPParam_MaxMsgSize()\n                    r = p.deserialize(buf[4+r:])\n                    if r != -1:\n                        self.addParam(p)\n\n                return self._validate()\n\n        return False\n\nclass SAPMessage_DISCONNECT_REQ(SAPMessage):\n    def __init__(self):\n        SAPMessage.__init__(self,\"DISCONNECT_REQ\",  SAPMessage.DISCONNECT_REQ)\n\nclass SAPMessage_DISCONNECT_RESP(SAPMessage):\n    def __init__(self):\n        SAPMessage.__init__(self,\"DISCONNECT_RESP\",  SAPMessage.DISCONNECT_RESP)\n\nclass SAPMessage_DISCONNECT_IND(SAPMessage):\n    def __init__(self,  Type = None):\n        SAPMessage.__init__(self,\"DISCONNECT_IND\",  SAPMessage.DISCONNECT_IND)\n        if Type is not None:\n            self.addParam(SAPParam_DisconnectionType(Type))\n\n    def _validate(self):\n        if len(self.params) == 1:\n            if self.params[0].getID() == SAPParam.DisconnectionType:\n                return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n        if SAPMessage._basicCheck(self,  buf):\n            p = SAPParam_DisconnectionType()\n            if p.deserialize(buf[4:]) == len(buf[4:]):\n                self.addParam(p)\n                return self._validate()\n\n        return False\n\n\nclass SAPMessage_TRANSFER_APDU_REQ(SAPMessage):\n    def __init__(self,  APDU = None,  T = False):\n        SAPMessage.__init__(self,\"TRANSFER_APDU_REQ\",  SAPMessage.TRANSFER_APDU_REQ)\n        if APDU is not None:\n            if T :\n                self.addParam(SAPParam_CommandAPDU(APDU))\n            else:\n                self.addParam(SAPParam_CommandAPDU7816(APDU))\n\n    def _validate(self):\n        if len(self.params) == 1:\n            if self.params[0].getID() == SAPParam.CommandAPDU or self.params[0].getID() == SAPParam.CommandAPDU7816:\n                return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n        if SAPMessage._basicCheck(self,  buf):\n\n            p = SAPParam_CommandAPDU()\n            p2 = SAPParam_CommandAPDU7816()\n            if p.deserialize(buf[4:]) == len(buf[4:]):\n                self.addParam(p)\n                return self._validate()\n            elif p2.deserialize(buf[4:]) == len(buf[4:]):\n                self.addParam(p2)\n                return self._validate()\n\n        return False\n\nclass SAPMessage_TRANSFER_APDU_RESP(SAPMessage):\n    def __init__(self,  ResultCode = None,  Response = None):\n        SAPMessage.__init__(self,\"TRANSFER_APDU_RESP\",  SAPMessage.TRANSFER_APDU_RESP)\n        if ResultCode is not None:\n            self.addParam(SAPParam_ResultCode(ResultCode))\n            if Response is not None:\n                self.addParam(SAPParam_ResponseAPDU(Response))\n\n    def _validate(self):\n        if len(self.params) > 0:\n            if self.params[0] .getID() == SAPParam.ResultCode:\n                if self.params[0].getValue() == 0x00:\n                    if len(self.params) == 2:\n                        return True\n                else:\n                    if len(self.params) == 1:\n                        return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n\n        if SAPMessage._basicCheck(self,  buf):\n            p = SAPParam_ResultCode()\n            r = p.deserialize(buf[4:])\n            if  r != -1:\n                self.addParam(p)\n                if buf[1] == 2:\n                    p = SAPParam_ResponseAPDU()\n                    r = p.deserialize(buf[4+r:])\n                    if r != -1:\n                        self.addParam(p)\n\n                return self._validate()\n\n        return False\n\nclass SAPMessage_TRANSFER_ATR_REQ(SAPMessage):\n    def __init__(self):\n        SAPMessage.__init__(self,\"TRANSFER_ATR_REQ\",  SAPMessage.TRANSFER_ATR_REQ)\n\nclass SAPMessage_TRANSFER_ATR_RESP(SAPMessage):\n    def __init__(self,  ResultCode = None,  ATR = None):\n        SAPMessage.__init__(self,\"TRANSFER_ATR_RESP\",  SAPMessage.TRANSFER_ATR_RESP)\n        if ResultCode is not None:\n            self.addParam(SAPParam_ResultCode(ResultCode))\n            if ATR is not None:\n                self.addParam(SAPParam_ATR(ATR))\n\n    def _validate(self):\n        if len(self.params) > 0:\n            if self.params[0] .getID() == SAPParam.ResultCode:\n                if self.params[0].getValue() == 0x00:\n                    if len(self.params) == 2:\n                        return True\n                else:\n                    if len(self.params) == 1:\n                        return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n\n        if SAPMessage._basicCheck(self,  buf):\n\n            p = SAPParam_ResultCode()\n            r = p.deserialize(buf[4:])\n\n            if  r != -1:\n\n                self.addParam(p)\n                if buf[1] == 2:\n\n                    p = SAPParam_ATR()\n                    r = p.deserialize(buf[4+r:])\n                    if r != -1:\n                        self.addParam(p)\n\n                return self._validate()\n\n        return False\n\nclass SAPMessage_POWER_SIM_OFF_REQ(SAPMessage):\n    def __init__(self):\n        SAPMessage.__init__(self,\"POWER_SIM_OFF_REQ\",  SAPMessage.POWER_SIM_OFF_REQ)\n\nclass SAPMessage_POWER_SIM_OFF_RESP(SAPMessage):\n    def __init__(self,  ResultCode = None):\n        SAPMessage.__init__(self,\"POWER_SIM_OFF_RESP\",  SAPMessage.POWER_SIM_OFF_RESP)\n        if ResultCode is not None:\n            self.addParam(SAPParam_ResultCode(ResultCode))\n\n    def _validate(self):\n        if len(self.params) == 1:\n            if self.params[0].getID() == SAPParam.ResultCode:\n                return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n        if SAPMessage._basicCheck(self,  buf):\n            p = SAPParam_ResultCode()\n            if p.deserialize(buf[4:]) == len(buf[4:]):\n                self.addParam(p)\n                return self._validate()\n\n        return False\n\nclass SAPMessage_POWER_SIM_ON_REQ(SAPMessage):\n    def __init__(self):\n        SAPMessage.__init__(self,\"POWER_SIM_ON_REQ\",  SAPMessage.POWER_SIM_ON_REQ)\n\nclass SAPMessage_POWER_SIM_ON_RESP(SAPMessage_POWER_SIM_OFF_RESP):\n    def __init__(self,  ResultCode = None):\n        SAPMessage.__init__(self,\"POWER_SIM_ON_RESP\",  SAPMessage.POWER_SIM_ON_RESP)\n        if ResultCode is not None:\n            self.addParam(SAPParam_ResultCode(ResultCode))\n\nclass SAPMessage_RESET_SIM_REQ(SAPMessage):\n    def __init__(self):\n        SAPMessage.__init__(self,\"RESET_SIM_REQ\",  SAPMessage.RESET_SIM_REQ)\n\nclass SAPMessage_RESET_SIM_RESP(SAPMessage_POWER_SIM_OFF_RESP):\n    def __init__(self,  ResultCode = None):\n        SAPMessage.__init__(self,\"RESET_SIM_RESP\",  SAPMessage.RESET_SIM_RESP)\n        if ResultCode is not None:\n            self.addParam(SAPParam_ResultCode(ResultCode))\n\nclass SAPMessage_STATUS_IND(SAPMessage):\n    def __init__(self,  StatusChange = None):\n        SAPMessage.__init__(self,\"STATUS_IND\",  SAPMessage.STATUS_IND)\n        if StatusChange is not None:\n            self.addParam(SAPParam_StatusChange(StatusChange))\n\n    def _validate(self):\n        if len(self.params) == 1:\n            if self.params[0].getID() == SAPParam.StatusChange:\n                return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n        if SAPMessage._basicCheck(self,  buf):\n            p = SAPParam_StatusChange()\n            if p.deserialize(buf[4:]) == len(buf[4:]):\n                self.addParam(p)\n                return self._validate()\n\n        return False\n\nclass SAPMessage_TRANSFER_CARD_READER_STATUS_REQ(SAPMessage):\n    def __init__(self):\n        SAPMessage.__init__(self,\"TRANSFER_CARD_READER_STATUS_REQ\",  SAPMessage.TRANSFER_CARD_READER_STATUS_REQ)\n\nclass SAPMessage_TRANSFER_CARD_READER_STATUS_RESP(SAPMessage):\n    def __init__(self,  ResultCode = None,  Status = None):\n        SAPMessage.__init__(self,\"TRANSFER_CARD_READER_STATUS_RESP\",  SAPMessage.TRANSFER_CARD_READER_STATUS_RESP)\n        if ResultCode is not None:\n            self.addParam(SAPParam_ResultCode(ResultCode))\n            if Status is not None:\n                self.addParam(SAPParam_CardReaderStatus(Status))\n\n    def _validate(self):\n        if len(self.params) > 0:\n            if self.params[0] .getID() == SAPParam.ResultCode:\n                if self.params[0].getValue() == 0x00:\n                    if len(self.params) == 2:\n                        return True\n                else:\n                    if len(self.params) == 1:\n                        return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n\n        if SAPMessage._basicCheck(self,  buf):\n            p = SAPParam_ResultCode()\n            r = p.deserialize(buf[4:])\n            if  r != -1:\n                self.addParam(p)\n                if buf[1] == 2:\n                    p = SAPParam_CardReaderStatus()\n                    r = p.deserialize(buf[4+r:])\n                    if r != -1:\n                        self.addParam(p)\n\n                return self._validate()\n\n        return False\n\nclass SAPMessage_ERROR_RESP(SAPMessage):\n    def __init__(self):\n        SAPMessage.__init__(self,\"ERROR_RESP\",  SAPMessage.ERROR_RESP)\n\n\nclass SAPMessage_SET_TRANSPORT_PROTOCOL_REQ(SAPMessage):\n    def __init__(self,  protocol = None):\n        SAPMessage.__init__(self,\"SET_TRANSPORT_PROTOCOL_REQ\",  SAPMessage.SET_TRANSPORT_PROTOCOL_REQ)\n        if protocol is not None:\n            self.addParam(SAPParam_TransportProtocol(protocol))\n\n    def _validate(self):\n        if len(self.params) == 1:\n            if self.params[0].getID() == SAPParam.TransportProtocol:\n                return True\n        return False\n\n    def deserialize(self,  buf):\n        self.buf = buf\n        self.params[:] = []\n        if SAPMessage._basicCheck(self,  buf):\n            p = SAPParam_TransportProtocol()\n            if p.deserialize(buf[4:]) == len(buf[4:]):\n                self.addParam(p)\n                return self._validate()\n\n        return False\n\nclass SAPMessage_SET_TRANSPORT_PROTOCOL_RESP(SAPMessage_POWER_SIM_OFF_RESP):\n    def __init__(self,  ResultCode = None):\n        SAPMessage.__init__(self,\"SET_TRANSPORT_PROTOCOL_RESP\",  SAPMessage.SET_TRANSPORT_PROTOCOL_RESP)\n        if ResultCode is not None:\n            self.addParam(SAPParam_ResultCode(ResultCode))\n\n\nclass SAPClient:\n\n    CONNECTED = 1\n    DISCONNECTED = 0\n\n    uuid = \"0000112D-0000-1000-8000-00805F9B34FB\"\n    bufsize = 1024\n    timeout = 20\n    state = DISCONNECTED\n\n    def __init__(self,  host = None,  port = None):\n        self.sock = None\n\n        if host is None or is_valid_address(host):\n            self.host = host\n        else:\n            raise BluetoothError (\"%s is not a valid BT address.\" % host)\n            self.host = None\n            return\n\n        if port is None:\n            self.__discover()\n        else:\n            self.port = port\n\n        self.__connectRFCOMM()\n\n    def __del__(self):\n        self.__disconnectRFCOMM()\n\n    def __disconnectRFCOMM(self):\n        if self.sock is not None:\n            self.sock.close()\n            self.state = self.DISCONNECTED\n\n    def __discover(self):\n        service_matches = find_service(self.uuid, self.host)\n\n        if len(service_matches) == 0:\n            raise BluetoothError (\"No SAP service found\")\n            return\n\n        first_match = service_matches[0]\n        self.port = first_match[\"port\"]\n        self.host = first_match[\"host\"]\n\n        print \"SAP Service found on %s(%s)\" % first_match[\"name\"] % self.host\n\n    def __connectRFCOMM(self):\n        self.sock=BluetoothSocket( RFCOMM )\n        self.sock.connect((self.host, self.port))\n        self.sock.settimeout(self.timeout)\n        self.state = self.CONNECTED\n\n    def __sendMsg(self, msg):\n        if isinstance(msg,  SAPMessage):\n            s = msg.serialize()\n            print \"\\tTX: \" + msg.getContent()\n            return self.sock.send(s.tostring())\n\n    def __rcvMsg(self,  msg):\n        if isinstance(msg,  SAPMessage):\n            print \"\\tRX Wait: %s(id = 0x%.2x)\" % (msg.name, msg.id)\n            data = self.sock.recv(self.bufsize)\n            if data:\n                if msg.deserialize(array('B',data)):\n                    print \"\\tRX: len(%d) %s\" % (len(data), msg.getContent())\n                    return msg\n                else:\n                    print \"msg: %s\" % array('B',data)\n                    raise BluetoothError (\"Message deserialization failed.\")\n            else:\n                raise BluetoothError (\"Timeout. No data received.\")\n\n    def connect(self):\n        self.__connectRFCOMM()\n\n    def disconnect(self):\n        self.__disconnectRFCOMM()\n\n    def isConnected(self):\n        return self.state\n\n    def proc_connect(self):\n        try:\n            self.__sendMsg(SAPMessage_CONNECT_REQ(self.bufsize))\n            params = self.__rcvMsg(SAPMessage_CONNECT_RESP()).getParams()\n\n            if params[0].getValue() in (0x00,  0x04):\n                pass\n            elif params[0].getValue() == 0x02:\n                self.bufsize = params[1].getValue()\n\n                self.__sendMsg(SAPMessage_CONNECT_REQ(self.bufsize))\n                params = self.__rcvMsg(SAPMessage_CONNECT_RESP()).getParams()\n\n                if params[0].getValue() not in (0x00,  0x04):\n                    return False\n            else:\n                return False\n\n            params = self.__rcvMsg(SAPMessage_STATUS_IND()).getParams()\n            if params[0].getValue() == 0x00:\n                return False\n            elif params[0].getValue() == 0x01:\n                \"\"\"OK, Card reset\"\"\"\n                return self.proc_transferATR()\n            elif params[0].getValue() == 0x02:\n                \"\"\"T0 not supported\"\"\"\n                if self.proc_transferATR():\n                    return self.proc_setTransportProtocol(1)\n                else:\n                    return False\n            else:\n                return False\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_disconnectByClient(self, timeout=0):\n        try:\n            self.__sendMsg(SAPMessage_DISCONNECT_REQ())\n            self.__rcvMsg(SAPMessage_DISCONNECT_RESP())\n            time.sleep(timeout) # let srv to close rfcomm\n            self.__disconnectRFCOMM()\n            return True\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_disconnectByServer(self, timeout=0):\n        try:\n            params = self.__rcvMsg(SAPMessage_DISCONNECT_IND()).getParams()\n\n            \"\"\"graceful\"\"\"\n            if params[0].getValue() == 0x00:\n                if not self.proc_transferAPDU():\n                    return False\n\n            return self.proc_disconnectByClient(timeout)\n\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_transferAPDU(self,  apdu = \"Sample APDU command\"):\n        try:\n            self.__sendMsg(SAPMessage_TRANSFER_APDU_REQ(apdu))\n            params = self.__rcvMsg(SAPMessage_TRANSFER_APDU_RESP()).getParams()\n            return True\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_transferATR(self):\n        try:\n            self.__sendMsg(SAPMessage_TRANSFER_ATR_REQ())\n            params = self.__rcvMsg(SAPMessage_TRANSFER_ATR_RESP()).getParams()\n            return True\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_powerSimOff(self):\n        try:\n            self.__sendMsg(SAPMessage_POWER_SIM_OFF_REQ())\n            params = self.__rcvMsg(SAPMessage_POWER_SIM_OFF_RESP()).getParams()\n            return True\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_powerSimOn(self):\n        try:\n            self.__sendMsg(SAPMessage_POWER_SIM_ON_REQ())\n            params = self.__rcvMsg(SAPMessage_POWER_SIM_ON_RESP()).getParams()\n            if params[0].getValue() == 0x00:\n                return self.proc_transferATR()\n\n            return True\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_resetSim(self):\n        try:\n            self.__sendMsg(SAPMessage_RESET_SIM_REQ())\n            params = self.__rcvMsg(SAPMessage_RESET_SIM_RESP()).getParams()\n            if params[0].getValue() == 0x00:\n                return self.proc_transferATR()\n\n            return True\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_reportStatus(self):\n        try:\n            params = self.__rcvMsg(SAPMessage_STATUS_IND()).getParams()\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_transferCardReaderStatus(self):\n        try:\n            self.__sendMsg(SAPMessage_TRANSFER_CARD_READER_STATUS_REQ())\n            params = self.__rcvMsg(SAPMessage_TRANSFER_CARD_READER_STATUS_RESP()).getParams()\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_errorResponse(self):\n        try:\n            \"\"\" send malformed message, no mandatory maxmsgsize parameter\"\"\"\n            self.__sendMsg(SAPMessage_CONNECT_REQ())\n\n            params = self.__rcvMsg(SAPMessage_ERROR_RESP()).getParams()\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\n    def proc_setTransportProtocol(self,  protocol = 0):\n        try:\n            self.__sendMsg(SAPMessage_SET_TRANSPORT_PROTOCOL_REQ(protocol))\n            params = self.__rcvMsg(SAPMessage_SET_TRANSPORT_PROTOCOL_RESP()).getParams()\n\n            if params[0].getValue() == 0x00:\n                params = self.__rcvMsg(SAPMessage_STATUS_IND()).getParams()\n                if params[0].getValue() in (0x01,  0x02):\n                    return self.proc_transferATR()\n                else:\n                    return True\n                    \"\"\"return False ???\"\"\"\n            elif params[0].getValue == 0x07:\n                \"\"\"not supported\"\"\"\n                return True\n                \"\"\"return False ???\"\"\"\n            else:\n                return False\n\n        except BluetoothError , e:\n            print \"Error. \" +str(e)\n            return False\n\nif __name__ == \"__main__\":\n    pass\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n##############################################################################\n#\n# Copyright (c) 2008 JAILLET Simon - CrysaLEAD - www.crysalead.fr\n#\n# WARNING: This program as such is intended to be used by professional\n# programmers who take the whole responsability of assessing all potential\n# consequences resulting from its eventual inadequacies and bugs\n# End users who are looking for a ready-to-use solution with commercial\n# garantees and support are strongly adviced to contract a Free Software\n# Service Company\n#\n# This program is Free Software; you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n#\n##############################################################################\n\nimport l10n_fr\nimport report\nimport wizard\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n\n","label":0}
{"content":"# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\n\ndef execute(filters=None):\n\tcolumns = get_columns()\n\tproj_details = get_project_details()\n\tpr_item_map = get_purchased_items_cost()\n\tse_item_map = get_issued_items_cost()\n\tdn_item_map = get_delivered_items_cost()\n\n\tdata = []\n\tfor project in proj_details:\n\t\tdata.append([project.name, pr_item_map.get(project.name, 0),\n\t\t\tse_item_map.get(project.name, 0), dn_item_map.get(project.name, 0),\n\t\t\tproject.project_name, project.status, project.company,\n\t\t\tproject.customer, project.estimated_costing, project.expected_start_date,\n\t\t\tproject.expected_end_date])\n\n\treturn columns, data\n\ndef get_columns():\n\treturn [_(\"Project Id\") + \":Link\/Project:140\", _(\"Cost of Purchased Items\") + \":Currency:160\",\n\t\t_(\"Cost of Issued Items\") + \":Currency:160\", _(\"Cost of Delivered Items\") + \":Currency:160\",\n\t\t_(\"Project Name\") + \"::120\", _(\"Project Status\") + \"::120\", _(\"Company\") + \":Link\/Company:100\",\n\t\t_(\"Customer\") + \":Link\/Customer:140\", _(\"Project Value\") + \":Currency:120\",\n\t\t_(\"Project Start Date\") + \":Date:120\", _(\"Completion Date\") + \":Date:120\"]\n\ndef get_project_details():\n\treturn frappe.db.sql(\"\"\" select name, project_name, status, company, customer, estimated_costing,\n\t\texpected_start_date, expected_end_date from tabProject where docstatus < 2\"\"\", as_dict=1)\n\ndef get_purchased_items_cost():\n\tpr_items = frappe.db.sql(\"\"\"select project_name, sum(base_net_amount) as amount\n\t\tfrom `tabPurchase Receipt Item` where ifnull(project_name, '') != ''\n\t\tand docstatus = 1 group by project_name\"\"\", as_dict=1)\n\n\tpr_item_map = {}\n\tfor item in pr_items:\n\t\tpr_item_map.setdefault(item.project_name, item.amount)\n\n\treturn pr_item_map\n\ndef get_issued_items_cost():\n\tse_items = frappe.db.sql(\"\"\"select se.project_name, sum(se_item.amount) as amount\n\t\tfrom `tabStock Entry` se, `tabStock Entry Detail` se_item\n\t\twhere se.name = se_item.parent and se.docstatus = 1 and ifnull(se_item.t_warehouse, '') = ''\n\t\tand ifnull(se.project_name, '') != '' group by se.project_name\"\"\", as_dict=1)\n\n\tse_item_map = {}\n\tfor item in se_items:\n\t\tse_item_map.setdefault(item.project_name, item.amount)\n\n\treturn se_item_map\n\ndef get_delivered_items_cost():\n\tdn_items = frappe.db.sql(\"\"\"select dn.project_name, sum(dn_item.base_net_amount) as amount\n\t\tfrom `tabDelivery Note` dn, `tabDelivery Note Item` dn_item\n\t\twhere dn.name = dn_item.parent and dn.docstatus = 1 and ifnull(dn.project_name, '') != ''\n\t\tgroup by dn.project_name\"\"\", as_dict=1)\n\n\tsi_items = frappe.db.sql(\"\"\"select si.project_name, sum(si_item.base_net_amount) as amount\n\t\tfrom `tabSales Invoice` si, `tabSales Invoice Item` si_item\n\t\twhere si.name = si_item.parent and si.docstatus = 1 and ifnull(si.update_stock, 0) = 1\n\t\tand ifnull(si.is_pos, 0) = 1 and ifnull(si.project_name, '') != ''\n\t\tgroup by si.project_name\"\"\", as_dict=1)\n\n\n\tdn_item_map = {}\n\tfor item in dn_items:\n\t\tdn_item_map.setdefault(item.project_name, item.amount)\n\n\tfor item in si_items:\n\t\tdn_item_map.setdefault(item.project_name, item.amount)\n\n\treturn dn_item_map\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"Documentation Flask Blueprint.\"\"\"\n\nimport os\n\nfrom flask import render_template, current_app, abort, url_for, Blueprint\nfrom flask.helpers import send_from_directory\nfrom werkzeug.utils import cached_property, import_string\n\nfrom sphinx.websupport import WebSupport\nfrom sphinx.websupport.errors import DocumentNotFoundError\n\nfrom invenio.base.globals import cfg\nfrom invenio.base.i18n import _\nfrom flask.ext.breadcrumbs import (default_breadcrumb_root,\n                                   register_breadcrumb,\n                                   current_breadcrumbs)\nfrom flask.ext.menu import register_menu\n\n\nclass DocsBlueprint(Blueprint):\n\n    \"\"\"Wrap blueprint with Sphinx ``WebSupport``.\"\"\"\n\n    @cached_property\n    def documentation_package(self):\n        \"\"\"Return documentation package.\"\"\"\n        try:\n            invenio_docs = import_string(cfg['DOCUMENTATION_PACKAGE'])\n        except ImportError:\n            import docs as invenio_docs\n        return invenio_docs\n\n    @cached_property\n    def support(self):\n        \"\"\"Return an instance of Sphinx ``WebSupport``.\"\"\"\n        builddir = os.path.abspath(os.path.join(\n            current_app.instance_path, 'docs'))\n        return WebSupport(\n            srcdir=self.documentation_package.__path__[0],\n            builddir=builddir,\n            staticroot=os.path.join(blueprint.url_prefix, 'static'),\n            docroot=blueprint.url_prefix\n        )\n\n    def send_static_file(self, filename):\n        \"\"\"Return static file.\"\"\"\n        try:\n            return super(self.__class__, self).send_static_file(filename)\n        except:\n            cache_timeout = self.get_send_file_max_age(filename)\n            return send_from_directory(\n                os.path.join(current_app.instance_path, \"docs\", \"static\"),\n                filename,\n                cache_timeout=cache_timeout)\n\n\nblueprint = DocsBlueprint('documentation', __name__,\n                          url_prefix=\"\/documentation\",\n                          template_folder='templates', static_folder='static')\n\ndefault_breadcrumb_root(blueprint, '.documentation')\n\n\n@blueprint.route('\/', strict_slashes=True)\n@blueprint.route('\/<path:docname>')\n@register_menu(blueprint, 'main.documentation', _('Help'), order=99)\n@register_breadcrumb(blueprint, '.', _('Help'))\ndef index(docname=None):\n    \"\"\"Render documentation page.\"\"\"\n    try:\n        document = blueprint.support.get_document(\n            docname or cfg[\"DOCUMENTATION_INDEX\"])\n    except DocumentNotFoundError:\n        abort(404)\n    additional_breadcrumbs = [{'text': document['title'],\n                               'url': url_for('.index', docname=docname)}]\n    return render_template(\n        'documentation\/index.html', document=document,\n        breadcrumbs=current_breadcrumbs + additional_breadcrumbs)\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n\"\"\"\nPelican Mathjax Markdown Extension\n==================================\nAn extension for the Python Markdown module that enables\nthe Pelican python blog to process mathjax. This extension\ngives Pelican the ability to use Mathjax as a \"first class\ncitizen\" of the blog\n\"\"\"\n\nimport markdown\n\nfrom markdown.util import etree\nfrom markdown.util import AtomicString\n\nclass PelicanMathJaxPattern(markdown.inlinepatterns.Pattern):\n    \"\"\"Inline markdown processing that matches mathjax\"\"\"\n\n    def __init__(self, pelican_mathjax_extension, tag, pattern):\n        super(PelicanMathJaxPattern,self).__init__(pattern)\n        self.math_tag_class = pelican_mathjax_extension.getConfig('math_tag_class')\n        self.pelican_mathjax_extension = pelican_mathjax_extension\n        self.tag = tag\n\n    def handleMatch(self, m):\n        node = markdown.util.etree.Element(self.tag)\n        node.set('class', self.math_tag_class)\n\n        prefix = '\\\\(' if m.group('prefix') == '$' else m.group('prefix')\n        suffix = '\\\\)' if m.group('suffix') == '$' else m.group('suffix')\n        node.text = markdown.util.AtomicString(prefix + m.group('math') + suffix)\n\n        # If mathjax was successfully matched, then JavaScript needs to be added\n        # for rendering. The boolean below indicates this\n        self.pelican_mathjax_extension.mathjax_needed = True\n        return node\n\nclass PelicanMathJaxCorrectDisplayMath(markdown.treeprocessors.Treeprocessor):\n    \"\"\"Corrects invalid html that results from a <div> being put inside\n    a <p> for displayed math\"\"\"\n\n    def __init__(self, pelican_mathjax_extension):\n        self.pelican_mathjax_extension = pelican_mathjax_extension\n\n    def correct_html(self, root, children, div_math, insert_idx, text):\n        \"\"\"Separates out <div class=\"math\"> from the parent tag <p>. Anything\n        in between is put into its own parent tag of <p>\"\"\"\n\n        current_idx = 0\n\n        for idx in div_math:\n            el = markdown.util.etree.Element('p')\n            el.text = text\n            el.extend(children[current_idx:idx])\n\n            # Test to ensure that empty <p> is not inserted  \n            if len(el) != 0 or (el.text and not el.text.isspace()):\n               root.insert(insert_idx, el)\n               insert_idx += 1\n\n            text = children[idx].tail\n            children[idx].tail = None\n            root.insert(insert_idx, children[idx])\n            insert_idx += 1\n            current_idx = idx+1\n\n        el = markdown.util.etree.Element('p')\n        el.text = text\n        el.extend(children[current_idx:])\n\n        if len(el) != 0 or (el.text and not el.text.isspace()):\n            root.insert(insert_idx, el)\n\n    def run(self, root):\n        \"\"\"Searches for <div class=\"math\"> that are children in <p> tags and corrects\n        the invalid HTML that results\"\"\"\n\n        math_tag_class = self.pelican_mathjax_extension.getConfig('math_tag_class')\n\n        for parent in root:\n            div_math = []\n            children = list(parent)\n\n            for div in parent.findall('div'):\n                if div.get('class') == math_tag_class:\n                    div_math.append(children.index(div))\n\n            # Do not process further if no displayed math has been found\n            if not div_math:\n                continue\n\n            insert_idx = list(root).index(parent)\n            self.correct_html(root, children, div_math, insert_idx, parent.text) \n            root.remove(parent)  # Parent must be removed last for correct insertion index\n\n        return root\n\nclass PelicanMathJaxAddJavaScript(markdown.treeprocessors.Treeprocessor):\n    \"\"\"Tree Processor for adding Mathjax JavaScript to the blog\"\"\"\n\n    def __init__(self, pelican_mathjax_extension):\n        self.pelican_mathjax_extension = pelican_mathjax_extension\n\n    def run(self, root):\n        # If no mathjax was present, then exit\n        if (not self.pelican_mathjax_extension.mathjax_needed):\n            return root\n\n        # Add the mathjax script to the html document\n        mathjax_script = etree.Element('script')\n        mathjax_script.set('type','text\/javascript')\n        mathjax_script.text = AtomicString(self.pelican_mathjax_extension.getConfig('mathjax_script'))\n        root.append(mathjax_script)\n\n        # Reset the boolean switch to false so that script is only added\n        # to other pages if needed\n        self.pelican_mathjax_extension.mathjax_needed = False\n        return root\n\nclass PelicanMathJaxExtension(markdown.Extension):\n    \"\"\"A markdown extension enabling mathjax processing in Markdown for Pelican\"\"\"\n    def __init__(self, config):\n\n        try:\n            # Needed for markdown versions >= 2.5\n            self.config['mathjax_script'] = ['', 'Mathjax JavaScript script']\n            self.config['math_tag_class'] = ['math', 'The class of the tag in which mathematics is wrapped']\n            self.config['auto_insert'] = [True, 'Determines if mathjax script is automatically inserted into content']\n            super(PelicanMathJaxExtension,self).__init__(**config)\n        except AttributeError:\n            # Markdown versions < 2.5\n            config['mathjax_script'] = [config['mathjax_script'], 'Mathjax JavaScript script']\n            config['math_tag_class'] = [config['math_tag_class'], 'The class of the tag in which mathematic is wrapped']\n            config['auto_insert'] = [config['auto_insert'], 'Determines if mathjax script is automatically inserted into content']\n            super(PelicanMathJaxExtension,self).__init__(config)\n\n        # Used as a flag to determine if javascript\n        # needs to be injected into a document\n        self.mathjax_needed = False\n\n    def extendMarkdown(self, md, md_globals):\n        # Regex to detect mathjax\n        mathjax_inline_regex = r'(?P<prefix>\\$)(?P<math>.+?)(?P<suffix>(?<!\\s)\\2)'\n        mathjax_display_regex = r'(?P<prefix>\\$\\$|\\\\begin\\{(.+?)\\})(?P<math>.+?)(?P<suffix>\\2|\\\\end\\{\\3\\})'\n\n        # Process mathjax before escapes are processed since escape processing will\n        # intefer with mathjax. The order in which the displayed and inlined math\n        # is registered below matters\n        md.inlinePatterns.add('mathjax_displayed', PelicanMathJaxPattern(self, 'div', mathjax_display_regex), '<escape')\n        md.inlinePatterns.add('mathjax_inlined', PelicanMathJaxPattern(self, 'span', mathjax_inline_regex), '<escape')\n\n        # Correct the invalid HTML that results from teh displayed math (<div> tag within a <p> tag) \n        md.treeprocessors.add('mathjax_correctdisplayedmath', PelicanMathJaxCorrectDisplayMath(self), '>inline')\n\n        # If necessary, add the JavaScript Mathjax library to the document. This must\n        # be last in the ordered dict (hence it is given the position '_end')\n        if self.getConfig('auto_insert'):\n            md.treeprocessors.add('mathjax_addjavascript', PelicanMathJaxAddJavaScript(self), '_end')\n","label":0}
{"content":"#!\/usr\/bin\/env python2\n# -*- coding: utf-8 -*-\n#\n# GuessIt - A library for guessing information from filenames\n# Copyright (c) 2012 Nicolas Wack <wackou@gmail.com>\n#\n# GuessIt is free software; you can redistribute it and\/or modify it under\n# the terms of the Lesser GNU General Public License as published by\n# the Free Software Foundation; either version 3 of the License, or\n# (at your option) any later version.\n#\n# GuessIt is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# Lesser GNU General Public License for more details.\n#\n# You should have received a copy of the Lesser GNU General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#\n\nfrom __future__ import unicode_literals\nfrom guessit import Guess\nfrom guessit.transfo import SingleNodeGuesser\nfrom guessit.patterns import weak_episode_rexps\nimport re\nimport logging\n\nlog = logging.getLogger(__name__)\n\n\ndef guess_weak_episodes_rexps(string, node):\n    if 'episodeNumber' in node.root.info:\n        return None, None\n\n    for rexp, span_adjust in weak_episode_rexps:\n        match = re.search(rexp, string, re.IGNORECASE)\n        if match:\n            metadata = match.groupdict()\n            span = (match.start() + span_adjust[0],\n                    match.end() + span_adjust[1])\n\n            epnum = int(metadata['episodeNumber'])\n            if epnum > 100:\n                season, epnum = epnum \/\/ 100, epnum % 100\n                # episodes which have a season > 25 are most likely errors\n                # (Simpsons is at 23!)\n                if season > 25:\n                    continue\n                return Guess({ 'season': season,\n                               'episodeNumber': epnum },\n                             confidence=0.6), span\n            else:\n                return Guess(metadata, confidence=0.3), span\n\n    return None, None\n\n\nguess_weak_episodes_rexps.use_node = True\n\n\ndef process(mtree):\n    SingleNodeGuesser(guess_weak_episodes_rexps, 0.6, log).process(mtree)\n","label":0}
{"content":"# coding: utf-8\nfrom __future__ import unicode_literals\n\nimport os.path\nimport re\n\nfrom .common import InfoExtractor\nfrom ..utils import (\n    ExtractorError,\n    remove_start,\n    sanitized_Request,\n    urlencode_postdata,\n)\n\n\nclass MonikerIE(InfoExtractor):\n    IE_DESC = 'allmyvideos.net and vidspot.net'\n    _VALID_URL = r'https?:\/\/(?:www\\.)?(?:allmyvideos|vidspot)\\.net\/(?:(?:2|v)\/v-)?(?P<id>[a-zA-Z0-9_-]+)'\n\n    _TESTS = [{\n        'url': 'http:\/\/allmyvideos.net\/jih3nce3x6wn',\n        'md5': '710883dee1bfc370ecf9fa6a89307c88',\n        'info_dict': {\n            'id': 'jih3nce3x6wn',\n            'ext': 'mp4',\n            'title': 'youtube-dl test video',\n        },\n    }, {\n        'url': 'http:\/\/allmyvideos.net\/embed-jih3nce3x6wn',\n        'md5': '710883dee1bfc370ecf9fa6a89307c88',\n        'info_dict': {\n            'id': 'jih3nce3x6wn',\n            'ext': 'mp4',\n            'title': 'youtube-dl test video',\n        },\n    }, {\n        'url': 'http:\/\/vidspot.net\/l2ngsmhs8ci5',\n        'md5': '710883dee1bfc370ecf9fa6a89307c88',\n        'info_dict': {\n            'id': 'l2ngsmhs8ci5',\n            'ext': 'mp4',\n            'title': 'youtube-dl test video',\n        },\n    }, {\n        'url': 'https:\/\/www.vidspot.net\/l2ngsmhs8ci5',\n        'only_matching': True,\n    }, {\n        'url': 'http:\/\/vidspot.net\/2\/v-ywDf99',\n        'md5': '5f8254ce12df30479428b0152fb8e7ba',\n        'info_dict': {\n            'id': 'ywDf99',\n            'ext': 'mp4',\n            'title': 'IL FAIT LE MALIN EN PORSHE CAYENNE ( mais pas pour longtemps)',\n            'description': 'IL FAIT LE MALIN EN PORSHE CAYENNE.',\n        },\n    }, {\n        'url': 'http:\/\/allmyvideos.net\/v\/v-HXZm5t',\n        'only_matching': True,\n    }]\n\n    def _real_extract(self, url):\n        orig_video_id = self._match_id(url)\n        video_id = remove_start(orig_video_id, 'embed-')\n        url = url.replace(orig_video_id, video_id)\n        assert re.match(self._VALID_URL, url) is not None\n        orig_webpage = self._download_webpage(url, video_id)\n\n        if '>File Not Found<' in orig_webpage:\n            raise ExtractorError('Video %s does not exist' % video_id, expected=True)\n\n        error = self._search_regex(\n            r'class=\"err\">([^<]+)<', orig_webpage, 'error', default=None)\n        if error:\n            raise ExtractorError(\n                '%s returned error: %s' % (self.IE_NAME, error), expected=True)\n\n        builtin_url = self._search_regex(\n            r'<iframe[^>]+src=([\"\\'])(?P<url>.+?\/builtin-.+?)\\1',\n            orig_webpage, 'builtin URL', default=None, group='url')\n\n        if builtin_url:\n            req = sanitized_Request(builtin_url)\n            req.add_header('Referer', url)\n            webpage = self._download_webpage(req, video_id, 'Downloading builtin page')\n            title = self._og_search_title(orig_webpage).strip()\n            description = self._og_search_description(orig_webpage).strip()\n        else:\n            fields = re.findall(r'type=\"hidden\" name=\"(.+?)\"\\s* value=\"?(.+?)\">', orig_webpage)\n            data = dict(fields)\n\n            post = urlencode_postdata(data)\n            headers = {\n                b'Content-Type': b'application\/x-www-form-urlencoded',\n            }\n            req = sanitized_Request(url, post, headers)\n            webpage = self._download_webpage(\n                req, video_id, note='Downloading video page ...')\n\n            title = os.path.splitext(data['fname'])[0]\n            description = None\n\n        # Could be several links with different quality\n        links = re.findall(r'\"file\" : \"?(.+?)\",', webpage)\n        # Assume the links are ordered in quality\n        formats = [{\n            'url': l,\n            'quality': i,\n        } for i, l in enumerate(links)]\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'title': title,\n            'description': description,\n            'formats': formats,\n        }\n","label":0}
{"content":"#!\/usr\/bin\/env python\n# Copyright 2009 Simon Arlott\n#\n# This program is free software; you can redistribute it and\/or modify it\n# under the terms of the GNU General Public License as published by the Free\n# Software Foundation; either version 2 of the License, or (at your option)\n# any later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n# more details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 59\n# Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# Usage: cxacru-cf.py < cxacru-cf.bin\n# Output: values string suitable for the sysfs adsl_config attribute\n#\n# Warning: cxacru-cf.bin with MD5 hash cdbac2689969d5ed5d4850f117702110\n# contains mis-aligned values which will stop the modem from being able\n# to make a connection. If the first and last two bytes are removed then\n# the values become valid, but the modulation will be forced to ANSI\n# T1.413 only which may not be appropriate.\n#\n# The original binary format is a packed list of le32 values.\n\nimport sys\nimport struct\n\ni = 0\nwhile True:\n\tbuf = sys.stdin.read(4)\n\n\tif len(buf) == 0:\n\t\tbreak\n\telif len(buf) != 4:\n\t\tsys.stdout.write(\"\\n\")\n\t\tsys.stderr.write(\"Error: read {0} not 4 bytes\\n\".format(len(buf)))\n\t\tsys.exit(1)\n\n\tif i > 0:\n\t\tsys.stdout.write(\" \")\n\tsys.stdout.write(\"{0:x}={1}\".format(i, struct.unpack(\"<I\", buf)[0]))\n\ti += 1\n\nsys.stdout.write(\"\\n\")\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n# taken from http:\/\/code.activestate.com\/recipes\/252524-length-limited-o1-lru-cache-implementation\/\nimport threading\nfrom func import synchronized\n\n__all__ = ['LRU']\n\nclass LRUNode(object):\n    __slots__ = ['prev', 'next', 'me']\n    def __init__(self, prev, me):\n        self.prev = prev\n        self.me = me\n        self.next = None\n\nclass LRU(object):\n    \"\"\"\n    Implementation of a length-limited O(1) LRU queue.\n    Built for and used by PyPE:\n    http:\/\/pype.sourceforge.net\n    Copyright 2003 Josiah Carlson.\n    \"\"\"\n    def __init__(self, count, pairs=[]):\n        self._lock = threading.RLock()\n        self.count = max(count, 1)\n        self.d = {}\n        self.first = None\n        self.last = None\n        for key, value in pairs:\n            self[key] = value\n\n    @synchronized()\n    def __contains__(self, obj):\n        return obj in self.d\n\n    @synchronized()\n    def __getitem__(self, obj):\n        a = self.d[obj].me\n        self[a[0]] = a[1]\n        return a[1]\n\n    @synchronized()\n    def __setitem__(self, obj, val):\n        if obj in self.d:\n            del self[obj]\n        nobj = LRUNode(self.last, (obj, val))\n        if self.first is None:\n            self.first = nobj\n        if self.last:\n            self.last.next = nobj\n        self.last = nobj\n        self.d[obj] = nobj\n        if len(self.d) > self.count:\n            if self.first == self.last:\n                self.first = None\n                self.last = None\n                return\n            a = self.first\n            a.next.prev = None\n            self.first = a.next\n            a.next = None\n            del self.d[a.me[0]]\n            del a\n\n    @synchronized()\n    def __delitem__(self, obj):\n        nobj = self.d[obj]\n        if nobj.prev:\n            nobj.prev.next = nobj.next\n        else:\n            self.first = nobj.next\n        if nobj.next:\n            nobj.next.prev = nobj.prev\n        else:\n            self.last = nobj.prev\n        del self.d[obj]\n\n    @synchronized()\n    def __iter__(self):\n        cur = self.first\n        while cur is not None:\n            cur2 = cur.next\n            yield cur.me[1]\n            cur = cur2\n\n    @synchronized()\n    def __len__(self):\n        return len(self.d)\n\n    @synchronized()\n    def iteritems(self):\n        cur = self.first\n        while cur is not None:\n            cur2 = cur.next\n            yield cur.me\n            cur = cur2\n\n    @synchronized()\n    def iterkeys(self):\n        return iter(self.d)\n\n    @synchronized()\n    def itervalues(self):\n        for i,j in self.iteritems():\n            yield j\n\n    @synchronized()\n    def keys(self):\n        return self.d.keys()\n\n    @synchronized()\n    def pop(self,key):\n        v=self[key]\n        del self[key]\n        return v\n\n    @synchronized()\n    def clear(self):\n        self.d = {}\n        self.first = None\n        self.last = None\n\n    @synchronized()\n    def clear_prefix(self, prefix):\n        \"\"\" Remove from `self` all the items with the given `prefix`. \"\"\"\n        n = len(prefix)\n        for key in self.keys():\n            if key[:n] == prefix:\n                del self[key]\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n# Generated by Django 1.10.8 on 2019-06-19 18:23\nfrom __future__ import unicode_literals\n\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n        ('organization-projects', '0084_auto_20190304_2221'),\n    ]\n\n    operations = [\n        migrations.AlterModelOptions(\n            name='projectpage',\n            options={'permissions': (('user_edit', 'Mezzo - User can edit its own content'), ('user_delete', 'Mezzo - User can delete its own content'), ('team_edit', \"Mezzo - User can edit his team's content\"), ('team_delete', \"Mezzo - User can delete his team's content\"))},\n        ),\n        migrations.AddField(\n            model_name='projectpage',\n            name='user',\n            field=models.ForeignKey(default=4, on_delete=django.db.models.deletion.CASCADE, related_name='projectpages', to=settings.AUTH_USER_MODEL, verbose_name='Author'),\n            preserve_default=False,\n        ),\n    ]\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n##\n## This file is part of Invenio.\n## Copyright (C) 2011, 2012, 2013 CERN.\n##\n## Invenio is free software; you can redistribute it and\/or\n## modify it under the terms of the GNU General Public License as\n## published by the Free Software Foundation; either version 2 of the\n## License, or (at your option) any later version.\n##\n## Invenio is distributed in the hope that it will be useful, but\n## WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n## General Public License for more details.\n##\n## You should have received a copy of the GNU General Public License\n## along with Invenio; if not, write to the Free Software Foundation, Inc.,\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\"\"\"Unit Tests for BibAuthority\"\"\"\n\nfrom invenio.testsuite import InvenioTestCase\nfrom invenio.testsuite import make_test_suite, run_test_suite\n\n\nclass TestBibAuthorityEngine(InvenioTestCase):\n    \"\"\"Unit tests for bibauthority_engine\"\"\"\n\n    def test_split_name_parts(self):\n        \"\"\"bibauthority - test get_type_from_authority_id\"\"\"\n        from invenio.legacy.bibauthority.config import CFG_BIBAUTHORITY_PREFIX_SEP\n        from invenio.legacy.bibauthority.engine import get_type_from_control_no\n        prefix = \"JOURNAL\"\n        control_no = \"(CERN)abcd1234\" # must start with a '('\n        self.assertEqual(get_type_from_control_no(\n                            prefix + CFG_BIBAUTHORITY_PREFIX_SEP + control_no),\n                         prefix)\n\nTEST_SUITE = make_test_suite(TestBibAuthorityEngine)\n\nif __name__ == \"__main__\":\n    run_test_suite(TEST_SUITE)\n","label":0}
{"content":"#!\/usr\/bin\/env python\n#\n# Copyright 2011 Free Software Foundation, Inc.\n# \n# This file is part of GNU Radio\n# \n# GNU Radio is free software; you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3, or (at your option)\n# any later version.\n# \n# GNU Radio is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with GNU Radio; see the file COPYING.  If not, write to\n# the Free Software Foundation, Inc., 51 Franklin Street,\n# Boston, MA 02110-1301, USA.\n# \n\nfrom gnuradio import gr, gr_unittest, digital\n\nclass test_digital(gr_unittest.TestCase):\n\n    def setUp(self):\n        self.tb = gr.top_block()\n\n    def tearDown(self):\n        self.tb = None\n\nif __name__ == '__main__':\n    gr_unittest.run(test_digital, \"test_digital.xml\")\n","label":0}
{"content":"#!\/usr\/bin\/python\n# -*- coding: utf-8 -*-\n\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nANSIBLE_METADATA = {'status': ['preview'],\n                    'supported_by': 'community',\n                    'version': '1.0'}\n\nDOCUMENTATION = \"\"\"\n---\nmodule: vertica_facts\nversion_added: '2.0'\nshort_description: Gathers Vertica database facts.\ndescription:\n  - Gathers Vertica database facts.\noptions:\n  cluster:\n    description:\n      - Name of the cluster running the schema.\n    required: false\n    default: localhost\n  port:\n    description:\n      Database port to connect to.\n    required: false\n    default: 5433\n  db:\n    description:\n      - Name of the database running the schema.\n    required: false\n    default: null\n  login_user:\n    description:\n      - The username used to authenticate with.\n    required: false\n    default: dbadmin\n  login_password:\n    description:\n      - The password used to authenticate with.\n    required: false\n    default: null\nnotes:\n  - The default authentication assumes that you are either logging in as or sudo'ing\n    to the C(dbadmin) account on the host.\n  - This module uses C(pyodbc), a Python ODBC database adapter. You must ensure\n    that C(unixODBC) and C(pyodbc) is installed on the host and properly configured.\n  - Configuring C(unixODBC) for Vertica requires C(Driver = \/opt\/vertica\/lib64\/libverticaodbc.so)\n    to be added to the C(Vertica) section of either C(\/etc\/odbcinst.ini) or C($HOME\/.odbcinst.ini)\n    and both C(ErrorMessagesPath = \/opt\/vertica\/lib64) and C(DriverManagerEncoding = UTF-16)\n    to be added to the C(Driver) section of either C(\/etc\/vertica.ini) or C($HOME\/.vertica.ini).\nrequirements: [ 'unixODBC', 'pyodbc' ]\nauthor: \"Dariusz Owczarek (@dareko)\"\n\"\"\"\n\nEXAMPLES = \"\"\"\n- name: gathering vertica facts\n  vertica_facts: db=db_name\n\"\"\"\n\ntry:\n    import pyodbc\nexcept ImportError:\n    pyodbc_found = False\nelse:\n    pyodbc_found = True\n\nfrom ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.pycompat24 import get_exception\n\n\nclass NotSupportedError(Exception):\n    pass\n\n# module specific functions\n\ndef get_schema_facts(cursor, schema=''):\n    facts = {}\n    cursor.execute(\"\"\"\n        select schema_name, schema_owner, create_time\n        from schemata\n        where not is_system_schema and schema_name not in ('public')\n        and (? = '' or schema_name ilike ?)\n    \"\"\", schema, schema)\n    while True:\n        rows = cursor.fetchmany(100)\n        if not rows:\n            break\n        for row in rows:\n            facts[row.schema_name.lower()] = {\n                'name': row.schema_name,\n                'owner': row.schema_owner,\n                'create_time': str(row.create_time),\n                'usage_roles': [],\n                'create_roles': []}\n    cursor.execute(\"\"\"\n        select g.object_name as schema_name, r.name as role_name,\n        lower(g.privileges_description) privileges_description\n        from roles r join grants g\n        on g.grantee = r.name and g.object_type='SCHEMA'\n        and g.privileges_description like '%USAGE%'\n        and g.grantee not in ('public', 'dbadmin')\n        and (? = '' or g.object_name ilike ?)\n    \"\"\", schema, schema)\n    while True:\n        rows = cursor.fetchmany(100)\n        if not rows:\n            break\n        for row in rows:\n            schema_key = row.schema_name.lower()\n            if 'create' in row.privileges_description:\n                facts[schema_key]['create_roles'].append(row.role_name)\n            else:\n                facts[schema_key]['usage_roles'].append(row.role_name)\n    return facts\n\ndef get_user_facts(cursor, user=''):\n    facts = {}\n    cursor.execute(\"\"\"\n        select u.user_name, u.is_locked, u.lock_time,\n        p.password, p.acctexpired as is_expired,\n        u.profile_name, u.resource_pool,\n        u.all_roles, u.default_roles\n        from users u join password_auditor p on p.user_id = u.user_id\n        where not u.is_super_user\n        and (? = '' or u.user_name ilike ?)\n     \"\"\", user, user)\n    while True:\n        rows = cursor.fetchmany(100)\n        if not rows:\n            break\n        for row in rows:\n            user_key = row.user_name.lower()\n            facts[user_key] = {\n                'name': row.user_name,\n                'locked': str(row.is_locked),\n                'password': row.password,\n                'expired': str(row.is_expired),\n                'profile': row.profile_name,\n                'resource_pool': row.resource_pool,\n                'roles': [],\n                'default_roles': []}\n            if row.is_locked:\n                facts[user_key]['locked_time'] = str(row.lock_time)\n            if row.all_roles:\n                facts[user_key]['roles'] = row.all_roles.replace(' ', '').split(',')\n            if row.default_roles:\n                facts[user_key]['default_roles'] = row.default_roles.replace(' ', '').split(',')\n    return facts\n\ndef get_role_facts(cursor, role=''):\n    facts = {}\n    cursor.execute(\"\"\"\n        select r.name, r.assigned_roles\n        from roles r\n        where (? = '' or r.name ilike ?)\n    \"\"\", role, role)\n    while True:\n        rows = cursor.fetchmany(100)\n        if not rows:\n            break\n        for row in rows:\n            role_key = row.name.lower()\n            facts[role_key] = {\n                'name': row.name,\n                'assigned_roles': []}\n            if row.assigned_roles:\n                facts[role_key]['assigned_roles'] = row.assigned_roles.replace(' ', '').split(',')\n    return facts\n\ndef get_configuration_facts(cursor, parameter=''):\n    facts = {}\n    cursor.execute(\"\"\"\n        select c.parameter_name, c.current_value, c.default_value\n        from configuration_parameters c\n        where c.node_name = 'ALL'\n        and (? = '' or c.parameter_name ilike ?)\n    \"\"\", parameter, parameter)\n    while True:\n        rows = cursor.fetchmany(100)\n        if not rows:\n            break\n        for row in rows:\n            facts[row.parameter_name.lower()] = {\n                'parameter_name': row.parameter_name,\n                'current_value': row.current_value,\n                'default_value': row.default_value}\n    return facts\n\ndef get_node_facts(cursor, schema=''):\n    facts = {}\n    cursor.execute(\"\"\"\n        select node_name, node_address, export_address, node_state, node_type,\n            catalog_path\n        from nodes\n    \"\"\")\n    while True:\n        rows = cursor.fetchmany(100)\n        if not rows:\n            break\n        for row in rows:\n            facts[row.node_address] = {\n                'node_name': row.node_name,\n                'export_address': row.export_address,\n                'node_state': row.node_state,\n                'node_type': row.node_type,\n                'catalog_path': row.catalog_path}\n    return facts\n\n# module logic\n\ndef main():\n\n    module = AnsibleModule(\n        argument_spec=dict(\n            cluster=dict(default='localhost'),\n            port=dict(default='5433'),\n            db=dict(default=None),\n            login_user=dict(default='dbadmin'),\n            login_password=dict(default=None),\n        ), supports_check_mode = True)\n\n    if not pyodbc_found:\n        module.fail_json(msg=\"The python pyodbc module is required.\")\n\n    db = ''\n    if module.params['db']:\n        db = module.params['db']\n\n    try:\n        dsn = (\n            \"Driver=Vertica;\"\n            \"Server=%s;\"\n            \"Port=%s;\"\n            \"Database=%s;\"\n            \"User=%s;\"\n            \"Password=%s;\"\n            \"ConnectionLoadBalance=%s\"\n            ) % (module.params['cluster'], module.params['port'], db,\n                module.params['login_user'], module.params['login_password'], 'true')\n        db_conn = pyodbc.connect(dsn, autocommit=True)\n        cursor = db_conn.cursor()\n    except Exception:\n        e = get_exception()\n        module.fail_json(msg=\"Unable to connect to database: %s.\" % str(e))\n\n    try:\n        schema_facts = get_schema_facts(cursor)\n        user_facts = get_user_facts(cursor)\n        role_facts = get_role_facts(cursor)\n        configuration_facts = get_configuration_facts(cursor)\n        node_facts = get_node_facts(cursor)\n        module.exit_json(changed=False,\n            ansible_facts={'vertica_schemas': schema_facts,\n                           'vertica_users': user_facts,\n                           'vertica_roles': role_facts,\n                           'vertica_configuration': configuration_facts,\n                           'vertica_nodes': node_facts})\n    except NotSupportedError:\n        e = get_exception()\n        module.fail_json(msg=str(e))\n    except SystemExit:\n        # avoid catching this on python 2.4\n        raise\n    except Exception:\n        e = get_exception()\n        module.fail_json(msg=e)\n\n\nif __name__ == '__main__':\n    main()\n","label":0}
{"content":"\"\"\"\nViews for groups info API\n\"\"\"\n\nfrom rest_framework import generics, status, mixins\nfrom rest_framework.response import Response\nfrom django.conf import settings\nimport facebook\n\nfrom ...utils import mobile_view\nfrom . import serializers\n\n\n@mobile_view()\nclass Groups(generics.CreateAPIView, mixins.DestroyModelMixin):\n    \"\"\"\n    **Use Case**\n\n        An API to Create or Delete course groups.\n\n        Note: The Delete is not invoked from the current version of the app\n        and is used only for testing with facebook dependencies.\n\n    **Creation Example request**:\n\n        POST \/api\/mobile\/v0.5\/social\/facebook\/groups\/\n\n        Parameters: name : string,\n                    description : string,\n                    privacy : open\/closed\n\n    **Creation Response Values**\n\n        {\"id\": group_id}\n\n    **Deletion Example request**:\n\n        DELETE \/api\/mobile\/v0.5\/social\/facebook\/groups\/<group_id>\n\n    **Deletion Response Values**\n\n        {\"success\" : \"true\"}\n\n    \"\"\"\n    serializer_class = serializers.GroupSerializer\n\n    def create(self, request, *args, **kwargs):\n        serializer = self.get_serializer(data=request.DATA, files=request.FILES)\n        if not serializer.is_valid():\n            return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)\n        try:\n            app_groups_response = facebook_graph_api().request(\n                settings.FACEBOOK_API_VERSION + '\/' + settings.FACEBOOK_APP_ID + \"\/groups\",\n                post_args=request.POST.dict()\n            )\n            return Response(app_groups_response)\n        except facebook.GraphAPIError, ex:\n            return Response({'error': ex.result['error']['message']}, status=status.HTTP_400_BAD_REQUEST)\n\n    def delete(self, request, *args, **kwargs):  # pylint: disable=unused-argument\n        \"\"\"\n        Deletes the course group.\n        \"\"\"\n        try:\n            return Response(\n                facebook_graph_api().request(\n                    settings.FACEBOOK_API_VERSION + '\/' + settings.FACEBOOK_APP_ID + \"\/groups\/\" + kwargs['group_id'],\n                    post_args={'method': 'delete'}\n                )\n            )\n        except facebook.GraphAPIError, ex:\n            return Response({'error': ex.result['error']['message']}, status=status.HTTP_400_BAD_REQUEST)\n\n\n@mobile_view()\nclass GroupsMembers(generics.CreateAPIView, mixins.DestroyModelMixin):\n    \"\"\"\n    **Use Case**\n\n        An API to Invite and Remove members to a group\n\n        Note: The Remove is not invoked from the current version\n        of the app and is used only for testing with facebook dependencies.\n\n    **Invite Example request**:\n\n        POST \/api\/mobile\/v0.5\/social\/facebook\/groups\/<group_id>\/member\/\n\n        Parameters: members : int,int,int...\n\n\n    **Invite Response Values**\n\n        {\"member_id\" : success\/error_message}\n        A response with each member_id and whether or not the member was added successfully.\n        If the member was not added successfully the Facebook error message is provided.\n\n    **Remove Example request**:\n\n        DELETE \/api\/mobile\/v0.5\/social\/facebook\/groups\/<group_id>\/member\/<member_id>\n\n    **Remove Response Values**\n\n        {\"success\" : \"true\"}\n    \"\"\"\n    serializer_class = serializers.GroupsMembersSerializer\n\n    def create(self, request, *args, **kwargs):\n        serializer = self.get_serializer(data=request.DATA, files=request.FILES)\n        if not serializer.is_valid():\n            return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)\n        graph = facebook_graph_api()\n        url = settings.FACEBOOK_API_VERSION + '\/' + kwargs['group_id'] + \"\/members\"\n        member_ids = serializer.object['member_ids'].split(',')\n        response = {}\n        for member_id in member_ids:\n            try:\n                if 'success' in graph.request(url, post_args={'member': member_id}):\n                    response[member_id] = 'success'\n            except facebook.GraphAPIError, ex:\n                response[member_id] = ex.result['error']['message']\n        return Response(response, status=status.HTTP_200_OK)\n\n    def delete(self, request, *args, **kwargs):  # pylint: disable=unused-argument\n        \"\"\"\n        Deletes the member from the course group.\n        \"\"\"\n        try:\n            return Response(\n                facebook_graph_api().request(\n                    settings.FACEBOOK_API_VERSION + '\/' + kwargs['group_id'] + \"\/members\",\n                    post_args={'method': 'delete', 'member': kwargs['member_id']}\n                )\n            )\n        except facebook.GraphAPIError, ex:\n            return Response({'error': ex.result['error']['message']}, status=status.HTTP_400_BAD_REQUEST)\n\n\ndef facebook_graph_api():\n    \"\"\"\n    Returns the result from calling Facebook's Graph API with the app's access token.\n    \"\"\"\n    return facebook.GraphAPI(facebook.get_app_access_token(settings.FACEBOOK_APP_ID, settings.FACEBOOK_APP_SECRET))\n","label":0}
{"content":"# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom __future__ import division\n\nfrom time import sleep\nfrom django.core.urlresolvers import reverse\nfrom django.core import mail\n\nfrom registration.models import RegistrationProfile\n\nfrom treemap.tests.ui import UITestCase\nfrom treemap.tests import make_user, create_mock_system_user\n\n\nclass LoginLogoutTest(UITestCase):\n    def setUp(self):\n        create_mock_system_user()\n\n        super(LoginLogoutTest, self).setUp()\n        self.user = make_user(username='username', password='password')\n        self.profile = RegistrationProfile.objects.create_profile(self.user)\n\n    def test_invalid_login(self):\n        self.browse_to_url(reverse('auth_login'))\n\n        login_url = self.driver.current_url\n\n        self.process_login_form(\n            self.user.username, 'passwordinvalid')\n\n        # There should be an error list with at least one element\n        self.wait_until_present('.errorlist li')\n\n        # We should be on the same page\n        self.assertEqual(login_url, self.driver.current_url)\n\n    def test_valid_login(self):\n        self.browse_to_url(reverse('auth_login'))\n\n        login_url = self.driver.current_url\n\n        self.process_login_form(self.user.username, 'password')\n\n        email_element = self.wait_until_present(\n            '[data-field=\"user.email\"][data-class=\"display\"]')\n\n        # We should not be on the same page\n        self.assertNotEqual(login_url, self.driver.current_url)\n\n        # We should expect our username in the url\n        self.assertIn(self.user.username, self.driver.current_url)\n\n        value = email_element.get_attribute('data-value')\n        self.assertEqual(self.user.email, value)\n\n        sleep(1)  # prevent hang\n\n\nclass ForgotUsernameTest(UITestCase):\n    def setUp(self):\n        create_mock_system_user()\n\n        super(ForgotUsernameTest, self).setUp()\n        self.user = make_user(username='username', password='password')\n\n    def tearDown(self):\n        mail.outbox = []\n        super(ForgotUsernameTest, self).tearDown()\n\n    def test_can_get_to_page(self):\n        self.browse_to_url(reverse('auth_login'))\n\n        forgot_username_url = reverse('forgot_username')\n\n        link = self.find_anchor_by_url(forgot_username_url)\n        link.click()\n        self.wait_until_present('input[name=\"email\"]')\n\n        self.assertEqual(self.live_server_url + forgot_username_url,\n                         self.driver.current_url)\n\n    def test_can_retrieve_username(self):\n        self.browse_to_url(reverse('forgot_username'))\n\n        email_elem = self.driver.find_element_by_name('email')\n\n        email_elem.send_keys(self.user.email)\n\n        self.click('form input[type=\"submit\"]')\n        self.wait_until_text_present('Email Sent')\n\n        self.assertEqual(len(mail.outbox), 1)\n","label":0}
{"content":"#!\/usr\/bin\/env python3\n\nimport builtins\n\nfrom pmlr import pmlr\n\ndebug_write = pmlr.util.debug_write\n\nERR_DATA = {\n    ZeroDivisionError:  {\"IS_FATAL\": False, \"TYPE\": \"DEBUG\"},\n    LookupError:        {\"IS_FATAL\": False, \"TYPE\": \"RANGE\"},\n    IndexError:         {\"IS_FATAL\": False, \"TYPE\": \"RANGE\"},\n    TypeError:          {\"IS_FATAL\": True,  \"TYPE\": \"ERROR\"},\n    NameError:          {\"IS_FATAL\": True,  \"TYPE\": \"FATAL\"},\n    ValueError:         {\"IS_FATAL\": True,  \"TYPE\": \"FATAL\"},\n    AssertionError:     {\"IS_FATAL\": True,  \"TYPE\": \"FATAL\"},\n}\n\n\ndef is_none(*args):\n    return None in args\n\n\ndef cmp_all(val, *tests):\n    return builtins.all([val == test for test in tests])\n\n\ndef all(*args):\n    return builtins.all(args)\n\n\ndef any(*args):\n    return builtins.any(args)\n\n\nclass Forth(object):\n\n    def __init__(self):\n        (self._stk, self._lopstk,\n            self._retstk, self._sftstk) = [Stack() for i in range(4)]\n\n        self.dict = {\n            \"\": ()\n        }\n\n        self.funcdict = {\n            \"\": ()\n        }\n\n    def run(self, prog, sandbox=False):\n        pass\n\n    def define(self, name, defn):\n        defn = \" \".join(defn).strip()\n        try:\n            self.run(defn, sandbox=True)\n        except MalformedExpressionException as err:\n            debug_write(err.msg, level=err.level)\n\n        return None  # {\"name\": \"None\", \"desc\": \"debug\"}\n\n\nclass OpCore():\n\n    \"\"\"bare stack operator mixin\"\"\"\n\n    def peek(self, from_idx=0, to_idx=-1):\n        return self._stk[:]\n\n    def pop(self, count=1, idx=-1):\n        \"\"\"( x -- )\n        take something and return it\"\"\"\n        if count > len(self._stk):\n            pmlr.util.debug_write(\n                \"popping more items than exist on stack!\\n\",\n                level=\"WARN\"\n            )\n\n        # http:\/\/stackoverflow.com\/a\/34633242\/4532996\n        # from testing it seems that pop(x) is slower than pop()\n        # pop(-1) doesn't seem to be optimised to pop(),\n        # so avoid it if possible\n\n        x = []\n        if -1 == idx:\n            for i in range(count):\n                try:\n                    x.append(self._stk.pop())\n                except LookupError as err:\n                    self.err(err, errtype=\"RANGE\")\n                    break\n        else:\n            for i in range(count):\n                try:\n                    x.append(self._stk.pop(idx))\n                except LookupError as err:\n                    self.err(err, errtype=\"RANGE\")\n                    break\n\n        return x[0] if len(x) == 1 else list(reversed(x))\n\n    def push(self, *args, idx=-1):\n        \"\"\"( -- x ... )\n        put somethings at idx\"\"\"\n        if idx == -1:\n            self._stk.extend(args)\n        else:\n            if idx < 0:\n                for arg in args:\n                    self._stk.insert(idx, arg)\n                    idx -= 1\n\n            else:\n                for arg in args:\n                    self._stk.insert(idx, arg)\n                    idx += 1\n\n    def clear(self):\n        \"\"\"( z y x -- )\n        clear the stack completely\"\"\"\n        y = self._stk.copy()\n        self._stk.clear()\n        return y\n\n    def pick(self, idx=-3, drop=False):\n        \"\"\"( x -- x )\n        pick somethings from a range of indicies\"\"\"\n        s = self._stk[idx]\n        if drop: self._stk[idx] = []\n\n        return s\n\n    def drop(self, count=1, idx=-1):\n        \"\"\"( x -- )\n        drop items without returning (cheaper pop)\"\"\"\n        [self.pop(idx=idx) for i in range(count)]\n\n    def dup(self, count=1, from_idx=-1):\n        \"\"\"( y -- y y )\n        duplicate something and push\"\"\"\n        try:\n            y = self._stk[from_idx] * count\n        except LookupError as err:\n            self.err(err, errtype=\"RANGE\")\n\n        self.push(*y, idx=idx)\n\n    def dupn(self, count=2, idx=-1):\n        \"\"\"( x y -- x y x y )\n        dup count items from an idx\"\"\"\n        y = []\n        for i in range(count):\n            try:\n                y.append(self._stk[idx - i])\n            except LookupError as err:\n                if idx == 1:\n                    continue\n                else:\n                    self.err(err, errtype=\"RANGE\")\n                    return None\n\n        self.push(*y, idx=idx)\n\n    def swap(self, idx=-1):\n        \"\"\"( x y -- y x )\n        swap two things at an index\"\"\"\n        self.push(*reversed([self.pop(idx=idx) for i in range(2)]), idx=idx)\n\n    def rot(self, idx=-1, count=3):\n        \"\"\"( w x y z -- x y z w )\n        rotate things left, at an index\"\"\"\n        l = [self.pop(idx=idx) for i in range(count)]\n        l.insert(0, l.pop())\n        self.push(*l, idx=idx)\n\n    def urot(self, idx=-1, count=3):\n        \"\"\"( w x y z -- z w x y )\n        rotate things right, at an index\"\"\"\n        l = [self.pop(idx=idx) for i in range(count)]\n        l.append(l.pop(0))\n        self.push(*l, idx=idx)\n\n\nclass OpLogik():\n    pass\n\nclass OpString():\n    pass\n\n\nclass Stack(OpCore, OpLogik, OpString):\n\n    \"the mixin mixer of the above mixins\"\n\n    def __init__(self):\n        self._stk = []\n\n    def err(self, err, errtype=None, framelevel=3):\n        if errtype is None:\n            errtype = ERR_DATA.get(err.__class__, {\"TYPE\": \"FATAL\"})[\"TYPE\"]\n\n        errtype = errtype.upper()\n\n        debug_write(*err.args, \"\\n\", level=errtype, framelevel=framelevel)\n\n        if ERR_DATA.get(err.__class__, {\"IS_FATAL\": True})[\"IS_FATAL\"]:\n            raise err.__class__(\n                pmlr.util.debug_fmt(\n                    errtype, framelevel=framelevel\n                ) + \" \" + \"\".join([str(i) for i in err.args])\n            )\n\n    def __repr__(self):\n        return \"<{}> {}\".format(len(self._stk), _fmt_collection(self._stk))\n\nis_collection = lambda c: any(issubclass(c.__class__, (list, tuple, dict, set)), isinstance(c, (list, tuple, dict, set)))\n\ndef _fmt_collection(col):\n    \"format a collection literal\"\n    t_super = col.__class__\n    try:\n        t_mro  = t_super.mro()\n        t_meta = t_mro[1]\n        if cmp_all(type(t_meta), object, type, type(object), type(type)): raise TypeError\n    except (NameError, TypeError, IndexError, AttributeError) as err:\n        if cmp_all(err.__class__, NameError, AttributeError) and not hasattr(t_super, \"mro\"): raise\n        else: raise TypeError(\"need object instance but found {} (class constructor, type or object object)\".format(type(col)))\n\n    is_iter      = hasattr(col, \"__iter__\")\n    is_meta_iter = hasattr(col.__class__, \"__iter__\")\n    if not any(is_iter, is_meta_iter):\n        raise TypeError(\"({}) {} object is not iterable\".format(col, col.__class__))\n\n    orderedary = (list, tuple, set)\n    if any(isinstance(col, orderedary), issubclass(col.__class__, orderedary)):\n        return \"[ {} ]\".format(\" \".join(repr(i) if not is_collection(i) else _fmt_collection(i) for i in col))\n\n    elif any(isinstance(col, dict), issubclass(col, dict)):\n        return \" \".join(\"{}:{}\".format(str(key), str(value)) for key, value in col.items())\n    else:\n        raise TypeError(\"don't know how to format that container\")\n\n    return locals()\n\nif __name__ == \"__main__\":\n    from tests import main as test_main\n    test_main()","label":0}
{"content":"#!\/usr\/bin\/env python\n\nfrom __future__ import print_function\n\nfrom ImageD11.grain import read_grain_file\nimport sys, os\n\ngf = read_grain_file(sys.argv[1])\nmapfile=open(sys.argv[2],\"w\")\n\ndef dodot(xyz,k):\n    mapfile.write(\"%f %f %f %d\\n\"%(xyz[0],xyz[1],xyz[2],k))\n\ndef getmedian(s):\n    items=s.split()\n    j = -1\n    for i in range(len(items)):\n        if items[i] == \"median\":\n            j = i\n    if j == -1:\n        return 0\n    return abs(float(items[j+2]))\n            \ntry:\n    outersf = float(sys.argv[3])\nexcept:\n    outersf = 1.0\n\nprint(\"Scale factor is\",outersf)\nfor g in gf:\n    #print g.translation, g.ubi\n    mapfile.write(\"\\n\\n\")\n    o = g.translation\n    try:\n        sf = pow(getmedian(g.intensity_info),0.3333)*outersf\n    except:\n        sf = outersf\n    try:\n        k = int(g.npks)\n    except:\n        k = 1\n    for u in g.ubi:\n        dodot(o,k)\n        dodot(o+u*sf,int(g.npks))\n    for u in g.ubi:\n        dodot(o,k)\n        dodot(o-u*sf,int(g.npks))\n#    dodot(o,k)\n#    dodot(o+sf*(-g.ubi[0]-g.ubi[1]),k)\n#    dodot(o,k)\n#    dodot(o+sf*(g.ubi[0]+g.ubi[1]),k)\n\nmapfile.close()\nterm = \" \"\nif \"linux\" in sys.platform:\n    term = \"set term x11\"\nif \"win32\" in sys.platform:\n    term = \"set term windows\"\n    \nopen(\"gnuplot.in\",\"w\").write(\"\"\"\n%s\nset ticslevel 0\nset title \"Color proportional to number of peaks\"\nset palette model RGB\nset palette defined ( 0 \"violet\", 1 \"blue\", 2 \"green\", 3 \"yellow\", 4 \"orange\", 5 \"red\" )\nset view equal xyz\nset view 75,0,1,1\n#set terminal gif animate delay 10 loop 1 optimize size 1024,768\nset nokey\nset hidden3d\n#set output \"ImageD11map.gif\"\nsplot \"%s\" u 1:2:3:4 w l lw 2 lc pal z\n\"\"\"%(term, sys.argv[2])\n# \"\".join([\"set view 75,%d\\n replot\\n\"%(i) for i in range(1,360,1)])\n                             )\n\n\n    \nos.system(\"gnuplot -background white gnuplot.in -\")\n\n    \n\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Business Applications\n#    Copyright (c) 2012-TODAY OpenERP S.A. <http:\/\/openerp.com>\n#\n#    This program is free software: you can redistribute it and\/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#\n##############################################################################\n\nimport base64\nfrom openerp.addons.mail.tests.common import TestMail\nfrom openerp.tools import mute_logger\n\n\nclass test_message_compose(TestMail):\n\n    def setUp(self):\n        super(test_message_compose, self).setUp()\n\n        # create a 'pigs' and 'bird' groups that will be used through the various tests\n        self.group_bird_id = self.mail_group.create(self.cr, self.uid,\n            {'name': 'Bird', 'description': 'I am angry !'})\n\n    def test_00_message_compose_wizard(self):\n        \"\"\" Tests designed for the mail.compose.message wizard updated by email_template. \"\"\"\n        cr, uid = self.cr, self.uid\n        mail_compose = self.registry('mail.compose.message')\n        self.res_users.write(cr, uid, [uid], {'signature': 'Admin', 'email': 'a@a.a'})\n        user_admin = self.res_users.browse(cr, uid, uid)\n        p_a_id = user_admin.partner_id.id\n        group_pigs = self.mail_group.browse(cr, uid, self.group_pigs_id)\n        group_bird = self.mail_group.browse(cr, uid, self.group_bird_id)\n\n        # Mail data\n        _subject1 = 'Pigs'\n        _subject2 = 'Bird'\n        _body_html1 = 'Fans of Pigs, unite !'\n        _body_html2 = 'I am angry !'\n        _attachments = [\n            {'name': 'First', 'datas_fname': 'first.txt', 'datas': base64.b64encode('My first attachment'), 'res_model': 'res.partner', 'res_id': self.partner_admin_id},\n            {'name': 'Second', 'datas_fname': 'second.txt', 'datas': base64.b64encode('My second attachment'), 'res_model': 'res.partner', 'res_id': self.partner_admin_id},\n            ]\n        _attachments_test = [('first.txt', 'My first attachment'), ('second.txt', 'My second attachment')]\n\n        # Create template on mail.group, with attachments\n        group_model_id = self.registry('ir.model').search(cr, uid, [('model', '=', 'mail.group')])[0]\n        email_template = self.registry('email.template')\n        email_template_id = email_template.create(cr, uid, {\n            'model_id': group_model_id,\n            'name': 'Pigs Template',\n            'subject': '${object.name}',\n            'body_html': '${object.description}',\n            'user_signature': False,\n            'attachment_ids': [(0, 0, _attachments[0]), (0, 0, _attachments[1])],\n            'email_to': 'b@b.b, c@c.c',\n            'email_cc': 'd@d.d'\n            })\n\n        # ----------------------------------------\n        # CASE1: comment and save as template\n        # ----------------------------------------\n\n        # 1. Comment on pigs\n        compose_id = mail_compose.create(cr, uid,\n            {'subject': 'Forget me subject', 'body': '<p>Dummy body<\/p>'},\n            {'default_composition_mode': 'comment',\n                'default_model': 'mail.group',\n                'default_res_id': self.group_pigs_id,\n                'active_ids': [self.group_pigs_id, self.group_bird_id]})\n        compose = mail_compose.browse(cr, uid, compose_id)\n\n        # 2. Save current composition form as a template\n        mail_compose.save_as_template(cr, uid, [compose_id], context={'default_model': 'mail.group'})\n        # Test: email_template subject, body_html, model\n        last_template_id = email_template.search(cr, uid, [('model', '=', 'mail.group'), ('subject', '=', 'Forget me subject')], limit=1)[0]\n        self.assertTrue(last_template_id, 'email_template not found for model mail.group, subject Forget me subject')\n        last_template = email_template.browse(cr, uid, last_template_id)\n        self.assertEqual(last_template.body_html, '<p>Dummy body<\/p>', 'email_template incorrect body_html')\n\n        # ----------------------------------------\n        # CASE2: comment with template, save as template\n        # ----------------------------------------\n\n        # 1. Comment on pigs\n        context = {\n            'default_composition_mode': 'comment',\n            'default_model': 'mail.group',\n            'default_res_id': self.group_pigs_id,\n            'default_use_template': False,\n            'default_template_id': email_template_id,\n            'active_ids': [self.group_pigs_id, self.group_bird_id]\n        }\n        compose_id = mail_compose.create(cr, uid, {'subject': 'Forget me subject', 'body': 'Dummy body'}, context)\n        compose = mail_compose.browse(cr, uid, compose_id, context)\n        onchange_res = compose.onchange_template_id(email_template_id, 'comment', 'mail.group', self.group_pigs_id)['value']\n        onchange_res['partner_ids'] = [(4, partner_id) for partner_id in onchange_res.pop('partner_ids', [])]\n        onchange_res['attachment_ids'] = [(4, attachment_id) for attachment_id in onchange_res.pop('attachment_ids', [])]\n        compose.write(onchange_res)\n        compose.refresh()\n\n        message_pids = [partner.id for partner in compose.partner_ids]\n        partner_ids = self.res_partner.search(cr, uid, [('email', 'in', ['b@b.b', 'c@c.c', 'd@d.d'])])\n        # Test: mail.compose.message: subject, body, partner_ids\n        self.assertEqual(compose.subject, _subject1, 'mail.compose.message subject incorrect')\n        self.assertIn(_body_html1, compose.body, 'mail.compose.message body incorrect')\n        self.assertEqual(set(message_pids), set(partner_ids), 'mail.compose.message partner_ids incorrect')\n        # Test: mail.compose.message: attachments (owner has not been modified)\n        for attach in compose.attachment_ids:\n            self.assertEqual(attach.res_model, 'res.partner', 'mail.compose.message attachment res_model through templat was overriden')\n            self.assertEqual(attach.res_id, self.partner_admin_id, 'mail.compose.message attachment res_id incorrect')\n            self.assertIn((attach.datas_fname, base64.b64decode(attach.datas)), _attachments_test,\n                'mail.message attachment name \/ data incorrect')\n        # Test: mail.message: attachments\n        mail_compose.send_mail(cr, uid, [compose_id])\n        group_pigs.refresh()\n        message_pigs = group_pigs.message_ids[0]\n        for attach in message_pigs.attachment_ids:\n            self.assertEqual(attach.res_model, 'mail.group', 'mail.compose.message attachment res_model through templat was overriden')\n            self.assertEqual(attach.res_id, self.group_pigs_id, 'mail.compose.message attachment res_id incorrect')\n            self.assertIn((attach.datas_fname, base64.b64decode(attach.datas)), _attachments_test,\n                'mail.message attachment name \/ data incorrect')\n\n        # ----------------------------------------\n        # CASE3: mass_mail with template\n        # ----------------------------------------\n\n        # 1. Mass_mail on pigs and bird, with a default_partner_ids set to check he is correctly added\n        context = {\n            'default_composition_mode': 'mass_mail',\n            'default_notify': True,\n            'default_model': 'mail.group',\n            'default_res_id': self.group_pigs_id,\n            'default_template_id': email_template_id,\n            'default_partner_ids': [p_a_id],\n            'active_ids': [self.group_pigs_id, self.group_bird_id]\n        }\n        compose_id = mail_compose.create(cr, uid, {'subject': 'Forget me subject', 'body': 'Dummy body'}, context)\n        compose = mail_compose.browse(cr, uid, compose_id, context)\n        onchange_res = compose.onchange_template_id(email_template_id, 'mass_mail', 'mail.group', self.group_pigs_id)['value']\n        onchange_res['partner_ids'] = [(4, partner_id) for partner_id in onchange_res.pop('partner_ids', [])]\n        onchange_res['attachment_ids'] = [(4, attachment_id) for attachment_id in onchange_res.pop('attachment_ids', [])]\n        compose.write(onchange_res)\n        compose.refresh()\n\n        message_pids = [partner.id for partner in compose.partner_ids]\n        partner_ids = [p_a_id]\n        self.assertEqual(compose.subject, '${object.name}', 'mail.compose.message subject incorrect')\n        self.assertEqual(compose.body, '<p>${object.description}<\/p>', 'mail.compose.message body incorrect')  # todo: check signature\n        self.assertEqual(set(message_pids), set(partner_ids), 'mail.compose.message partner_ids incorrect')\n\n        # 2. Post the comment, get created message\n        mail_compose.send_mail(cr, uid, [compose_id],  {'default_res_id': -1, 'active_ids': [self.group_pigs_id, self.group_bird_id]})\n        group_pigs.refresh()\n        group_bird.refresh()\n        message_pigs = group_pigs.message_ids[0]\n        message_bird = group_bird.message_ids[0]\n        # Test: subject, body\n        self.assertEqual(message_pigs.subject, _subject1, 'mail.message subject on Pigs incorrect')\n        self.assertEqual(message_bird.subject, _subject2, 'mail.message subject on Bird incorrect')\n        self.assertIn(_body_html1, message_pigs.body, 'mail.message body on Pigs incorrect')\n        self.assertIn(_body_html2, message_bird.body, 'mail.message body on Bird incorrect')\n        # Test: partner_ids: p_a_id (default) + 3 newly created partners\n        # message_pigs_pids = [partner.id for partner in message_pigs.notified_partner_ids]\n        # message_bird_pids = [partner.id for partner in message_bird.notified_partner_ids]\n        # partner_ids = self.res_partner.search(cr, uid, [('email', 'in', ['b@b.b', 'c@c.c', 'd@d.d'])])\n        # partner_ids.append(p_a_id)\n        # self.assertEqual(set(message_pigs_pids), set(partner_ids), 'mail.message on pigs incorrect number of notified_partner_ids')\n        # self.assertEqual(set(message_bird_pids), set(partner_ids), 'mail.message on bird notified_partner_ids incorrect')\n\n        # ----------------------------------------\n        # CASE4: test newly introduced partner_to field\n        # ----------------------------------------\n\n        # get already-created partners back\n        p_b_id = self.res_partner.search(cr, uid, [('email', '=', 'b@b.b')])[0]\n        p_c_id = self.res_partner.search(cr, uid, [('email', '=', 'c@c.c')])[0]\n        p_d_id = self.res_partner.search(cr, uid, [('email', '=', 'd@d.d')])[0]\n        # modify template: use partner_to, use template and email address in email_to to test all features together\n        user_model_id = self.registry('ir.model').search(cr, uid, [('model', '=', 'res.users')])[0]\n        email_template.write(cr, uid, [email_template_id], {\n            'model_id': user_model_id,\n            'body_html': '${object.login}',\n            'email_to': '${object.email}, c@c',\n            'partner_to': '%i,%i' % (p_b_id, p_c_id),\n            'email_cc': 'd@d',\n            })\n        # patner by email + partner by id (no double)\n        send_to = [p_a_id, p_b_id, p_c_id, p_d_id]\n        # Generate messsage with default email and partner on template\n        mail_value = mail_compose.generate_email_for_composer(cr, uid, email_template_id, uid)\n        self.assertEqual(set(mail_value['partner_ids']), set(send_to), 'mail.message partner_ids list created by template is incorrect')\n\n    @mute_logger('openerp.models')\n    def test_10_email_templating(self):\n        \"\"\" Tests designed for the mail.compose.message wizard updated by email_template. \"\"\"\n        cr, uid, context = self.cr, self.uid, {}\n\n        # create the email.template on mail.group model\n        group_model_id = self.registry('ir.model').search(cr, uid, [('model', '=', 'mail.group')])[0]\n        email_template = self.registry('email.template')\n        email_template_id = email_template.create(cr, uid, {\n            'model_id': group_model_id,\n            'name': 'Pigs Template',\n            'email_from': 'Raoul Grosbedon <raoul@example.com>',\n            'subject': '${object.name}',\n            'body_html': '${object.description}',\n            'user_signature': True,\n            'email_to': 'b@b.b, c@c.c',\n            'email_cc': 'd@d.d',\n            'partner_to': '${user.partner_id.id},%s,%s,-1' % (self.user_raoul.partner_id.id, self.user_bert.partner_id.id)\n        })\n\n        # not force send: email_recipients is not taken into account\n        msg_id = email_template.send_mail(cr, uid, email_template_id, self.group_pigs_id, context=context)\n        mail = self.mail_mail.browse(cr, uid, msg_id, context=context)\n        self.assertEqual(mail.subject, 'Pigs', 'email_template: send_mail: wrong subject')\n        self.assertEqual(mail.email_to, 'b@b.b, c@c.c', 'email_template: send_mail: wrong email_to')\n        self.assertEqual(mail.email_cc, 'd@d.d', 'email_template: send_mail: wrong email_cc')\n        self.assertEqual(\n            set([partner.id for partner in mail.recipient_ids]),\n            set((self.partner_admin_id, self.user_raoul.partner_id.id, self.user_bert.partner_id.id)),\n            'email_template: send_mail: wrong management of partner_to')\n\n        # force send: take email_recipients into account\n        email_template.send_mail(cr, uid, email_template_id, self.group_pigs_id, force_send=True, context=context)\n        sent_emails = self._build_email_kwargs_list\n        email_to_lst = [\n            ['b@b.b', 'c@c.c'], ['Administrator <admin@yourcompany.example.com>'],\n            ['Raoul Grosbedon <raoul@raoul.fr>'], ['Bert Tartignole <bert@bert.fr>']]\n        self.assertEqual(len(sent_emails), 4, 'email_template: send_mail: 3 valid email recipients + email_to -> should send 4 emails')\n        for email in sent_emails:\n            self.assertIn(email['email_to'], email_to_lst, 'email_template: send_mail: wrong email_recipients')\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    Copyright (C) 2016 Compassion CH (http:\/\/www.compassion.ch)\n#    Releasing children from poverty in Jesus' name\n#    @author: Emanuel Cino <ecino@compassion.ch>\n#\n#    The licence is in the file __manifest__.py\n#\n##############################################################################\nfrom odoo.addons.message_center_compassion.mappings.base_mapping import \\\n    OnrampMapping\n\n\nclass HouseHoldMapping(OnrampMapping):\n    ODOO_MODEL = 'compassion.household'\n\n    CONNECT_MAPPING = {\n        \"BeneficiaryHouseholdMemberList\": ('member_ids',\n                                           'compassion.household.member'),\n        \"BeneficiaryHouseholdMemberDetails\": ('member_ids',\n                                              'compassion.household.member'),\n        \"FemaleGuardianEmploymentStatus\": 'female_guardian_job_type',\n        \"FemaleGuardianOccupation\": 'female_guardian_job',\n        \"Household_ID\": \"household_id\",\n        \"Household_Name\": \"name\",\n        \"IsNaturalFatherLivingWithChild\": 'father_living_with_child',\n        \"IsNaturalMotherLivingWithChild\": 'mother_living_with_child',\n        \"MaleGuardianEmploymentStatus\": 'male_guardian_job_type',\n        \"MaleGuardianOccupation\": \"male_guardian_job\",\n        \"NaturalFatherAlive\": \"father_alive\",\n        \"NaturalMotherAlive\": \"mother_alive\",\n        \"NumberOfSiblingBeneficiaries\": \"number_beneficiaries\",\n        \"ParentsMaritalStatus\": \"marital_status\",\n        \"ParentsTogether\": \"parents_together\",\n        'RevisedValues': 'revised_value_ids',\n\n        # Not define\n        \"SourceKitName\": None,\n    }\n\n    def _process_odoo_data(self, odoo_data):\n        # Unlink old revised values and create new ones\n        if isinstance(odoo_data.get('revised_value_ids'), list):\n            household = self.env[self.ODOO_MODEL].search(\n                [('household_id', '=', odoo_data['household_id'])])\n            household.revised_value_ids.unlink()\n            for value in odoo_data['revised_value_ids']:\n                self.env['compassion.major.revision'].create({\n                    'name': value,\n                    'household_id': household.id,\n                })\n            del odoo_data['revised_value_ids']\n\n        # Replace dict by a tuple for the ORM update\/create\n        if 'member_ids' in odoo_data:\n            # Remove all members\n            household = self.env[self.ODOO_MODEL].search(\n                [('household_id', '=', odoo_data['household_id'])])\n            household.member_ids.unlink()\n\n            member_list = list()\n            for member in odoo_data['member_ids']:\n                orm_tuple = (0, 0, member)\n                member_list.append(orm_tuple)\n            odoo_data['member_ids'] = member_list or False\n\n        for key in odoo_data.iterkeys():\n            val = odoo_data[key]\n            if isinstance(val, basestring) and val.lower() in (\n                    'null', 'false', 'none', 'other', 'unknown'):\n                odoo_data[key] = False\n\n\nclass HouseholdMemberMapping(OnrampMapping):\n    ODOO_MODEL = 'compassion.household.member'\n\n    CONNECT_MAPPING = {\n        \"Beneficiary_GlobalID\": ('child_id.global_id', 'compassion.child'),\n        \"Beneficiary_LocalID\": 'beneficiary_local_id',\n        \"FullName\": None,\n        \"HouseholdMemberRole\": 'role',\n        \"HouseholdMember_Name\": 'name',\n        \"IsCaregiver\": 'is_caregiver',\n        \"IsPrimaryCaregiver\": 'is_primary_caregiver',\n    }\n","label":0}
{"content":"# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU Affero Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU Affero Public License for more details.\n#\n# You should have received a copy of the GNU Affero Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses.\n#\n# http:\/\/numenta.org\/licenses\/\n# ----------------------------------------------------------------------\n\nimport numpy\n\nfrom nupic.data.fieldmeta import FieldMetaType\nfrom nupic.data import SENTINEL_VALUE_FOR_MISSING_DATA\nfrom nupic.encoders.base import Encoder, EncoderResult\nfrom nupic.encoders.scalar import ScalarEncoder\n\n\n\nUNKNOWN = \"<UNKNOWN>\"\n\n\n\nclass CategoryEncoder(Encoder):\n  \"\"\"Encodes a list of discrete categories (described by strings), that aren't\n  related to each other, so we never emit a mixture of categories.\n\n  The value of zero is reserved for \"unknown category\"\n\n  Internally we use a ScalarEncoder with a radius of 1, but since we only encode\n  integers, we never get mixture outputs.\n\n  The SDRCategoryEncoder uses a different method to encode categories\"\"\"\n\n\n  def __init__(self, w, categoryList, name=\"category\", verbosity=0, forced=False):\n    \"\"\"params:\n       forced (default False) : if True, skip checks for parameters' settings; see encoders\/scalar.py for details\n    \"\"\"\n\n    self.encoders = None\n    self.verbosity = verbosity\n\n    # number of categories includes \"unknown\"\n    self.ncategories = len(categoryList) + 1\n\n    self.categoryToIndex = dict()\n    self.indexToCategory = dict()\n    self.indexToCategory[0] = UNKNOWN\n    for i in xrange(len(categoryList)):\n      self.categoryToIndex[categoryList[i]] = i+1\n      self.indexToCategory[i+1] = categoryList[i]\n\n    self.encoder = ScalarEncoder(w, minval=0, maxval=self.ncategories - 1,\n                      radius=1, periodic=False, forced=forced)\n    self.width = w * self.ncategories\n    assert self.encoder.getWidth() == self.width\n\n    self.description = [(name, 0)]\n    self.name = name\n\n    # These are used to support the topDownCompute method\n    self._topDownMappingM = None\n\n    # This gets filled in by getBucketValues\n    self._bucketValues = None\n\n\n  def getDecoderOutputFieldTypes(self):\n    \"\"\" [Encoder class virtual method override]\n    \"\"\"\n    # TODO: change back to string meta-type after the decoding logic is fixed\n    #       to output strings instead of internal index values.\n    #return (FieldMetaType.string,)\n    return (FieldMetaType.integer,)\n\n\n  def getWidth(self):\n    return self.width\n\n\n  def getDescription(self):\n    return self.description\n\n\n  def getScalars(self, input):\n    \"\"\" See method description in base.py \"\"\"\n    if input == SENTINEL_VALUE_FOR_MISSING_DATA:\n      return numpy.array([None])\n    else:\n      return numpy.array([self.categoryToIndex.get(input, 0)])\n\n\n  def getBucketIndices(self, input):\n    \"\"\" See method description in base.py \"\"\"\n\n    # Get the bucket index from the underlying scalar encoder\n    if input == SENTINEL_VALUE_FOR_MISSING_DATA:\n      return [None]\n    else:\n      return self.encoder.getBucketIndices(self.categoryToIndex.get(input, 0))\n\n\n  def encodeIntoArray(self, input, output):\n    # if not found, we encode category 0\n    if input == SENTINEL_VALUE_FOR_MISSING_DATA:\n      output[0:] = 0\n      val = \"<missing>\"\n    else:\n      val = self.categoryToIndex.get(input, 0)\n      self.encoder.encodeIntoArray(val, output)\n\n    if self.verbosity >= 2:\n      print \"input:\", input, \"va:\", val, \"output:\", output\n      print \"decoded:\", self.decodedToStr(self.decode(output))\n\n\n  def decode(self, encoded, parentFieldName=''):\n    \"\"\" See the function description in base.py\n    \"\"\"\n\n    # Get the scalar values from the underlying scalar encoder\n    (fieldsDict, fieldNames) = self.encoder.decode(encoded)\n    if len(fieldsDict) == 0:\n      return (fieldsDict, fieldNames)\n\n    # Expect only 1 field\n    assert(len(fieldsDict) == 1)\n\n    # Get the list of categories the scalar values correspond to and\n    #  generate the description from the category name(s).\n    (inRanges, inDesc) = fieldsDict.values()[0]\n    outRanges = []\n    desc = \"\"\n    for (minV, maxV) in inRanges:\n      minV = int(round(minV))\n      maxV = int(round(maxV))\n      outRanges.append((minV, maxV))\n      while minV <= maxV:\n        if len(desc) > 0:\n          desc += \", \"\n        desc += self.indexToCategory[minV]\n        minV += 1\n\n    # Return result\n    if parentFieldName != '':\n      fieldName = \"%s.%s\" % (parentFieldName, self.name)\n    else:\n      fieldName = self.name\n    return ({fieldName: (outRanges, desc)}, [fieldName])\n\n\n  def closenessScores(self, expValues, actValues, fractional=True,):\n    \"\"\" See the function description in base.py\n\n    kwargs will have the keyword \"fractional\", which is ignored by this encoder\n    \"\"\"\n\n    expValue = expValues[0]\n    actValue = actValues[0]\n\n    if expValue == actValue:\n      closeness = 1.0\n    else:\n      closeness = 0.0\n\n    if not fractional:\n      closeness = 1.0 - closeness\n\n    return numpy.array([closeness])\n\n\n  def getBucketValues(self):\n    \"\"\" See the function description in base.py \"\"\"\n\n    if self._bucketValues is None:\n      numBuckets = len(self.encoder.getBucketValues())\n      self._bucketValues = []\n      for bucketIndex in range(numBuckets):\n        self._bucketValues.append(self.getBucketInfo([bucketIndex])[0].value)\n\n    return self._bucketValues\n\n\n  def getBucketInfo(self, buckets):\n    \"\"\" See the function description in base.py\n    \"\"\"\n\n    # For the category encoder, the bucket index is the category index\n    bucketInfo = self.encoder.getBucketInfo(buckets)[0]\n\n    categoryIndex = int(round(bucketInfo.value))\n    category = self.indexToCategory[categoryIndex]\n\n    return [EncoderResult(value=category, scalar=categoryIndex,\n                         encoding=bucketInfo.encoding)]\n\n\n  def topDownCompute(self, encoded):\n    \"\"\" See the function description in base.py\n    \"\"\"\n\n    encoderResult = self.encoder.topDownCompute(encoded)[0]\n    value = encoderResult.value\n    categoryIndex = int(round(value))\n    category = self.indexToCategory[categoryIndex]\n\n    return EncoderResult(value=category, scalar=categoryIndex,\n                         encoding=encoderResult.encoding)\n\n\n  @classmethod\n  def read(cls, proto):\n    encoder = object.__new__(cls)\n\n    encoder.verbosity = proto.verbosity\n    encoder.encoder = ScalarEncoder.read(proto.encoder)\n    encoder.width = proto.width\n    encoder.description = [(proto.name, 0)]\n    encoder.name = proto.name\n    encoder.indexToCategory = {x.index: x.category\n                               for x in proto.indexToCategory}\n    encoder.categoryToIndex = {category: index\n                               for index, category\n                               in encoder.indexToCategory.items()\n                               if category != UNKNOWN}\n    encoder._topDownMappingM = None\n    encoder._bucketValues = None\n\n    return encoder\n\n\n  def write(self, proto):\n    proto.width = self.width\n    proto.indexToCategory = [\n      {\"index\": index, \"category\": category}\n      for index, category in self.indexToCategory.items()\n    ]\n    proto.name = self.name\n    proto.verbosity = self.verbosity\n    self.encoder.write(proto.encoder)\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n\nimport babel.dates\nimport re\nimport werkzeug\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\n\nfrom odoo import fields, http, _\nfrom odoo.addons.website.models.website import slug\nfrom odoo.http import request\n\n\nclass WebsiteEventController(http.Controller):\n\n    @http.route(['\/event', '\/event\/page\/<int:page>', '\/events', '\/events\/page\/<int:page>'], type='http', auth=\"public\", website=True)\n    def events(self, page=1, **searches):\n        Event = request.env['event.event']\n        EventType = request.env['event.type']\n\n        searches.setdefault('date', 'all')\n        searches.setdefault('type', 'all')\n        searches.setdefault('country', 'all')\n\n        domain_search = {}\n\n        def sdn(date):\n            return fields.Datetime.to_string(date.replace(hour=23, minute=59, second=59))\n\n        def sd(date):\n            return fields.Datetime.to_string(date)\n        today = datetime.today()\n        dates = [\n            ['all', _('Next Events'), [(\"date_end\", \">\", sd(today))], 0],\n            ['today', _('Today'), [\n                (\"date_end\", \">\", sd(today)),\n                (\"date_begin\", \"<\", sdn(today))],\n                0],\n            ['week', _('This Week'), [\n                (\"date_end\", \">=\", sd(today + relativedelta(days=-today.weekday()))),\n                (\"date_begin\", \"<\", sdn(today + relativedelta(days=6-today.weekday())))],\n                0],\n            ['nextweek', _('Next Week'), [\n                (\"date_end\", \">=\", sd(today + relativedelta(days=7-today.weekday()))),\n                (\"date_begin\", \"<\", sdn(today + relativedelta(days=13-today.weekday())))],\n                0],\n            ['month', _('This month'), [\n                (\"date_end\", \">=\", sd(today.replace(day=1))),\n                (\"date_begin\", \"<\", (today.replace(day=1) + relativedelta(months=1)).strftime('%Y-%m-%d 00:00:00'))],\n                0],\n            ['nextmonth', _('Next month'), [\n                (\"date_end\", \">=\", sd(today.replace(day=1) + relativedelta(months=1))),\n                (\"date_begin\", \"<\", (today.replace(day=1) + relativedelta(months=2)).strftime('%Y-%m-%d 00:00:00'))],\n                0],\n            ['old', _('Old Events'), [\n                (\"date_end\", \"<\", today.strftime('%Y-%m-%d 00:00:00'))],\n                0],\n        ]\n\n        # search domains\n        # TDE note: WTF ???\n        current_date = None\n        current_type = None\n        current_country = None\n        for date in dates:\n            if searches[\"date\"] == date[0]:\n                domain_search[\"date\"] = date[2]\n                if date[0] != 'all':\n                    current_date = date[1]\n        if searches[\"type\"] != 'all':\n            current_type = EventType.browse(int(searches['type']))\n            domain_search[\"type\"] = [(\"event_type_id\", \"=\", int(searches[\"type\"]))]\n\n        if searches[\"country\"] != 'all' and searches[\"country\"] != 'online':\n            current_country = request.env['res.country'].browse(int(searches['country']))\n            domain_search[\"country\"] = ['|', (\"country_id\", \"=\", int(searches[\"country\"])), (\"country_id\", \"=\", False)]\n        elif searches[\"country\"] == 'online':\n            domain_search[\"country\"] = [(\"country_id\", \"=\", False)]\n\n        def dom_without(without):\n            domain = [('state', \"in\", ['draft', 'confirm', 'done'])]\n            for key, search in domain_search.items():\n                if key != without:\n                    domain += search\n            return domain\n\n        # count by domains without self search\n        for date in dates:\n            if date[0] != 'old':\n                date[3] = Event.search_count(dom_without('date') + date[2])\n\n        domain = dom_without('type')\n        types = Event.read_group(domain, [\"id\", \"event_type_id\"], groupby=[\"event_type_id\"], orderby=\"event_type_id\")\n        types.insert(0, {\n            'event_type_id_count': sum([int(type['event_type_id_count']) for type in types]),\n            'event_type_id': (\"all\", _(\"All Categories\"))\n        })\n\n        domain = dom_without('country')\n        countries = Event.read_group(domain, [\"id\", \"country_id\"], groupby=\"country_id\", orderby=\"country_id\")\n        countries.insert(0, {\n            'country_id_count': sum([int(country['country_id_count']) for country in countries]),\n            'country_id': (\"all\", _(\"All Countries\"))\n        })\n\n        step = 10  # Number of events per page\n        event_count = Event.search_count(dom_without(\"none\"))\n        pager = request.website.pager(\n            url=\"\/event\",\n            url_args={'date': searches.get('date'), 'type': searches.get('type'), 'country': searches.get('country')},\n            total=event_count,\n            page=page,\n            step=step,\n            scope=5)\n\n        order = 'website_published desc, date_begin'\n        if searches.get('date', 'all') == 'old':\n            order = 'website_published desc, date_begin desc'\n        events = Event.search(dom_without(\"none\"), limit=step, offset=pager['offset'], order=order)\n\n        values = {\n            'current_date': current_date,\n            'current_country': current_country,\n            'current_type': current_type,\n            'event_ids': events,  # event_ids used in website_event_track so we keep name as it is\n            'dates': dates,\n            'types': types,\n            'countries': countries,\n            'pager': pager,\n            'searches': searches,\n            'search_path': \"?%s\" % werkzeug.url_encode(searches),\n        }\n\n        return request.render(\"website_event.index\", values)\n\n    @http.route(['\/event\/<model(\"event.event\"):event>\/page\/<path:page>'], type='http', auth=\"public\", website=True)\n    def event_page(self, event, page, **post):\n        values = {\n            'event': event,\n            'main_object': event\n        }\n\n        if '.' not in page:\n            page = 'website_event.%s' % page\n\n        try:\n            request.website.get_template(page)\n        except ValueError:\n            # page not found\n            values['path'] = re.sub(r\"^website_event\\.\", '', page)\n            values['from_template'] = 'website_event.default_page'  # .strip('website_event.')\n            page = 'website.page_404'\n\n        return request.render(page, values)\n\n    @http.route(['\/event\/<model(\"event.event\"):event>'], type='http', auth=\"public\", website=True)\n    def event(self, event, **post):\n        if event.menu_id and event.menu_id.child_id:\n            target_url = event.menu_id.child_id[0].url\n        else:\n            target_url = '\/event\/%s\/register' % str(event.id)\n        if post.get('enable_editor') == '1':\n            target_url += '?enable_editor=1'\n        return request.redirect(target_url)\n\n    @http.route(['\/event\/<model(\"event.event\"):event>\/register'], type='http', auth=\"public\", website=True)\n    def event_register(self, event, **post):\n        values = {\n            'event': event,\n            'main_object': event,\n            'range': range,\n        }\n        return request.render(\"website_event.event_description_full\", values)\n\n    @http.route('\/event\/add_event', type='http', auth=\"user\", methods=['POST'], website=True)\n    def add_event(self, event_name=\"New Event\", **kwargs):\n        event = self._add_event(event_name, request.context)\n        return request.redirect(\"\/event\/%s\/register?enable_editor=1\" % slug(event))\n\n    def _add_event(self, event_name=None, context=None, **kwargs):\n        if not event_name:\n            event_name = _(\"New Event\")\n        date_begin = datetime.today() + timedelta(days=(14))\n        vals = {\n            'name': event_name,\n            'date_begin': fields.Date.to_string(date_begin),\n            'date_end': fields.Date.to_string((date_begin + timedelta(days=(1)))),\n            'seats_available': 1000,\n        }\n        return request.env['event.event'].with_context(context or {}).create(vals)\n\n    def get_formated_date(self, event):\n        start_date = fields.Datetime.from_string(event.date_begin).date()\n        end_date = fields.Datetime.from_string(event.date_end).date()\n        month = babel.dates.get_month_names('abbreviated', locale=event.env.context.get('lang', 'en_US'))[start_date.month]\n        return ('%s %s%s') % (month, start_date.strftime(\"%e\"), (end_date != start_date and (\"-\" + end_date.strftime(\"%e\")) or \"\"))\n\n    @http.route('\/event\/get_country_event_list', type='http', auth='public', website=True)\n    def get_country_events(self, **post):\n        Event = request.env['event.event']\n        country_code = request.session['geoip'].get('country_code')\n        result = {'events': [], 'country': False}\n        events = None\n        if country_code:\n            country = request.env['res.country'].search([('code', '=', country_code)], limit=1)\n            events = Event.search(['|', ('address_id', '=', None), ('country_id.code', '=', country_code), ('date_begin', '>=', '%s 00:00:00' % fields.Date.today()), ('state', '=', 'confirm')], order=\"date_begin\")\n        if not events:\n            events = Event.search([('date_begin', '>=', '%s 00:00:00' % fields.Date.today()), ('state', '=', 'confirm')], order=\"date_begin\")\n        for event in events:\n            if country_code and event.country_id.code == country_code:\n                result['country'] = country\n            result['events'].append({\n                \"date\": self.get_formated_date(event),\n                \"event\": event,\n                \"url\": event.website_url})\n        return request.render(\"website_event.country_events_list\", result)\n\n    def _process_tickets_details(self, data):\n        nb_register = int(data.get('nb_register-0', 0))\n        if nb_register:\n            return [{'id': 0, 'name': 'Registration', 'quantity': nb_register, 'price': 0}]\n        return []\n\n    @http.route(['\/event\/<model(\"event.event\"):event>\/registration\/new'], type='json', auth=\"public\", methods=['POST'], website=True)\n    def registration_new(self, event, **post):\n        tickets = self._process_tickets_details(post)\n        if not tickets:\n            return request.redirect(\"\/event\/%s\" % slug(event))\n        return request.env['ir.ui.view'].render_template(\"website_event.registration_attendee_details\", {'tickets': tickets, 'event': event})\n\n    def _process_registration_details(self, details):\n        ''' Process data posted from the attendee details form. '''\n        registrations = {}\n        global_values = {}\n        for key, value in details.iteritems():\n            counter, field_name = key.split('-', 1)\n            if counter == '0':\n                global_values[field_name] = value\n            else:\n                registrations.setdefault(counter, dict())[field_name] = value\n        for key, value in global_values.iteritems():\n            for registration in registrations.values():\n                registration[key] = value\n        return registrations.values()\n\n    @http.route(['\/event\/<model(\"event.event\"):event>\/registration\/confirm'], type='http', auth=\"public\", methods=['POST'], website=True)\n    def registration_confirm(self, event, **post):\n        Attendees = request.env['event.registration']\n        registrations = self._process_registration_details(post)\n\n        for registration in registrations:\n            registration['event_id'] = event\n            Attendees += Attendees.sudo().create(\n                Attendees._prepare_attendee_values(registration))\n\n        return request.render(\"website_event.registration_complete\", {\n            'attendees': Attendees,\n            'event': event,\n        })\n","label":0}
{"content":"import sys\n\nfrom django import http\nfrom django.core import signals\nfrom django.utils.encoding import force_unicode\nfrom django.utils.importlib import import_module\n\nclass BaseHandler(object):\n    # Changes that are always applied to a response (in this order).\n    response_fixes = [\n        http.fix_location_header,\n        http.conditional_content_removal,\n        http.fix_IE_for_attach,\n        http.fix_IE_for_vary,\n    ]\n\n    def __init__(self):\n        self._request_middleware = self._view_middleware = self._response_middleware = self._exception_middleware = None\n\n    def load_middleware(self):\n        \"\"\"\n        Populate middleware lists from settings.MIDDLEWARE_CLASSES.\n\n        Must be called after the environment is fixed (see __call__).\n        \"\"\"\n        from django.conf import settings\n        from django.core import exceptions\n        self._view_middleware = []\n        self._response_middleware = []\n        self._exception_middleware = []\n\n        request_middleware = []\n        for middleware_path in settings.MIDDLEWARE_CLASSES:\n            try:\n                dot = middleware_path.rindex('.')\n            except ValueError:\n                raise exceptions.ImproperlyConfigured('%s isn\\'t a middleware module' % middleware_path)\n            mw_module, mw_classname = middleware_path[:dot], middleware_path[dot+1:]\n            try:\n                mod = import_module(mw_module)\n            except ImportError, e:\n                raise exceptions.ImproperlyConfigured('Error importing middleware %s: \"%s\"' % (mw_module, e))\n            try:\n                mw_class = getattr(mod, mw_classname)\n            except AttributeError:\n                raise exceptions.ImproperlyConfigured('Middleware module \"%s\" does not define a \"%s\" class' % (mw_module, mw_classname))\n\n            try:\n                mw_instance = mw_class()\n            except exceptions.MiddlewareNotUsed:\n                continue\n\n            if hasattr(mw_instance, 'process_request'):\n                request_middleware.append(mw_instance.process_request)\n            if hasattr(mw_instance, 'process_view'):\n                self._view_middleware.append(mw_instance.process_view)\n            if hasattr(mw_instance, 'process_response'):\n                self._response_middleware.insert(0, mw_instance.process_response)\n            if hasattr(mw_instance, 'process_exception'):\n                self._exception_middleware.insert(0, mw_instance.process_exception)\n\n        # We only assign to this when initialization is complete as it is used\n        # as a flag for initialization being complete.\n        self._request_middleware = request_middleware\n\n    def get_response(self, request):\n        \"Returns an HttpResponse object for the given HttpRequest\"\n        from django.core import exceptions, urlresolvers\n        from django.conf import settings\n\n        try:\n            try:\n                # Setup default url resolver for this thread.\n                urlconf = settings.ROOT_URLCONF\n                urlresolvers.set_urlconf(urlconf)\n                resolver = urlresolvers.RegexURLResolver(r'^\/', urlconf)\n\n                # Apply request middleware\n                for middleware_method in self._request_middleware:\n                    response = middleware_method(request)\n                    if response:\n                        return response\n\n                if hasattr(request, \"urlconf\"):\n                    # Reset url resolver with a custom urlconf.\n                    urlconf = request.urlconf\n                    urlresolvers.set_urlconf(urlconf)\n                    resolver = urlresolvers.RegexURLResolver(r'^\/', urlconf)\n\n                callback, callback_args, callback_kwargs = resolver.resolve(\n                        request.path_info)\n\n                # Apply view middleware\n                for middleware_method in self._view_middleware:\n                    response = middleware_method(request, callback, callback_args, callback_kwargs)\n                    if response:\n                        return response\n\n                try:\n                    response = callback(request, *callback_args, **callback_kwargs)\n                except Exception, e:\n                    # If the view raised an exception, run it through exception\n                    # middleware, and if the exception middleware returns a\n                    # response, use that. Otherwise, reraise the exception.\n                    for middleware_method in self._exception_middleware:\n                        response = middleware_method(request, e)\n                        if response:\n                            return response\n                    raise\n\n                # Complain if the view returned None (a common error).\n                if response is None:\n                    try:\n                        view_name = callback.func_name # If it's a function\n                    except AttributeError:\n                        view_name = callback.__class__.__name__ + '.__call__' # If it's a class\n                    raise ValueError(\"The view %s.%s didn't return an HttpResponse object.\" % (callback.__module__, view_name))\n\n                return response\n            except http.Http404, e:\n                if settings.DEBUG:\n                    from django.views import debug\n                    return debug.technical_404_response(request, e)\n                else:\n                    try:\n                        callback, param_dict = resolver.resolve404()\n                        return callback(request, **param_dict)\n                    except:\n                        try:\n                            return self.handle_uncaught_exception(request, resolver, sys.exc_info())\n                        finally:\n                            receivers = signals.got_request_exception.send(sender=self.__class__, request=request)\n            except exceptions.PermissionDenied:\n                return http.HttpResponseForbidden('<h1>Permission denied<\/h1>')\n            except SystemExit:\n                # Allow sys.exit() to actually exit. See tickets #1023 and #4701\n                raise\n            except: # Handle everything else, including SuspiciousOperation, etc.\n                # Get the exception info now, in case another exception is thrown later.\n                receivers = signals.got_request_exception.send(sender=self.__class__, request=request)\n                return self.handle_uncaught_exception(request, resolver, sys.exc_info())\n        finally:\n            # Reset URLconf for this thread on the way out for complete\n            # isolation of request.urlconf\n            urlresolvers.set_urlconf(None)\n\n    def handle_uncaught_exception(self, request, resolver, exc_info):\n        \"\"\"\n        Processing for any otherwise uncaught exceptions (those that will\n        generate HTTP 500 responses). Can be overridden by subclasses who want\n        customised 500 handling.\n\n        Be *very* careful when overriding this because the error could be\n        caused by anything, so assuming something like the database is always\n        available would be an error.\n        \"\"\"\n        from django.conf import settings\n        from django.core.mail import mail_admins\n\n        if settings.DEBUG_PROPAGATE_EXCEPTIONS:\n            raise\n\n        if settings.DEBUG:\n            from django.views import debug\n            return debug.technical_500_response(request, *exc_info)\n\n        # When DEBUG is False, send an error message to the admins.\n        subject = 'Error (%s IP): %s' % ((request.META.get('REMOTE_ADDR') in settings.INTERNAL_IPS and 'internal' or 'EXTERNAL'), request.path)\n        try:\n            request_repr = repr(request)\n        except:\n            request_repr = \"Request repr() unavailable\"\n        message = \"%s\\n\\n%s\" % (self._get_traceback(exc_info), request_repr)\n        mail_admins(subject, message, fail_silently=True)\n        # If Http500 handler is not installed, re-raise last exception\n        if resolver.urlconf_module is None:\n            raise exc_info[1], None, exc_info[2]\n        # Return an HttpResponse that displays a friendly error message.\n        callback, param_dict = resolver.resolve500()\n        return callback(request, **param_dict)\n\n    def _get_traceback(self, exc_info=None):\n        \"Helper function to return the traceback as a string\"\n        import traceback\n        return '\\n'.join(traceback.format_exception(*(exc_info or sys.exc_info())))\n\n    def apply_response_fixes(self, request, response):\n        \"\"\"\n        Applies each of the functions in self.response_fixes to the request and\n        response, modifying the response in the process. Returns the new\n        response.\n        \"\"\"\n        for func in self.response_fixes:\n            response = func(request, response)\n        return response\n\ndef get_script_name(environ):\n    \"\"\"\n    Returns the equivalent of the HTTP request's SCRIPT_NAME environment\n    variable. If Apache mod_rewrite has been used, returns what would have been\n    the script name prior to any rewriting (so it's the script name as seen\n    from the client's perspective), unless DJANGO_USE_POST_REWRITE is set (to\n    anything).\n    \"\"\"\n    from django.conf import settings\n    if settings.FORCE_SCRIPT_NAME is not None:\n        return force_unicode(settings.FORCE_SCRIPT_NAME)\n\n    # If Apache's mod_rewrite had a whack at the URL, Apache set either\n    # SCRIPT_URL or REDIRECT_URL to the full resource URL before applying any\n    # rewrites. Unfortunately not every Web server (lighttpd!) passes this\n    # information through all the time, so FORCE_SCRIPT_NAME, above, is still\n    # needed.\n    script_url = environ.get('SCRIPT_URL', u'')\n    if not script_url:\n        script_url = environ.get('REDIRECT_URL', u'')\n    if script_url:\n        return force_unicode(script_url[:-len(environ.get('PATH_INFO', ''))])\n    return force_unicode(environ.get('SCRIPT_NAME', u''))\n\n","label":0}
{"content":"\"\"\"\nThis file contains implementation override of SearchFilterGenerator which will allow\n    * Filter by all courses in which the user is enrolled in\n\"\"\"\nfrom microsite_configuration import microsite\n\nfrom student.models import CourseEnrollment\nfrom opaque_keys import InvalidKeyError\nfrom opaque_keys.edx.keys import CourseKey\nfrom opaque_keys.edx.locations import SlashSeparatedCourseKey\nfrom xmodule.modulestore.django import modulestore\n\nfrom search.filter_generator import SearchFilterGenerator\nfrom openedx.core.djangoapps.user_api.partition_schemes import RandomUserPartitionScheme\nfrom openedx.core.djangoapps.course_groups.partition_scheme import CohortPartitionScheme\nfrom courseware.access import get_user_role\n\n\nINCLUDE_SCHEMES = [CohortPartitionScheme, RandomUserPartitionScheme, ]\nSCHEME_SUPPORTS_ASSIGNMENT = [RandomUserPartitionScheme, ]\n\n\nclass LmsSearchFilterGenerator(SearchFilterGenerator):\n    \"\"\" SearchFilterGenerator for LMS Search \"\"\"\n\n    _user_enrollments = {}\n\n    def _enrollments_for_user(self, user):\n        \"\"\" Return the specified user's course enrollments \"\"\"\n        if user not in self._user_enrollments:\n            self._user_enrollments[user] = CourseEnrollment.enrollments_for_user(user)\n        return self._user_enrollments[user]\n\n    def filter_dictionary(self, **kwargs):\n        \"\"\" LMS implementation, adds filtering by user partition, course id and user \"\"\"\n\n        def get_group_for_user_partition(user_partition, course_key, user):\n            \"\"\" Returns the specified user's group for user partition \"\"\"\n            if user_partition.scheme in SCHEME_SUPPORTS_ASSIGNMENT:\n                return user_partition.scheme.get_group_for_user(\n                    course_key,\n                    user,\n                    user_partition,\n                    assign=False,\n                )\n            else:\n                return user_partition.scheme.get_group_for_user(\n                    course_key,\n                    user,\n                    user_partition,\n                )\n\n        def get_group_ids_for_user(course, user):\n            \"\"\" Collect user partition group ids for user for this course \"\"\"\n            partition_groups = []\n            for user_partition in course.user_partitions:\n                if user_partition.scheme in INCLUDE_SCHEMES:\n                    group = get_group_for_user_partition(user_partition, course.id, user)\n                    if group:\n                        partition_groups.append(group)\n            partition_group_ids = [unicode(partition_group.id) for partition_group in partition_groups]\n            return partition_group_ids if partition_group_ids else None\n\n        filter_dictionary = super(LmsSearchFilterGenerator, self).filter_dictionary(**kwargs)\n        if 'user' in kwargs:\n            user = kwargs['user']\n\n            if 'course_id' in kwargs and kwargs['course_id']:\n                try:\n                    course_key = CourseKey.from_string(kwargs['course_id'])\n                except InvalidKeyError:\n                    course_key = SlashSeparatedCourseKey.from_deprecated_string(kwargs['course_id'])\n\n                # Staff user looking at course as staff user\n                if get_user_role(user, course_key) in ('instructor', 'staff'):\n                    return filter_dictionary\n                # Need to check course exist (if course gets deleted enrollments don't get cleaned up)\n                course = modulestore().get_course(course_key)\n                if course:\n                    filter_dictionary['content_groups'] = get_group_ids_for_user(course, user)\n            else:\n                user_enrollments = self._enrollments_for_user(user)\n                content_groups = []\n                for enrollment in user_enrollments:\n                    course = modulestore().get_course(enrollment.course_id)\n                    if course:\n                        enrollment_group_ids = get_group_ids_for_user(course, user)\n                        if enrollment_group_ids:\n                            content_groups.extend(enrollment_group_ids)\n\n                filter_dictionary['content_groups'] = content_groups if content_groups else None\n\n        return filter_dictionary\n\n    def field_dictionary(self, **kwargs):\n        \"\"\" add course if provided otherwise add courses in which the user is enrolled in \"\"\"\n        field_dictionary = super(LmsSearchFilterGenerator, self).field_dictionary(**kwargs)\n        if not kwargs.get('user'):\n            field_dictionary['course'] = []\n        elif not kwargs.get('course_id'):\n            user_enrollments = self._enrollments_for_user(kwargs['user'])\n            field_dictionary['course'] = [unicode(enrollment.course_id) for enrollment in user_enrollments]\n\n        # if we have an org filter, only include results for this org filter\n        course_org_filter = microsite.get_value('course_org_filter')\n        if course_org_filter:\n            field_dictionary['org'] = course_org_filter\n\n        return field_dictionary\n\n    def exclude_dictionary(self, **kwargs):\n        \"\"\" If we are not on a microsite, then exclude any microsites that are defined \"\"\"\n        exclude_dictionary = super(LmsSearchFilterGenerator, self).exclude_dictionary(**kwargs)\n        course_org_filter = microsite.get_value('course_org_filter')\n        # If we have a course filter we are ensuring that we only get those courses above\n        if not course_org_filter:\n            org_filter_out_set = microsite.get_all_orgs()\n            if org_filter_out_set:\n                exclude_dictionary['org'] = list(org_filter_out_set)\n\n        return exclude_dictionary\n","label":0}
{"content":"# coding: utf-8\nfrom __future__ import unicode_literals\n\nfrom .common import InfoExtractor\nfrom ..utils import (\n    parse_iso8601,\n    int_or_none,\n)\n\n\nclass TwentyFourVideoIE(InfoExtractor):\n    IE_NAME = '24video'\n    _VALID_URL = r'https?:\/\/(?:www\\.)?24video\\.net\/(?:video\/(?:view|xml)\/|player\/new24_play\\.swf\\?id=)(?P<id>\\d+)'\n\n    _TESTS = [\n        {\n            'url': 'http:\/\/www.24video.net\/video\/view\/1044982',\n            'md5': '48dd7646775690a80447a8dca6a2df76',\n            'info_dict': {\n                'id': '1044982',\n                'ext': 'mp4',\n                'title': '\u042d\u0440\u043e\u0442\u0438\u043a\u0430 \u043a\u0430\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0432\u0435\u043a\u0430',\n                'description': '\u041a\u0430\u043a \u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u043f\u043e\u0440\u043d\u043e \u0432 \u043a\u0430\u043c\u0435\u043d\u043d\u043e\u043c \u0432\u0435\u043a\u0435.',\n                'thumbnail': 're:^https?:\/\/.*\\.jpg$',\n                'uploader': 'SUPERTELO',\n                'duration': 31,\n                'timestamp': 1275937857,\n                'upload_date': '20100607',\n                'age_limit': 18,\n                'like_count': int,\n                'dislike_count': int,\n            },\n        },\n        {\n            'url': 'http:\/\/www.24video.net\/player\/new24_play.swf?id=1044982',\n            'only_matching': True,\n        }\n    ]\n\n    def _real_extract(self, url):\n        video_id = self._match_id(url)\n\n        webpage = self._download_webpage(\n            'http:\/\/www.24video.net\/video\/view\/%s' % video_id, video_id)\n\n        title = self._og_search_title(webpage)\n        description = self._html_search_regex(\n            r'<span itemprop=\"description\">([^<]+)<\/span>', webpage, 'description', fatal=False)\n        thumbnail = self._og_search_thumbnail(webpage)\n        duration = int_or_none(self._og_search_property(\n            'duration', webpage, 'duration', fatal=False))\n        timestamp = parse_iso8601(self._search_regex(\n            r'<time id=\"video-timeago\" datetime=\"([^\"]+)\" itemprop=\"uploadDate\">',\n            webpage, 'upload date'))\n\n        uploader = self._html_search_regex(\n            r'\u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043b\\s*<a href=\"\/jsecUser\/movies\/[^\"]+\" class=\"link\">([^<]+)<\/a>',\n            webpage, 'uploader', fatal=False)\n\n        view_count = int_or_none(self._html_search_regex(\n            r'<span class=\"video-views\">(\\d+) \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440',\n            webpage, 'view count', fatal=False))\n        comment_count = int_or_none(self._html_search_regex(\n            r'<div class=\"comments-title\" id=\"comments-count\">(\\d+) \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438',\n            webpage, 'comment count', fatal=False))\n\n        formats = []\n\n        pc_video = self._download_xml(\n            'http:\/\/www.24video.net\/video\/xml\/%s?mode=play' % video_id,\n            video_id, 'Downloading PC video URL').find('.\/\/video')\n\n        formats.append({\n            'url': pc_video.attrib['url'],\n            'format_id': 'pc',\n            'quality': 1,\n        })\n\n        like_count = int_or_none(pc_video.get('ratingPlus'))\n        dislike_count = int_or_none(pc_video.get('ratingMinus'))\n        age_limit = 18 if pc_video.get('adult') == 'true' else 0\n\n        mobile_video = self._download_xml(\n            'http:\/\/www.24video.net\/video\/xml\/%s' % video_id,\n            video_id, 'Downloading mobile video URL').find('.\/\/video')\n\n        formats.append({\n            'url': mobile_video.attrib['url'],\n            'format_id': 'mobile',\n            'quality': 0,\n        })\n\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'title': title,\n            'description': description,\n            'thumbnail': thumbnail,\n            'uploader': uploader,\n            'duration': duration,\n            'timestamp': timestamp,\n            'view_count': view_count,\n            'comment_count': comment_count,\n            'like_count': like_count,\n            'dislike_count': dislike_count,\n            'age_limit': age_limit,\n            'formats': formats,\n        }\n","label":0}
{"content":"##\n# Copyright 2009-2014 Ghent University\n#\n# This file is part of EasyBuild,\n# originally created by the HPC team of Ghent University (http:\/\/ugent.be\/hpc\/en),\n# with support of Ghent University (http:\/\/ugent.be\/hpc),\n# the Flemish Supercomputer Centre (VSC) (https:\/\/vscentrum.be\/nl\/en),\n# the Hercules foundation (http:\/\/www.herculesstichting.be\/in_English)\n# and the Department of Economy, Science and Innovation (EWI) (http:\/\/www.ewi-vlaanderen.be\/en).\n#\n# http:\/\/github.com\/hpcugent\/easybuild\n#\n# EasyBuild is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation v2.\n#\n# EasyBuild is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with EasyBuild.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n##\n\"\"\"\nModule that takes control of versioning.\n\n@author: Stijn De Weirdt (Ghent University)\n@author: Dries Verdegem (Ghent University)\n@author: Kenneth Hoste (Ghent University)\n@author: Pieter De Baets (Ghent University)\n@author: Jens Timmerman (Ghent University)\n\"\"\"\nimport os\nfrom distutils.version import LooseVersion\nfrom socket import gethostname\n\n# note: release candidates should be versioned as a pre-release, e.g. \"1.1rc1\"\n# 1.1-rc1 would indicate a post-release, i.e., and update of 1.1, so beware!\nVERSION = LooseVersion(\"1.14.0\")\nUNKNOWN = \"UNKNOWN\"\n\ndef get_git_revision():\n    \"\"\"\n    Returns the git revision (e.g. aab4afc016b742c6d4b157427e192942d0e131fe),\n    or UNKNOWN is getting the git revision fails\n\n    relies on GitPython (see http:\/\/gitorious.org\/git-python)\n    \"\"\"\n    try:\n        import git\n    except ImportError:\n        return UNKNOWN\n    try:\n        path = os.path.dirname(__file__)\n        gitrepo = git.Git(path)\n        return gitrepo.rev_list(\"HEAD\").splitlines()[0]\n    except git.GitCommandError:\n        return UNKNOWN\n\ngit_rev = get_git_revision()\nif git_rev == UNKNOWN:\n    VERBOSE_VERSION = VERSION\nelse:\n    VERBOSE_VERSION = LooseVersion(\"%s-r%s\" % (VERSION, get_git_revision()))\n\n# alias\nFRAMEWORK_VERSION = VERBOSE_VERSION\n\n# EasyBlock version\ntry:\n    from easybuild.easyblocks import VERBOSE_VERSION as EASYBLOCKS_VERSION\nexcept:\n    EASYBLOCKS_VERSION = '0.0.UNKNOWN.EASYBLOCKS'  # make sure it is smaller then anything\n\ndef this_is_easybuild():\n    \"\"\"Standard starting message\"\"\"\n    top_version = max(FRAMEWORK_VERSION, EASYBLOCKS_VERSION)\n    # !!! bootstrap_eb.py script checks hard on the string below, so adjust with sufficient care !!!\n    msg = \"This is EasyBuild %s (framework: %s, easyblocks: %s) on host %s.\" \\\n         % (top_version, FRAMEWORK_VERSION, EASYBLOCKS_VERSION, gethostname())\n\n    return msg\n","label":0}
{"content":"#!\/usr\/bin\/env python\n#\n# rt-mutex tester\n#\n# (C) 2006 Thomas Gleixner <tglx@linutronix.de>\n#\n# This program is free software; you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License version 2 as\n# published by the Free Software Foundation.\n#\nimport os\nimport sys\nimport getopt\nimport shutil\nimport string\n\n# Globals\nquiet = 0\ntest = 0\ncomments = 0\n\nsysfsprefix = \"\/sys\/devices\/system\/rttest\/rttest\"\nstatusfile = \"\/status\"\ncommandfile = \"\/command\"\n\n# Command opcodes\ncmd_opcodes = {\n    \"schedother\"    : \"1\",\n    \"schedfifo\"     : \"2\",\n    \"lock\"          : \"3\",\n    \"locknowait\"    : \"4\",\n    \"lockint\"       : \"5\",\n    \"lockintnowait\" : \"6\",\n    \"lockcont\"      : \"7\",\n    \"unlock\"        : \"8\",\n    \"lockbkl\"       : \"9\",\n    \"unlockbkl\"     : \"10\",\n    \"signal\"        : \"11\",\n    \"resetevent\"    : \"98\",\n    \"reset\"         : \"99\",\n    }\n\ntest_opcodes = {\n    \"prioeq\"        : [\"P\" , \"eq\" , None],\n    \"priolt\"        : [\"P\" , \"lt\" , None],\n    \"priogt\"        : [\"P\" , \"gt\" , None],\n    \"nprioeq\"       : [\"N\" , \"eq\" , None],\n    \"npriolt\"       : [\"N\" , \"lt\" , None],\n    \"npriogt\"       : [\"N\" , \"gt\" , None],\n    \"unlocked\"      : [\"M\" , \"eq\" , 0],\n    \"trylock\"       : [\"M\" , \"eq\" , 1],\n    \"blocked\"       : [\"M\" , \"eq\" , 2],\n    \"blockedwake\"   : [\"M\" , \"eq\" , 3],\n    \"locked\"        : [\"M\" , \"eq\" , 4],\n    \"opcodeeq\"      : [\"O\" , \"eq\" , None],\n    \"opcodelt\"      : [\"O\" , \"lt\" , None],\n    \"opcodegt\"      : [\"O\" , \"gt\" , None],\n    \"eventeq\"       : [\"E\" , \"eq\" , None],\n    \"eventlt\"       : [\"E\" , \"lt\" , None],\n    \"eventgt\"       : [\"E\" , \"gt\" , None],\n    }\n\n# Print usage information\ndef usage():\n    print \"rt-tester.py <-c -h -q -t> <testfile>\"\n    print \" -c    display comments after first command\"\n    print \" -h    help\"\n    print \" -q    quiet mode\"\n    print \" -t    test mode (syntax check)\"\n    print \" testfile: read test specification from testfile\"\n    print \" otherwise from stdin\"\n    return\n\n# Print progress when not in quiet mode\ndef progress(str):\n    if not quiet:\n        print str\n\n# Analyse a status value\ndef analyse(val, top, arg):\n\n    intval = int(val)\n\n    if top[0] == \"M\":\n        intval = intval \/ (10 ** int(arg))\n\tintval = intval % 10\n        argval = top[2]\n    elif top[0] == \"O\":\n        argval = int(cmd_opcodes.get(arg, arg))\n    else:\n        argval = int(arg)\n\n    # progress(\"%d %s %d\" %(intval, top[1], argval))\n\n    if top[1] == \"eq\" and intval == argval:\n\treturn 1\n    if top[1] == \"lt\" and intval < argval:\n        return 1\n    if top[1] == \"gt\" and intval > argval:\n\treturn 1\n    return 0\n\n# Parse the commandline\ntry:\n    (options, arguments) = getopt.getopt(sys.argv[1:],'chqt')\nexcept getopt.GetoptError, ex:\n    usage()\n    sys.exit(1)\n\n# Parse commandline options\nfor option, value in options:\n    if option == \"-c\":\n        comments = 1\n    elif option == \"-q\":\n        quiet = 1\n    elif option == \"-t\":\n        test = 1\n    elif option == '-h':\n        usage()\n        sys.exit(0)\n\n# Select the input source\nif arguments:\n    try:\n        fd = open(arguments[0])\n    except Exception,ex:\n        sys.stderr.write(\"File not found %s\\n\" %(arguments[0]))\n        sys.exit(1)\nelse:\n    fd = sys.stdin\n\nlinenr = 0\n\n# Read the test patterns\nwhile 1:\n\n    linenr = linenr + 1\n    line = fd.readline()\n    if not len(line):\n        break\n\n    line = line.strip()\n    parts = line.split(\":\")\n\n    if not parts or len(parts) < 1:\n        continue\n\n    if len(parts[0]) == 0:\n        continue\n\n    if parts[0].startswith(\"#\"):\n\tif comments > 1:\n\t    progress(line)\n\tcontinue\n\n    if comments == 1:\n\tcomments = 2\n\n    progress(line)\n\n    cmd = parts[0].strip().lower()\n    opc = parts[1].strip().lower()\n    tid = parts[2].strip()\n    dat = parts[3].strip()\n\n    try:\n        # Test or wait for a status value\n        if cmd == \"t\" or cmd == \"w\":\n            testop = test_opcodes[opc]\n\n            fname = \"%s%s%s\" %(sysfsprefix, tid, statusfile)\n            if test:\n\t\tprint fname\n                continue\n\n            while 1:\n                query = 1\n                fsta = open(fname, 'r')\n                status = fsta.readline().strip()\n                fsta.close()\n                stat = status.split(\",\")\n                for s in stat:\n\t\t    s = s.strip()\n                    if s.startswith(testop[0]):\n                        # Seperate status value\n                        val = s[2:].strip()\n                        query = analyse(val, testop, dat)\n                        break\n                if query or cmd == \"t\":\n                    break\n\n            progress(\"   \" + status)\n\n            if not query:\n                sys.stderr.write(\"Test failed in line %d\\n\" %(linenr))\n\t\tsys.exit(1)\n\n        # Issue a command to the tester\n        elif cmd == \"c\":\n            cmdnr = cmd_opcodes[opc]\n            # Build command string and sys filename\n            cmdstr = \"%s:%s\" %(cmdnr, dat)\n            fname = \"%s%s%s\" %(sysfsprefix, tid, commandfile)\n            if test:\n\t\tprint fname\n                continue\n            fcmd = open(fname, 'w')\n            fcmd.write(cmdstr)\n            fcmd.close()\n\n    except Exception,ex:\n    \tsys.stderr.write(str(ex))\n        sys.stderr.write(\"\\nSyntax error in line %d\\n\" %(linenr))\n        if not test:\n            fd.close()\n            sys.exit(1)\n\n# Normal exit pass\nprint \"Pass\"\nsys.exit(0)\n\n\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n#\n# Copyright: (c) 2017, F5 Networks Inc.\n# GNU General Public License v3.0 (see COPYING or https:\/\/www.gnu.org\/licenses\/gpl-3.0.txt)\n\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nimport os\nimport json\nimport pytest\nimport sys\n\nif sys.version_info < (2, 7):\n    pytestmark = pytest.mark.skip(\"F5 Ansible modules require Python >= 2.7\")\n\nfrom ansible.module_utils.basic import AnsibleModule\n\ntry:\n    from library.modules.bigip_service_policy import ApiParameters\n    from library.modules.bigip_service_policy import ModuleParameters\n    from library.modules.bigip_service_policy import ModuleManager\n    from library.modules.bigip_service_policy import ArgumentSpec\n\n    # In Ansible 2.8, Ansible changed import paths.\n    from test.units.compat import unittest\n    from test.units.compat.mock import Mock\n    from test.units.compat.mock import patch\n\n    from test.units.modules.utils import set_module_args\nexcept ImportError:\n    from ansible.modules.network.f5.bigip_service_policy import ApiParameters\n    from ansible.modules.network.f5.bigip_service_policy import ModuleParameters\n    from ansible.modules.network.f5.bigip_service_policy import ModuleManager\n    from ansible.modules.network.f5.bigip_service_policy import ArgumentSpec\n\n    # Ansible 2.8 imports\n    from units.compat import unittest\n    from units.compat.mock import Mock\n    from units.compat.mock import patch\n\n    from units.modules.utils import set_module_args\n\n\nfixture_path = os.path.join(os.path.dirname(__file__), 'fixtures')\nfixture_data = {}\n\n\ndef load_fixture(name):\n    path = os.path.join(fixture_path, name)\n\n    if path in fixture_data:\n        return fixture_data[path]\n\n    with open(path) as f:\n        data = f.read()\n\n    try:\n        data = json.loads(data)\n    except Exception:\n        pass\n\n    fixture_data[path] = data\n    return data\n\n\nclass TestParameters(unittest.TestCase):\n    def test_module_parameters(self):\n        args = dict(\n            name='foo',\n            description='my description',\n            timer_policy='timer1',\n            port_misuse_policy='misuse1',\n        )\n\n        p = ModuleParameters(params=args)\n        assert p.name == 'foo'\n        assert p.description == 'my description'\n        assert p.timer_policy == '\/Common\/timer1'\n        assert p.port_misuse_policy == '\/Common\/misuse1'\n\n    def test_api_parameters(self):\n        args = load_fixture('load_net_service_policy_1.json')\n        p = ApiParameters(params=args)\n        assert p.name == 'baz'\n        assert p.description == 'my description'\n        assert p.timer_policy == '\/Common\/foo'\n        assert p.port_misuse_policy == '\/Common\/bar'\n\n\nclass TestManager(unittest.TestCase):\n\n    def setUp(self):\n        self.spec = ArgumentSpec()\n        try:\n            self.p1 = patch('library.modules.bigip_service_policy.module_provisioned')\n            self.m1 = self.p1.start()\n            self.m1.return_value = True\n        except Exception:\n            self.p1 = patch('ansible.modules.network.f5.bigip_service_policy.module_provisioned')\n            self.m1 = self.p1.start()\n            self.m1.return_value = True\n\n    def test_create_selfip(self, *args):\n        set_module_args(dict(\n            name='foo',\n            description='my description',\n            timer_policy='timer1',\n            port_misuse_policy='misuse1',\n            partition='Common',\n            state='present',\n            provider=dict(\n                server='localhost',\n                password='password',\n                user='admin'\n            )\n        ))\n\n        module = AnsibleModule(\n            argument_spec=self.spec.argument_spec,\n            supports_check_mode=self.spec.supports_check_mode\n        )\n        mm = ModuleManager(module=module)\n\n        # Override methods to force specific logic in the module to happen\n        mm.exists = Mock(side_effect=[False, True])\n        mm.create_on_device = Mock(return_value=True)\n        mm.module_provisioned = Mock(return_value=True)\n\n        results = mm.exec_module()\n\n        assert results['changed'] is True\n","label":0}
{"content":"# -*- encoding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#\n#    Copyright (c) 2011 Noviat nv\/sa (www.noviat.be). All rights reserved.\n#\n#    This program is free software: you can redistribute it and\/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#\n##############################################################################\n\nimport time\nfrom osv import osv, fields\nimport decimal_precision as dp\nimport netsvc\nfrom tools.translate import _\n\nclass account_bank_statement(osv.osv):\n    _inherit = 'account.bank.statement'\n\n    def write(self, cr, uid, ids, vals, context=None):\n        if context is None:\n            context = {}\n        # bypass obsolete statement line resequencing\n        if vals.get('line_ids', False) or context.get('ebanking_import', False):\n            res = super(osv.osv, self).write(cr, uid, ids, vals, context=context)\n        else: \n            res = super(account_bank_statement, self).write(cr, uid, ids, vals, context=context)\n        return res\n\n    def button_confirm_bank(self, cr, uid, ids, context=None):\n        super(account_bank_statement, self).button_confirm_bank(cr, uid, ids, context=context)\n        for st in self.browse(cr, uid, ids, context=context):\n            cr.execute(\"UPDATE account_bank_statement_line  \\\n                SET state='confirm' WHERE id in %s \",\n                (tuple([x.id for x in st.line_ids]),))\n        return True\n\n    def button_cancel(self, cr, uid, ids, context=None):\n        super(account_bank_statement, self).button_cancel(cr, uid, ids, context=context)\n        for st in self.browse(cr, uid, ids, context=context):\n            if st.line_ids:\n                cr.execute(\"UPDATE account_bank_statement_line  \\\n                    SET state='draft' WHERE id in %s \",\n                    (tuple([x.id for x in st.line_ids]),))\n        return True\n\naccount_bank_statement()\n\nclass account_bank_statement_line_global(osv.osv):\n    _name = 'account.bank.statement.line.global'\n    _description = 'Batch Payment Info'\n\n    _columns = {\n        'name': fields.char('Communication', size=128, required=True),\n        'code': fields.char('Code', size=64, required=True),\n        'parent_id': fields.many2one('account.bank.statement.line.global', 'Parent Code', ondelete='cascade'),\n        'child_ids': fields.one2many('account.bank.statement.line.global', 'parent_id', 'Child Codes'),\n        'type': fields.selection([\n            ('iso20022', 'ISO 20022'),\n            ('coda', 'CODA'),\n            ('manual', 'Manual'), \n            ], 'Type', required=True),\n        'amount': fields.float('Amount', digits_compute=dp.get_precision('Account')),\n        'bank_statement_line_ids': fields.one2many('account.bank.statement.line', 'globalisation_id', 'Bank Statement Lines'),\n    }\n    _rec_name = 'code'\n    _defaults = {\n        'code': lambda s,c,u,ctx={}: s.pool.get('ir.sequence').get(c, u, 'account.bank.statement.line.global'),\n        'name': '\/',\n    }\n    _sql_constraints = [\n        ('code_uniq', 'unique (code)', 'The code must be unique !'),\n    ]\n\n    def name_search(self, cr, user, name, args=None, operator='ilike', context=None, limit=100):\n        if not args:\n            args = []\n        ids = []\n        if name:\n            ids = self.search(cr, user, [('code', 'ilike', name)] + args, limit=limit)\n            if not ids:\n                ids = self.search(cr, user, [('name', operator, name)] + args, limit=limit)\n            if not ids and len(name.split()) >= 2:\n                #Separating code and name for searching\n                operand1, operand2 = name.split(' ', 1) #name can contain spaces\n                ids = self.search(cr, user, [('code', 'like', operand1), ('name', operator, operand2)] + args, limit=limit)\n        else:\n            ids = self.search(cr, user, args, context=context, limit=limit)\n        return self.name_get(cr, user, ids, context=context)\n\naccount_bank_statement_line_global()\n\nclass account_bank_statement_line(osv.osv):\n    _inherit = 'account.bank.statement.line'\n    _columns = {\n        'date': fields.date('Entry Date', required=True, states={'confirm': [('readonly', True)]}),\n        'val_date': fields.date('Valuta Date', states={'confirm': [('readonly', True)]}),\n        'globalisation_id': fields.many2one('account.bank.statement.line.global', 'Globalisation ID',\n            states={'confirm': [('readonly', True)]}, \n            help=\"Code to identify transactions belonging to the same globalisation level within a batch payment\"),\n        'globalisation_amount': fields.related('globalisation_id', 'amount', type='float',\n            relation='account.bank.statement.line.global', string='Glob. Amount', readonly=True),\n        'journal_id': fields.related('statement_id', 'journal_id', type='many2one', relation='account.journal', string='Journal', store=True, readonly=True),\n        'state': fields.selection([('draft', 'Draft'), ('confirm', 'Confirmed')],\n            'State', required=True, readonly=True),    \n        'counterparty_name': fields.char('Counterparty Name', size=35),\n        'counterparty_bic': fields.char('Counterparty BIC', size=11),\n        'counterparty_number': fields.char('Counterparty Number', size=34),\n        'counterparty_currency': fields.char('Counterparty Currency', size=3),\n    }\n    _defaults = {\n        'state': 'draft',\n    }\n\n    def unlink(self, cr, uid, ids, context=None):\n        if context is None:\n            context = {}\n        if context.get('block_statement_line_delete', False):\n            raise osv.except_osv(_('Warning'), _('Delete operation not allowed ! \\\n            Please go to the associated bank statement in order to delete and\/or modify this bank statement line'))\n        return super(account_bank_statement_line, self).unlink(cr, uid, ids, context=context)\n\naccount_bank_statement_line()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:","label":0}
{"content":"\n\nimport requests\nimport json\nimport numpy as np\nimport pandas as pd\nimport CoHouseToken\nfrom difflib import SequenceMatcher\n\n\n\n# In[3]:\n\n\ndef exactMatch(line1, line2):\n    line1=line1.upper().rstrip()    \n    line2=line2.upper().rstrip()\n    #print(\"|\"+line1+\"|\"+line2+\"|\",line1==line2)\n    return line1==line2\n\n\n\n\ndef aStopWord(word):\n    return word.upper().replace(\"COMPANY\",\"CO\").replace(\"LIMITED\",\"LTD\").replace(\"&\",\"AND\").rstrip() \n\n\n\n\n\ndef spaces(word):\n    w = word.upper().replace(\"\/\",\" \")\n    w = w.replace(\".\",\" \").replace(\",\",\" \").replace(\"-\",\" \").rstrip() \n    return w\n\n\n\n\n\ndef removeAStopWord(word):\n    w = word.upper().replace(\"LTD\",\" \").replace(\"CO\",\" \").replace(\"AND\",\" \").replace(\"(\",\" \").replace(\"\/\",\" \")\n    w = w.replace(\")\",\" \").replace(\".\",\" \").replace(\",\",\" \").replace(\"-\",\" \").rstrip() \n    return w\n\n\n\n\n\ndef removeABlank(word):\n    w = word.replace(\" \",\"\")\n    return w\n\n\n\n\n\ndef removeABracket (line):\n    flag = False\n    word=\"\"\n    for a in line:\n        if a==\"(\":\n            flag = True\n            a=\"\"\n        if a==\")\":\n            a=\"\"\n            flag = False\n        if flag:\n            a=\"\"\n        word+=a\n    return word\n    \n\n\n\n\n\ndef stopWord(line1, line2):\n    line1=aStopWord(line1)  \n    line2=aStopWord(line2)\n    #print(\"|\"+line1+\"|\"+line2+\"|\",line1==line2)\n    return line1==line2\n\n\n\n\n\ndef removeStopWord(line1, line2):\n    line1=spaces(line1)  \n    line2=spaces(line2)\n    line1=aStopWord(line1)  \n    line2=aStopWord(line2)\n    line1=removeAStopWord(line1)  \n    line2=removeAStopWord(line2)\n    #print(\"|\"+line1+\"|\"+line2+\"|\",line1==line2)\n    return line1==line2\n\n\n\n\n\ndef removeBlanks(line1, line2):\n    line1=spaces(line1)  \n    line2=spaces(line2)\n    line1=aStopWord(line1)  \n    line2=aStopWord(line2)\n    line1=removeAStopWord(line1)  \n    line2=removeAStopWord(line2)\n    line1=removeABlank(line1)  \n    line2=removeABlank(line2)\n    return line1==line2\n\n\n\n\n\ndef removeBrackets(line1, line2):\n    line1=removeABracket(line1)  \n    line2=removeABracket(line2)\n    line1=spaces(line1)  \n    line2=spaces(line2)\n    line1=aStopWord(line1)  \n    line2=aStopWord(line2)\n    line1=removeAStopWord(line1)  \n    line2=removeAStopWord(line2)\n    line1=removeABlank(line1)  \n    line2=removeABlank(line2)\n   #print(\"|\"+line1+\"|\"+line2+\"|\",line1==line2)\n    \n    return line1==line2\n\n\n\n\n\ndef strip(line1, line2):\n    line1=removeABracket(line1)  \n    line2=removeABracket(line2)\n    line1=spaces(line1)  \n    line2=spaces(line2)\n    line1=aStopWord(line1)  \n    line2=aStopWord(line2)\n    line1=removeAStopWord(line1)  \n    line2=removeAStopWord(line2)\n    line1=removeABlank(line1)  \n    line2=removeABlank(line2)\n    \n    return line1,line2\n\n\n\n\n\ndef match(company,results):\n    for i in results['items']:\n        line = i['title']\n        number = i['company_number']\n        if(exactMatch(company,line)):\n            return True,line,number\n            \n    for i in results['items']:\n        line = i['title']\n        number = i['company_number']\n        if(stopWord(company,line)):\n            return True,line,number\n            \n    for i in results['items']:\n        line = i['title']\n        number = i['company_number']\n        if(removeStopWord(company,line)):\n            return True,line,number\n            \n    for i in results['items']:\n        line = i['title']\n        number = i['company_number']\n        if(removeBlanks(company,line)):\n            return True,line,number\n            \n    for i in results['items']:\n        line = i['title']\n        number = i['company_number']\n        if(removeBrackets(company,line)):\n            return True,line,number\n        \n        #old_match(company,results)\n    return False,\"\",\"\"\n\n\n\n\ndef main(args):\n    print(args[0])\n    search_url =\"https:\/\/api.companieshouse.gov.uk\/search\/companies?q=\"\n    token = CoHouseToken.getToken()\n    pw = ''\n    base_url = 'https:\/\/api.companieshouse.gov.uk'\n    file = args[1]\n    print(file)\n    df = pd.read_csv(file,names=['Organisation'])\n    companies = df.Organisation\n    count=0\n    found = open(\"found2.csv\",'w')\n    missing = open(\"missing2.csv\",'w')\n\n    for c in companies:\n        c =c.upper().replace(\"&\",\"AND\")\n        c = c.split(\" T\/A \")[0]\n        c = c.split(\"WAS \")[0]\n        c= spaces(c)\n        url=search_url+c\n        results = json.loads(requests.get(url, auth=(token,pw)).text)\n        for i , key  in enumerate(results['items']):\n            a,b = strip(c, key['title'])\n            r = SequenceMatcher(None, a, b).ratio()\n            print(\"%s \\t %s\\t %.2f \\t %s \\t %s\"%(i,c,r,key['company_number'],key['title']))\n        \n        v = input('type number or return to reject: ')\n        if v ==\"\":\n            print(\"reject\")\n            missing.write(\"%s\\n\"%(c))\n        else:\n            key = results['items'][int(v)]\n            print(\"%s \\t %s\\t %.2f \\t %s \\t %s\"%(v,c,r,key['company_number'],key['title']))\n            print(\"*************************\")\n            found.write(\"%s,%s,%s,\\n\"%(c,key['title'],key['company_number']))\n        \n            \n    print()\n    #print(count\/len(companies))\n\n\n\n\n    return 0\n\nif __name__ == '__main__':\n    import sys\n    sys.exit(main(sys.argv))\n\n\n\n\n\n","label":0}
{"content":"#!\/usr\/bin\/env python\n\"\"\"html2text: Turn HTML into equivalent Markdown-structured text.\"\"\"\n__version__ = \"2.36\"\n__author__ = \"Aaron Swartz (me@aaronsw.com)\"\n__copyright__ = \"(C) 2004-2008 Aaron Swartz. GNU GPL 3.\"\n__contributors__ = [\"Martin 'Joey' Schulze\", \"Ricardo Reyes\", \"Kevin Jay North\"]\n\n# TODO:\n#   Support decoded entities with unifiable.\n\nif not hasattr(__builtins__, 'True'): True, False = 1, 0\nimport re, sys, urllib, htmlentitydefs, codecs\nimport sgmllib\nimport urlparse\nsgmllib.charref = re.compile('&#([xX]?[0-9a-fA-F]+)[^0-9a-fA-F]')\n\ntry: from textwrap import wrap\nexcept: pass\n\n# Use Unicode characters instead of their ascii psuedo-replacements\nUNICODE_SNOB = 0\n\n# Put the links after each paragraph instead of at the end.\nLINKS_EACH_PARAGRAPH = 0\n\n# Wrap long lines at position. 0 for no wrapping. (Requires Python 2.3.)\nBODY_WIDTH = 78\n\n# Don't show internal links (href=\"#local-anchor\") -- corresponding link targets\n# won't be visible in the plain text file anyway.\nSKIP_INTERNAL_LINKS = False\n\n### Entity Nonsense ###\n\ndef name2cp(k):\n    if k == 'apos': return ord(\"'\")\n    if hasattr(htmlentitydefs, \"name2codepoint\"): # requires Python 2.3\n        return htmlentitydefs.name2codepoint[k]\n    else:\n        k = htmlentitydefs.entitydefs[k]\n        if k.startswith(\"&#\") and k.endswith(\";\"): return int(k[2:-1]) # not in latin-1\n        return ord(codecs.latin_1_decode(k)[0])\n\nunifiable = {'rsquo':\"'\", 'lsquo':\"'\", 'rdquo':'\"', 'ldquo':'\"',\n'copy':'(C)', 'mdash':'--', 'nbsp':' ', 'rarr':'->', 'larr':'<-', 'middot':'*',\n'ndash':'-', 'oelig':'oe', 'aelig':'ae',\n'agrave':'a', 'aacute':'a', 'acirc':'a', 'atilde':'a', 'auml':'a', 'aring':'a',\n'egrave':'e', 'eacute':'e', 'ecirc':'e', 'euml':'e',\n'igrave':'i', 'iacute':'i', 'icirc':'i', 'iuml':'i',\n'ograve':'o', 'oacute':'o', 'ocirc':'o', 'otilde':'o', 'ouml':'o',\n'ugrave':'u', 'uacute':'u', 'ucirc':'u', 'uuml':'u'}\n\nunifiable_n = {}\n\nfor k in unifiable.keys():\n    unifiable_n[name2cp(k)] = unifiable[k]\n\ndef charref(name):\n    if name[0] in ['x','X']:\n        c = int(name[1:], 16)\n    else:\n        c = int(name)\n\n    if not UNICODE_SNOB and c in unifiable_n.keys():\n        return unifiable_n[c]\n    else:\n        return unichr(c)\n\ndef entityref(c):\n    if not UNICODE_SNOB and c in unifiable.keys():\n        return unifiable[c]\n    else:\n        try: name2cp(c)\n        except KeyError: return \"&\" + c\n        else: return unichr(name2cp(c))\n\ndef replaceEntities(s):\n    s = s.group(1)\n    if s[0] == \"#\":\n        return charref(s[1:])\n    else: return entityref(s)\n\nr_unescape = re.compile(r\"&(#?[xX]?(?:[0-9a-fA-F]+|\\w{1,8}));\")\ndef unescape(s):\n    return r_unescape.sub(replaceEntities, s)\n\ndef fixattrs(attrs):\n    # Fix bug in sgmllib.py\n    if not attrs: return attrs\n    newattrs = []\n    for attr in attrs:\n        newattrs.append((attr[0], unescape(attr[1])))\n    return newattrs\n\n### End Entity Nonsense ###\n\ndef onlywhite(line):\n    \"\"\"Return true if the line does only consist of whitespace characters.\"\"\"\n    for c in line:\n        if c is not ' ' and c is not '  ':\n            return c is ' '\n    return line\n\ndef optwrap(text):\n    \"\"\"Wrap all paragraphs in the provided text.\"\"\"\n    if not BODY_WIDTH:\n        return text\n\n    assert wrap, \"Requires Python 2.3.\"\n    result = ''\n    newlines = 0\n    for para in text.split(\"\\n\"):\n        if len(para) > 0:\n            if para[0] is not ' ' and para[0] is not '-' and para[0] is not '*':\n                for line in wrap(para, BODY_WIDTH):\n                    result += line + \"\\n\"\n                result += \"\\n\"\n                newlines = 2\n            else:\n                if not onlywhite(para):\n                    result += para + \"\\n\"\n                    newlines = 1\n        else:\n            if newlines < 2:\n                result += \"\\n\"\n                newlines += 1\n    return result\n\ndef hn(tag):\n    if tag[0] == 'h' and len(tag) == 2:\n        try:\n            n = int(tag[1])\n            if n in range(1, 10): return n\n        except ValueError: return 0\n\nclass _html2text(sgmllib.SGMLParser):\n    def __init__(self, out=sys.stdout.write, baseurl=''):\n        sgmllib.SGMLParser.__init__(self)\n\n        if out is None: self.out = self.outtextf\n        else: self.out = out\n        self.outtext = u''\n        self.quiet = 0\n        self.p_p = 0\n        self.outcount = 0\n        self.start = 1\n        self.space = 0\n        self.a = []\n        self.astack = []\n        self.acount = 0\n        self.list = []\n        self.blockquote = 0\n        self.pre = 0\n        self.startpre = 0\n        self.lastWasNL = 0\n        self.abbr_title = None # current abbreviation definition\n        self.abbr_data = None # last inner HTML (for abbr being defined)\n        self.abbr_list = {} # stack of abbreviations to write later\n        self.baseurl = baseurl\n\n    def outtextf(self, s):\n        self.outtext += s\n\n    def close(self):\n        sgmllib.SGMLParser.close(self)\n\n        self.pbr()\n        self.o('', 0, 'end')\n\n        return self.outtext\n\n    def handle_charref(self, c):\n        self.o(charref(c))\n\n    def handle_entityref(self, c):\n        self.o(entityref(c))\n\n    def unknown_starttag(self, tag, attrs):\n        self.handle_tag(tag, attrs, 1)\n\n    def unknown_endtag(self, tag):\n        self.handle_tag(tag, None, 0)\n\n    def previousIndex(self, attrs):\n        \"\"\" returns the index of certain set of attributes (of a link) in the\n            self.a list\n\n            If the set of attributes is not found, returns None\n        \"\"\"\n        if not attrs.has_key('href'): return None\n\n        i = -1\n        for a in self.a:\n            i += 1\n            match = 0\n\n            if a.has_key('href') and a['href'] == attrs['href']:\n                if a.has_key('title') or attrs.has_key('title'):\n                        if (a.has_key('title') and attrs.has_key('title') and\n                            a['title'] == attrs['title']):\n                            match = True\n                else:\n                    match = True\n\n            if match: return i\n\n    def handle_tag(self, tag, attrs, start):\n        attrs = fixattrs(attrs)\n\n        if hn(tag):\n            self.p()\n            if start: self.o(hn(tag)*\"#\" + ' ')\n\n        if tag in ['p', 'div']: self.p()\n\n        if tag == \"br\" and start: self.o(\"  \\n\")\n\n        if tag == \"hr\" and start:\n            self.p()\n            self.o(\"* * *\")\n            self.p()\n\n        if tag in [\"head\", \"style\", 'script']:\n            if start: self.quiet += 1\n            else: self.quiet -= 1\n\n        if tag in [\"body\"]:\n            self.quiet = 0 # sites like 9rules.com never close <head>\n\n        if tag == \"blockquote\":\n            if start:\n                self.p(); self.o('> ', 0, 1); self.start = 1\n                self.blockquote += 1\n            else:\n                self.blockquote -= 1\n                self.p()\n\n        if tag in ['em', 'i', 'u']: self.o(\"_\")\n        if tag in ['strong', 'b']: self.o(\"**\")\n        if tag == \"code\" and not self.pre: self.o('`') #TODO: `` `this` ``\n        if tag == \"abbr\":\n            if start:\n                attrsD = {}\n                for (x, y) in attrs: attrsD[x] = y\n                attrs = attrsD\n\n                self.abbr_title = None\n                self.abbr_data = ''\n                if attrs.has_key('title'):\n                    self.abbr_title = attrs['title']\n            else:\n                if self.abbr_title != None:\n                    self.abbr_list[self.abbr_data] = self.abbr_title\n                    self.abbr_title = None\n                self.abbr_data = ''\n\n        if tag == \"a\":\n            if start:\n                attrsD = {}\n                for (x, y) in attrs: attrsD[x] = y\n                attrs = attrsD\n                if attrs.has_key('href') and not (SKIP_INTERNAL_LINKS and attrs['href'].startswith('#')):\n                    self.astack.append(attrs)\n                    self.o(\"[\")\n                else:\n                    self.astack.append(None)\n            else:\n                if self.astack:\n                    a = self.astack.pop()\n                    if a:\n                        i = self.previousIndex(a)\n                        if i is not None:\n                            a = self.a[i]\n                        else:\n                            self.acount += 1\n                            a['count'] = self.acount\n                            a['outcount'] = self.outcount\n                            self.a.append(a)\n                        self.o(\"][\" + `a['count']` + \"]\")\n\n        if tag == \"img\" and start:\n            attrsD = {}\n            for (x, y) in attrs: attrsD[x] = y\n            attrs = attrsD\n            if attrs.has_key('src'):\n                attrs['href'] = attrs['src']\n                alt = attrs.get('alt', '')\n                i = self.previousIndex(attrs)\n                if i is not None:\n                    attrs = self.a[i]\n                else:\n                    self.acount += 1\n                    attrs['count'] = self.acount\n                    attrs['outcount'] = self.outcount\n                    self.a.append(attrs)\n                self.o(\"![\")\n                self.o(alt)\n                self.o(\"][\"+`attrs['count']`+\"]\")\n\n        if tag == 'dl' and start: self.p()\n        if tag == 'dt' and not start: self.pbr()\n        if tag == 'dd' and start: self.o('    ')\n        if tag == 'dd' and not start: self.pbr()\n\n        if tag in [\"ol\", \"ul\"]:\n            if start:\n                self.list.append({'name':tag, 'num':0})\n            else:\n                if self.list: self.list.pop()\n\n            self.p()\n\n        if tag == 'li':\n            if start:\n                self.pbr()\n                if self.list: li = self.list[-1]\n                else: li = {'name':'ul', 'num':0}\n                self.o(\"  \"*len(self.list)) #TODO: line up <ol><li>s > 9 correctly.\n                if li['name'] == \"ul\": self.o(\"* \")\n                elif li['name'] == \"ol\":\n                    li['num'] += 1\n                    self.o(`li['num']`+\". \")\n                self.start = 1\n            else:\n                self.pbr()\n\n        if tag in [\"table\", \"tr\"] and start: self.p()\n        if tag == 'td': self.pbr()\n\n        if tag == \"pre\":\n            if start:\n                self.startpre = 1\n                self.pre = 1\n            else:\n                self.pre = 0\n            self.p()\n\n    def pbr(self):\n        if self.p_p == 0: self.p_p = 1\n\n    def p(self):\n        self.p_p = 2\n\n    def o(self, data, puredata=0, force=0):\n        if self.abbr_data is not None: self.abbr_data += data\n\n        if not self.quiet:\n            if puredata and not self.pre:\n                data = re.sub('\\s+', ' ', data)\n                if data and data[0] == ' ':\n                    self.space = 1\n                    data = data[1:]\n            if not data and not force: return\n\n            if self.startpre:\n                #self.out(\" :\") #TODO: not output when already one there\n                self.startpre = 0\n\n            bq = (\">\" * self.blockquote)\n            if not (force and data and data[0] == \">\") and self.blockquote: bq += \" \"\n\n            if self.pre:\n                bq += \"    \"\n                data = data.replace(\"\\n\", \"\\n\"+bq)\n\n            if self.start:\n                self.space = 0\n                self.p_p = 0\n                self.start = 0\n\n            if force == 'end':\n                # It's the end.\n                self.p_p = 0\n                self.out(\"\\n\")\n                self.space = 0\n\n\n            if self.p_p:\n                self.out(('\\n'+bq)*self.p_p)\n                self.space = 0\n\n            if self.space:\n                if not self.lastWasNL: self.out(' ')\n                self.space = 0\n\n            if self.a and ((self.p_p == 2 and LINKS_EACH_PARAGRAPH) or force == \"end\"):\n                if force == \"end\": self.out(\"\\n\")\n\n                newa = []\n                for link in self.a:\n                    if self.outcount > link['outcount']:\n                        self.out(\"   [\"+`link['count']`+\"]: \" + urlparse.urljoin(self.baseurl, link['href']))\n                        if link.has_key('title'): self.out(\" (\"+link['title']+\")\")\n                        self.out(\"\\n\")\n                    else:\n                        newa.append(link)\n\n                if self.a != newa: self.out(\"\\n\") # Don't need an extra line when nothing was done.\n\n                self.a = newa\n\n            if self.abbr_list and force == \"end\":\n                for abbr, definition in self.abbr_list.items():\n                    self.out(\"  *[\" + abbr + \"]: \" + definition + \"\\n\")\n\n            self.p_p = 0\n            self.out(data)\n            self.lastWasNL = data and data[-1] == '\\n'\n            self.outcount += 1\n\n    def handle_data(self, data):\n        if r'\\\/script>' in data: self.quiet -= 1\n        self.o(data, 1)\n\n    def unknown_decl(self, data):\n        pass\n\ndef wrapwrite(text): sys.stdout.write(text.encode('utf8'))\n\ndef html2text_file(html, out=wrapwrite, baseurl=''):\n    h = _html2text(out, baseurl)\n    h.feed(html)\n    h.feed(\"\")\n    return h.close()\n\ndef html2text(html, baseurl=''):\n    return optwrap(html2text_file(html, None, baseurl))\n\nif __name__ == \"__main__\":\n    baseurl = ''\n    if sys.argv[1:]:\n        arg = sys.argv[1]\n        if arg.startswith('http:\/\/'):\n            baseurl = arg\n            j = urllib.urlopen(baseurl)\n            try:\n                from feedparser import _getCharacterEncoding as enc\n            except ImportError:\n                   enc = lambda x, y: ('utf-8', 1)\n            text = j.read()\n            encoding = enc(j.headers, text)[0]\n            if encoding == 'us-ascii': encoding = 'utf-8'\n            data = text.decode(encoding)\n\n        else:\n            encoding = 'utf8'\n            if len(sys.argv) > 2:\n                encoding = sys.argv[2]\n            f = open(arg, 'r')\n            try:\n                    data = f.read().decode(encoding)\n            finally:\n                    f.close()\n    else:\n        data = sys.stdin.read().decode('utf8')\n    wrapwrite(html2text(data, baseurl))\n\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n\n# Copyright(C) 2012 Romain Bignon\n#\n# This file is part of weboob.\n#\n# weboob is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU Affero General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# weboob is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with weboob. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n\nimport datetime\nfrom decimal import Decimal\nimport re\n\nfrom weboob.deprecated.browser import Page\nfrom weboob.capabilities.bank import Account\nfrom weboob.tools.capabilities.bank.transactions import FrenchTransaction\n\n\nclass LoginPage(Page):\n    def login(self, login, passwd):\n        self.browser.select_form(name='frmLogin')\n        self.browser['username'] = login.encode(self.browser.ENCODING)\n        self.browser['password'] = passwd.encode(self.browser.ENCODING)\n        self.browser.submit(nologin=True)\n\n    def has_redirect(self):\n        if len(self.document.getroot().xpath('\/\/form')) > 0:\n            return False\n        else:\n            return True\n\n\nclass Login2Page(Page):\n    def login(self, secret):\n        label = self.document.xpath('\/\/span[@class=\"PF_LABEL\"]')[0].text.strip()\n        letters = ''\n        for n in re.findall('(\\d+)', label):\n            letters += secret[int(n) - 1]\n\n        self.browser.select_form(name='frmControl')\n        self.browser['word'] = letters\n        self.browser.submit(name='valider', nologin=True)\n\n\nclass IndexPage(Page):\n    pass\n\n\nclass AccountsPage(Page):\n    ACCOUNT_TYPES = {u'Epargne':                Account.TYPE_SAVINGS,\n                     u'Liquidit\u00e9s':             Account.TYPE_CHECKING,\n                     u'Titres':                 Account.TYPE_MARKET,\n                     u'Pr\u00eats':                  Account.TYPE_LOAN,\n                    }\n\n    def get_list(self):\n        accounts = []\n\n        for block in self.document.xpath('\/\/div[@class=\"pave\"]\/div'):\n            head_type = block.xpath('.\/div\/span[@class=\"accGroupLabel\"]')[0].text.strip()\n            account_type = self.ACCOUNT_TYPES.get(head_type, Account.TYPE_UNKNOWN)\n            for tr in block.cssselect('ul li.tbord_account'):\n                id = tr.attrib.get('id', '')\n                if id.find('contratId') != 0:\n                    self.logger.warning('Unable to parse contract ID: %r' % id)\n                    continue\n                id = id[id.find('contratId')+len('contratId'):]\n\n                link = tr.cssselect('span.accountLabel a')[0]\n                balance = Decimal(FrenchTransaction.clean_amount(tr.cssselect('span.accountTotal')[0].text))\n\n                if id.endswith('CRT'):\n                    account = accounts[-1]\n                    account._card_links.append(link.attrib['href'])\n                    if not account.coming:\n                        account.coming = Decimal('0.0')\n                    account.coming += balance\n                    continue\n\n                account = Account()\n                account.id = id\n                account.label = unicode(link.text.strip())\n                account.type = account_type\n                account.balance = balance\n                account.currency = account.get_currency(tr.cssselect('span.accountDev')[0].text)\n                account._link = link.attrib['href']\n                account._card_links = []\n                accounts.append(account)\n\n        if len(accounts) == 0:\n            # Sometimes, accounts are only in javascript...\n            for script in self.document.xpath('\/\/script'):\n                text = script.text\n                if text is None:\n                    continue\n                if 'remotePerso' not in text:\n                    continue\n\n                account = None\n                attribs = {}\n                account_type = Account.TYPE_UNKNOWN\n                for line in text.split('\\n'):\n                    line = line.strip()\n                    m = re.match(\"data.libelle = '(.*)';\", line)\n                    if m:\n                        account_type = self.ACCOUNT_TYPES.get(m.group(1), Account.TYPE_UNKNOWN)\n                    elif line == 'var remotePerso = new Object;':\n                        account = Account()\n                    elif account is not None:\n                        m = re.match(\"remotePerso.(\\w+) = '?(.*?)'?;\", line)\n                        if m:\n                            attribs[m.group(1)] = m.group(2)\n                        elif line.startswith('listProduitsGroup'):\n                            account.id = attribs['refContrat']\n\n                            account.label = attribs['libelle']\n                            account.type = account_type\n                            account.balance = Decimal(FrenchTransaction.clean_amount(attribs['soldeDateOpeValeurFormatted']))\n                            account.currency = account.get_currency(attribs['codeDevise'])\n                            account._link = 'tbord.do?id=%s' % attribs['id']\n                            account._card_links = []\n\n                            if account.id.endswith('CRT'):\n                                a = accounts[-1]\n                                a._card_links.append(account._link)\n                                if not a.coming:\n                                    a.coming = Decimal('0.0')\n                                a.coming += account.balance\n                            else:\n                                accounts.append(account)\n                            account = None\n\n        return accounts\n\n\nclass Transaction(FrenchTransaction):\n    PATTERNS = [(re.compile('^RET DAB (?P<text>.*?) RETRAIT DU (?P<dd>\\d{2})(?P<mm>\\d{2})(?P<yy>\\d{2}).*'),\n                                                              FrenchTransaction.TYPE_WITHDRAWAL),\n                (re.compile('^RET DAB (?P<text>.*?) CARTE ?:.*'),\n                                                              FrenchTransaction.TYPE_WITHDRAWAL),\n                (re.compile('^RET DAB (?P<dd>\\d{2})\/(?P<mm>\\d{2})\/(?P<yy>\\d{2}) (?P<text>.*?) CARTE .*'),\n                                                              FrenchTransaction.TYPE_WITHDRAWAL),\n                (re.compile('^(?P<text>.*) RETRAIT DU (?P<dd>\\d{2})(?P<mm>\\d{2})(?P<yy>\\d{2}) .*'),\n                                                              FrenchTransaction.TYPE_WITHDRAWAL),\n                (re.compile('(\\w+) (?P<dd>\\d{2})(?P<mm>\\d{2})(?P<yy>\\d{2}) CB[:\\*][^ ]+ (?P<text>.*)'),\n                                                              FrenchTransaction.TYPE_CARD),\n                (re.compile('^(?P<category>VIR(EMEN)?T? (SEPA)?(RECU|FAVEUR)?)( \/FRM)?(?P<text>.*)'),\n                                                              FrenchTransaction.TYPE_TRANSFER),\n                (re.compile('^PRLV (?P<text>.*) (REF \\w+)?$'),FrenchTransaction.TYPE_ORDER),\n                (re.compile('^CHEQUE.*? (REF \\w+)?$'),        FrenchTransaction.TYPE_CHECK),\n                (re.compile('^(AGIOS \/|FRAIS) (?P<text>.*)'), FrenchTransaction.TYPE_BANK),\n                (re.compile('^(CONVENTION \\d+ )?COTIS(ATION)? (?P<text>.*)'),\n                                                              FrenchTransaction.TYPE_BANK),\n                (re.compile('^REMISE (?P<text>.*)'),          FrenchTransaction.TYPE_DEPOSIT),\n                (re.compile('^(?P<text>.*)( \\d+)? QUITTANCE .*'),\n                                                              FrenchTransaction.TYPE_ORDER),\n                (re.compile('^.* LE (?P<dd>\\d{2})\/(?P<mm>\\d{2})\/(?P<yy>\\d{2})$'),\n                                                              FrenchTransaction.TYPE_UNKNOWN),\n               ]\n\n\nclass HistoryBasePage(Page):\n    def get_history(self):\n        self.logger.warning('Do not support account of type %s' % type(self).__name__)\n        return iter([])\n\n\nclass TransactionsPage(HistoryBasePage):\n    def get_history(self):\n        for tr in self.document.xpath('\/\/table[@id=\"operation\"]\/tbody\/tr'):\n            tds = tr.findall('td')\n\n            if len(tds) < 5:\n                continue\n\n            t = Transaction(tds[-1].findall('img')[-1].attrib.get('id', ''))\n\n            date = u''.join([txt.strip() for txt in tds[0].itertext()])\n            raw = u' '.join([txt.strip() for txt in tds[1].itertext()])\n            debit = u''.join([txt.strip() for txt in tds[-3].itertext()])\n            credit = u''.join([txt.strip() for txt in tds[-2].itertext()])\n            t.parse(date, re.sub(r'[ ]+', ' ', raw))\n            t.set_amount(credit, debit)\n            t._coming = False\n\n            if t.raw.startswith('ACHAT CARTE -DEBIT DIFFERE'):\n                continue\n\n            yield t\n\n\nclass CardPage(HistoryBasePage):\n    def get_history(self):\n        debit_date = None\n        coming = True\n        for tr in self.document.xpath('\/\/table[@class=\"report\"]\/tbody\/tr'):\n            tds = tr.findall('td')\n\n            if len(tds) == 2:\n                # headers\n                m = re.match('.* (\\d+)\/(\\d+)\/(\\d+)', tds[0].text.strip())\n                debit_date = datetime.date(int(m.group(3)), int(m.group(2)), int(m.group(1)))\n                if debit_date < datetime.date.today():\n                    coming = False\n\n            if len(tds) != 3:\n                continue\n\n            t = Transaction(0)\n            date = u''.join([txt.strip() for txt in tds[0].itertext()])\n            raw = u' '.join([txt.strip() for txt in tds[1].itertext()])\n            amount = u''.join([txt.strip() for txt in tds[-1].itertext()])\n            t.parse(date, re.sub(r'[ ]+', ' ', raw))\n            if debit_date is not None:\n                t.date = debit_date\n            t.label = unicode(tds[1].find('span').text.strip())\n            t.type = t.TYPE_CARD\n            t._coming = coming\n            t.set_amount(amount)\n            yield t\n\n\nclass ValuationPage(HistoryBasePage):\n    pass\n\n\nclass LoanPage(HistoryBasePage):\n    pass\n\n\nclass MarketPage(HistoryBasePage):\n    pass\n\nclass AssurancePage(HistoryBasePage):\n    pass\n","label":0}
{"content":"\"\"\"\nHTMLParser-based link extractor\n\"\"\"\n\nfrom HTMLParser import HTMLParser\nfrom urlparse import urljoin\n\nfrom w3lib.url import safe_url_string\n\nfrom scrapy.link import Link\nfrom scrapy.utils.python import unique as unique_list\n\nclass HtmlParserLinkExtractor(HTMLParser):\n\n    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n        HTMLParser.__init__(self)\n\n        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n        self.process_attr = process if callable(process) else lambda v: v\n        self.unique = unique\n\n    def _extract_links(self, response_text, response_url, response_encoding):\n        self.reset()\n        self.feed(response_text)\n        self.close()\n\n        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n\n        ret = []\n        base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n        for link in links:\n            if isinstance(link.url, unicode):\n                link.url = link.url.encode(response_encoding)\n            link.url = urljoin(base_url, link.url)\n            link.url = safe_url_string(link.url, response_encoding)\n            link.text = link.text.decode(response_encoding)\n            ret.append(link)\n\n        return ret\n\n    def extract_links(self, response):\n        # wrapper needed to allow to work directly with text\n        return self._extract_links(response.body, response.url, response.encoding)\n\n    def reset(self):\n        HTMLParser.reset(self)\n\n        self.base_url = None\n        self.current_link = None\n        self.links = []\n\n    def handle_starttag(self, tag, attrs):\n        if tag == 'base':\n            self.base_url = dict(attrs).get('href')\n        if self.scan_tag(tag):\n            for attr, value in attrs:\n                if self.scan_attr(attr):\n                    url = self.process_attr(value)\n                    link = Link(url=url)\n                    self.links.append(link)\n                    self.current_link = link\n\n    def handle_endtag(self, tag):\n        if self.scan_tag(tag):\n            self.current_link = None\n\n    def handle_data(self, data):\n        if self.current_link:\n            self.current_link.text = self.current_link.text + data\n\n    def matches(self, url):\n        \"\"\"This extractor matches with any url, since\n        it doesn't contain any patterns\"\"\"\n        return True\n","label":0}
{"content":"#!\/usr\/bin\/env python\n# Copyright (C) 2009-2014:\n#    Gabes Jean, naparuba@gmail.com\n#    Gerhard Lausser, Gerhard.Lausser@consol.de\n#\n# This file is part of Shinken.\n#\n# Shinken is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU Affero General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Shinken is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with Shinken.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n#\n# This file is used to test reading and processing of config files\n#\n\nfrom shinken_test import *\n\n\nclass TestsericeTplNoHostname(ShinkenTest):\n\n    def setUp(self):\n        self.setup_with_file('etc\/shinken_servicetpl_no_hostname.cfg')\n\n    def test_dummy(self):\n        #\n        # Config is not correct because of a wrong relative path\n        # in the main config file\n        #\n        print \"Get the hosts and services\"\n        now = time.time()\n        host = self.sched.hosts.find_by_name(\"test_host_0\")\n        host.checks_in_progress = []\n        host.act_depend_of = []  # ignore the router\n        router = self.sched.hosts.find_by_name(\"test_router_0\")\n        router.checks_in_progress = []\n        router.act_depend_of = []  # ignore the router\n        svc = self.sched.services.find_srv_by_name_and_hostname(\"test_host_0\", \"test_ok_0\")\n        svc.checks_in_progress = []\n        svc.act_depend_of = []  # no hostchecks on critical checkresults\n        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])\n        self.assertEqual('UP', host.state)\n        self.assertEqual('HARD', host.state_type)\n\n\nif __name__ == '__main__':\n    unittest.main()\n","label":0}
{"content":"#!\/usr\/bin\/python3\n\nimport csv\nimport pymysql\n\nsingleList = []\nmultipleList = []\n\n# Connect to the database\nconnection = pymysql.connect(\n                host='localhost',\n                user='operator',\n                passwd='operator',\n                db='chunk4_images',\n                charset='utf8mb4',\n                cursorclass=pymysql.cursors.DictCursor)\n\nwith open('updated4_images.csv', 'r') as csvfile:\n    for myupdatedCSV in csv.reader(csvfile):\n        if myupdatedCSV[3] == \"UML\":\n            updatedCSV = myupdatedCSV[2].split('\/')\n            repo = updatedCSV[3] + \"\/\" + updatedCSV[4]\n            fileurl = myupdatedCSV[2]\n            filename = fileurl.split('\/')[-1]\n            filepath = '\/'.join(fileurl.split('\/')[6:])\n            if 'https:\/\/raw.githubusercontent.com\/' not in fileurl:\n                continue\n            # Get repo id from database\n            cursor = connection.cursor()\n            sql = 'SELECT id FROM repositories WHERE uri=\"'\n            sql += 'https:\/\/github.com\/{0}\"'.format(repo)\n    #        print(sql)\n            cursor.execute(sql)\n            result = cursor.fetchone()\n            try:\n                repo_id = result['id']\n    #            print(repo_id)\n            except:\n                #print(\"# Error\", result, repo)\n                continue\n\n            # Get file id from database\n            sql = 'SELECT id FROM files WHERE repository_id={0} and file_name=\"{1}\"'.format(repo_id, filename)\n    #        print(sql)\n            cursor.execute(sql)\n            if cursor.rowcount == 1:\n                result = cursor.fetchone()\n                file_id = result['id']\n                singleList.append((file_id, repo_id, fileurl.replace(\"'\", \"\\\\'\"), filepath.replace(\"'\", \"\\\\'\")))\n            else:\n                result = cursor.fetchall()\n    #            print(\"Warning:\", result, filepath)\n                found = 0\n                for file in result:\n                    sql = 'SELECT file_path from file_links WHERE file_id={0}'.format(file['id'])\n    #                print(sql)\n                    cursor.execute(sql)\n                    result = cursor.fetchone()\n                    db_path = result['file_path']\n                    if db_path == filepath:\n                        singleList.append((file['id'], repo_id, fileurl.replace(\"'\", \"\\\\'\"), filepath.replace(\"'\", \"\\\\'\")))\n                        found = 1\n                        break\n                #if not found:\n                    #print(\"# ERROR:\", filepath, \"not found\")\n\n\nconnection.close()\n\n# Write data into database\n\ncreate = \"\"\"\n\nUSE chunk4_images;\n\nCREATE TABLE uml_files (\n  id int,\n  repository_id int,\n  file_url VARCHAR(255),\n  file_path VARCHAR(255)\n);\n\"\"\"\n\nprint(create)\n\nfor entry in singleList:\n    print(\"INSERT INTO uml_files (id, repository_id, file_url, file_path) VALUES ({0}, {1}, '{2}', '{3}');\".format(*entry))\n","label":0}
{"content":"import honeyd\nimport time\nimport support\nfrom htmltmpl import TemplateManager, TemplateProcessor\n\nglobal counter\n\nself.send_response(200)\nself.send_header(\"Content-Type\", \"text\/html\")\nself.send_nocache()\nself.end_headers()\n\n# Compile or load already precompiled template.\ntemplate = TemplateManager().prepare(self.root+\"\/templates\/index.tmpl\")\ntproc = TemplateProcessor(0)\n\n# Process commands given to us\nmessage = support.parse_query(self.query)\n\n# Set the title.\ntproc.set(\"title\", \"Honeyd Administration Interface\")\n\n# Test\ntry:\n    counter += 1\nexcept:\n    counter = 1\n\ngreeting = (\"Welcome to the Honeyd Administration Interface.\"\n            \"You are visitor %d.<p>\") % counter\n\ncontent = support.interface_table()\ncontent += \"<p>\" + support.stats_table(self.root) + \"<\/p>\\n\"\ncontent += \"<p>\" + support.status_connections(self.root, \"tcp\") + \"<\/p>\\n\"\ncontent += \"<p>\" + support.status_connections(self.root, \"udp\") + \"<\/p>\\n\"\n\nside_content = (\"<div class=graphs>\"\n                \"<img height=155 width=484 src=\/graphs\/traffic_hourly.gif><br>\"\n                \"<img height=155 width=484 src=\/graphs\/traffic_daily.gif>\"\n                \"<\/div>\")\n\nsupport.security_check(tproc)\n\nif message:\n    tproc.set(\"message\", message)\n\ntproc.set(\"greeting\", greeting)\ntproc.set(\"content\", content)\ntproc.set(\"side_content\", side_content)\ntproc.set(\"uptime\", support.uptime())\n\n# Print the processed template.\nself.wfile.write(tproc.process(template))\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n\nimport time\nimport unittest\n\nfrom nive.security import AdminUser, UserFound\nfrom db_app import *\n\n\n\nclass ObjectTest(unittest.TestCase):\n\n    def setUp(self):\n        self.app = app()\n\n    def tearDown(self):\n        self.app.Close()\n        pass\n    \n    def test_add(self):\n        a=self.app\n        root=a.root()\n        user = User(\"test\")\n        # root\n        root.DeleteUser(\"user1\")\n        root.DeleteUser(\"user2\")\n        root.DeleteUser(\"user3\")\n        data = {\"password\": \"11111\", \"surname\": \"surname\", \"lastname\": \"lastname\", \"organistion\": \"organisation\"}\n        \n        data[\"name\"] = \"user1\"\n        data[\"email\"] = \"user1@aaa.ccc\"\n        o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups=\"\", currentUser=user)\n        self.assert_(o,r)\n        o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups=\"\", currentUser=user)\n        self.assertFalse(o,r)\n\n        data[\"name\"] = \"user2\"\n        data[\"email\"] = \"user2@aaa.ccc\"\n        o,r = root.AddUser(data, activate=1, generatePW=1, mail=None, notify=False, groups=\"group:author\", currentUser=user)\n        self.assert_(o,r)\n\n        data[\"name\"] = \"user3\"\n        data[\"email\"] = \"user3@aaa.ccc\"\n        o,r = root.AddUser(data, activate=0, generatePW=1, mail=None, notify=False, groups=\"group:editor\", currentUser=user)\n        self.assert_(o,r)\n        self.assert_(\"group:editor\" in o.data.groups, o.data.groups)\n        self.assert_(o.data.password != \"11111\")\n        self.assertFalse(o.meta.pool_state)\n        \n        root.MailUserPass(email = \"user1\", mailtmpl = None)\n        root.MailUserPass(email = \"user2@aaa.ccc\", mailtmpl = None, createNewPasswd=False)\n        root.MailUserPass(email = \"user3@aaa.ccc\", mailtmpl = None)\n        \n        self.assert_(root.GetUserByName(\"user2\", activeOnly=1))\n        self.assert_(root.GetUserByID(o.id, activeOnly=0))\n        self.assert_(root.GetUserByMail(\"user2@aaa.ccc\", activeOnly=1))\n        \n        self.assert_(root.LookupUser(name=\"user1\", id=None, activeOnly=1))\n        self.assertFalse(root.LookupUser(name=\"user3\", id=None, activeOnly=1))\n        self.assert_(root.LookupUser(name=\"user3\", id=None, activeOnly=0))\n        \n        self.assert_(len(root.GetUserInfos([\"user1\", \"user2\"], fields=[\"name\", \"email\", \"title\"], activeOnly=True)))\n        self.assert_(len(root.GetUsersWithGroup(\"group:author\", fields=[\"name\"], activeOnly=True)))\n        self.assert_(len(root.GetUsersWithGroup(\"group:editor\", fields=[\"name\"], activeOnly=False)))\n        self.assertFalse(len(root.GetUsersWithGroup(\"group:editor\", fields=[\"name\"], activeOnly=True)))\n        self.assert_(len(root.GetUsers()))\n        \n        root.DeleteUser(\"user1\")\n        root.DeleteUser(\"user2\")\n        root.DeleteUser(\"user3\")\n\n        \n    def test_login(self):\n        a=self.app\n        root=a.root()\n        root.identityField=u\"name\"\n        user = User(\"test\")\n        # root\n        root.DeleteUser(\"user1\")\n        root.DeleteUser(\"user2\")\n        root.DeleteUser(\"user3\")\n        data = {\"password\": \"11111\", \"surname\": \"surname\", \"lastname\": \"lastname\", \"organistion\": \"organisation\"}\n        \n        data[\"name\"] = \"user1\"\n        data[\"email\"] = \"user1@aaa.ccc\"\n        o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups=\"\", currentUser=user)\n        self.assert_(o,r)\n        l,r = root.Login(\"user1\", \"11111\", raiseUnauthorized = 0)\n        self.assert_(l,r)\n        self.assert_(root.Logout(\"user1\"))\n        l,r = root.Login(\"user1\", \"aaaaa\", raiseUnauthorized = 0)\n        self.assertFalse(l,r)\n        l,r = root.Login(\"user1\", \"\", raiseUnauthorized = 0)\n        self.assertFalse(l,r)\n\n        data[\"name\"] = \"user2\"\n        data[\"email\"] = \"user2@aaa.ccc\"\n        o,r = root.AddUser(data, activate=1, generatePW=1, mail=None, notify=False, groups=\"\", currentUser=user)\n        self.assert_(o,r)\n        l,r = root.Login(\"user2\", o.data.password, raiseUnauthorized = 0)\n        self.assertFalse(l,r)\n        self.assert_(root.Logout(\"user1\"))\n        l,r = root.Login(\"user2\", \"11111\", raiseUnauthorized = 0)\n        self.assertFalse(l,r)\n\n        data[\"name\"] = \"user3\"\n        data[\"email\"] = \"user3@aaa.ccc\"\n        o,r = root.AddUser(data, activate=0, generatePW=1, mail=None, notify=False, groups=\"group:author\", currentUser=user)\n        self.assert_(o,r)\n        l,r = root.Login(\"user3\", o.data.password, raiseUnauthorized = 0)\n        self.assertFalse(l,r)\n        self.assertFalse(root.Logout(\"user3\"))\n        \n        root.DeleteUser(\"user1\")\n        root.DeleteUser(\"user2\")\n        root.DeleteUser(\"user3\")\n\n\n    def test_user(self):\n        a=self.app\n        root=a.root()\n        user = User(\"test\")\n        # root\n        root.DeleteUser(\"user1\")\n        data = {\"password\": \"11111\", \"surname\": \"surname\", \"lastname\": \"lastname\", \"organistion\": \"organisation\"}\n        \n        data[\"name\"] = \"user1\"\n        data[\"email\"] = \"user1@aaa.ccc\"\n        o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups=\"\", currentUser=user)\n\n        self.assert_(o.SecureUpdate(data, user))\n        self.assert_(o.UpdateGroups([\"group:author\"]))\n        self.assert_(o.GetGroups()==(\"group:author\",), o.GetGroups())\n        self.assert_(o.AddGroup(\"group:editor\", user))\n        self.assert_(o.GetGroups()==(\"group:author\",\"group:editor\"), o.GetGroups())\n        self.assert_(o.InGroups(\"group:editor\"))\n        self.assert_(o.InGroups(\"group:author\"))\n    \n        self.assert_(o.ReadableName()==\"surname lastname\")\n\n        root.DeleteUser(\"user1\")\n\n\n\nclass AdminuserTest(unittest.TestCase):\n\n    def setUp(self):\n        self.app = app()\n        self.app.configuration.unlock()\n        self.app.configuration.admin = {\"name\":\"admin\", \"password\":\"11111\", \"email\":\"admin@aaa.ccc\", \"groups\":(\"group:admin\",)}\n        self.app.configuration.loginByEmail = True\n        self.app.configuration.lock()\n        \n    def tearDown(self):\n        self.app.Close()\n        pass\n    \n    def test_login(self):\n        user = User(\"test\")\n        a=self.app\n        root=a.root()\n        root.identityField=u\"name\"\n        root.DeleteUser(\"adminXXXXX\")\n        root.DeleteUser(\"admin\")\n\n        data = {\"password\": \"11111\", \"surname\": \"surname\", \"lastname\": \"lastname\", \"organistion\": \"organisation\"}\n        data[\"name\"] = \"admin\"\n        data[\"email\"] = \"admin@aaa.cccXXXXX\"\n        o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups=\"\", currentUser=user)\n        self.assertFalse(o,r)\n        data[\"name\"] = \"adminXXXXX\"\n        data[\"email\"] = \"admin@aaa.ccc\"\n        o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups=\"\", currentUser=user)\n        self.assertFalse(o,r)\n\n        l,r = root.Login(\"admin\", \"11111\", raiseUnauthorized = 0)\n        self.assert_(l,r)\n        self.assert_(root.Logout(\"admin\"))\n        l,r = root.Login(\"admin\", \"aaaaa\", raiseUnauthorized = 0)\n        self.assertFalse(l,r)\n        l,r = root.Login(\"admin\", \"\", raiseUnauthorized = 0)\n        self.assertFalse(l,r)\n\n        \n","label":0}
{"content":"# -*- coding: utf-8 -*-\n#########################################################################\n#\n# Copyright (C) 2018 OSGeo\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#\n#########################################################################\n\n\"\"\"unit tests for geonode.upload.files module\"\"\"\n\nfrom geonode.tests.base import GeoNodeBaseTestSupport\n\nfrom geonode.upload import files\n\n\nclass FilesTestCase(GeoNodeBaseTestSupport):\n\n    def test_scan_hint_kml_ground_overlay(self):\n        result = files.get_scan_hint([\"kml\", \"other\"])\n        kml_file_type = files.get_type(\"KML Ground Overlay\")\n        self.assertEqual(result, kml_file_type.code)\n\n    def test_scan_hint_kmz_ground_overlay(self):\n        result = files.get_scan_hint([\"kmz\", \"other\"])\n        self.assertEqual(result, \"kmz\")\n\n    def test_get_type_non_existing_type(self):\n        self.assertIsNone(files.get_type(\"fake\"))\n\n    def test_get_type_kml_ground_overlay(self):\n        file_type = files.get_type(\"KML Ground Overlay\")\n        self.assertEqual(file_type.code, \"kml-overlay\")\n        self.assertIn(\"kmz\", file_type.aliases)\n","label":0}
{"content":"#    Copyright 2013 X35\n#\n#    This file is part of gamemod.\n#\n#    gamemod is free software: you can redistribute it and\/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    gamemod is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with gamemod.  If not, see <http:#www.gnu.org\/licenses\/>.\n\nfrom requestcounter import requestcounter\nfrom debug import debug\n\n# provide the gamemod gui & server list\nclass guiprovider:\n\tFILECHECK_INTERVAL = 60*60 # 1h\n\t\n\tDBGTAG = \"guiprovider\"\n\tDBGTAG_REQUEST = DBGTAG+\"\/request\"\n\tDBGTAG_REPLY = DBGTAG+\"\/reply\"\n\t\n\tLIST_REQUEST = \"list\"\n\tREADABLELIST_REQUEST = \"readablelist\"\n\n\tdef __init__(self, reqfunc):\n\t\tself.reqfunc = reqfunc\n\t\tself.counter = requestcounter()\n\t\t\n\tdef request(self, readable=False):\n\t\treturn self.reqfunc(readable)\n\t\n\tdef onrequest(self, line, addr, build): # return (reply, close)\n\t\tif line == guiprovider.LIST_REQUEST:\n\t\t\tdebug.msg(guiprovider.DBGTAG_REQUEST, \"%s request from %s:%d (%sbuild)\" % ((line,)+addr+(\"\" if build else \"don't \",)))\n\t\t\tself.counter.add(addr[0])\n\t\t\ts = (self.request() if build else True)\n\t\t\tdebug.msg(guiprovider.DBGTAG_REQUEST, \"sending reply to %s request to %s:%d\" % ((line,)+addr))\n\t\t\treturn s, True\n\t\telif line == guiprovider.READABLELIST_REQUEST:\n\t\t\tdebug.msg(guiprovider.DBGTAG_REQUEST, \"%s request from %s:%d (%sbuild)\" % ((line,)+addr+(\"\" if build else \"don't \",)))\n\t\t\ts = (self.request(True) if build else True)\n\t\t\tdebug.msg(guiprovider.DBGTAG_REQUEST, \"sending reply to %s request to %s:%d\" % ((line,)+addr))\n\t\t\treturn s, True\n\t\treturn None, False\n\t\n\tdef differentips(self):\n\t\treturn self.counter.differentips()\n\t\n\tdef requests(self):\n\t\treturn self.counter.requests()\n","label":0}
{"content":"# mysql\/gaerdbms.py\n# Copyright (C) 2005-2015 the SQLAlchemy authors and contributors\n# <see AUTHORS file>\n#\n# This module is part of SQLAlchemy and is released under\n# the MIT License: http:\/\/www.opensource.org\/licenses\/mit-license.php\n\"\"\"\n.. dialect:: mysql+gaerdbms\n    :name: Google Cloud SQL\n    :dbapi: rdbms\n    :connectstring: mysql+gaerdbms:\/\/\/<dbname>?instance=<instancename>\n    :url: https:\/\/developers.google.com\/appengine\/docs\/python\/cloud-sql\/\\\ndevelopers-guide\n\n    This dialect is based primarily on the :mod:`.mysql.mysqldb` dialect with\n    minimal changes.\n\n    .. versionadded:: 0.7.8\n\n    .. deprecated:: 1.0 This dialect is **no longer necessary** for\n        Google Cloud SQL; the MySQLdb dialect can be used directly.\n        Cloud SQL now recommends creating connections via the\n        mysql dialect using the URL format\n\n        ``mysql+mysqldb:\/\/root@\/<dbname>?unix_socket=\/cloudsql\/<projectid>:<instancename>``\n\n\nPooling\n-------\n\nGoogle App Engine connections appear to be randomly recycled,\nso the dialect does not pool connections.  The :class:`.NullPool`\nimplementation is installed within the :class:`.Engine` by\ndefault.\n\n\"\"\"\n\nimport os\n\nfrom .mysqldb import MySQLDialect_mysqldb\nfrom ...pool import NullPool\nimport re\nfrom sqlalchemy.util import warn_deprecated\n\n\ndef _is_dev_environment():\n    return os.environ.get('SERVER_SOFTWARE', '').startswith('Development\/')\n\n\nclass MySQLDialect_gaerdbms(MySQLDialect_mysqldb):\n\n    @classmethod\n    def dbapi(cls):\n\n        warn_deprecated(\n            \"Google Cloud SQL now recommends creating connections via the \"\n            \"MySQLdb dialect directly, using the URL format \"\n            \"mysql+mysqldb:\/\/root@\/<dbname>?unix_socket=\/cloudsql\/\"\n            \"<projectid>:<instancename>\"\n        )\n\n        # from django:\n        # http:\/\/code.google.com\/p\/googleappengine\/source\/\n        #     browse\/trunk\/python\/google\/storage\/speckle\/\n        # python\/django\/backend\/base.py#118\n        # see also [ticket:2649]\n        # see also http:\/\/stackoverflow.com\/q\/14224679\/34549\n        from google.appengine.api import apiproxy_stub_map\n\n        if _is_dev_environment():\n            from google.appengine.api import rdbms_mysqldb\n            return rdbms_mysqldb\n        elif apiproxy_stub_map.apiproxy.GetStub('rdbms'):\n            from google.storage.speckle.python.api import rdbms_apiproxy\n            return rdbms_apiproxy\n        else:\n            from google.storage.speckle.python.api import rdbms_googleapi\n            return rdbms_googleapi\n\n    @classmethod\n    def get_pool_class(cls, url):\n        # Cloud SQL connections die at any moment\n        return NullPool\n\n    def create_connect_args(self, url):\n        opts = url.translate_connect_args()\n        if not _is_dev_environment():\n            # 'dsn' and 'instance' are because we are skipping\n            # the traditional google.api.rdbms wrapper\n            opts['dsn'] = ''\n            opts['instance'] = url.query['instance']\n        return [], opts\n\n    def _extract_error_code(self, exception):\n        match = re.compile(r\"^(\\d+)L?:|^\\((\\d+)L?,\").match(str(exception))\n        # The rdbms api will wrap then re-raise some types of errors\n        # making this regex return no matches.\n        code = match.group(1) or match.group(2) if match else None\n        if code:\n            return int(code)\n\ndialect = MySQLDialect_gaerdbms\n","label":0}
{"content":"'''\nDDS: DDS image loader\n'''\n\n__all__ = ('ImageLoaderDDS', )\n\nfrom kivy.lib.ddsfile import DDSFile\nfrom kivy.logger import Logger\nfrom kivy.core.image import ImageLoaderBase, ImageData, ImageLoader\n\n\nclass ImageLoaderDDS(ImageLoaderBase):\n\n    @staticmethod\n    def extensions():\n        return ('dds', )\n\n    def load(self, filename):\n        try:\n            dds = DDSFile(filename=filename)\n        except:\n            Logger.warning('Image: Unable to load image <%s>' % filename)\n            raise\n\n        self.filename = filename\n        width, height = dds.size\n        im = ImageData(width, height, dds.dxt, dds.images[0], source=filename,\n                       flip_vertical=False)\n        if len(dds.images) > 1:\n            images = dds.images\n            images_size = dds.images_size\n            for index in range(1, len(dds.images)):\n                w, h = images_size[index]\n                data = images[index]\n                im.add_mipmap(index, w, h, data)\n        return [im]\n\n# register\nImageLoader.register(ImageLoaderDDS)\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n\"\"\"QGIS Unit tests for QgsMessageLog.\n\n.. note:: This program is free software; you can redistribute it and\/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\n\"\"\"\n__author__ = 'Nyall Dawson'\n__date__ = '18\/06\/2018'\n__copyright__ = 'Copyright 2018, The QGIS Project'\n\nimport qgis  # NOQA\n\nfrom qgis.core import (Qgis,\n                       QgsApplication,\n                       QgsMessageLog,\n                       QgsMessageLogNotifyBlocker)\n\nfrom qgis.PyQt.QtTest import QSignalSpy\n\nfrom qgis.testing import start_app, unittest\nfrom utilities import (unitTestDataPath)\n\napp = start_app()\nTEST_DATA_DIR = unitTestDataPath()\n\n\nclass TestQgsMessageLog(unittest.TestCase):\n\n    def testSignals(self):\n        app_log = QgsApplication.messageLog()\n\n        # signals should be emitted by application log\n        app_spy = QSignalSpy(app_log.messageReceived)\n        app_spy_received = QSignalSpy(app_log.messageReceived[bool])\n\n        QgsMessageLog.logMessage('test', 'tag', Qgis.Info, notifyUser=True)\n        self.assertEqual(len(app_spy), 1)\n        self.assertEqual(app_spy[-1], ['test', 'tag', Qgis.Info])\n        # info message, so messageReceived(bool) should not be emitted\n        self.assertEqual(len(app_spy_received), 0)\n\n        QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True)\n        self.assertEqual(len(app_spy), 2)\n        self.assertEqual(app_spy[-1], ['test', 'tag', Qgis.Warning])\n        # warning message, so messageReceived(bool) should be emitted\n        self.assertEqual(len(app_spy_received), 1)\n\n        QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=False)\n        self.assertEqual(len(app_spy), 3)\n        # notifyUser was False\n        self.assertEqual(len(app_spy_received), 1)\n\n    def testBlocker(self):\n        app_log = QgsApplication.messageLog()\n\n        spy = QSignalSpy(app_log.messageReceived)\n        spy_received = QSignalSpy(app_log.messageReceived[bool])\n\n        QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True)\n        self.assertEqual(len(spy), 1)\n        self.assertEqual(spy[-1], ['test', 'tag', Qgis.Warning])\n        self.assertEqual(len(spy_received), 1)\n\n        # block notifications\n        b = QgsMessageLogNotifyBlocker()\n        QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True)\n        self.assertEqual(len(spy), 2)  # should not be blocked\n        self.assertEqual(len(spy_received), 1)  # should be blocked\n\n        # another blocker\n        b2 = QgsMessageLogNotifyBlocker()\n        QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True)\n        self.assertEqual(len(spy), 3)  # should not be blocked\n        self.assertEqual(len(spy_received), 1)  # should be blocked\n\n        del b\n        # still blocked because of b2\n        QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True)\n        self.assertEqual(len(spy), 4)  # should not be blocked\n        self.assertEqual(len(spy_received), 1)  # should be blocked\n\n        del b2\n        # not blocked\n        QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True)\n        self.assertEqual(len(spy), 5)  # should not be blocked\n        self.assertEqual(len(spy_received), 2)  # should not be blocked\n\n\nif __name__ == '__main__':\n    unittest.main()\n","label":0}
{"content":"# \/*\n#  * This program is free software; you can redistribute it and\/or modify\n#  * it under the terms of the GNU General Public License version 2 as\n#  * published by the Free Software Foundation;\n#  *\n#  * This program is distributed in the hope that it will be useful,\n#  * but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  * GNU General Public License for more details.\n#  *\n#  * You should have received a copy of the GNU General Public License\n#  * along with this program; if not, write to the Free Software\n#  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n#  *\n#  *\/\n\n# \n#  This ns-3 example demonstrates the use of helper functions to ease \n#  the construction of simulation scenarios.  \n#  \n#  The simulation topology consists of a mixed wired and wireless\n#  scenario in which a hierarchical mobility model is used.\n# \n#  The simulation layout consists of N backbone routers interconnected\n#  by an ad hoc wifi network.\n#  Each backbone router also has a local 802.11 network and is connected\n#  to a local LAN.  An additional set of(K-1) nodes are connected to\n#  this backbone.  Finally, a local LAN is connected to each router\n#  on the backbone, with L-1 additional hosts.  \n# \n#  The nodes are populated with TCP\/IP stacks, and OLSR unicast routing\n#  on the backbone.  An example UDP transfer is shown.  The simulator\n#  be configured to output tcpdumps or traces from different nodes.\n# \n# \n#           +--------------------------------------------------------+\n#           |                                                        |\n#           |              802.11 ad hoc, ns-2 mobility              | \n#           |                                                        |\n#           +--------------------------------------------------------+\n#                    |       o o o(N backbone routers)       |\n#                +--------+                               +--------+\n#      wired LAN | mobile |                     wired LAN | mobile |\n#     -----------| router |                    -----------| router |\n#                ---------                                ---------\n#                    |                                        |\n#           +----------------+                       +----------------+\n#           |     802.11     |                       |     802.11     |\n#           |      net       |                       |       net      |\n#           |   K-1 hosts    |                       |   K-1 hosts    |\n#           +----------------+                       +----------------+\n# \n\nimport ns.applications\nimport ns.core\nimport ns.csma\nimport ns.internet\nimport ns.mobility\nimport ns.network\nimport ns.olsr\nimport ns.wifi\n\n# # \n# #  This function will be used below as a trace sink\n# #  \n# static void\n# CourseChangeCallback(std.string path, Ptr<const MobilityModel> model)\n# {\n#   Vector position = model.GetPosition();\n#   std.cout << \"CourseChange \" << path << \" x=\" << position.x << \", y=\" << position.y << \", z=\" << position.z << std.endl;\n# }\n\ndef main(argv): \n    # \n    #  First, we initialize a few local variables that control some \n    #  simulation parameters.\n    #\n\n    cmd = ns.core.CommandLine()\n    cmd.backboneNodes = 10\n    cmd.infraNodes = 2\n    cmd.lanNodes = 2\n    cmd.stopTime = 20\n\n    # \n    #  Simulation defaults are typically set next, before command line\n    #  arguments are parsed.\n    # \n    ns.core.Config.SetDefault(\"ns3::OnOffApplication::PacketSize\", ns.core.StringValue(\"1472\"))\n    ns.core.Config.SetDefault(\"ns3::OnOffApplication::DataRate\", ns.core.StringValue(\"100kb\/s\"))\n\n    # \n    #  For convenience, we add the local variables to the command line argument\n    #  system so that they can be overridden with flags such as \n    #  \"--backboneNodes=20\"\n    # \n    \n    cmd.AddValue(\"backboneNodes\", \"number of backbone nodes\")\n    cmd.AddValue(\"infraNodes\", \"number of leaf nodes\")\n    cmd.AddValue(\"lanNodes\", \"number of LAN nodes\")\n    cmd.AddValue(\"stopTime\", \"simulation stop time(seconds)\")\n    \n    # \n    #  The system global variables and the local values added to the argument\n    #  system can be overridden by command line arguments by using this call.\n    # \n    cmd.Parse(argv)\n\n    backboneNodes = int(cmd.backboneNodes)\n    infraNodes = int(cmd.infraNodes) \n    lanNodes = int(cmd.lanNodes)\n    stopTime = int(cmd.stopTime)\n\n    if (stopTime < 10):\n        print \"Use a simulation stop time >= 10 seconds\"\n        exit(1)\n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n    #                                                                        # \n    #  Construct the backbone                                                # \n    #                                                                        # \n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n\n    # \n    #  Create a container to manage the nodes of the adhoc(backbone) network.\n    #  Later we'll create the rest of the nodes we'll need.\n    # \n    backbone = ns.network.NodeContainer()\n    backbone.Create(backboneNodes)\n    # \n    #  Create the backbone wifi net devices and install them into the nodes in \n    #  our container\n    # \n    wifi = ns.wifi.WifiHelper()\n    mac = ns.wifi.WifiMacHelper()\n    mac.SetType(\"ns3::AdhocWifiMac\")\n    wifi.SetRemoteStationManager(\"ns3::ConstantRateWifiManager\",\n                                  \"DataMode\", ns.core.StringValue(\"OfdmRate54Mbps\"))\n    wifiPhy = ns.wifi.YansWifiPhyHelper.Default()\n    wifiChannel = ns.wifi.YansWifiChannelHelper.Default()\n    wifiPhy.SetChannel(wifiChannel.Create())\n    backboneDevices = wifi.Install(wifiPhy, mac, backbone)\n    # \n    #  Add the IPv4 protocol stack to the nodes in our container\n    # \n    print \"Enabling OLSR routing on all backbone nodes\"\n    internet = ns.internet.InternetStackHelper()\n    olsr = ns.olsr.OlsrHelper()\n    internet.SetRoutingHelper(olsr); # has effect on the next Install ()\n    internet.Install(backbone);\n    # re-initialize for non-olsr routing.\n    # internet.Reset()\n    # \n    #  Assign IPv4 addresses to the device drivers(actually to the associated\n    #  IPv4 interfaces) we just created.\n    # \n    ipAddrs = ns.internet.Ipv4AddressHelper()\n    ipAddrs.SetBase(ns.network.Ipv4Address(\"192.168.0.0\"), ns.network.Ipv4Mask(\"255.255.255.0\"))\n    ipAddrs.Assign(backboneDevices)\n\n    # \n    #  The ad-hoc network nodes need a mobility model so we aggregate one to \n    #  each of the nodes we just finished building.  \n    # \n    mobility = ns.mobility.MobilityHelper()\n    mobility.SetPositionAllocator(\"ns3::GridPositionAllocator\",\n                                  \"MinX\", ns.core.DoubleValue(20.0),\n                                  \"MinY\", ns.core.DoubleValue(20.0),\n                                  \"DeltaX\", ns.core.DoubleValue(20.0),\n                                  \"DeltaY\", ns.core.DoubleValue(20.0),\n                                  \"GridWidth\", ns.core.UintegerValue(5),\n                                  \"LayoutType\", ns.core.StringValue(\"RowFirst\"))\n    mobility.SetMobilityModel(\"ns3::RandomDirection2dMobilityModel\",\n                               \"Bounds\", ns.mobility.RectangleValue(ns.mobility.Rectangle(-500, 500, -500, 500)),\n                               \"Speed\", ns.core.StringValue (\"ns3::ConstantRandomVariable[Constant=2]\"),\n                               \"Pause\", ns.core.StringValue (\"ns3::ConstantRandomVariable[Constant=0.2]\"))\n    mobility.Install(backbone)\n\n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n    #                                                                        # \n    #  Construct the LANs                                                    # \n    #                                                                        # \n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n\n    #  Reset the address base-- all of the CSMA networks will be in\n    #  the \"172.16 address space\n    ipAddrs.SetBase(ns.network.Ipv4Address(\"172.16.0.0\"), ns.network.Ipv4Mask(\"255.255.255.0\"))\n\n    for i in range(backboneNodes):\n        print \"Configuring local area network for backbone node \", i\n        # \n        #  Create a container to manage the nodes of the LAN.  We need\n        #  two containers here; one with all of the new nodes, and one\n        #  with all of the nodes including new and existing nodes\n        # \n        newLanNodes = ns.network.NodeContainer()\n        newLanNodes.Create(lanNodes - 1)\n        #  Now, create the container with all nodes on this link\n        lan = ns.network.NodeContainer(ns.network.NodeContainer(backbone.Get(i)), newLanNodes)\n        # \n        #  Create the CSMA net devices and install them into the nodes in our \n        #  collection.\n        # \n        csma = ns.csma.CsmaHelper()\n        csma.SetChannelAttribute(\"DataRate\", ns.network.DataRateValue(ns.network.DataRate(5000000)))\n        csma.SetChannelAttribute(\"Delay\", ns.core.TimeValue(ns.core.MilliSeconds(2)))\n        lanDevices = csma.Install(lan)\n        # \n        #  Add the IPv4 protocol stack to the new LAN nodes\n        # \n        internet.Install(newLanNodes)\n        # \n        #  Assign IPv4 addresses to the device drivers(actually to the \n        #  associated IPv4 interfaces) we just created.\n        # \n        ipAddrs.Assign(lanDevices)\n        # \n        #  Assign a new network prefix for the next LAN, according to the\n        #  network mask initialized above\n        # \n        ipAddrs.NewNetwork()\n        #\n        # The new LAN nodes need a mobility model so we aggregate one\n        # to each of the nodes we just finished building.\n        #\n        mobilityLan = ns.mobility.MobilityHelper() \n        positionAlloc = ns.mobility.ListPositionAllocator()\n        for j in range(newLanNodes.GetN()):\n            positionAlloc.Add(ns.core.Vector(0.0, (j*10 + 10), 0.0))\n\n        mobilityLan.SetPositionAllocator(positionAlloc)\n        mobilityLan.PushReferenceMobilityModel(backbone.Get(i))\n        mobilityLan.SetMobilityModel(\"ns3::ConstantPositionMobilityModel\")\n        mobilityLan.Install(newLanNodes);\n\n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n    #                                                                        # \n    #  Construct the mobile networks                                         # \n    #                                                                        # \n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n\n    #  Reset the address base-- all of the 802.11 networks will be in\n    #  the \"10.0\" address space\n    ipAddrs.SetBase(ns.network.Ipv4Address(\"10.0.0.0\"), ns.network.Ipv4Mask(\"255.255.255.0\"))\n\n    for i in range(backboneNodes):\n        print \"Configuring wireless network for backbone node \", i\n        # \n        #  Create a container to manage the nodes of the LAN.  We need\n        #  two containers here; one with all of the new nodes, and one\n        #  with all of the nodes including new and existing nodes\n        # \n        stas = ns.network.NodeContainer()\n        stas.Create(infraNodes - 1)\n        #  Now, create the container with all nodes on this link\n        infra = ns.network.NodeContainer(ns.network.NodeContainer(backbone.Get(i)), stas)\n        # \n        #  Create another ad hoc network and devices\n        # \n        ssid = ns.wifi.Ssid('wifi-infra' + str(i))\n        wifiInfra = ns.wifi.WifiHelper.Default()\n        wifiPhy.SetChannel(wifiChannel.Create())\n        wifiInfra.SetRemoteStationManager('ns3::ArfWifiManager')\n        macInfra = ns.wifi.WifiMacHelper();\n        macInfra.SetType(\"ns3::StaWifiMac\",\n                         \"Ssid\", ns.wifi.SsidValue(ssid),\n                         \"ActiveProbing\", ns.core.BooleanValue(False))\n\n        # setup stas\n        staDevices = wifiInfra.Install(wifiPhy, macInfra, stas)\n        # setup ap.\n        macInfra.SetType(\"ns3::ApWifiMac\",\n                         \"Ssid\", ns.wifi.SsidValue(ssid),\n                         \"BeaconGeneration\", ns.core.BooleanValue(True),\n                         \"BeaconInterval\", ns.core.TimeValue(ns.core.Seconds(2.5)))\n        apDevices = wifiInfra.Install(wifiPhy, macInfra, backbone.Get(i))\n        # Collect all of these new devices\n        infraDevices = ns.network.NetDeviceContainer(apDevices, staDevices)\n\n        #  Add the IPv4 protocol stack to the nodes in our container\n        # \n        internet.Install(stas)\n        # \n        #  Assign IPv4 addresses to the device drivers(actually to the associated\n        #  IPv4 interfaces) we just created.\n        # \n        ipAddrs.Assign(infraDevices)\n        # \n        #  Assign a new network prefix for each mobile network, according to \n        #  the network mask initialized above\n        # \n        ipAddrs.NewNetwork()\n        # \n        #  The new wireless nodes need a mobility model so we aggregate one \n        #  to each of the nodes we just finished building.\n        # \n        subnetAlloc = ns.mobility.ListPositionAllocator()\n        for j in range(infra.GetN()):\n            subnetAlloc.Add(ns.core.Vector(0.0, j, 0.0))\n\n        mobility.PushReferenceMobilityModel(backbone.Get(i))\n        mobility.SetPositionAllocator(subnetAlloc)\n        mobility.SetMobilityModel(\"ns3::RandomDirection2dMobilityModel\",\n                                  \"Bounds\", ns.mobility.RectangleValue(ns.mobility.Rectangle(-10, 10, -10, 10)),\n                                  \"Speed\", ns.core.StringValue (\"ns3::ConstantRandomVariable[Constant=3]\"),\n                                  \"Pause\", ns.core.StringValue (\"ns3::ConstantRandomVariable[Constant=0.4]\"))\n        mobility.Install(stas)\n\n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n    #                                                                        # \n    #  Application configuration                                             # \n    #                                                                        # \n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n\n    #  Create the OnOff application to send UDP datagrams of size\n    #  210 bytes at a rate of 448 Kb\/s, between two nodes\n    print \"Create Applications.\"\n    port = 9   #  Discard port(RFC 863)\n\n    appSource = ns.network.NodeList.GetNode(backboneNodes)\n    lastNodeIndex = backboneNodes + backboneNodes*(lanNodes - 1) + backboneNodes*(infraNodes - 1) - 1\n    appSink = ns.network.NodeList.GetNode(lastNodeIndex)\n    # Let's fetch the IP address of the last node, which is on Ipv4Interface 1\n    remoteAddr = appSink.GetObject(ns.internet.Ipv4.GetTypeId()).GetAddress(1,0).GetLocal()\n\n    onoff = ns.applications.OnOffHelper(\"ns3::UdpSocketFactory\", \n                            ns.network.Address(ns.network.InetSocketAddress(remoteAddr, port)))\n    apps = onoff.Install(ns.network.NodeContainer(appSource))\n    apps.Start(ns.core.Seconds(3))\n    apps.Stop(ns.core.Seconds(stopTime - 1))\n\n    #  Create a packet sink to receive these packets\n    sink = ns.applications.PacketSinkHelper(\"ns3::UdpSocketFactory\", \n                                ns.network.InetSocketAddress(ns.network.Ipv4Address.GetAny(), port))\n    apps = sink.Install(ns.network.NodeContainer(appSink))\n    apps.Start(ns.core.Seconds(3))\n\n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n    #                                                                        # \n    #  Tracing configuration                                                 # \n    #                                                                        # \n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \/ \n\n    print \"Configure Tracing.\"\n    csma = ns.csma.CsmaHelper()\n    # \n    #  Let's set up some ns-2-like ascii traces, using another helper class\n    # \n    ascii = ns.network.AsciiTraceHelper();\n    stream = ascii.CreateFileStream(\"mixed-wireless.tr\");\n    wifiPhy.EnableAsciiAll(stream);\n    csma.EnableAsciiAll(stream);\n    internet.EnableAsciiIpv4All(stream);\n\n    #  Csma captures in non-promiscuous mode\n    csma.EnablePcapAll(\"mixed-wireless\", False)\n    #  Let's do a pcap trace on the backbone devices\n    wifiPhy.EnablePcap(\"mixed-wireless\", backboneDevices)\n    wifiPhy.EnablePcap(\"mixed-wireless\", appSink.GetId(), 0)\n\n#   #ifdef ENABLE_FOR_TRACING_EXAMPLE\n#     Config.Connect(\"\/NodeList\/*\/$MobilityModel\/CourseChange\",\n#       MakeCallback(&CourseChangeCallback))\n#   #endif\n\n\n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #  \n    #                                                                        # \n    #  Run simulation                                                        # \n    #                                                                        # \n    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #  \n\n    print \"Run Simulation.\"\n    ns.core.Simulator.Stop(ns.core.Seconds(stopTime))\n    ns.core.Simulator.Run()\n    ns.core.Simulator.Destroy()\n\n\nif __name__ == '__main__':\n    import sys\n    main(sys.argv)\n\n\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n\"\"\"\nPlot oscilloscope files from MultiSim\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport os\nfrom matplotlib import rc\n\nrc('font',family=\"Consolas\")\nfiles=[\"real_zad5_05f_p2.txt\"]\nfor NazwaPliku in files:\n    print NazwaPliku\n    Plik=open(NazwaPliku)\n    #print DeltaT\n    Dane=Plik.readlines()#[4:]\n    DeltaT=float(Dane[2].split()[3].replace(\",\",\".\"))\n    #M=len(Dane[4].split())\/2\n    M=2\n    Dane=Dane[5:]\n    Plik.close()\n\n    print M\n    Ys=[np.zeros(len(Dane)) for i in range(M)]\n\n    for m in range(M):\n        for i in range(len(Dane)):\n            try:\n                Ys[m][i]=float(Dane[i].split()[2+3*m].replace(\",\",\".\"))\n            except:\n                print m, i, 2+3*m, len(Dane[i].split()), Dane[i].split()\n        #print i, Y[i]\n    X=np.zeros_like(Ys[0])\n    for i in range(len(X)):\n        X[i]=i*DeltaT\n\n    for y in Ys:\n        print max(y)-min(y)\n    Opis=u\"Uk\u0142ad szeregowy\\nPo\u0142owa cz\u0119stotliwo\u015bci rezonansowej\"\n    Nazwa=u\"Z5W2\"\n    plt.title(u\"Przebieg napi\u0119ciowy\\n\"+Opis)\n    plt.xlabel(u\"Czas t [s]\")\n    plt.ylabel(u\"Napi\u0119cie [V]\")\n    plt.plot(X,Ys[0],label=u\"Wej\u015bcie\")\n    plt.plot(X,Ys[1],label=u\"Wyj\u015bcie\")\n    plt.grid()\n    plt.legend(loc=\"best\")\n    plt.savefig(Nazwa + \".png\", bbox_inches='tight')\n    plt.show()\n\n\n\n","label":0}
{"content":"# Copyright (C) 2008 One Laptop Per Child\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport os\nimport logging\nfrom gettext import gettext as _\n\nfrom gi.repository import GObject\nfrom gi.repository import Gtk\nfrom gi.repository import Gdk\nfrom gi.repository import GdkX11\n\nfrom sugar3.graphics.icon import Icon\nfrom sugar3.graphics import style\nfrom sugar3.graphics.alert import Alert, TimeoutAlert\n\nfrom jarabe.model.session import get_session_manager\nfrom jarabe.controlpanel.toolbar import MainToolbar\nfrom jarabe.controlpanel.toolbar import SectionToolbar\nfrom jarabe import config\nfrom jarabe.model import shell\n\n_logger = logging.getLogger('ControlPanel')\n\n\nclass ControlPanel(Gtk.Window):\n    __gtype_name__ = 'SugarControlPanel'\n\n    def __init__(self, window_xid=0):\n        self.parent_window_xid = window_xid\n        Gtk.Window.__init__(self)\n\n        self._calculate_max_columns()\n        self.set_border_width(style.LINE_WIDTH)\n        self.set_position(Gtk.WindowPosition.CENTER_ALWAYS)\n        self.set_decorated(False)\n        self.set_resizable(False)\n        self.set_modal(True)\n\n        self.set_can_focus(True)\n        self.connect('key-press-event', self.__key_press_event_cb)\n\n        self._toolbar = None\n        self._canvas = None\n        self._table = None\n        self._scrolledwindow = None\n        self._separator = None\n        self._section_view = None\n        self._section_toolbar = None\n        self._main_toolbar = None\n\n        self._vbox = Gtk.VBox()\n        self._hbox = Gtk.HBox()\n        self._vbox.pack_start(self._hbox, True, True, 0)\n        self._hbox.show()\n\n        self._main_view = Gtk.EventBox()\n        self._hbox.pack_start(self._main_view, True, True, 0)\n        self._main_view.modify_bg(Gtk.StateType.NORMAL,\n                                  style.COLOR_BLACK.get_gdk_color())\n        self._main_view.show()\n\n        self.add(self._vbox)\n        self._vbox.show()\n\n        self.connect('realize', self.__realize_cb)\n\n        self._options = self._get_options()\n        self._current_option = None\n        self._setup_main()\n        self._setup_section()\n        self._show_main_view()\n        Gdk.Screen.get_default().connect(\n            'size-changed', self.__size_changed_cb)\n\n        self._busy_count = 0\n        self._selected = []\n\n    def __realize_cb(self, widget):\n        self.set_type_hint(Gdk.WindowTypeHint.DIALOG)\n        window = self.get_window()\n        window.set_accept_focus(True)\n        if self.parent_window_xid > 0:\n            display = Gdk.Display.get_default()\n            parent = GdkX11.X11Window.foreign_new_for_display(\n                display, self.parent_window_xid)\n            window.set_transient_for(parent)\n\n        # the modal windows counter is updated to disable hot keys - SL#4601\n        shell.get_model().push_modal()\n\n    def __size_changed_cb(self, event):\n        self._calculate_max_columns()\n\n    def busy(self):\n        if self._busy_count == 0:\n            self._old_cursor = self.get_window().get_cursor()\n            self._set_cursor(Gdk.Cursor.new(Gdk.CursorType.WATCH))\n        self._busy_count += 1\n\n    def unbusy(self):\n        self._busy_count -= 1\n        if self._busy_count == 0:\n            self._set_cursor(self._old_cursor)\n\n    def _set_cursor(self, cursor):\n        self.get_window().set_cursor(cursor)\n        Gdk.flush()\n\n    def add_alert(self, alert):\n        self._vbox.pack_start(alert, False, False, 0)\n        self._vbox.reorder_child(alert, 2)\n\n    def remove_alert(self, alert):\n        self._vbox.remove(alert)\n\n    def grab_focus(self):\n        # overwrite grab focus in order to grab focus on the view\n        self._main_view.get_child().grab_focus()\n\n    def _calculate_max_columns(self):\n        self._max_columns = int(0.285 * (float(Gdk.Screen.width()) \/\n                                         style.GRID_CELL_SIZE - 3))\n        offset = style.GRID_CELL_SIZE\n        width = Gdk.Screen.width() - offset * 2\n        height = Gdk.Screen.height() - offset * 2\n        self.set_size_request(width, height)\n        if hasattr(self, '_table'):\n            for child in self._table.get_children():\n                child.destroy()\n            self._setup_options()\n\n    def _set_canvas(self, canvas):\n        if self._canvas in self._main_view:\n            self._main_view.remove(self._canvas)\n        if canvas:\n            self._main_view.add(canvas)\n        self._canvas = canvas\n\n    def _set_toolbar(self, toolbar):\n        if self._toolbar:\n            self._vbox.remove(self._toolbar)\n        self._vbox.pack_start(toolbar, False, False, 0)\n        self._vbox.reorder_child(toolbar, 0)\n        self._toolbar = toolbar\n        if not self._separator:\n            self._separator = Gtk.HSeparator()\n            self._vbox.pack_start(self._separator, False, False, 0)\n            self._vbox.reorder_child(self._separator, 1)\n            self._separator.show()\n\n    def _setup_main(self):\n        self._main_toolbar = MainToolbar()\n\n        self._table = Gtk.Table()\n        self._table.set_col_spacings(style.GRID_CELL_SIZE)\n        self._table.set_row_spacings(style.GRID_CELL_SIZE)\n        self._table.set_border_width(style.GRID_CELL_SIZE)\n\n        self._scrolledwindow = Gtk.ScrolledWindow()\n        self._scrolledwindow.set_can_focus(False)\n        self._scrolledwindow.set_policy(Gtk.PolicyType.AUTOMATIC,\n                                        Gtk.PolicyType.AUTOMATIC)\n        self._scrolledwindow.add_with_viewport(self._table)\n        child = self._scrolledwindow.get_child()\n        child.modify_bg(\n            Gtk.StateType.NORMAL, style.COLOR_BLACK.get_gdk_color())\n\n        self._setup_options()\n        self._main_toolbar.connect('stop-clicked',\n                                   self.__stop_clicked_cb)\n        self._main_toolbar.connect('search-changed',\n                                   self.__search_changed_cb)\n\n    def _setup_options(self):\n        # If the screen width only supports two columns, start\n        # placing from the second row.\n        if self._max_columns == 2:\n            row = 1\n            column = 0\n        else:\n            # About Me and About my computer are hardcoded below to use the\n            # first two slots so we need to leave them free.\n            row = 0\n            column = 2\n\n        options = self._options.keys()\n        options.sort()\n\n        for option in options:\n            sectionicon = _SectionIcon(icon_name=self._options[option]['icon'],\n                                       title=self._options[option]['title'],\n                                       xo_color=self._options[option]['color'],\n                                       pixel_size=style.GRID_CELL_SIZE)\n            sectionicon.connect('button_press_event',\n                                self.__select_option_cb, option)\n            sectionicon.show()\n\n            if option == 'aboutme':\n                self._table.attach(sectionicon, 0, 1, 0, 1)\n            elif option == 'aboutcomputer':\n                self._table.attach(sectionicon, 1, 2, 0, 1)\n            else:\n                self._table.attach(sectionicon,\n                                   column, column + 1,\n                                   row, row + 1)\n                column += 1\n                if column == self._max_columns:\n                    column = 0\n                    row += 1\n\n            self._options[option]['button'] = sectionicon\n\n    def _show_main_view(self):\n        if self._section_view is not None:\n            self._section_view.destroy()\n            self._section_view = None\n\n        self._set_toolbar(self._main_toolbar)\n        self._main_toolbar.show()\n        self._set_canvas(self._scrolledwindow)\n        self._main_view.modify_bg(Gtk.StateType.NORMAL,\n                                  style.COLOR_BLACK.get_gdk_color())\n        self._table.show()\n        self._scrolledwindow.show()\n        entry = self._main_toolbar.get_entry()\n        entry.set_text('')\n        entry.connect('icon-press', self.__clear_icon_pressed_cb)\n        self.grab_focus()\n\n    def __key_press_event_cb(self, window, event):\n        if event.keyval == Gdk.KEY_Return:\n            if len(self._selected) == 1:\n                self.show_section_view(self._selected[0])\n                return True\n\n        if event.keyval == Gdk.KEY_Escape:\n            if self._toolbar == self._main_toolbar:\n                self.__stop_clicked_cb(None)\n                self.destroy()\n            else:\n                self.__cancel_clicked_cb(None)\n            return True\n\n        # if the user clicked out of the window - fix SL #3188\n        if not self.is_active():\n            self.present()\n\n        entry = self._main_toolbar.get_entry()\n        if not entry.has_focus():\n            entry.grab_focus()\n        return False\n\n    def __clear_icon_pressed_cb(self, entry, icon_pos, event):\n        self.grab_focus()\n\n    def _update(self, query):\n        self._selected = []\n        for option in self._options:\n            found = False\n            for key in self._options[option]['keywords']:\n                if query.lower() in key.lower():\n                    self._options[option]['button'].set_sensitive(True)\n                    self._selected.append(option)\n                    found = True\n                    break\n            if not found:\n                self._options[option]['button'].set_sensitive(False)\n\n    def _setup_section(self):\n        self._section_toolbar = SectionToolbar()\n        self._section_toolbar.connect('cancel-clicked',\n                                      self.__cancel_clicked_cb)\n        self._section_toolbar.connect('accept-clicked',\n                                      self.__accept_clicked_cb)\n\n    def show_section_view(self, option):\n        self._set_toolbar(self._section_toolbar)\n\n        icon = self._section_toolbar.get_icon()\n        icon.set_from_icon_name(self._options[option]['icon'],\n                                Gtk.IconSize.LARGE_TOOLBAR)\n        icon.props.xo_color = self._options[option]['color']\n        title = self._section_toolbar.get_title()\n        title.set_text(self._options[option]['title'])\n        self._section_toolbar.show()\n\n        self._current_option = option\n\n        mod = __import__('.'.join(('cpsection', option, 'view')),\n                         globals(), locals(), ['view'])\n        view_class = getattr(mod, self._options[option]['view'], None)\n\n        mod = __import__('.'.join(('cpsection', option, 'model')),\n                         globals(), locals(), ['model'])\n        model = ModelWrapper(mod)\n\n        try:\n            self.busy()\n            self._section_view = view_class(model,\n                                            self._options[option]['alerts'])\n\n            self._set_canvas(self._section_view)\n            self._section_view.show()\n        finally:\n            self.unbusy()\n\n        self._section_view.connect('notify::is-valid',\n                                   self.__valid_section_cb)\n        self._section_view.connect('notify::is-cancellable',\n                                   self.__cancellable_section_cb)\n        self._section_view.connect('request-close',\n                                   self.__close_request_cb)\n        self._section_view.connect('add-alert',\n                                   self.__create_restart_alert_cb)\n        self._section_view.connect('set-toolbar-sensitivity',\n                                   self.__set_toolbar_sensitivity_cb)\n        self._main_view.modify_bg(Gtk.StateType.NORMAL,\n                                  style.COLOR_WHITE.get_gdk_color())\n\n    def set_section_view_auto_close(self):\n        \"\"\"Automatically close the control panel if there is \"nothing to do\"\n        \"\"\"\n        self._section_view.auto_close = True\n\n    def _get_options(self):\n        \"\"\"Get the available option information from the extensions\n        \"\"\"\n        options = {}\n\n        path = os.path.join(config.ext_path, 'cpsection')\n        folder = os.listdir(path)\n\n        for item in folder:\n            if os.path.isdir(os.path.join(path, item)) and \\\n                    os.path.exists(os.path.join(path, item, '__init__.py')):\n                try:\n                    mod = __import__('.'.join(('cpsection', item)),\n                                     globals(), locals(), [item])\n                    view_class = getattr(mod, 'CLASS', None)\n                    if view_class is not None:\n                        options[item] = {}\n                        options[item]['alerts'] = []\n                        options[item]['view'] = view_class\n                        options[item]['icon'] = getattr(mod, 'ICON', item)\n                        options[item]['title'] = getattr(mod, 'TITLE', item)\n                        options[item]['color'] = getattr(mod, 'COLOR', None)\n                        keywords = getattr(mod, 'KEYWORDS', [])\n                        keywords.append(options[item]['title'].lower())\n                        if item not in keywords:\n                            keywords.append(item)\n                        options[item]['keywords'] = keywords\n                    else:\n                        _logger.debug('no CLASS attribute in %r', item)\n                except Exception:\n                    logging.exception('Exception while loading extension:')\n\n        return options\n\n    def __cancel_clicked_cb(self, widget):\n        self._section_view.undo()\n        self._options[self._current_option]['alerts'] = []\n        self._section_toolbar.accept_button.set_sensitive(True)\n        self._show_main_view()\n\n    def __accept_clicked_cb(self, widget):\n        if hasattr(self._section_view, \"apply\"):\n            self._section_view.apply()\n\n        if self._section_view.needs_restart:\n            self.__set_toolbar_sensitivity_cb(False)\n            if self._section_view.show_restart_alert:\n                self.__create_restart_alert_cb()\n        else:\n            self._show_main_view()\n\n    def __set_toolbar_sensitivity_cb(self, value=True,\n                                     widget=None, event=None):\n        self._section_toolbar.accept_button.set_sensitive(value)\n        self._section_toolbar.cancel_button.set_sensitive(value)\n\n    def __create_restart_alert_cb(self, widget=None, event=None):\n        alert = Alert()\n        alert.props.title = _('Warning')\n        alert.props.msg = self._section_view.restart_msg\n\n        if self._section_view.props.is_cancellable:\n            icon = Icon(icon_name='dialog-cancel')\n            alert.add_button(Gtk.ResponseType.CANCEL,\n                             _('Cancel changes'), icon)\n            icon.show()\n\n        if self._section_view.props.is_deferrable:\n            icon = Icon(icon_name='dialog-ok')\n            alert.add_button(Gtk.ResponseType.ACCEPT, _('Later'), icon)\n            icon.show()\n\n        icon = Icon(icon_name='system-restart')\n        alert.add_button(Gtk.ResponseType.APPLY, _('Restart now'), icon)\n        icon.show()\n\n        self.add_alert(alert)\n        alert.connect('response', self.__response_cb)\n        alert.show()\n\n    def __response_cb(self, alert, response_id):\n        self.remove_alert(alert)\n        self._section_toolbar.accept_button.set_sensitive(True)\n        self._section_toolbar.cancel_button.set_sensitive(True)\n        if response_id is Gtk.ResponseType.CANCEL:\n            self._section_view.undo()\n            self._section_view.setup()\n            self._options[self._current_option]['alerts'] = []\n        elif response_id is Gtk.ResponseType.ACCEPT:\n            self._options[self._current_option]['alerts'] = \\\n                self._section_view.restart_alerts\n            self._show_main_view()\n        elif response_id is Gtk.ResponseType.APPLY:\n            self.busy()\n            self._section_toolbar.accept_button.set_sensitive(False)\n            self._section_toolbar.cancel_button.set_sensitive(False)\n            get_session_manager().logout()\n            GObject.timeout_add_seconds(4, self.__quit_timeout_cb)\n\n    def __quit_timeout_cb(self):\n        self.unbusy()\n        alert = TimeoutAlert(30)\n        alert.props.title = _('An activity is not responding.')\n        alert.props.msg = _('You may lose unsaved work if you continue.')\n        alert.connect('response', self.__quit_accept_cb)\n\n        self.add_alert(alert)\n        alert.show()\n\n    def __quit_accept_cb(self, alert, response_id):\n        self.remove_alert(alert)\n        if response_id is Gtk.ResponseType.CANCEL:\n            get_session_manager().cancel_shutdown()\n            self._section_toolbar.accept_button.set_sensitive(True)\n            self._section_toolbar.cancel_button.set_sensitive(True)\n        else:\n            self.busy()\n            get_session_manager().shutdown_completed()\n\n    def __select_option_cb(self, button, event, option):\n        self.show_section_view(option)\n\n    def __search_changed_cb(self, maintoolbar, query):\n        self._update(query)\n\n    def __stop_clicked_cb(self, widget):\n        shell.get_model().pop_modal()\n        self.destroy()\n\n    def __close_request_cb(self, widget, event=None):\n        self.destroy()\n\n    def __valid_section_cb(self, section_view, pspec):\n        section_is_valid = section_view.props.is_valid\n        self._section_toolbar.accept_button.set_sensitive(section_is_valid)\n\n    def __cancellable_section_cb(self, section_view, pspec):\n        cancellable = section_view.props.is_cancellable\n        self._section_toolbar.cancel_button.set_sensitive(cancellable)\n\n\nclass ModelWrapper(object):\n\n    def __init__(self, module):\n        self._module = module\n        self._options = {}\n        self._setup()\n\n    def _setup(self):\n        methods = dir(self._module)\n        for method in methods:\n            if method.startswith('get_') and method[4:] != 'color':\n                try:\n                    self._options[method[4:]] = getattr(self._module, method)()\n                except Exception:\n                    self._options[method[4:]] = None\n\n    def __getattr__(self, name):\n        return getattr(self._module, name)\n\n    def undo(self):\n        for key in self._options.keys():\n            method = getattr(self._module, 'set_' + key, None)\n            if method and self._options[key] is not None:\n                try:\n                    method(self._options[key])\n                except Exception as detail:\n                    _logger.debug('Error undo option: %s', detail)\nif hasattr(ControlPanel, 'set_css_name'):\n    ControlPanel.set_css_name('controlpanel')\n\n\nclass _SectionIcon(Gtk.EventBox):\n    __gtype_name__ = 'SugarSectionIcon'\n\n    __gproperties__ = {\n        'icon-name': (str, None, None, None, GObject.PARAM_READWRITE),\n        'pixel-size': (object, None, None, GObject.PARAM_READWRITE),\n        'xo-color': (object, None, None, GObject.PARAM_READWRITE),\n        'title': (str, None, None, None, GObject.PARAM_READWRITE),\n    }\n\n    def __init__(self, **kwargs):\n        self._icon_name = None\n        self._pixel_size = style.GRID_CELL_SIZE\n        self._xo_color = None\n        self._title = 'No Title'\n\n        Gtk.EventBox.__init__(self, **kwargs)\n\n        self._vbox = Gtk.VBox()\n        self._icon = Icon(icon_name=self._icon_name,\n                          pixel_size=self._pixel_size,\n                          xo_color=self._xo_color)\n        self._vbox.pack_start(self._icon, expand=False, fill=False, padding=0)\n\n        self._label = Gtk.Label(label=self._title)\n        self._label.modify_fg(Gtk.StateType.NORMAL,\n                              style.COLOR_WHITE.get_gdk_color())\n        self._vbox.pack_start(self._label, expand=False, fill=False, padding=0)\n\n        self._vbox.set_spacing(style.DEFAULT_SPACING)\n        self.set_visible_window(False)\n        self.set_app_paintable(True)\n        self.set_events(Gdk.EventMask.BUTTON_PRESS_MASK)\n\n        self.add(self._vbox)\n        self._vbox.show()\n        self._label.show()\n        self._icon.show()\n\n    def get_icon(self):\n        return self._icon\n\n    def do_set_property(self, pspec, value):\n        if pspec.name == 'icon-name':\n            if self._icon_name != value:\n                self._icon_name = value\n        elif pspec.name == 'pixel-size':\n            if self._pixel_size != value:\n                self._pixel_size = value\n        elif pspec.name == 'xo-color':\n            if self._xo_color != value:\n                self._xo_color = value\n        elif pspec.name == 'title':\n            if self._title != value:\n                self._title = value\n\n    def do_get_property(self, pspec):\n        if pspec.name == 'icon-name':\n            return self._icon_name\n        elif pspec.name == 'pixel-size':\n            return self._pixel_size\n        elif pspec.name == 'xo-color':\n            return self._xo_color\n        elif pspec.name == 'title':\n            return self._title\n","label":0}
{"content":"#!\/usr\/bin\/env python\n\n\"\"\"\n   Get rid of lines containing duplicate copies of the same atom in the \"Atoms\"\n   section of a LAMMPS data file.  Duplicate lines which occur later are\n   preserved and the earlier lines are erased.\n   The file is read from sys.stdin.  This program does not parse the entire\n   data file.  The text from the \"Atoms\" section of the LAMMPS file must\n   be extracted in advance before it is sent to this program.)\n\n\"\"\"\n\nimport sys\n\ndef main():\n    in_stream = sys.stdin\n    f = None\n    fname = None\n    if len(sys.argv) == 2:\n        fname = sys.argv[1]\n        f = open(fname, 'r')\n        in_stream = f\n\n    atom_ids_in_use = set([])\n\n    lines = in_stream.readlines()\n\n    # Start at the end of the file and read backwards.\n    # If duplicate lines exist, eliminate the ones that occur earlier in the file.\n    i = len(lines)\n    while i > 0:\n        i -= 1\n        line_orig = lines[i]\n        line = line_orig.rstrip('\\n')\n        if '#' in line_orig:\n            ic = line.find('#')\n            line = line_orig[:ic]\n\n        tokens = line.strip().split()\n        if len(tokens) > 0:\n            atom_id = tokens[0]\n            if atom_id in atom_ids_in_use:\n                del lines[i]\n            else:\n                atom_ids_in_use.add(atom_id)\n        else:\n            del lines[i]\n\n\n    for line in lines:\n        sys.stdout.write(line)\n\n    if f != None:\n        f.close()\n\n    return\n\n\nif __name__ == '__main__':\n    main()\n","label":0}
{"content":"# Copyright (C) 2007-2011 Canonical Ltd\n#\n# This program is free software; you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n\n\"\"\"Indexing facilities.\"\"\"\n\nfrom __future__ import absolute_import\n\n__all__ = [\n    'CombinedGraphIndex',\n    'GraphIndex',\n    'GraphIndexBuilder',\n    'GraphIndexPrefixAdapter',\n    'InMemoryGraphIndex',\n    ]\n\nfrom bisect import bisect_right\nfrom cStringIO import StringIO\nimport re\nimport sys\n\nfrom bzrlib.lazy_import import lazy_import\nlazy_import(globals(), \"\"\"\nfrom bzrlib import (\n    bisect_multi,\n    revision as _mod_revision,\n    trace,\n    )\n\"\"\")\nfrom bzrlib import (\n    debug,\n    errors,\n    )\nfrom bzrlib.static_tuple import StaticTuple\n\n_HEADER_READV = (0, 200)\n_OPTION_KEY_ELEMENTS = \"key_elements=\"\n_OPTION_LEN = \"len=\"\n_OPTION_NODE_REFS = \"node_ref_lists=\"\n_SIGNATURE = \"Bazaar Graph Index 1\\n\"\n\n\n_whitespace_re = re.compile('[\\t\\n\\x0b\\x0c\\r\\x00 ]')\n_newline_null_re = re.compile('[\\n\\0]')\n\n\ndef _has_key_from_parent_map(self, key):\n    \"\"\"Check if this index has one key.\n\n    If it's possible to check for multiple keys at once through\n    calling get_parent_map that should be faster.\n    \"\"\"\n    return (key in self.get_parent_map([key]))\n\n\ndef _missing_keys_from_parent_map(self, keys):\n    return set(keys) - set(self.get_parent_map(keys))\n\n\nclass GraphIndexBuilder(object):\n    \"\"\"A builder that can build a GraphIndex.\n\n    The resulting graph has the structure::\n\n      _SIGNATURE OPTIONS NODES NEWLINE\n      _SIGNATURE     := 'Bazaar Graph Index 1' NEWLINE\n      OPTIONS        := 'node_ref_lists=' DIGITS NEWLINE\n      NODES          := NODE*\n      NODE           := KEY NULL ABSENT? NULL REFERENCES NULL VALUE NEWLINE\n      KEY            := Not-whitespace-utf8\n      ABSENT         := 'a'\n      REFERENCES     := REFERENCE_LIST (TAB REFERENCE_LIST){node_ref_lists - 1}\n      REFERENCE_LIST := (REFERENCE (CR REFERENCE)*)?\n      REFERENCE      := DIGITS  ; digits is the byte offset in the index of the\n                                ; referenced key.\n      VALUE          := no-newline-no-null-bytes\n    \"\"\"\n\n    def __init__(self, reference_lists=0, key_elements=1):\n        \"\"\"Create a GraphIndex builder.\n\n        :param reference_lists: The number of node references lists for each\n            entry.\n        :param key_elements: The number of bytestrings in each key.\n        \"\"\"\n        self.reference_lists = reference_lists\n        # A dict of {key: (absent, ref_lists, value)}\n        self._nodes = {}\n        # Keys that are referenced but not actually present in this index\n        self._absent_keys = set()\n        self._nodes_by_key = None\n        self._key_length = key_elements\n        self._optimize_for_size = False\n        self._combine_backing_indices = True\n\n    def _check_key(self, key):\n        \"\"\"Raise BadIndexKey if key is not a valid key for this index.\"\"\"\n        if type(key) not in (tuple, StaticTuple):\n            raise errors.BadIndexKey(key)\n        if self._key_length != len(key):\n            raise errors.BadIndexKey(key)\n        for element in key:\n            if not element or _whitespace_re.search(element) is not None:\n                raise errors.BadIndexKey(element)\n\n    def _external_references(self):\n        \"\"\"Return references that are not present in this index.\n        \"\"\"\n        keys = set()\n        refs = set()\n        # TODO: JAM 2008-11-21 This makes an assumption about how the reference\n        #       lists are used. It is currently correct for pack-0.92 through\n        #       1.9, which use the node references (3rd column) second\n        #       reference list as the compression parent. Perhaps this should\n        #       be moved into something higher up the stack, since it\n        #       makes assumptions about how the index is used.\n        if self.reference_lists > 1:\n            for node in self.iter_all_entries():\n                keys.add(node[1])\n                refs.update(node[3][1])\n            return refs - keys\n        else:\n            # If reference_lists == 0 there can be no external references, and\n            # if reference_lists == 1, then there isn't a place to store the\n            # compression parent\n            return set()\n\n    def _get_nodes_by_key(self):\n        if self._nodes_by_key is None:\n            nodes_by_key = {}\n            if self.reference_lists:\n                for key, (absent, references, value) in self._nodes.iteritems():\n                    if absent:\n                        continue\n                    key_dict = nodes_by_key\n                    for subkey in key[:-1]:\n                        key_dict = key_dict.setdefault(subkey, {})\n                    key_dict[key[-1]] = key, value, references\n            else:\n                for key, (absent, references, value) in self._nodes.iteritems():\n                    if absent:\n                        continue\n                    key_dict = nodes_by_key\n                    for subkey in key[:-1]:\n                        key_dict = key_dict.setdefault(subkey, {})\n                    key_dict[key[-1]] = key, value\n            self._nodes_by_key = nodes_by_key\n        return self._nodes_by_key\n\n    def _update_nodes_by_key(self, key, value, node_refs):\n        \"\"\"Update the _nodes_by_key dict with a new key.\n\n        For a key of (foo, bar, baz) create\n        _nodes_by_key[foo][bar][baz] = key_value\n        \"\"\"\n        if self._nodes_by_key is None:\n            return\n        key_dict = self._nodes_by_key\n        if self.reference_lists:\n            key_value = StaticTuple(key, value, node_refs)\n        else:\n            key_value = StaticTuple(key, value)\n        for subkey in key[:-1]:\n            key_dict = key_dict.setdefault(subkey, {})\n        key_dict[key[-1]] = key_value\n\n    def _check_key_ref_value(self, key, references, value):\n        \"\"\"Check that 'key' and 'references' are all valid.\n\n        :param key: A key tuple. Must conform to the key interface (be a tuple,\n            be of the right length, not have any whitespace or nulls in any key\n            element.)\n        :param references: An iterable of reference lists. Something like\n            [[(ref, key)], [(ref, key), (other, key)]]\n        :param value: The value associate with this key. Must not contain\n            newlines or null characters.\n        :return: (node_refs, absent_references)\n        \n            * node_refs: basically a packed form of 'references' where all\n              iterables are tuples\n            * absent_references: reference keys that are not in self._nodes.\n              This may contain duplicates if the same key is referenced in\n              multiple lists.\n        \"\"\"\n        as_st = StaticTuple.from_sequence\n        self._check_key(key)\n        if _newline_null_re.search(value) is not None:\n            raise errors.BadIndexValue(value)\n        if len(references) != self.reference_lists:\n            raise errors.BadIndexValue(references)\n        node_refs = []\n        absent_references = []\n        for reference_list in references:\n            for reference in reference_list:\n                # If reference *is* in self._nodes, then we know it has already\n                # been checked.\n                if reference not in self._nodes:\n                    self._check_key(reference)\n                    absent_references.append(reference)\n            reference_list = as_st([as_st(ref).intern()\n                                    for ref in reference_list])\n            node_refs.append(reference_list)\n        return as_st(node_refs), absent_references\n\n    def add_node(self, key, value, references=()):\n        \"\"\"Add a node to the index.\n\n        :param key: The key. keys are non-empty tuples containing\n            as many whitespace-free utf8 bytestrings as the key length\n            defined for this index.\n        :param references: An iterable of iterables of keys. Each is a\n            reference to another key.\n        :param value: The value to associate with the key. It may be any\n            bytes as long as it does not contain \\\\0 or \\\\n.\n        \"\"\"\n        (node_refs,\n         absent_references) = self._check_key_ref_value(key, references, value)\n        if key in self._nodes and self._nodes[key][0] != 'a':\n            raise errors.BadIndexDuplicateKey(key, self)\n        for reference in absent_references:\n            # There may be duplicates, but I don't think it is worth worrying\n            # about\n            self._nodes[reference] = ('a', (), '')\n        self._absent_keys.update(absent_references)\n        self._absent_keys.discard(key)\n        self._nodes[key] = ('', node_refs, value)\n        if self._nodes_by_key is not None and self._key_length > 1:\n            self._update_nodes_by_key(key, value, node_refs)\n\n    def clear_cache(self):\n        \"\"\"See GraphIndex.clear_cache()\n\n        This is a no-op, but we need the api to conform to a generic 'Index'\n        abstraction.\n        \"\"\"\n        \n    def finish(self):\n        \"\"\"Finish the index.\n\n        :returns: cStringIO holding the full context of the index as it \n        should be written to disk.\n        \"\"\"\n        lines = [_SIGNATURE]\n        lines.append(_OPTION_NODE_REFS + str(self.reference_lists) + '\\n')\n        lines.append(_OPTION_KEY_ELEMENTS + str(self._key_length) + '\\n')\n        key_count = len(self._nodes) - len(self._absent_keys)\n        lines.append(_OPTION_LEN + str(key_count) + '\\n')\n        prefix_length = sum(len(x) for x in lines)\n        # references are byte offsets. To avoid having to do nasty\n        # polynomial work to resolve offsets (references to later in the\n        # file cannot be determined until all the inbetween references have\n        # been calculated too) we pad the offsets with 0's to make them be\n        # of consistent length. Using binary offsets would break the trivial\n        # file parsing.\n        # to calculate the width of zero's needed we do three passes:\n        # one to gather all the non-reference data and the number of references.\n        # one to pad all the data with reference-length and determine entry\n        # addresses.\n        # One to serialise.\n\n        # forward sorted by key. In future we may consider topological sorting,\n        # at the cost of table scans for direct lookup, or a second index for\n        # direct lookup\n        nodes = sorted(self._nodes.items())\n        # if we do not prepass, we don't know how long it will be up front.\n        expected_bytes = None\n        # we only need to pre-pass if we have reference lists at all.\n        if self.reference_lists:\n            key_offset_info = []\n            non_ref_bytes = prefix_length\n            total_references = 0\n            # TODO use simple multiplication for the constants in this loop.\n            for key, (absent, references, value) in nodes:\n                # record the offset known *so far* for this key:\n                # the non reference bytes to date, and the total references to\n                # date - saves reaccumulating on the second pass\n                key_offset_info.append((key, non_ref_bytes, total_references))\n                # key is literal, value is literal, there are 3 null's, 1 NL\n                # key is variable length tuple, \\x00 between elements\n                non_ref_bytes += sum(len(element) for element in key)\n                if self._key_length > 1:\n                    non_ref_bytes += self._key_length - 1\n                # value is literal bytes, there are 3 null's, 1 NL.\n                non_ref_bytes += len(value) + 3 + 1\n                # one byte for absent if set.\n                if absent:\n                    non_ref_bytes += 1\n                elif self.reference_lists:\n                    # (ref_lists -1) tabs\n                    non_ref_bytes += self.reference_lists - 1\n                    # (ref-1 cr's per ref_list)\n                    for ref_list in references:\n                        # how many references across the whole file?\n                        total_references += len(ref_list)\n                        # accrue reference separators\n                        if ref_list:\n                            non_ref_bytes += len(ref_list) - 1\n            # how many digits are needed to represent the total byte count?\n            digits = 1\n            possible_total_bytes = non_ref_bytes + total_references*digits\n            while 10 ** digits < possible_total_bytes:\n                digits += 1\n                possible_total_bytes = non_ref_bytes + total_references*digits\n            expected_bytes = possible_total_bytes + 1 # terminating newline\n            # resolve key addresses.\n            key_addresses = {}\n            for key, non_ref_bytes, total_references in key_offset_info:\n                key_addresses[key] = non_ref_bytes + total_references*digits\n            # serialise\n            format_string = '%%0%sd' % digits\n        for key, (absent, references, value) in nodes:\n            flattened_references = []\n            for ref_list in references:\n                ref_addresses = []\n                for reference in ref_list:\n                    ref_addresses.append(format_string % key_addresses[reference])\n                flattened_references.append('\\r'.join(ref_addresses))\n            string_key = '\\x00'.join(key)\n            lines.append(\"%s\\x00%s\\x00%s\\x00%s\\n\" % (string_key, absent,\n                '\\t'.join(flattened_references), value))\n        lines.append('\\n')\n        result = StringIO(''.join(lines))\n        if expected_bytes and len(result.getvalue()) != expected_bytes:\n            raise errors.BzrError('Failed index creation. Internal error:'\n                ' mismatched output length and expected length: %d %d' %\n                (len(result.getvalue()), expected_bytes))\n        return result\n\n    def set_optimize(self, for_size=None, combine_backing_indices=None):\n        \"\"\"Change how the builder tries to optimize the result.\n\n        :param for_size: Tell the builder to try and make the index as small as\n            possible.\n        :param combine_backing_indices: If the builder spills to disk to save\n            memory, should the on-disk indices be combined. Set to True if you\n            are going to be probing the index, but to False if you are not. (If\n            you are not querying, then the time spent combining is wasted.)\n        :return: None\n        \"\"\"\n        # GraphIndexBuilder itself doesn't pay attention to the flag yet, but\n        # other builders do.\n        if for_size is not None:\n            self._optimize_for_size = for_size\n        if combine_backing_indices is not None:\n            self._combine_backing_indices = combine_backing_indices\n\n    def find_ancestry(self, keys, ref_list_num):\n        \"\"\"See CombinedGraphIndex.find_ancestry()\"\"\"\n        pending = set(keys)\n        parent_map = {}\n        missing_keys = set()\n        while pending:\n            next_pending = set()\n            for _, key, value, ref_lists in self.iter_entries(pending):\n                parent_keys = ref_lists[ref_list_num]\n                parent_map[key] = parent_keys\n                next_pending.update([p for p in parent_keys if p not in\n                                     parent_map])\n                missing_keys.update(pending.difference(parent_map))\n            pending = next_pending\n        return parent_map, missing_keys\n\n\nclass GraphIndex(object):\n    \"\"\"An index for data with embedded graphs.\n\n    The index maps keys to a list of key reference lists, and a value.\n    Each node has the same number of key reference lists. Each key reference\n    list can be empty or an arbitrary length. The value is an opaque NULL\n    terminated string without any newlines. The storage of the index is\n    hidden in the interface: keys and key references are always tuples of\n    bytestrings, never the internal representation (e.g. dictionary offsets).\n\n    It is presumed that the index will not be mutated - it is static data.\n\n    Successive iter_all_entries calls will read the entire index each time.\n    Additionally, iter_entries calls will read the index linearly until the\n    desired keys are found. XXX: This must be fixed before the index is\n    suitable for production use. :XXX\n    \"\"\"\n\n    def __init__(self, transport, name, size, unlimited_cache=False, offset=0):\n        \"\"\"Open an index called name on transport.\n\n        :param transport: A bzrlib.transport.Transport.\n        :param name: A path to provide to transport API calls.\n        :param size: The size of the index in bytes. This is used for bisection\n            logic to perform partial index reads. While the size could be\n            obtained by statting the file this introduced an additional round\n            trip as well as requiring stat'able transports, both of which are\n            avoided by having it supplied. If size is None, then bisection\n            support will be disabled and accessing the index will just stream\n            all the data.\n        :param offset: Instead of starting the index data at offset 0, start it\n            at an arbitrary offset.\n        \"\"\"\n        self._transport = transport\n        self._name = name\n        # Becomes a dict of key:(value, reference-list-byte-locations) used by\n        # the bisection interface to store parsed but not resolved keys.\n        self._bisect_nodes = None\n        # Becomes a dict of key:(value, reference-list-keys) which are ready to\n        # be returned directly to callers.\n        self._nodes = None\n        # a sorted list of slice-addresses for the parsed bytes of the file.\n        # e.g. (0,1) would mean that byte 0 is parsed.\n        self._parsed_byte_map = []\n        # a sorted list of keys matching each slice address for parsed bytes\n        # e.g. (None, 'foo@bar') would mean that the first byte contained no\n        # key, and the end byte of the slice is the of the data for 'foo@bar'\n        self._parsed_key_map = []\n        self._key_count = None\n        self._keys_by_offset = None\n        self._nodes_by_key = None\n        self._size = size\n        # The number of bytes we've read so far in trying to process this file\n        self._bytes_read = 0\n        self._base_offset = offset\n\n    def __eq__(self, other):\n        \"\"\"Equal when self and other were created with the same parameters.\"\"\"\n        return (\n            type(self) == type(other) and\n            self._transport == other._transport and\n            self._name == other._name and\n            self._size == other._size)\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __repr__(self):\n        return \"%s(%r)\" % (self.__class__.__name__,\n            self._transport.abspath(self._name))\n\n    def _buffer_all(self, stream=None):\n        \"\"\"Buffer all the index data.\n\n        Mutates self._nodes and self.keys_by_offset.\n        \"\"\"\n        if self._nodes is not None:\n            # We already did this\n            return\n        if 'index' in debug.debug_flags:\n            trace.mutter('Reading entire index %s',\n                          self._transport.abspath(self._name))\n        if stream is None:\n            stream = self._transport.get(self._name)\n            if self._base_offset != 0:\n                # This is wasteful, but it is better than dealing with\n                # adjusting all the offsets, etc.\n                stream = StringIO(stream.read()[self._base_offset:])\n        self._read_prefix(stream)\n        self._expected_elements = 3 + self._key_length\n        line_count = 0\n        # raw data keyed by offset\n        self._keys_by_offset = {}\n        # ready-to-return key:value or key:value, node_ref_lists\n        self._nodes = {}\n        self._nodes_by_key = None\n        trailers = 0\n        pos = stream.tell()\n        lines = stream.read().split('\\n')\n        # GZ 2009-09-20: Should really use a try\/finally block to ensure close\n        stream.close()\n        del lines[-1]\n        _, _, _, trailers = self._parse_lines(lines, pos)\n        for key, absent, references, value in self._keys_by_offset.itervalues():\n            if absent:\n                continue\n            # resolve references:\n            if self.node_ref_lists:\n                node_value = (value, self._resolve_references(references))\n            else:\n                node_value = value\n            self._nodes[key] = node_value\n        # cache the keys for quick set intersections\n        if trailers != 1:\n            # there must be one line - the empty trailer line.\n            raise errors.BadIndexData(self)\n\n    def clear_cache(self):\n        \"\"\"Clear out any cached\/memoized values.\n\n        This can be called at any time, but generally it is used when we have\n        extracted some information, but don't expect to be requesting any more\n        from this index.\n        \"\"\"\n\n    def external_references(self, ref_list_num):\n        \"\"\"Return references that are not present in this index.\n        \"\"\"\n        self._buffer_all()\n        if ref_list_num + 1 > self.node_ref_lists:\n            raise ValueError('No ref list %d, index has %d ref lists'\n                % (ref_list_num, self.node_ref_lists))\n        refs = set()\n        nodes = self._nodes\n        for key, (value, ref_lists) in nodes.iteritems():\n            ref_list = ref_lists[ref_list_num]\n            refs.update([ref for ref in ref_list if ref not in nodes])\n        return refs\n\n    def _get_nodes_by_key(self):\n        if self._nodes_by_key is None:\n            nodes_by_key = {}\n            if self.node_ref_lists:\n                for key, (value, references) in self._nodes.iteritems():\n                    key_dict = nodes_by_key\n                    for subkey in key[:-1]:\n                        key_dict = key_dict.setdefault(subkey, {})\n                    key_dict[key[-1]] = key, value, references\n            else:\n                for key, value in self._nodes.iteritems():\n                    key_dict = nodes_by_key\n                    for subkey in key[:-1]:\n                        key_dict = key_dict.setdefault(subkey, {})\n                    key_dict[key[-1]] = key, value\n            self._nodes_by_key = nodes_by_key\n        return self._nodes_by_key\n\n    def iter_all_entries(self):\n        \"\"\"Iterate over all keys within the index.\n\n        :return: An iterable of (index, key, value) or (index, key, value, reference_lists).\n            The former tuple is used when there are no reference lists in the\n            index, making the API compatible with simple key:value index types.\n            There is no defined order for the result iteration - it will be in\n            the most efficient order for the index.\n        \"\"\"\n        if 'evil' in debug.debug_flags:\n            trace.mutter_callsite(3,\n                \"iter_all_entries scales with size of history.\")\n        if self._nodes is None:\n            self._buffer_all()\n        if self.node_ref_lists:\n            for key, (value, node_ref_lists) in self._nodes.iteritems():\n                yield self, key, value, node_ref_lists\n        else:\n            for key, value in self._nodes.iteritems():\n                yield self, key, value\n\n    def _read_prefix(self, stream):\n        signature = stream.read(len(self._signature()))\n        if not signature == self._signature():\n            raise errors.BadIndexFormatSignature(self._name, GraphIndex)\n        options_line = stream.readline()\n        if not options_line.startswith(_OPTION_NODE_REFS):\n            raise errors.BadIndexOptions(self)\n        try:\n            self.node_ref_lists = int(options_line[len(_OPTION_NODE_REFS):-1])\n        except ValueError:\n            raise errors.BadIndexOptions(self)\n        options_line = stream.readline()\n        if not options_line.startswith(_OPTION_KEY_ELEMENTS):\n            raise errors.BadIndexOptions(self)\n        try:\n            self._key_length = int(options_line[len(_OPTION_KEY_ELEMENTS):-1])\n        except ValueError:\n            raise errors.BadIndexOptions(self)\n        options_line = stream.readline()\n        if not options_line.startswith(_OPTION_LEN):\n            raise errors.BadIndexOptions(self)\n        try:\n            self._key_count = int(options_line[len(_OPTION_LEN):-1])\n        except ValueError:\n            raise errors.BadIndexOptions(self)\n\n    def _resolve_references(self, references):\n        \"\"\"Return the resolved key references for references.\n\n        References are resolved by looking up the location of the key in the\n        _keys_by_offset map and substituting the key name, preserving ordering.\n\n        :param references: An iterable of iterables of key locations. e.g.\n            [[123, 456], [123]]\n        :return: A tuple of tuples of keys.\n        \"\"\"\n        node_refs = []\n        for ref_list in references:\n            node_refs.append(tuple([self._keys_by_offset[ref][0] for ref in ref_list]))\n        return tuple(node_refs)\n\n    def _find_index(self, range_map, key):\n        \"\"\"Helper for the _parsed_*_index calls.\n\n        Given a range map - [(start, end), ...], finds the index of the range\n        in the map for key if it is in the map, and if it is not there, the\n        immediately preceeding range in the map.\n        \"\"\"\n        result = bisect_right(range_map, key) - 1\n        if result + 1 < len(range_map):\n            # check the border condition, it may be in result + 1\n            if range_map[result + 1][0] == key[0]:\n                return result + 1\n        return result\n\n    def _parsed_byte_index(self, offset):\n        \"\"\"Return the index of the entry immediately before offset.\n\n        e.g. if the parsed map has regions 0,10 and 11,12 parsed, meaning that\n        there is one unparsed byte (the 11th, addressed as[10]). then:\n        asking for 0 will return 0\n        asking for 10 will return 0\n        asking for 11 will return 1\n        asking for 12 will return 1\n        \"\"\"\n        key = (offset, 0)\n        return self._find_index(self._parsed_byte_map, key)\n\n    def _parsed_key_index(self, key):\n        \"\"\"Return the index of the entry immediately before key.\n\n        e.g. if the parsed map has regions (None, 'a') and ('b','c') parsed,\n        meaning that keys from None to 'a' inclusive, and 'b' to 'c' inclusive\n        have been parsed, then:\n        asking for '' will return 0\n        asking for 'a' will return 0\n        asking for 'b' will return 1\n        asking for 'e' will return 1\n        \"\"\"\n        search_key = (key, None)\n        return self._find_index(self._parsed_key_map, search_key)\n\n    def _is_parsed(self, offset):\n        \"\"\"Returns True if offset has been parsed.\"\"\"\n        index = self._parsed_byte_index(offset)\n        if index == len(self._parsed_byte_map):\n            return offset < self._parsed_byte_map[index - 1][1]\n        start, end = self._parsed_byte_map[index]\n        return offset >= start and offset < end\n\n    def _iter_entries_from_total_buffer(self, keys):\n        \"\"\"Iterate over keys when the entire index is parsed.\"\"\"\n        # Note: See the note in BTreeBuilder.iter_entries for why we don't use\n        #       .intersection() here\n        nodes = self._nodes\n        keys = [key for key in keys if key in nodes]\n        if self.node_ref_lists:\n            for key in keys:\n                value, node_refs = nodes[key]\n                yield self, key, value, node_refs\n        else:\n            for key in keys:\n                yield self, key, nodes[key]\n\n    def iter_entries(self, keys):\n        \"\"\"Iterate over keys within the index.\n\n        :param keys: An iterable providing the keys to be retrieved.\n        :return: An iterable as per iter_all_entries, but restricted to the\n            keys supplied. No additional keys will be returned, and every\n            key supplied that is in the index will be returned.\n        \"\"\"\n        keys = set(keys)\n        if not keys:\n            return []\n        if self._size is None and self._nodes is None:\n            self._buffer_all()\n\n        # We fit about 20 keys per minimum-read (4K), so if we are looking for\n        # more than 1\/20th of the index its likely (assuming homogenous key\n        # spread) that we'll read the entire index. If we're going to do that,\n        # buffer the whole thing. A better analysis might take key spread into\n        # account - but B+Tree indices are better anyway.\n        # We could look at all data read, and use a threshold there, which will\n        # trigger on ancestry walks, but that is not yet fully mapped out.\n        if self._nodes is None and len(keys) * 20 > self.key_count():\n            self._buffer_all()\n        if self._nodes is not None:\n            return self._iter_entries_from_total_buffer(keys)\n        else:\n            return (result[1] for result in bisect_multi.bisect_multi_bytes(\n                self._lookup_keys_via_location, self._size, keys))\n\n    def iter_entries_prefix(self, keys):\n        \"\"\"Iterate over keys within the index using prefix matching.\n\n        Prefix matching is applied within the tuple of a key, not to within\n        the bytestring of each key element. e.g. if you have the keys ('foo',\n        'bar'), ('foobar', 'gam') and do a prefix search for ('foo', None) then\n        only the former key is returned.\n\n        WARNING: Note that this method currently causes a full index parse\n        unconditionally (which is reasonably appropriate as it is a means for\n        thunking many small indices into one larger one and still supplies\n        iter_all_entries at the thunk layer).\n\n        :param keys: An iterable providing the key prefixes to be retrieved.\n            Each key prefix takes the form of a tuple the length of a key, but\n            with the last N elements 'None' rather than a regular bytestring.\n            The first element cannot be 'None'.\n        :return: An iterable as per iter_all_entries, but restricted to the\n            keys with a matching prefix to those supplied. No additional keys\n            will be returned, and every match that is in the index will be\n            returned.\n        \"\"\"\n        keys = set(keys)\n        if not keys:\n            return\n        # load data - also finds key lengths\n        if self._nodes is None:\n            self._buffer_all()\n        if self._key_length == 1:\n            for key in keys:\n                # sanity check\n                if key[0] is None:\n                    raise errors.BadIndexKey(key)\n                if len(key) != self._key_length:\n                    raise errors.BadIndexKey(key)\n                if self.node_ref_lists:\n                    value, node_refs = self._nodes[key]\n                    yield self, key, value, node_refs\n                else:\n                    yield self, key, self._nodes[key]\n            return\n        nodes_by_key = self._get_nodes_by_key()\n        for key in keys:\n            # sanity check\n            if key[0] is None:\n                raise errors.BadIndexKey(key)\n            if len(key) != self._key_length:\n                raise errors.BadIndexKey(key)\n            # find what it refers to:\n            key_dict = nodes_by_key\n            elements = list(key)\n            # find the subdict whose contents should be returned.\n            try:\n                while len(elements) and elements[0] is not None:\n                    key_dict = key_dict[elements[0]]\n                    elements.pop(0)\n            except KeyError:\n                # a non-existant lookup.\n                continue\n            if len(elements):\n                dicts = [key_dict]\n                while dicts:\n                    key_dict = dicts.pop(-1)\n                    # can't be empty or would not exist\n                    item, value = key_dict.iteritems().next()\n                    if type(value) == dict:\n                        # push keys\n                        dicts.extend(key_dict.itervalues())\n                    else:\n                        # yield keys\n                        for value in key_dict.itervalues():\n                            # each value is the key:value:node refs tuple\n                            # ready to yield.\n                            yield (self, ) + value\n            else:\n                # the last thing looked up was a terminal element\n                yield (self, ) + key_dict\n\n    def _find_ancestors(self, keys, ref_list_num, parent_map, missing_keys):\n        \"\"\"See BTreeIndex._find_ancestors.\"\"\"\n        # The api can be implemented as a trivial overlay on top of\n        # iter_entries, it is not an efficient implementation, but it at least\n        # gets the job done.\n        found_keys = set()\n        search_keys = set()\n        for index, key, value, refs in self.iter_entries(keys):\n            parent_keys = refs[ref_list_num]\n            found_keys.add(key)\n            parent_map[key] = parent_keys\n            search_keys.update(parent_keys)\n        # Figure out what, if anything, was missing\n        missing_keys.update(set(keys).difference(found_keys))\n        search_keys = search_keys.difference(parent_map)\n        return search_keys\n\n    def key_count(self):\n        \"\"\"Return an estimate of the number of keys in this index.\n\n        For GraphIndex the estimate is exact.\n        \"\"\"\n        if self._key_count is None:\n            self._read_and_parse([_HEADER_READV])\n        return self._key_count\n\n    def _lookup_keys_via_location(self, location_keys):\n        \"\"\"Public interface for implementing bisection.\n\n        If _buffer_all has been called, then all the data for the index is in\n        memory, and this method should not be called, as it uses a separate\n        cache because it cannot pre-resolve all indices, which buffer_all does\n        for performance.\n\n        :param location_keys: A list of location(byte offset), key tuples.\n        :return: A list of (location_key, result) tuples as expected by\n            bzrlib.bisect_multi.bisect_multi_bytes.\n        \"\"\"\n        # Possible improvements:\n        #  - only bisect lookup each key once\n        #  - sort the keys first, and use that to reduce the bisection window\n        # -----\n        # this progresses in three parts:\n        # read data\n        # parse it\n        # attempt to answer the question from the now in memory data.\n        # build the readv request\n        # for each location, ask for 800 bytes - much more than rows we've seen\n        # anywhere.\n        readv_ranges = []\n        for location, key in location_keys:\n            # can we answer from cache?\n            if self._bisect_nodes and key in self._bisect_nodes:\n                # We have the key parsed.\n                continue\n            index = self._parsed_key_index(key)\n            if (len(self._parsed_key_map) and\n                self._parsed_key_map[index][0] <= key and\n                (self._parsed_key_map[index][1] >= key or\n                 # end of the file has been parsed\n                 self._parsed_byte_map[index][1] == self._size)):\n                # the key has been parsed, so no lookup is needed even if its\n                # not present.\n                continue\n            # - if we have examined this part of the file already - yes\n            index = self._parsed_byte_index(location)\n            if (len(self._parsed_byte_map) and\n                self._parsed_byte_map[index][0] <= location and\n                self._parsed_byte_map[index][1] > location):\n                # the byte region has been parsed, so no read is needed.\n                continue\n            length = 800\n            if location + length > self._size:\n                length = self._size - location\n            # todo, trim out parsed locations.\n            if length > 0:\n                readv_ranges.append((location, length))\n        # read the header if needed\n        if self._bisect_nodes is None:\n            readv_ranges.append(_HEADER_READV)\n        self._read_and_parse(readv_ranges)\n        result = []\n        if self._nodes is not None:\n            # _read_and_parse triggered a _buffer_all because we requested the\n            # whole data range\n            for location, key in location_keys:\n                if key not in self._nodes: # not present\n                    result.append(((location, key), False))\n                elif self.node_ref_lists:\n                    value, refs = self._nodes[key]\n                    result.append(((location, key),\n                        (self, key, value, refs)))\n                else:\n                    result.append(((location, key),\n                        (self, key, self._nodes[key])))\n            return result\n        # generate results:\n        #  - figure out <, >, missing, present\n        #  - result present references so we can return them.\n        # keys that we cannot answer until we resolve references\n        pending_references = []\n        pending_locations = set()\n        for location, key in location_keys:\n            # can we answer from cache?\n            if key in self._bisect_nodes:\n                # the key has been parsed, so no lookup is needed\n                if self.node_ref_lists:\n                    # the references may not have been all parsed.\n                    value, refs = self._bisect_nodes[key]\n                    wanted_locations = []\n                    for ref_list in refs:\n                        for ref in ref_list:\n                            if ref not in self._keys_by_offset:\n                                wanted_locations.append(ref)\n                    if wanted_locations:\n                        pending_locations.update(wanted_locations)\n                        pending_references.append((location, key))\n                        continue\n                    result.append(((location, key), (self, key,\n                        value, self._resolve_references(refs))))\n                else:\n                    result.append(((location, key),\n                        (self, key, self._bisect_nodes[key])))\n                continue\n            else:\n                # has the region the key should be in, been parsed?\n                index = self._parsed_key_index(key)\n                if (self._parsed_key_map[index][0] <= key and\n                    (self._parsed_key_map[index][1] >= key or\n                     # end of the file has been parsed\n                     self._parsed_byte_map[index][1] == self._size)):\n                    result.append(((location, key), False))\n                    continue\n            # no, is the key above or below the probed location:\n            # get the range of the probed & parsed location\n            index = self._parsed_byte_index(location)\n            # if the key is below the start of the range, its below\n            if key < self._parsed_key_map[index][0]:\n                direction = -1\n            else:\n                direction = +1\n            result.append(((location, key), direction))\n        readv_ranges = []\n        # lookup data to resolve references\n        for location in pending_locations:\n            length = 800\n            if location + length > self._size:\n                length = self._size - location\n            # TODO: trim out parsed locations (e.g. if the 800 is into the\n            # parsed region trim it, and dont use the adjust_for_latency\n            # facility)\n            if length > 0:\n                readv_ranges.append((location, length))\n        self._read_and_parse(readv_ranges)\n        if self._nodes is not None:\n            # The _read_and_parse triggered a _buffer_all, grab the data and\n            # return it\n            for location, key in pending_references:\n                value, refs = self._nodes[key]\n                result.append(((location, key), (self, key, value, refs)))\n            return result\n        for location, key in pending_references:\n            # answer key references we had to look-up-late.\n            value, refs = self._bisect_nodes[key]\n            result.append(((location, key), (self, key,\n                value, self._resolve_references(refs))))\n        return result\n\n    def _parse_header_from_bytes(self, bytes):\n        \"\"\"Parse the header from a region of bytes.\n\n        :param bytes: The data to parse.\n        :return: An offset, data tuple such as readv yields, for the unparsed\n            data. (which may length 0).\n        \"\"\"\n        signature = bytes[0:len(self._signature())]\n        if not signature == self._signature():\n            raise errors.BadIndexFormatSignature(self._name, GraphIndex)\n        lines = bytes[len(self._signature()):].splitlines()\n        options_line = lines[0]\n        if not options_line.startswith(_OPTION_NODE_REFS):\n            raise errors.BadIndexOptions(self)\n        try:\n            self.node_ref_lists = int(options_line[len(_OPTION_NODE_REFS):])\n        except ValueError:\n            raise errors.BadIndexOptions(self)\n        options_line = lines[1]\n        if not options_line.startswith(_OPTION_KEY_ELEMENTS):\n            raise errors.BadIndexOptions(self)\n        try:\n            self._key_length = int(options_line[len(_OPTION_KEY_ELEMENTS):])\n        except ValueError:\n            raise errors.BadIndexOptions(self)\n        options_line = lines[2]\n        if not options_line.startswith(_OPTION_LEN):\n            raise errors.BadIndexOptions(self)\n        try:\n            self._key_count = int(options_line[len(_OPTION_LEN):])\n        except ValueError:\n            raise errors.BadIndexOptions(self)\n        # calculate the bytes we have processed\n        header_end = (len(signature) + len(lines[0]) + len(lines[1]) +\n            len(lines[2]) + 3)\n        self._parsed_bytes(0, None, header_end, None)\n        # setup parsing state\n        self._expected_elements = 3 + self._key_length\n        # raw data keyed by offset\n        self._keys_by_offset = {}\n        # keys with the value and node references\n        self._bisect_nodes = {}\n        return header_end, bytes[header_end:]\n\n    def _parse_region(self, offset, data):\n        \"\"\"Parse node data returned from a readv operation.\n\n        :param offset: The byte offset the data starts at.\n        :param data: The data to parse.\n        \"\"\"\n        # trim the data.\n        # end first:\n        end = offset + len(data)\n        high_parsed = offset\n        while True:\n            # Trivial test - if the current index's end is within the\n            # low-matching parsed range, we're done.\n            index = self._parsed_byte_index(high_parsed)\n            if end < self._parsed_byte_map[index][1]:\n                return\n            # print \"[%d:%d]\" % (offset, end), \\\n            #     self._parsed_byte_map[index:index + 2]\n            high_parsed, last_segment = self._parse_segment(\n                offset, data, end, index)\n            if last_segment:\n                return\n\n    def _parse_segment(self, offset, data, end, index):\n        \"\"\"Parse one segment of data.\n\n        :param offset: Where 'data' begins in the file.\n        :param data: Some data to parse a segment of.\n        :param end: Where data ends\n        :param index: The current index into the parsed bytes map.\n        :return: True if the parsed segment is the last possible one in the\n            range of data.\n        :return: high_parsed_byte, last_segment.\n            high_parsed_byte is the location of the highest parsed byte in this\n            segment, last_segment is True if the parsed segment is the last\n            possible one in the data block.\n        \"\"\"\n        # default is to use all data\n        trim_end = None\n        # accomodate overlap with data before this.\n        if offset < self._parsed_byte_map[index][1]:\n            # overlaps the lower parsed region\n            # skip the parsed data\n            trim_start = self._parsed_byte_map[index][1] - offset\n            # don't trim the start for \\n\n            start_adjacent = True\n        elif offset == self._parsed_byte_map[index][1]:\n            # abuts the lower parsed region\n            # use all data\n            trim_start = None\n            # do not trim anything\n            start_adjacent = True\n        else:\n            # does not overlap the lower parsed region\n            # use all data\n            trim_start = None\n            # but trim the leading \\n\n            start_adjacent = False\n        if end == self._size:\n            # lines up to the end of all data:\n            # use it all\n            trim_end = None\n            # do not strip to the last \\n\n            end_adjacent = True\n            last_segment = True\n        elif index + 1 == len(self._parsed_byte_map):\n            # at the end of the parsed data\n            # use it all\n            trim_end = None\n            # but strip to the last \\n\n            end_adjacent = False\n            last_segment = True\n        elif end == self._parsed_byte_map[index + 1][0]:\n            # buts up against the next parsed region\n            # use it all\n            trim_end = None\n            # do not strip to the last \\n\n            end_adjacent = True\n            last_segment = True\n        elif end > self._parsed_byte_map[index + 1][0]:\n            # overlaps into the next parsed region\n            # only consider the unparsed data\n            trim_end = self._parsed_byte_map[index + 1][0] - offset\n            # do not strip to the last \\n as we know its an entire record\n            end_adjacent = True\n            last_segment = end < self._parsed_byte_map[index + 1][1]\n        else:\n            # does not overlap into the next region\n            # use it all\n            trim_end = None\n            # but strip to the last \\n\n            end_adjacent = False\n            last_segment = True\n        # now find bytes to discard if needed\n        if not start_adjacent:\n            # work around python bug in rfind\n            if trim_start is None:\n                trim_start = data.find('\\n') + 1\n            else:\n                trim_start = data.find('\\n', trim_start) + 1\n            if not (trim_start != 0):\n                raise AssertionError('no \\n was present')\n            # print 'removing start', offset, trim_start, repr(data[:trim_start])\n        if not end_adjacent:\n            # work around python bug in rfind\n            if trim_end is None:\n                trim_end = data.rfind('\\n') + 1\n            else:\n                trim_end = data.rfind('\\n', None, trim_end) + 1\n            if not (trim_end != 0):\n                raise AssertionError('no \\n was present')\n            # print 'removing end', offset, trim_end, repr(data[trim_end:])\n        # adjust offset and data to the parseable data.\n        trimmed_data = data[trim_start:trim_end]\n        if not (trimmed_data):\n            raise AssertionError('read unneeded data [%d:%d] from [%d:%d]'\n                % (trim_start, trim_end, offset, offset + len(data)))\n        if trim_start:\n            offset += trim_start\n        # print \"parsing\", repr(trimmed_data)\n        # splitlines mangles the \\r delimiters.. don't use it.\n        lines = trimmed_data.split('\\n')\n        del lines[-1]\n        pos = offset\n        first_key, last_key, nodes, _ = self._parse_lines(lines, pos)\n        for key, value in nodes:\n            self._bisect_nodes[key] = value\n        self._parsed_bytes(offset, first_key,\n            offset + len(trimmed_data), last_key)\n        return offset + len(trimmed_data), last_segment\n\n    def _parse_lines(self, lines, pos):\n        key = None\n        first_key = None\n        trailers = 0\n        nodes = []\n        for line in lines:\n            if line == '':\n                # must be at the end\n                if self._size:\n                    if not (self._size == pos + 1):\n                        raise AssertionError(\"%s %s\" % (self._size, pos))\n                trailers += 1\n                continue\n            elements = line.split('\\0')\n            if len(elements) != self._expected_elements:\n                raise errors.BadIndexData(self)\n            # keys are tuples. Each element is a string that may occur many\n            # times, so we intern them to save space. AB, RC, 200807\n            key = tuple([intern(element) for element in elements[:self._key_length]])\n            if first_key is None:\n                first_key = key\n            absent, references, value = elements[-3:]\n            ref_lists = []\n            for ref_string in references.split('\\t'):\n                ref_lists.append(tuple([\n                    int(ref) for ref in ref_string.split('\\r') if ref\n                    ]))\n            ref_lists = tuple(ref_lists)\n            self._keys_by_offset[pos] = (key, absent, ref_lists, value)\n            pos += len(line) + 1 # +1 for the \\n\n            if absent:\n                continue\n            if self.node_ref_lists:\n                node_value = (value, ref_lists)\n            else:\n                node_value = value\n            nodes.append((key, node_value))\n            # print \"parsed \", key\n        return first_key, key, nodes, trailers\n\n    def _parsed_bytes(self, start, start_key, end, end_key):\n        \"\"\"Mark the bytes from start to end as parsed.\n\n        Calling self._parsed_bytes(1,2) will mark one byte (the one at offset\n        1) as parsed.\n\n        :param start: The start of the parsed region.\n        :param end: The end of the parsed region.\n        \"\"\"\n        index = self._parsed_byte_index(start)\n        new_value = (start, end)\n        new_key = (start_key, end_key)\n        if index == -1:\n            # first range parsed is always the beginning.\n            self._parsed_byte_map.insert(index, new_value)\n            self._parsed_key_map.insert(index, new_key)\n            return\n        # four cases:\n        # new region\n        # extend lower region\n        # extend higher region\n        # combine two regions\n        if (index + 1 < len(self._parsed_byte_map) and\n            self._parsed_byte_map[index][1] == start and\n            self._parsed_byte_map[index + 1][0] == end):\n            # combine two regions\n            self._parsed_byte_map[index] = (self._parsed_byte_map[index][0],\n                self._parsed_byte_map[index + 1][1])\n            self._parsed_key_map[index] = (self._parsed_key_map[index][0],\n                self._parsed_key_map[index + 1][1])\n            del self._parsed_byte_map[index + 1]\n            del self._parsed_key_map[index + 1]\n        elif self._parsed_byte_map[index][1] == start:\n            # extend the lower entry\n            self._parsed_byte_map[index] = (\n                self._parsed_byte_map[index][0], end)\n            self._parsed_key_map[index] = (\n                self._parsed_key_map[index][0], end_key)\n        elif (index + 1 < len(self._parsed_byte_map) and\n            self._parsed_byte_map[index + 1][0] == end):\n            # extend the higher entry\n            self._parsed_byte_map[index + 1] = (\n                start, self._parsed_byte_map[index + 1][1])\n            self._parsed_key_map[index + 1] = (\n                start_key, self._parsed_key_map[index + 1][1])\n        else:\n            # new entry\n            self._parsed_byte_map.insert(index + 1, new_value)\n            self._parsed_key_map.insert(index + 1, new_key)\n\n    def _read_and_parse(self, readv_ranges):\n        \"\"\"Read the ranges and parse the resulting data.\n\n        :param readv_ranges: A prepared readv range list.\n        \"\"\"\n        if not readv_ranges:\n            return\n        if self._nodes is None and self._bytes_read * 2 >= self._size:\n            # We've already read more than 50% of the file and we are about to\n            # request more data, just _buffer_all() and be done\n            self._buffer_all()\n            return\n\n        base_offset = self._base_offset\n        if base_offset != 0:\n            # Rewrite the ranges for the offset\n            readv_ranges = [(start+base_offset, size)\n                            for start, size in readv_ranges]\n        readv_data = self._transport.readv(self._name, readv_ranges, True,\n            self._size + self._base_offset)\n        # parse\n        for offset, data in readv_data:\n            offset -= base_offset\n            self._bytes_read += len(data)\n            if offset < 0:\n                # transport.readv() expanded to extra data which isn't part of\n                # this index\n                data = data[-offset:]\n                offset = 0\n            if offset == 0 and len(data) == self._size:\n                # We read the whole range, most likely because the\n                # Transport upcast our readv ranges into one long request\n                # for enough total data to grab the whole index.\n                self._buffer_all(StringIO(data))\n                return\n            if self._bisect_nodes is None:\n                # this must be the start\n                if not (offset == 0):\n                    raise AssertionError()\n                offset, data = self._parse_header_from_bytes(data)\n            # print readv_ranges, \"[%d:%d]\" % (offset, offset + len(data))\n            self._parse_region(offset, data)\n\n    def _signature(self):\n        \"\"\"The file signature for this index type.\"\"\"\n        return _SIGNATURE\n\n    def validate(self):\n        \"\"\"Validate that everything in the index can be accessed.\"\"\"\n        # iter_all validates completely at the moment, so just do that.\n        for node in self.iter_all_entries():\n            pass\n\n\nclass CombinedGraphIndex(object):\n    \"\"\"A GraphIndex made up from smaller GraphIndices.\n\n    The backing indices must implement GraphIndex, and are presumed to be\n    static data.\n\n    Queries against the combined index will be made against the first index,\n    and then the second and so on. The order of indices can thus influence\n    performance significantly. For example, if one index is on local disk and a\n    second on a remote server, the local disk index should be before the other\n    in the index list.\n    \n    Also, queries tend to need results from the same indices as previous\n    queries.  So the indices will be reordered after every query to put the\n    indices that had the result(s) of that query first (while otherwise\n    preserving the relative ordering).\n    \"\"\"\n\n    def __init__(self, indices, reload_func=None):\n        \"\"\"Create a CombinedGraphIndex backed by indices.\n\n        :param indices: An ordered list of indices to query for data.\n        :param reload_func: A function to call if we find we are missing an\n            index. Should have the form reload_func() => True\/False to indicate\n            if reloading actually changed anything.\n        \"\"\"\n        self._indices = indices\n        self._reload_func = reload_func\n        # Sibling indices are other CombinedGraphIndex that we should call\n        # _move_to_front_by_name on when we auto-reorder ourself.\n        self._sibling_indices = []\n        # A list of names that corresponds to the instances in self._indices,\n        # so _index_names[0] is always the name for _indices[0], etc.  Sibling\n        # indices must all use the same set of names as each other.\n        self._index_names = [None] * len(self._indices)\n\n    def __repr__(self):\n        return \"%s(%s)\" % (\n                self.__class__.__name__,\n                ', '.join(map(repr, self._indices)))\n\n    def clear_cache(self):\n        \"\"\"See GraphIndex.clear_cache()\"\"\"\n        for index in self._indices:\n            index.clear_cache()\n\n    def get_parent_map(self, keys):\n        \"\"\"See graph.StackedParentsProvider.get_parent_map\"\"\"\n        search_keys = set(keys)\n        if _mod_revision.NULL_REVISION in search_keys:\n            search_keys.discard(_mod_revision.NULL_REVISION)\n            found_parents = {_mod_revision.NULL_REVISION:[]}\n        else:\n            found_parents = {}\n        for index, key, value, refs in self.iter_entries(search_keys):\n            parents = refs[0]\n            if not parents:\n                parents = (_mod_revision.NULL_REVISION,)\n            found_parents[key] = parents\n        return found_parents\n\n    has_key = _has_key_from_parent_map\n\n    def insert_index(self, pos, index, name=None):\n        \"\"\"Insert a new index in the list of indices to query.\n\n        :param pos: The position to insert the index.\n        :param index: The index to insert.\n        :param name: a name for this index, e.g. a pack name.  These names can\n            be used to reflect index reorderings to related CombinedGraphIndex\n            instances that use the same names.  (see set_sibling_indices)\n        \"\"\"\n        self._indices.insert(pos, index)\n        self._index_names.insert(pos, name)\n\n    def iter_all_entries(self):\n        \"\"\"Iterate over all keys within the index\n\n        Duplicate keys across child indices are presumed to have the same\n        value and are only reported once.\n\n        :return: An iterable of (index, key, reference_lists, value).\n            There is no defined order for the result iteration - it will be in\n            the most efficient order for the index.\n        \"\"\"\n        seen_keys = set()\n        while True:\n            try:\n                for index in self._indices:\n                    for node in index.iter_all_entries():\n                        if node[1] not in seen_keys:\n                            yield node\n                            seen_keys.add(node[1])\n                return\n            except errors.NoSuchFile:\n                self._reload_or_raise()\n\n    def iter_entries(self, keys):\n        \"\"\"Iterate over keys within the index.\n\n        Duplicate keys across child indices are presumed to have the same\n        value and are only reported once.\n\n        :param keys: An iterable providing the keys to be retrieved.\n        :return: An iterable of (index, key, reference_lists, value). There is\n            no defined order for the result iteration - it will be in the most\n            efficient order for the index.\n        \"\"\"\n        keys = set(keys)\n        hit_indices = []\n        while True:\n            try:\n                for index in self._indices:\n                    if not keys:\n                        break\n                    index_hit = False\n                    for node in index.iter_entries(keys):\n                        keys.remove(node[1])\n                        yield node\n                        index_hit = True\n                    if index_hit:\n                        hit_indices.append(index)\n                break\n            except errors.NoSuchFile:\n                self._reload_or_raise()\n        self._move_to_front(hit_indices)\n\n    def iter_entries_prefix(self, keys):\n        \"\"\"Iterate over keys within the index using prefix matching.\n\n        Duplicate keys across child indices are presumed to have the same\n        value and are only reported once.\n\n        Prefix matching is applied within the tuple of a key, not to within\n        the bytestring of each key element. e.g. if you have the keys ('foo',\n        'bar'), ('foobar', 'gam') and do a prefix search for ('foo', None) then\n        only the former key is returned.\n\n        :param keys: An iterable providing the key prefixes to be retrieved.\n            Each key prefix takes the form of a tuple the length of a key, but\n            with the last N elements 'None' rather than a regular bytestring.\n            The first element cannot be 'None'.\n        :return: An iterable as per iter_all_entries, but restricted to the\n            keys with a matching prefix to those supplied. No additional keys\n            will be returned, and every match that is in the index will be\n            returned.\n        \"\"\"\n        keys = set(keys)\n        if not keys:\n            return\n        seen_keys = set()\n        hit_indices = []\n        while True:\n            try:\n                for index in self._indices:\n                    index_hit = False\n                    for node in index.iter_entries_prefix(keys):\n                        if node[1] in seen_keys:\n                            continue\n                        seen_keys.add(node[1])\n                        yield node\n                        index_hit = True\n                    if index_hit:\n                        hit_indices.append(index)\n                break\n            except errors.NoSuchFile:\n                self._reload_or_raise()\n        self._move_to_front(hit_indices)\n\n    def _move_to_front(self, hit_indices):\n        \"\"\"Rearrange self._indices so that hit_indices are first.\n\n        Order is maintained as much as possible, e.g. the first unhit index\n        will be the first index in _indices after the hit_indices, and the\n        hit_indices will be present in exactly the order they are passed to\n        _move_to_front.\n\n        _move_to_front propagates to all objects in self._sibling_indices by\n        calling _move_to_front_by_name.\n        \"\"\"\n        if self._indices[:len(hit_indices)] == hit_indices:\n            # The 'hit_indices' are already at the front (and in the same\n            # order), no need to re-order\n            return\n        hit_names = self._move_to_front_by_index(hit_indices)\n        for sibling_idx in self._sibling_indices:\n            sibling_idx._move_to_front_by_name(hit_names)\n\n    def _move_to_front_by_index(self, hit_indices):\n        \"\"\"Core logic for _move_to_front.\n        \n        Returns a list of names corresponding to the hit_indices param.\n        \"\"\"\n        indices_info = zip(self._index_names, self._indices)\n        if 'index' in debug.debug_flags:\n            trace.mutter('CombinedGraphIndex reordering: currently %r, '\n                         'promoting %r', indices_info, hit_indices)\n        hit_names = []\n        unhit_names = []\n        new_hit_indices = []\n        unhit_indices = []\n\n        for offset, (name, idx) in enumerate(indices_info):\n            if idx in hit_indices:\n                hit_names.append(name)\n                new_hit_indices.append(idx)\n                if len(new_hit_indices) == len(hit_indices):\n                    # We've found all of the hit entries, everything else is\n                    # unhit\n                    unhit_names.extend(self._index_names[offset+1:])\n                    unhit_indices.extend(self._indices[offset+1:])\n                    break\n            else:\n                unhit_names.append(name)\n                unhit_indices.append(idx)\n\n        self._indices = new_hit_indices + unhit_indices\n        self._index_names = hit_names + unhit_names\n        if 'index' in debug.debug_flags:\n            trace.mutter('CombinedGraphIndex reordered: %r', self._indices)\n        return hit_names\n\n    def _move_to_front_by_name(self, hit_names):\n        \"\"\"Moves indices named by 'hit_names' to front of the search order, as\n        described in _move_to_front.\n        \"\"\"\n        # Translate names to index instances, and then call\n        # _move_to_front_by_index.\n        indices_info = zip(self._index_names, self._indices)\n        hit_indices = []\n        for name, idx in indices_info:\n            if name in hit_names:\n                hit_indices.append(idx)\n        self._move_to_front_by_index(hit_indices)\n\n    def find_ancestry(self, keys, ref_list_num):\n        \"\"\"Find the complete ancestry for the given set of keys.\n\n        Note that this is a whole-ancestry request, so it should be used\n        sparingly.\n\n        :param keys: An iterable of keys to look for\n        :param ref_list_num: The reference list which references the parents\n            we care about.\n        :return: (parent_map, missing_keys)\n        \"\"\"\n        # XXX: make this call _move_to_front?\n        missing_keys = set()\n        parent_map = {}\n        keys_to_lookup = set(keys)\n        generation = 0\n        while keys_to_lookup:\n            # keys that *all* indexes claim are missing, stop searching them\n            generation += 1\n            all_index_missing = None\n            # print 'gen\\tidx\\tsub\\tn_keys\\tn_pmap\\tn_miss'\n            # print '%4d\\t\\t\\t%4d\\t%5d\\t%5d' % (generation, len(keys_to_lookup),\n            #                                   len(parent_map),\n            #                                   len(missing_keys))\n            for index_idx, index in enumerate(self._indices):\n                # TODO: we should probably be doing something with\n                #       'missing_keys' since we've already determined that\n                #       those revisions have not been found anywhere\n                index_missing_keys = set()\n                # Find all of the ancestry we can from this index\n                # keep looking until the search_keys set is empty, which means\n                # things we didn't find should be in index_missing_keys\n                search_keys = keys_to_lookup\n                sub_generation = 0\n                # print '    \\t%2d\\t\\t%4d\\t%5d\\t%5d' % (\n                #     index_idx, len(search_keys),\n                #     len(parent_map), len(index_missing_keys))\n                while search_keys:\n                    sub_generation += 1\n                    # TODO: ref_list_num should really be a parameter, since\n                    #       CombinedGraphIndex does not know what the ref lists\n                    #       mean.\n                    search_keys = index._find_ancestors(search_keys,\n                        ref_list_num, parent_map, index_missing_keys)\n                    # print '    \\t  \\t%2d\\t%4d\\t%5d\\t%5d' % (\n                    #     sub_generation, len(search_keys),\n                    #     len(parent_map), len(index_missing_keys))\n                # Now set whatever was missing to be searched in the next index\n                keys_to_lookup = index_missing_keys\n                if all_index_missing is None:\n                    all_index_missing = set(index_missing_keys)\n                else:\n                    all_index_missing.intersection_update(index_missing_keys)\n                if not keys_to_lookup:\n                    break\n            if all_index_missing is None:\n                # There were no indexes, so all search keys are 'missing'\n                missing_keys.update(keys_to_lookup)\n                keys_to_lookup = None\n            else:\n                missing_keys.update(all_index_missing)\n                keys_to_lookup.difference_update(all_index_missing)\n        return parent_map, missing_keys\n\n    def key_count(self):\n        \"\"\"Return an estimate of the number of keys in this index.\n\n        For CombinedGraphIndex this is approximated by the sum of the keys of\n        the child indices. As child indices may have duplicate keys this can\n        have a maximum error of the number of child indices * largest number of\n        keys in any index.\n        \"\"\"\n        while True:\n            try:\n                return sum((index.key_count() for index in self._indices), 0)\n            except errors.NoSuchFile:\n                self._reload_or_raise()\n\n    missing_keys = _missing_keys_from_parent_map\n\n    def _reload_or_raise(self):\n        \"\"\"We just got a NoSuchFile exception.\n\n        Try to reload the indices, if it fails, just raise the current\n        exception.\n        \"\"\"\n        if self._reload_func is None:\n            raise\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        trace.mutter('Trying to reload after getting exception: %s',\n                     exc_value)\n        if not self._reload_func():\n            # We tried to reload, but nothing changed, so we fail anyway\n            trace.mutter('_reload_func indicated nothing has changed.'\n                         ' Raising original exception.')\n            raise exc_type, exc_value, exc_traceback\n\n    def set_sibling_indices(self, sibling_combined_graph_indices):\n        \"\"\"Set the CombinedGraphIndex objects to reorder after reordering self.\n        \"\"\"\n        self._sibling_indices = sibling_combined_graph_indices\n\n    def validate(self):\n        \"\"\"Validate that everything in the index can be accessed.\"\"\"\n        while True:\n            try:\n                for index in self._indices:\n                    index.validate()\n                return\n            except errors.NoSuchFile:\n                self._reload_or_raise()\n\n\nclass InMemoryGraphIndex(GraphIndexBuilder):\n    \"\"\"A GraphIndex which operates entirely out of memory and is mutable.\n\n    This is designed to allow the accumulation of GraphIndex entries during a\n    single write operation, where the accumulated entries need to be immediately\n    available - for example via a CombinedGraphIndex.\n    \"\"\"\n\n    def add_nodes(self, nodes):\n        \"\"\"Add nodes to the index.\n\n        :param nodes: An iterable of (key, node_refs, value) entries to add.\n        \"\"\"\n        if self.reference_lists:\n            for (key, value, node_refs) in nodes:\n                self.add_node(key, value, node_refs)\n        else:\n            for (key, value) in nodes:\n                self.add_node(key, value)\n\n    def iter_all_entries(self):\n        \"\"\"Iterate over all keys within the index\n\n        :return: An iterable of (index, key, reference_lists, value). There is no\n            defined order for the result iteration - it will be in the most\n            efficient order for the index (in this case dictionary hash order).\n        \"\"\"\n        if 'evil' in debug.debug_flags:\n            trace.mutter_callsite(3,\n                \"iter_all_entries scales with size of history.\")\n        if self.reference_lists:\n            for key, (absent, references, value) in self._nodes.iteritems():\n                if not absent:\n                    yield self, key, value, references\n        else:\n            for key, (absent, references, value) in self._nodes.iteritems():\n                if not absent:\n                    yield self, key, value\n\n    def iter_entries(self, keys):\n        \"\"\"Iterate over keys within the index.\n\n        :param keys: An iterable providing the keys to be retrieved.\n        :return: An iterable of (index, key, value, reference_lists). There is no\n            defined order for the result iteration - it will be in the most\n            efficient order for the index (keys iteration order in this case).\n        \"\"\"\n        # Note: See BTreeBuilder.iter_entries for an explanation of why we\n        #       aren't using set().intersection() here\n        nodes = self._nodes\n        keys = [key for key in keys if key in nodes]\n        if self.reference_lists:\n            for key in keys:\n                node = nodes[key]\n                if not node[0]:\n                    yield self, key, node[2], node[1]\n        else:\n            for key in keys:\n                node = nodes[key]\n                if not node[0]:\n                    yield self, key, node[2]\n\n    def iter_entries_prefix(self, keys):\n        \"\"\"Iterate over keys within the index using prefix matching.\n\n        Prefix matching is applied within the tuple of a key, not to within\n        the bytestring of each key element. e.g. if you have the keys ('foo',\n        'bar'), ('foobar', 'gam') and do a prefix search for ('foo', None) then\n        only the former key is returned.\n\n        :param keys: An iterable providing the key prefixes to be retrieved.\n            Each key prefix takes the form of a tuple the length of a key, but\n            with the last N elements 'None' rather than a regular bytestring.\n            The first element cannot be 'None'.\n        :return: An iterable as per iter_all_entries, but restricted to the\n            keys with a matching prefix to those supplied. No additional keys\n            will be returned, and every match that is in the index will be\n            returned.\n        \"\"\"\n        # XXX: To much duplication with the GraphIndex class; consider finding\n        # a good place to pull out the actual common logic.\n        keys = set(keys)\n        if not keys:\n            return\n        if self._key_length == 1:\n            for key in keys:\n                # sanity check\n                if key[0] is None:\n                    raise errors.BadIndexKey(key)\n                if len(key) != self._key_length:\n                    raise errors.BadIndexKey(key)\n                node = self._nodes[key]\n                if node[0]:\n                    continue\n                if self.reference_lists:\n                    yield self, key, node[2], node[1]\n                else:\n                    yield self, key, node[2]\n            return\n        nodes_by_key = self._get_nodes_by_key()\n        for key in keys:\n            # sanity check\n            if key[0] is None:\n                raise errors.BadIndexKey(key)\n            if len(key) != self._key_length:\n                raise errors.BadIndexKey(key)\n            # find what it refers to:\n            key_dict = nodes_by_key\n            elements = list(key)\n            # find the subdict to return\n            try:\n                while len(elements) and elements[0] is not None:\n                    key_dict = key_dict[elements[0]]\n                    elements.pop(0)\n            except KeyError:\n                # a non-existant lookup.\n                continue\n            if len(elements):\n                dicts = [key_dict]\n                while dicts:\n                    key_dict = dicts.pop(-1)\n                    # can't be empty or would not exist\n                    item, value = key_dict.iteritems().next()\n                    if type(value) == dict:\n                        # push keys\n                        dicts.extend(key_dict.itervalues())\n                    else:\n                        # yield keys\n                        for value in key_dict.itervalues():\n                            yield (self, ) + value\n            else:\n                yield (self, ) + key_dict\n\n    def key_count(self):\n        \"\"\"Return an estimate of the number of keys in this index.\n\n        For InMemoryGraphIndex the estimate is exact.\n        \"\"\"\n        return len(self._nodes) - len(self._absent_keys)\n\n    def validate(self):\n        \"\"\"In memory index's have no known corruption at the moment.\"\"\"\n\n\nclass GraphIndexPrefixAdapter(object):\n    \"\"\"An adapter between GraphIndex with different key lengths.\n\n    Queries against this will emit queries against the adapted Graph with the\n    prefix added, queries for all items use iter_entries_prefix. The returned\n    nodes will have their keys and node references adjusted to remove the\n    prefix. Finally, an add_nodes_callback can be supplied - when called the\n    nodes and references being added will have prefix prepended.\n    \"\"\"\n\n    def __init__(self, adapted, prefix, missing_key_length,\n        add_nodes_callback=None):\n        \"\"\"Construct an adapter against adapted with prefix.\"\"\"\n        self.adapted = adapted\n        self.prefix_key = prefix + (None,)*missing_key_length\n        self.prefix = prefix\n        self.prefix_len = len(prefix)\n        self.add_nodes_callback = add_nodes_callback\n\n    def add_nodes(self, nodes):\n        \"\"\"Add nodes to the index.\n\n        :param nodes: An iterable of (key, node_refs, value) entries to add.\n        \"\"\"\n        # save nodes in case its an iterator\n        nodes = tuple(nodes)\n        translated_nodes = []\n        try:\n            # Add prefix_key to each reference node_refs is a tuple of tuples,\n            # so split it apart, and add prefix_key to the internal reference\n            for (key, value, node_refs) in nodes:\n                adjusted_references = (\n                    tuple(tuple(self.prefix + ref_node for ref_node in ref_list)\n                        for ref_list in node_refs))\n                translated_nodes.append((self.prefix + key, value,\n                    adjusted_references))\n        except ValueError:\n            # XXX: TODO add an explicit interface for getting the reference list\n            # status, to handle this bit of user-friendliness in the API more\n            # explicitly.\n            for (key, value) in nodes:\n                translated_nodes.append((self.prefix + key, value))\n        self.add_nodes_callback(translated_nodes)\n\n    def add_node(self, key, value, references=()):\n        \"\"\"Add a node to the index.\n\n        :param key: The key. keys are non-empty tuples containing\n            as many whitespace-free utf8 bytestrings as the key length\n            defined for this index.\n        :param references: An iterable of iterables of keys. Each is a\n            reference to another key.\n        :param value: The value to associate with the key. It may be any\n            bytes as long as it does not contain \\0 or \\n.\n        \"\"\"\n        self.add_nodes(((key, value, references), ))\n\n    def _strip_prefix(self, an_iter):\n        \"\"\"Strip prefix data from nodes and return it.\"\"\"\n        for node in an_iter:\n            # cross checks\n            if node[1][:self.prefix_len] != self.prefix:\n                raise errors.BadIndexData(self)\n            for ref_list in node[3]:\n                for ref_node in ref_list:\n                    if ref_node[:self.prefix_len] != self.prefix:\n                        raise errors.BadIndexData(self)\n            yield node[0], node[1][self.prefix_len:], node[2], (\n                tuple(tuple(ref_node[self.prefix_len:] for ref_node in ref_list)\n                for ref_list in node[3]))\n\n    def iter_all_entries(self):\n        \"\"\"Iterate over all keys within the index\n\n        iter_all_entries is implemented against the adapted index using\n        iter_entries_prefix.\n\n        :return: An iterable of (index, key, reference_lists, value). There is no\n            defined order for the result iteration - it will be in the most\n            efficient order for the index (in this case dictionary hash order).\n        \"\"\"\n        return self._strip_prefix(self.adapted.iter_entries_prefix([self.prefix_key]))\n\n    def iter_entries(self, keys):\n        \"\"\"Iterate over keys within the index.\n\n        :param keys: An iterable providing the keys to be retrieved.\n        :return: An iterable of (index, key, value, reference_lists). There is no\n            defined order for the result iteration - it will be in the most\n            efficient order for the index (keys iteration order in this case).\n        \"\"\"\n        return self._strip_prefix(self.adapted.iter_entries(\n            self.prefix + key for key in keys))\n\n    def iter_entries_prefix(self, keys):\n        \"\"\"Iterate over keys within the index using prefix matching.\n\n        Prefix matching is applied within the tuple of a key, not to within\n        the bytestring of each key element. e.g. if you have the keys ('foo',\n        'bar'), ('foobar', 'gam') and do a prefix search for ('foo', None) then\n        only the former key is returned.\n\n        :param keys: An iterable providing the key prefixes to be retrieved.\n            Each key prefix takes the form of a tuple the length of a key, but\n            with the last N elements 'None' rather than a regular bytestring.\n            The first element cannot be 'None'.\n        :return: An iterable as per iter_all_entries, but restricted to the\n            keys with a matching prefix to those supplied. No additional keys\n            will be returned, and every match that is in the index will be\n            returned.\n        \"\"\"\n        return self._strip_prefix(self.adapted.iter_entries_prefix(\n            self.prefix + key for key in keys))\n\n    def key_count(self):\n        \"\"\"Return an estimate of the number of keys in this index.\n\n        For GraphIndexPrefixAdapter this is relatively expensive - key\n        iteration with the prefix is done.\n        \"\"\"\n        return len(list(self.iter_all_entries()))\n\n    def validate(self):\n        \"\"\"Call the adapted's validate.\"\"\"\n        self.adapted.validate()\n","label":0}
{"content":"#!\/usr\/bin\/env python\n\nfrom gnuradio import gr\nfrom gnuradio import trellis, digital, filter, blocks\nfrom gnuradio import eng_notation\nimport math\nimport sys\nimport random\nimport fsm_utils\n\ntry:\n    from gnuradio import analog\nexcept ImportError:\n    sys.stderr.write(\"Error: Program requires gr-analog.\\n\")\n    sys.exit(1)\n\ndef make_rx(tb,fo,fi,dimensionality,tot_constellation,K,interleaver,IT,Es,N0,type):\n    metrics_in = trellis.metrics_f(fi.O(),dimensionality,tot_constellation,digital.TRELLIS_EUCLIDEAN) # data preprocessing to generate metrics for innner SISO\n    scale = blocks.multiply_const_ff(1.0\/N0)\n    gnd = blocks.vector_source_f([0],True);\n\n    inter=[]\n    deinter=[]\n    siso_in=[]\n    siso_out=[]\n\n    # generate all blocks\n    for it in range(IT):\n      inter.append( trellis.permutation(interleaver.K(),interleaver.INTER(),fi.I(),gr.sizeof_float) )\n      siso_in.append( trellis.siso_f(fi,K,0,-1,True,False,type) )\n      deinter.append( trellis.permutation(interleaver.K(),interleaver.DEINTER(),fi.I(),gr.sizeof_float) )\n      if it < IT-1:\n        siso_out.append( trellis.siso_f(fo,K,0,-1,False,True,type) )\n      else:\n        siso_out.append( trellis.viterbi_s(fo,K,0,-1) ) # no soft outputs needed\n\n    # connect first stage\n    tb.connect (gnd,inter[0])\n    tb.connect (metrics_in,scale)\n    tb.connect (scale,(siso_in[0],1))\n\n    # connect the rest\n    for it in range(IT):\n      if it < IT-1:\n        tb.connect (scale,(siso_in[it+1],1))\n        tb.connect (siso_in[it],deinter[it],(siso_out[it],1))\n        tb.connect (gnd,(siso_out[it],0))\n        tb.connect (siso_out[it],inter[it+1])\n        tb.connect (inter[it],(siso_in[it],0))\n      else:\n        tb.connect (siso_in[it],deinter[it],siso_out[it])\n        tb.connect (inter[it],(siso_in[it],0))\n\n    return (metrics_in,siso_out[IT-1])\n\n\ndef run_test (fo,fi,interleaver,Kb,bitspersymbol,K,channel,modulation,dimensionality,tot_constellation,Es,N0,IT,seed):\n    tb = gr.top_block ()\n    L = len(channel)\n\n    # TX\n    # this for loop is TOO slow in python!!!\n    packet = [0]*(K)\n    random.seed(seed)\n    for i in range(len(packet)):\n        packet[i] = random.randint(0, 2**bitspersymbol - 1) # random symbols\n    src = blocks.vector_source_s(packet,False)\n    enc_out = trellis.encoder_ss(fo,0) # initial state = 0\n    inter = trellis.permutation(interleaver.K(),interleaver.INTER(),1,gr.sizeof_short)\n    mod = digital.chunks_to_symbols_sf(modulation[1],modulation[0])\n\n    # CHANNEL\n    isi = filter.fir_filter_fff(1,channel)\n    add = blocks.add_ff()\n    noise = analog.noise_source_f(analog.GR_GAUSSIAN,math.sqrt(N0\/2),seed)\n\n    # RX\n    (head,tail) = make_rx(tb,fo,fi,dimensionality,tot_constellation,K,interleaver,IT,Es,N0,trellis.TRELLIS_MIN_SUM)\n    dst = blocks.vector_sink_s();\n\n    tb.connect (src,enc_out,inter,mod)\n    tb.connect (mod,isi,(add,0))\n    tb.connect (noise,(add,1))\n    tb.connect (add,head)\n    tb.connect (tail,dst)\n\n    tb.run()\n\n    data = dst.data()\n    ntotal = len(data)\n    nright=0\n    for i in range(ntotal):\n        if packet[i]==data[i]:\n            nright=nright+1\n        #else:\n            #print \"Error in \", i\n\n    return (ntotal,ntotal-nright)\n\n\n\n\ndef main(args):\n    nargs = len (args)\n    if nargs == 3:\n        fname_out=args[0]\n        esn0_db=float(args[1])\n        rep=int(args[2])\n    else:\n        sys.stderr.write ('usage: test_turbo_equalization.py fsm_name_out Es\/No_db  repetitions\\n')\n        sys.exit (1)\n\n    # system parameters\n    Kb=64*16  # packet size in bits (multiple of 16)\n    modulation = fsm_utils.pam4 # see fsm_utlis.py for available predefined modulations\n    channel = fsm_utils.c_channel # see fsm_utlis.py for available predefined test channels\n    fo=trellis.fsm(fname_out) # get the outer FSM specification from a file\n    fi=trellis.fsm(len(modulation[1]),len(channel)) # generate the FSM automatically\n    if fo.O() != fi.I():\n        sys.stderr.write ('Incompatible cardinality between outer and inner FSM.\\n')\n        sys.exit (1)\n    bitspersymbol = int(round(math.log(fo.I())\/math.log(2))) # bits per FSM input symbol\n    K=Kb\/bitspersymbol # packet size in trellis steps\n    interleaver=trellis.interleaver(K,666) # construct a random interleaver\n    tot_channel = fsm_utils.make_isi_lookup(modulation,channel,True) # generate the lookup table (normalize energy to 1)\n    dimensionality = tot_channel[0]\n    tot_constellation = tot_channel[1]\n    if len(tot_constellation)\/dimensionality != fi.O():\n        sys.stderr.write ('Incompatible FSM output cardinality and lookup table size.\\n')\n        sys.exit (1)\n    N0=pow(10.0,-esn0_db\/10.0); # noise variance\n    IT = 3 # number of turbo iterations\n\n    tot_s=0 # total number of transmitted shorts\n    terr_s=0 # total number of shorts in error\n    terr_p=0 # total number of packets in error\n\n    for i in range(rep):\n        (s,e)=run_test(fo,fi,interleaver,Kb,bitspersymbol,K,channel,modulation,dimensionality,tot_constellation,1,N0,IT,-long(666+i)) # run experiment with different seed to get different noise realizations\n        tot_s=tot_s+s\n        terr_s=terr_s+e\n        terr_p=terr_p+(terr_s!=0)\n        if ((i+1)%10==0) : # display progress\n            print i+1,terr_p, '%.2e' % ((1.0*terr_p)\/(i+1)),tot_s,terr_s, '%.2e' % ((1.0*terr_s)\/tot_s)\n    # estimate of the (short or bit) error rate\n    print rep,terr_p, '%.2e' % ((1.0*terr_p)\/(i+1)),tot_s,terr_s, '%.2e' % ((1.0*terr_s)\/tot_s)\n\n\n\nif __name__ == '__main__':\n    main (sys.argv[1:])\n\n","label":0}
{"content":"# Copyright 2006 Georg Brandl.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Fixer for intern().\n\nintern(s) -> sys.intern(s)\"\"\"\n\n# Local imports\nfrom .. import pytree\nfrom .. import fixer_base\nfrom ..fixer_util import Name, Attr, touch_import\n\n\nclass FixIntern(fixer_base.BaseFix):\n\n    PATTERN = \"\"\"\n    power< 'intern'\n           trailer< lpar='('\n                    ( not(arglist | argument<any '=' any>) obj=any\n                      | obj=arglist<(not argument<any '=' any>) any ','> )\n                    rpar=')' >\n           after=any*\n    >\n    \"\"\"\n\n    def transform(self, node, results):\n        syms = self.syms\n        obj = results[\"obj\"].clone()\n        if obj.type == syms.arglist:\n            newarglist = obj.clone()\n        else:\n            newarglist = pytree.Node(syms.arglist, [obj.clone()])\n        after = results[\"after\"]\n        if after:\n            after = [n.clone() for n in after]\n        new = pytree.Node(syms.power,\n                          Attr(Name(u\"sys\"), Name(u\"intern\")) +\n                          [pytree.Node(syms.trailer,\n                                       [results[\"lpar\"].clone(),\n                                        newarglist,\n                                        results[\"rpar\"].clone()])] + after)\n        new.prefix = node.prefix\n        touch_import(None, u'sys', node)\n        return new\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2009 Sharoon Thomas\n#    Copyright (C) 2010-Today OpenERP SA (<http:\/\/www.openerp.com>)\n#\n#    This program is free software: you can redistribute it and\/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>\n#\n##############################################################################\nimport email_template_preview\nimport mail_compose_message\n\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n","label":0}
{"content":"#!\/usr\/bin\/env python\n\"\"\"Python version of cairo-demo\/cairo_snippets\/cairo_snippets_pdf.c\ncreate a file for each example rather than one large file for all examples\n\"\"\"\n\nfrom __future__ import division\nfrom math import pi as M_PI  # used by many snippets\nimport sys\n\nimport cairo\nif not cairo.HAS_PDF_SURFACE:\n  raise SystemExit ('cairo was not compiled with PDF support')\n\nfrom snippets import snip_list, snippet_normalize\n\n\nwidth_in_inches, height_in_inches = 2, 2\nwidth_in_points, height_in_points = width_in_inches * 72, height_in_inches * 72\nwidth, height = width_in_points, height_in_points # used by snippet_normalize()\n\n\ndef do_snippet (snippet):\n  if verbose_mode:\n    print('processing %s' % snippet)\n\n  filename = 'snippets\/%s.pdf' % snippet\n  surface = cairo.PDFSurface (filename, width_in_points, height_in_points)\n  cr = cairo.Context (surface)\n\n  cr.save()\n  try:\n    fName = 'snippets\/%s.py' % snippet\n    code = open(fName).read()\n    exec (code, globals(), locals())\n  except:\n    exc_type, exc_value = sys.exc_info()[:2]\n    print(exc_type, exc_value, file=sys.stderr)\n  else:\n    cr.restore()\n    cr.show_page()\n    surface.finish()\n\n  if verbose_mode:\n    print\n\nif __name__ == '__main__':\n  verbose_mode = True\n  if len(sys.argv) > 1 and sys.argv[1] == '-s':\n    verbose_mode = False\n    del sys.argv[1]\n\n  if len(sys.argv) > 1: # do specified snippets\n    snippet_list = sys.argv[1:]\n  else:                 # do all snippets\n    snippet_list = snip_list\n\n  for s in snippet_list:\n    do_snippet (s)\n","label":0}
{"content":"# Copyright 2013 Dean Gardiner <gardiner91@gmail.com>\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nfrom logr import Logr\n\n\nGROUP_MATCHES = ['identifier']\n\n\nclass CaperNode(object):\n    def __init__(self, closure, parent=None, match=None):\n        \"\"\"\n        :type parent: CaperNode\n        :type weight: float\n        \"\"\"\n\n        #: :type: caper.objects.CaperClosure\n        self.closure = closure\n\n        #: :type: CaperNode\n        self.parent = parent\n\n        #: :type: CaptureMatch\n        self.match = match\n\n        #: :type: list of CaptureGroup\n        self.finished_groups = []\n\n    def next(self):\n        raise NotImplementedError()\n\n    def captured(self):\n        cur = self\n\n        if cur.match:\n            yield cur.match.tag, cur.match.result\n\n        while cur.parent:\n            cur = cur.parent\n\n            if cur.match:\n                yield cur.match.tag, cur.match.result\n\n\nclass CaperRootNode(CaperNode):\n    def __init__(self, closure):\n        \"\"\"\n        :type closure: caper.objects.CaperClosure or list of caper.objects.CaperClosure\n        \"\"\"\n        super(CaperRootNode, self).__init__(closure)\n\n    def next(self):\n        return self.closure\n\n\nclass CaperClosureNode(CaperNode):\n    def __init__(self, closure, parent=None, match=None):\n        \"\"\"\n        :type closure: caper.objects.CaperClosure or list of caper.objects.CaperClosure\n        \"\"\"\n        super(CaperClosureNode, self).__init__(closure, parent, match)\n\n    def next(self):\n        if not self.closure:\n            return None\n\n        if self.match:\n            # Jump to next closure if we have a match\n            return self.closure.right\n        elif len(self.closure.fragments) > 0:\n            # Otherwise parse the fragments\n            return self.closure.fragments[0]\n\n        return None\n\n    def __str__(self):\n        return \"<CaperClosureNode match: %s>\" % repr(self.match)\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass CaperFragmentNode(CaperNode):\n    def __init__(self, closure, fragments, parent=None, match=None):\n        \"\"\"\n        :type closure: caper.objects.CaperClosure\n        :type fragments: list of caper.objects.CaperFragment\n        \"\"\"\n        super(CaperFragmentNode, self).__init__(closure, parent, match)\n\n        #: :type: caper.objects.CaperFragment or list of caper.objects.CaperFragment\n        self.fragments = fragments\n\n    def next(self):\n        if len(self.fragments) > 0 and self.fragments[-1] and self.fragments[-1].right:\n            return self.fragments[-1].right\n\n        if self.closure.right:\n            return self.closure.right\n\n        return None\n\n    def __str__(self):\n        return \"<CaperFragmentNode match: %s>\" % repr(self.match)\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass CaperResult(object):\n    def __init__(self):\n        #: :type: list of CaperNode\n        self.heads = []\n\n        self.chains = []\n\n    def build(self):\n        max_matched = 0\n\n        for head in self.heads:\n            for chain in self.combine_chain(head):\n                if chain.num_matched > max_matched:\n                    max_matched = chain.num_matched\n\n                self.chains.append(chain)\n\n        for chain in self.chains:\n            chain.weights.append(chain.num_matched \/ float(max_matched or chain.num_matched or 1))\n            chain.finish()\n\n        self.chains.sort(key=lambda chain: chain.weight, reverse=True)\n\n        for chain in self.chains:\n            Logr.debug(\"chain weight: %.02f\", chain.weight)\n            Logr.debug(\"\\tInfo: %s\", chain.info)\n\n            Logr.debug(\"\\tWeights: %s\", chain.weights)\n            Logr.debug(\"\\tNumber of Fragments Matched: %s\", chain.num_matched)\n\n    def combine_chain(self, subject, chain=None):\n        nodes = subject if type(subject) is list else [subject]\n\n        if chain is None:\n            chain = CaperResultChain()\n\n        result = []\n\n        for x, node in enumerate(nodes):\n            node_chain = chain if x == len(nodes) - 1 else chain.copy()\n\n            if not node.parent:\n                result.append(node_chain)\n                continue\n\n            node_chain.update(node)\n            result.extend(self.combine_chain(node.parent, node_chain))\n\n        return result\n\n\nclass CaperResultChain(object):\n    def __init__(self):\n        #: :type: float\n        self.weight = None\n        self.info = {}\n        self.num_matched = 0\n\n        self.weights = []\n\n    def update(self, subject):\n        \"\"\"\n        :type subject: CaperFragmentNode\n        \"\"\"\n        if not subject.match or not subject.match.success:\n            return\n\n        # TODO this should support closure nodes\n        if type(subject) is CaperFragmentNode:\n            self.num_matched += len(subject.fragments) if subject.fragments is not None else 0\n\n        self.weights.append(subject.match.weight)\n\n        if subject.match:\n            if subject.match.tag not in self.info:\n                self.info[subject.match.tag] = []\n\n            self.info[subject.match.tag].insert(0, subject.match.result)\n\n    def finish(self):\n        self.weight = sum(self.weights) \/ len(self.weights)\n\n    def copy(self):\n        chain = CaperResultChain()\n\n        chain.weight = self.weight\n        chain.info = copy.deepcopy(self.info)\n\n        chain.num_matched = self.num_matched\n        chain.weights = copy.copy(self.weights)\n\n        return chain","label":0}
{"content":"#!\/usr\/bin\/python\n# -*- coding: utf-8 -*-\n\n# (c) 2015, Billy Kimble <basslines@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n\nDOCUMENTATION = \"\"\"\nmodule: hall\nshort_description: Send notification to Hall\ndescription:\n    - \"The M(hall) module connects to the U(https:\/\/hall.com) messaging API and allows you to deliver notication messages to rooms.\"\nversion_added: \"2.0\"\nauthor: Billy Kimble (@bkimble) <basslines@gmail.com>\noptions:\n  room_token:\n    description:\n      - \"Room token provided to you by setting up the Ansible room integation on U(https:\/\/hall.com)\"\n    required: true\n  msg:\n    description:\n      - The message you wish to deliver as a notifcation\n    required: true\n  title:\n    description:\n      - The title of the message\n    required: true\n  picture:\n    description:\n      - \"The full URL to the image you wish to use for the Icon of the message. Defaults to U(http:\/\/cdn2.hubspot.net\/hub\/330046\/file-769078210-png\/Official_Logos\/ansible_logo_black_square_small.png?t=1421076128627)\"\n    required: false\n\"\"\"\n\nEXAMPLES = \"\"\"\n- name: Send Hall notifiation\n  local_action:\n    module: hall\n    room_token: <hall room integration token>\n    title: Nginx\n    msg: Created virtual host file on {{ inventory_hostname }}\n\n- name: Send Hall notification if EC2 servers were created.\n  when: ec2.instances|length > 0\n  local_action:\n    module: hall\n    room_token: <hall room integration token>\n    title: Server Creation\n    msg: \"Created EC2 instance {{ item.id }} of type {{ item.instance_type }}.\\\\nInstance can be reached at {{ item.public_ip }} in the {{ item.region }} region.\"\n  with_items: ec2.instances\n\"\"\"\n\nHALL_API_ENDPOINT  = 'https:\/\/hall.com\/api\/1\/services\/generic\/%s'\n\ndef send_request_to_hall(module, room_token, payload):\n    headers = {'Content-Type': 'application\/json'}\n    payload=module.jsonify(payload)\n    api_endpoint = HALL_API_ENDPOINT % (room_token)\n    response, info = fetch_url(module, api_endpoint, data=payload, headers=headers)\n    if info['status'] != 200:\n        secure_url = HALL_API_ENDPOINT % ('[redacted]')\n        module.fail_json(msg=\" failed to send %s to %s: %s\" % (payload, secure_url, info['msg']))\n\ndef main():\n    module = AnsibleModule(\n        argument_spec = dict(\n            room_token  = dict(type='str', required=True),\n            msg     = dict(type='str', required=True),\n            title       = dict(type='str', required=True),\n            picture     = dict(type='str', default='http:\/\/cdn2.hubspot.net\/hub\/330046\/file-769078210-png\/Official_Logos\/ansible_logo_black_square_small.png?t=1421076128627'),\n        )\n    )\n\n    room_token = module.params['room_token']\n    message = module.params['msg']\n    title = module.params['title']\n    picture = module.params['picture']\n    payload = {'title': title, 'message': message, 'picture': picture}\n    send_request_to_hall(module, room_token, payload)\n    module.exit_json(msg=\"OK\")\n\nfrom ansible.module_utils.basic import *\nfrom ansible.module_utils.urls import *\nmain()\n","label":0}
{"content":"from __future__ import print_function\nimport os, string, tempfile, shutil\nfrom subprocess import Popen\nfrom ase.io import write\nfrom ase.units import Bohr\n\n\nclass Bader:\n    '''class for running bader analysis and extracting data from it.\n\n    The class runs bader, extracts the charge density and outputs it\n    to a cube file. Then you call different functions of the class to\n    extract the charges, volumes, etc...\n\n    ACF.dat contains the coordinates of each atom, the charge\n    associated with it according to Bader partitioning, percentage of\n    the whole according to Bader partitioning and the minimum distance\n    to the surface. This distance should be compared to maximum\n    cut-off radius for the core region if pseudo potentials have been\n    used.\n\n    BCF.dat contains the coordinates of each Bader maxima, the charge\n    within that volume, the nearest atom and the distance to that\n    atom.\n\n    AtomVolumes.dat contains the number of each volume that has been\n    assigned to each atom. These numbers correspond to the number of\n    the BvAtxxxx.dat files.\n\n    The options for the executable are::\n\n        bader [ -c bader | voronoi ]\n              [ -n bader | voronoi ]\n              [ -b neargrid | ongrid ]\n              [ -r refine_edge_iterations ]\n              [ -ref reference_charge ]\n              [ -p all_atom | all_bader ]\n              [ -p sel_atom | sel_bader ] [volume list]\n              [ -p atom_index | bader_index ]\n              [ -i cube | chgcar ]\n              [ -h ] [ -v ]\n              chargefile\n\n    References:\n    \n    G. Henkelman, A. Arnaldsson, and H. Jonsson, A fast and robust\n    algorithm for Bader decomposition of charge density,\n    Comput. Mater. Sci. 36 254-360 (2006).\n\n    E. Sanville, S. D. Kenny, R. Smith, and G. Henkelman An improved\n    grid-based algorithm for Bader charge allocation,\n    J. Comp. Chem. 28 899-908 (2007).\n\n    W. Tang, E. Sanville, and G. Henkelman A grid-based Bader analysis\n    algorithm without lattice bias, J. Phys.: Condens. Matter 21\n    084204 (2009).\n    '''\n    def __init__(self, atoms):\n        '''\n \n        '''\n        self.atoms = atoms\n\n        #get density and write cube file\n        calc = atoms.get_calculator()\n        ncfile = calc.get_nc()\n        base, ext = os.path.splitext(ncfile)\n\n        x, y, z, density = calc.get_charge_density()\n        cubefile = base + '_charge_density.cube'\n        self.densityfile = cubefile\n\n        if not os.path.exists(cubefile):\n            write(cubefile, atoms, data=density * Bohr ** 3)\n        \n        #cmd to run for bader analysis. check if output exists so we\n        #don't run this too often.\n        acf_file = base + '_ACF.dat'\n        if not os.path.exists(acf_file):\n            #mk tempdir\n            tempdir = tempfile.mkdtemp()\n            \n            cwd = os.getcwd()\n            abscubefile = os.path.abspath(cubefile)\n            os.chdir(tempdir)\n            cmd = 'bader %s' % abscubefile\n\n            process = Popen(cmd)\n            status = Popen.wait()\n            \n            if status != 0:\n                print(process)\n\n            shutil.copy2('ACF.dat', os.path.join(cwd, acf_file))\n            \n            os.chdir(cwd)\n            shutil.rmtree(tempdir)\n\n        self.charges = []\n        self.volumes = []\n\n        #now parse the output\n        f = open(acf_file, 'r')\n        #skip 2 lines\n        f.readline()\n        f.readline()\n\n        for i, atom in enumerate(self.atoms):\n            line = f.readline()\n            fields = line.split()\n            n = int(fields[0])\n            x = float(fields[1])\n            y = float(fields[2])\n            z = float(fields[3])\n            chg = float(fields[4])\n            mindist = float(fields[5])\n            vol = float(fields[6])\n\n            self.charges.append(chg)\n            self.volumes.append(vol)\n\n        f.close()\n\n    def get_bader_charges(self):\n        return self.charges\n\n    def get_bader_volumes(self):\n        'return volumes in Ang**3'\n        return [x * Bohr ** 3 for x in self.volumes]\n\n    def write_atom_volume(self, atomlist):\n        '''write bader atom volumes to cube files.\n        atomlist = [0,2] #for example\n\n        -p sel_atom Write the selected atomic volumes, read from the\n        subsequent list of volumes.\n        '''\n        alist = string.join([str(x) for x in atomlist], ' ')\n        cmd = 'bader -p sel_atom %s %s' % (alist, self.densityfile)\n        print(cmd)\n        os.system(cmd)\n        \n    def write_bader_volume(self, atomlist):\n        \"\"\"write bader atom volumes to cube files.\n\n        ::\n        \n          atomlist = [0,2] #  for example\n          \n        -p sel_bader Write the selected Bader volumes, read from the\n        subsequent list of volumes.\n        \"\"\"\n        alist = string.join([str(x) for x in atomlist], ' ')\n        cmd = 'bader -p sel_bader %s %s' % (alist, self.densityfile)\n        print(cmd)\n        os.system(cmd)\n\n    def write_atom_index(self):\n        ''' -p atom_index Write the atomic volume index to a charge\n        density file.\n        '''\n        cmd = 'bader -p atom_index %s' % (self.densityfile)\n        print(cmd)\n        os.system(cmd)\n\n    def write_bader_index(self):\n        '''\n        -p bader_index Write the Bader volume index to a charge\n        density file.\n        '''\n        cmd = 'bader -p bader_index %s' % (self.densityfile)\n        print(cmd)\n        os.system(cmd)\n\n    def write_all_atom(self):\n        '''\n        -p all_atom Combine all volumes associated with an atom and\n        write to file. This is done for all atoms and written to files\n        named BvAtxxxx.dat. The volumes associated with atoms are\n        those for which the maximum in charge density within the\n        volume is closest to the atom.\n        '''\n        cmd = 'bader -p all_atom %s' % (self.densityfile)\n        print(cmd)\n        os.system(cmd)\n\n    def write_all_bader(self):\n        '''\n        -p all_bader Write all Bader volumes (containing charge above\n        threshold of 0.0001) to a file. The charge distribution in\n        each volume is written to a separate file, named\n        Bvolxxxx.dat. It will either be of a CHGCAR format or a CUBE\n        file format, depending on the format of the initial charge\n        density file. These files can be quite large, so this option\n        should be used with caution.\n        '''\n        cmd = 'bader -p all_bader %s' % (self.densityfile)\n        print(cmd)\n        os.system(cmd)\n        \nif __name__ == '__main__':\n\n    from ase.calculators.jacapo import Jacapo\n\n    atoms = Jacapo.read_atoms('ethylene.nc')\n\n    b = Bader(atoms)\n\n    print(b.get_bader_charges())\n    print(b.get_bader_volumes())\n    b.write_atom_volume([3, 4])\n","label":0}
{"content":"\"\"\"\nThis module provides views that proxy to the staff grading backend service.\n\"\"\"\n\nimport json\nimport logging\n\nfrom django.conf import settings\nfrom django.http import HttpResponse, Http404\nfrom django.utils.translation import ugettext as _\n\nfrom opaque_keys.edx.locations import SlashSeparatedCourseKey\nfrom xmodule.open_ended_grading_classes.grading_service_module import GradingService, GradingServiceError\n\nfrom courseware.access import has_access\nfrom edxmako.shortcuts import render_to_string\nfrom student.models import unique_id_for_user\n\nfrom open_ended_grading.utils import does_location_exist\nimport dogstats_wrapper as dog_stats_api\n\nlog = logging.getLogger(__name__)\n\nSTAFF_ERROR_MESSAGE = _(\n    u'Could not contact the external grading server. Please contact the '\n    u'development team at {email}.'\n).format(\n    email=u'<a href=\"mailto:{tech_support_email}>{tech_support_email}<\/a>'.format(\n        tech_support_email=settings.TECH_SUPPORT_EMAIL\n    )\n)\nMAX_ALLOWED_FEEDBACK_LENGTH = 5000\n\n\nclass MockStaffGradingService(object):\n    \"\"\"\n    A simple mockup of a staff grading service, testing.\n    \"\"\"\n\n    def __init__(self):\n        self.cnt = 0\n\n    def get_next(self, course_id, location, grader_id):\n        self.cnt += 1\n        return {'success': True,\n                'submission_id': self.cnt,\n                'submission': 'Test submission {cnt}'.format(cnt=self.cnt),\n                'num_graded': 3,\n                'min_for_ml': 5,\n                'num_pending': 4,\n                'prompt': 'This is a fake prompt',\n                'ml_error_info': 'ML info',\n                'max_score': 2 + self.cnt % 3,\n                'rubric': 'A rubric'}\n\n    def get_problem_list(self, course_id, grader_id):\n        self.cnt += 1\n        return {\n            'success': True,\n            'problem_list': [\n                json.dumps({\n                    'location': 'i4x:\/\/MITx\/3.091x\/problem\/open_ended_demo1',\n                    'problem_name': \"Problem 1\",\n                    'num_graded': 3,\n                    'num_pending': 5,\n                    'min_for_ml': 10,\n                }),\n                json.dumps({\n                    'location': 'i4x:\/\/MITx\/3.091x\/problem\/open_ended_demo2',\n                    'problem_name': \"Problem 2\",\n                    'num_graded': 1,\n                    'num_pending': 5,\n                    'min_for_ml': 10,\n                }),\n            ],\n        }\n\n    def save_grade(self, course_id, grader_id, submission_id, score, feedback, skipped, rubric_scores,\n                   submission_flagged):\n        return self.get_next(course_id, 'fake location', grader_id)\n\n\nclass StaffGradingService(GradingService):\n    \"\"\"\n    Interface to staff grading backend.\n    \"\"\"\n\n    METRIC_NAME = 'edxapp.open_ended_grading.staff_grading_service'\n\n    def __init__(self, config):\n        config['render_template'] = render_to_string\n        super(StaffGradingService, self).__init__(config)\n        self.url = config['url'] + config['staff_grading']\n        self.login_url = self.url + '\/login\/'\n        self.get_next_url = self.url + '\/get_next_submission\/'\n        self.save_grade_url = self.url + '\/save_grade\/'\n        self.get_problem_list_url = self.url + '\/get_problem_list\/'\n        self.get_notifications_url = self.url + \"\/get_notifications\/\"\n\n    def get_problem_list(self, course_id, grader_id):\n        \"\"\"\n        Get the list of problems for a given course.\n\n        Args:\n            course_id: course id that we want the problems of\n            grader_id: who is grading this?  The anonymous user_id of the grader.\n\n        Returns:\n            dict with the response from the service.  (Deliberately not\n            writing out the fields here--see the docs on the staff_grading view\n            in the grading_controller repo)\n\n        Raises:\n            GradingServiceError: something went wrong with the connection.\n        \"\"\"\n        params = {'course_id': course_id.to_deprecated_string(), 'grader_id': grader_id}\n        result = self.get(self.get_problem_list_url, params)\n        tags = [u'course_id:{}'.format(course_id)]\n        self._record_result('get_problem_list', result, tags)\n        dog_stats_api.histogram(\n            self._metric_name('get_problem_list.result.length'),\n            len(result.get('problem_list', []))\n        )\n        return result\n\n    def get_next(self, course_id, location, grader_id):\n        \"\"\"\n        Get the next thing to grade.\n\n        Args:\n            course_id: the course that this problem belongs to\n            location: location of the problem that we are grading and would like the\n                next submission for\n            grader_id: who is grading this?  The anonymous user_id of the grader.\n\n        Returns:\n            dict with the response from the service.  (Deliberately not\n            writing out the fields here--see the docs on the staff_grading view\n            in the grading_controller repo)\n\n        Raises:\n            GradingServiceError: something went wrong with the connection.\n        \"\"\"\n        result = self._render_rubric(\n            self.get(\n                self.get_next_url,\n                params={\n                    'location': location.to_deprecated_string(),\n                    'grader_id': grader_id\n                }\n            )\n        )\n        tags = [u'course_id:{}'.format(course_id)]\n        self._record_result('get_next', result, tags)\n        return result\n\n    def save_grade(self, course_id, grader_id, submission_id, score, feedback, skipped, rubric_scores,\n                   submission_flagged):\n        \"\"\"\n        Save a score and feedback for a submission.\n\n        Returns:\n            dict with keys\n                'success': bool\n                'error': error msg, if something went wrong.\n\n        Raises:\n            GradingServiceError if there's a problem connecting.\n        \"\"\"\n        data = {'course_id': course_id.to_deprecated_string(),\n                'submission_id': submission_id,\n                'score': score,\n                'feedback': feedback,\n                'grader_id': grader_id,\n                'skipped': skipped,\n                'rubric_scores': rubric_scores,\n                'rubric_scores_complete': True,\n                'submission_flagged': submission_flagged}\n\n        result = self._render_rubric(self.post(self.save_grade_url, data=data))\n        tags = [u'course_id:{}'.format(course_id)]\n        self._record_result('save_grade', result, tags)\n        return result\n\n    def get_notifications(self, course_id):\n        params = {'course_id': course_id.to_deprecated_string()}\n        result = self.get(self.get_notifications_url, params)\n        tags = [\n            u'course_id:{}'.format(course_id),\n            u'staff_needs_to_grade:{}'.format(result.get('staff_needs_to_grade'))\n        ]\n        self._record_result('get_notifications', result, tags)\n        return result\n\n\n# don't initialize until staff_grading_service() is called--means that just\n# importing this file doesn't create objects that may not have the right config\n_service = None\n\n\ndef staff_grading_service():\n    \"\"\"\n    Return a staff grading service instance--if settings.MOCK_STAFF_GRADING is True,\n    returns a mock one, otherwise a real one.\n\n    Caches the result, so changing the setting after the first call to this\n    function will have no effect.\n    \"\"\"\n    global _service\n    if _service is not None:\n        return _service\n\n    if settings.MOCK_STAFF_GRADING:\n        _service = MockStaffGradingService()\n    else:\n        _service = StaffGradingService(settings.OPEN_ENDED_GRADING_INTERFACE)\n\n    return _service\n\n\ndef _err_response(msg):\n    \"\"\"\n    Return a HttpResponse with a json dump with success=False, and the given error message.\n    \"\"\"\n    return HttpResponse(json.dumps({'success': False, 'error': msg}),\n                        mimetype=\"application\/json\")\n\n\ndef _check_access(user, course_id):\n    \"\"\"\n    Raise 404 if user doesn't have staff access to course_id\n    \"\"\"\n    if not has_access(user, 'staff', course_id):\n        raise Http404\n\n    return\n\n\ndef get_next(request, course_id):\n    \"\"\"\n    Get the next thing to grade for course_id and with the location specified\n    in the request.\n\n    Returns a json dict with the following keys:\n\n    'success': bool\n\n    'submission_id': a unique identifier for the submission, to be passed back\n                     with the grade.\n\n    'submission': the submission, rendered as read-only html for grading\n\n    'rubric': the rubric, also rendered as html.\n\n    'message': if there was no submission available, but nothing went wrong,\n            there will be a message field.\n\n    'error': if success is False, will have an error message with more info.\n    \"\"\"\n    assert isinstance(course_id, basestring)\n    course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id)\n    _check_access(request.user, course_key)\n\n    required = set(['location'])\n    if request.method != 'POST':\n        raise Http404\n    actual = set(request.POST.keys())\n    missing = required - actual\n    if len(missing) > 0:\n        return _err_response('Missing required keys {0}'.format(\n            ', '.join(missing)))\n    grader_id = unique_id_for_user(request.user)\n    p = request.POST\n    location = course_key.make_usage_key_from_deprecated_string(p['location'])\n\n    return HttpResponse(json.dumps(_get_next(course_key, grader_id, location)),\n                        mimetype=\"application\/json\")\n\n\ndef get_problem_list(request, course_id):\n    \"\"\"\n    Get all the problems for the given course id\n\n    Returns a json dict with the following keys:\n        success: bool\n\n        problem_list: a list containing json dicts with the following keys:\n            each dict represents a different problem in the course\n\n            location: the location of the problem\n\n            problem_name: the name of the problem\n\n            num_graded: the number of responses that have been graded\n\n            num_pending: the number of responses that are sitting in the queue\n\n            min_for_ml: the number of responses that need to be graded before\n                the ml can be run\n\n        'error': if success is False, will have an error message with more info.\n    \"\"\"\n    assert isinstance(course_id, basestring)\n    course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id)\n    _check_access(request.user, course_key)\n    try:\n        response = staff_grading_service().get_problem_list(course_key, unique_id_for_user(request.user))\n\n        # If 'problem_list' is in the response, then we got a list of problems from the ORA server.\n        # If it is not, then ORA could not find any problems.\n        if 'problem_list' in response:\n            problem_list = response['problem_list']\n        else:\n            problem_list = []\n            # Make an error messages to reflect that we could not find anything to grade.\n            response['error'] = _(\n                u'Cannot find any open response problems in this course. '\n                u'Have you submitted answers to any open response assessment questions? '\n                u'If not, please do so and return to this page.'\n            )\n        valid_problem_list = []\n        for i in xrange(len(problem_list)):\n            # Needed to ensure that the 'location' key can be accessed.\n            try:\n                problem_list[i] = json.loads(problem_list[i])\n            except Exception:\n                pass\n            if does_location_exist(course_key.make_usage_key_from_deprecated_string(problem_list[i]['location'])):\n                valid_problem_list.append(problem_list[i])\n        response['problem_list'] = valid_problem_list\n        response = json.dumps(response)\n\n        return HttpResponse(response,\n                            mimetype=\"application\/json\")\n    except GradingServiceError:\n        #This is a dev_facing_error\n        log.exception(\n            \"Error from staff grading service in open \"\n            \"ended grading.  server url: {0}\".format(staff_grading_service().url)\n        )\n        #This is a staff_facing_error\n        return HttpResponse(json.dumps({'success': False,\n                                        'error': STAFF_ERROR_MESSAGE}))\n\n\ndef _get_next(course_id, grader_id, location):\n    \"\"\"\n    Implementation of get_next (also called from save_grade) -- returns a json string\n    \"\"\"\n    try:\n        return staff_grading_service().get_next(course_id, location, grader_id)\n    except GradingServiceError:\n        #This is a dev facing error\n        log.exception(\n            \"Error from staff grading service in open \"\n            \"ended grading.  server url: {0}\".format(staff_grading_service().url)\n        )\n        #This is a staff_facing_error\n        return json.dumps({'success': False,\n                           'error': STAFF_ERROR_MESSAGE})\n\n\ndef save_grade(request, course_id):\n    \"\"\"\n    Save the grade and feedback for a submission, and, if all goes well, return\n    the next thing to grade.\n\n    Expects the following POST parameters:\n    'score': int\n    'feedback': string\n    'submission_id': int\n\n    Returns the same thing as get_next, except that additional error messages\n    are possible if something goes wrong with saving the grade.\n    \"\"\"\n\n    course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id)\n    _check_access(request.user, course_key)\n\n    if request.method != 'POST':\n        raise Http404\n    p = request.POST\n    required = set(['score', 'feedback', 'submission_id', 'location', 'submission_flagged'])\n    skipped = 'skipped' in p\n    #If the instructor has skipped grading the submission, then there will not be any rubric scores.\n    #Only add in the rubric scores if the instructor has not skipped.\n    if not skipped:\n        required.add('rubric_scores[]')\n    actual = set(p.keys())\n    missing = required - actual\n    if len(missing) > 0:\n        return _err_response('Missing required keys {0}'.format(\n            ', '.join(missing)))\n\n    success, message = check_feedback_length(p)\n    if not success:\n        return _err_response(message)\n\n    grader_id = unique_id_for_user(request.user)\n\n    location = course_key.make_usage_key_from_deprecated_string(p['location'])\n\n    try:\n        result = staff_grading_service().save_grade(course_key,\n                                                    grader_id,\n                                                    p['submission_id'],\n                                                    p['score'],\n                                                    p['feedback'],\n                                                    skipped,\n                                                    p.getlist('rubric_scores[]'),\n                                                    p['submission_flagged'])\n    except GradingServiceError:\n        #This is a dev_facing_error\n        log.exception(\n            \"Error saving grade in the staff grading interface in open ended grading.  Request: {0} Course ID: {1}\".format(\n                request, course_id))\n        #This is a staff_facing_error\n        return _err_response(STAFF_ERROR_MESSAGE)\n    except ValueError:\n        #This is a dev_facing_error\n        log.exception(\n            \"save_grade returned broken json in the staff grading interface in open ended grading: {0}\".format(\n                result_json))\n        #This is a staff_facing_error\n        return _err_response(STAFF_ERROR_MESSAGE)\n\n    if not result.get('success', False):\n        #This is a dev_facing_error\n        log.warning(\n            'Got success=False from staff grading service in open ended grading.  Response: {0}'.format(result_json))\n        return _err_response(STAFF_ERROR_MESSAGE)\n\n    # Ok, save_grade seemed to work.  Get the next submission to grade.\n    return HttpResponse(json.dumps(_get_next(course_id, grader_id, location)),\n                        mimetype=\"application\/json\")\n\n\ndef check_feedback_length(data):\n    feedback = data.get(\"feedback\")\n    if feedback and len(feedback) > MAX_ALLOWED_FEEDBACK_LENGTH:\n        return False, \"Feedback is too long, Max length is {0} characters.\".format(\n            MAX_ALLOWED_FEEDBACK_LENGTH\n        )\n    else:\n        return True, \"\"\n","label":0}
{"content":"import _surface\nimport chimera\ntry:\n  import chimera.runCommand\nexcept:\n  pass\nfrom VolumePath import markerset as ms\ntry:\n  from VolumePath import Marker_Set, Link\n  new_marker_set=Marker_Set\nexcept:\n  from VolumePath import volume_path_dialog\n  d= volume_path_dialog(True)\n  new_marker_set= d.new_marker_set\nmarker_sets={}\nsurf_sets={}\nif \"particle_0 geometry\" not in marker_sets:\n  s=new_marker_set('particle_0 geometry')\n  marker_sets[\"particle_0 geometry\"]=s\ns= marker_sets[\"particle_0 geometry\"]\nmark=s.place_marker((14969.5, 9355.22, 4884.95), (0.7, 0.7, 0.7), 507.685)\nif \"particle_1 geometry\" not in marker_sets:\n  s=new_marker_set('particle_1 geometry')\n  marker_sets[\"particle_1 geometry\"]=s\ns= marker_sets[\"particle_1 geometry\"]\nmark=s.place_marker((15857.7, 8984.62, 4617.48), (0.7, 0.7, 0.7), 479.978)\nif \"particle_2 geometry\" not in marker_sets:\n  s=new_marker_set('particle_2 geometry')\n  marker_sets[\"particle_2 geometry\"]=s\ns= marker_sets[\"particle_2 geometry\"]\nmark=s.place_marker((14048.8, 8305.7, 4500.15), (0.7, 0.7, 0.7), 681.834)\nif \"particle_3 geometry\" not in marker_sets:\n  s=new_marker_set('particle_3 geometry')\n  marker_sets[\"particle_3 geometry\"]=s\ns= marker_sets[\"particle_3 geometry\"]\nmark=s.place_marker((11860.6, 7514.14, 4365.92), (0.7, 0.7, 0.7), 522.532)\nif \"particle_4 geometry\" not in marker_sets:\n  s=new_marker_set('particle_4 geometry')\n  marker_sets[\"particle_4 geometry\"]=s\ns= marker_sets[\"particle_4 geometry\"]\nmark=s.place_marker((11168.8, 7253.91, 4353.22), (0, 1, 0), 751.925)\nif \"particle_5 geometry\" not in marker_sets:\n  s=new_marker_set('particle_5 geometry')\n  marker_sets[\"particle_5 geometry\"]=s\ns= marker_sets[\"particle_5 geometry\"]\nmark=s.place_marker((12710.6, 5841.18, 3924.41), (0.7, 0.7, 0.7), 437.001)\nif \"particle_6 geometry\" not in marker_sets:\n  s=new_marker_set('particle_6 geometry')\n  marker_sets[\"particle_6 geometry\"]=s\ns= marker_sets[\"particle_6 geometry\"]\nmark=s.place_marker((11163.9, 4708.08, 4115.8), (0.7, 0.7, 0.7), 710.767)\nif \"particle_7 geometry\" not in marker_sets:\n  s=new_marker_set('particle_7 geometry')\n  marker_sets[\"particle_7 geometry\"]=s\ns= marker_sets[\"particle_7 geometry\"]\nmark=s.place_marker((11220.3, 3116.23, 3445.88), (0.7, 0.7, 0.7), 762.077)\nif \"particle_8 geometry\" not in marker_sets:\n  s=new_marker_set('particle_8 geometry')\n  marker_sets[\"particle_8 geometry\"]=s\ns= marker_sets[\"particle_8 geometry\"]\nmark=s.place_marker((10024.2, 2278.2, 2811.32), (0.7, 0.7, 0.7), 726.799)\nif \"particle_9 geometry\" not in marker_sets:\n  s=new_marker_set('particle_9 geometry')\n  marker_sets[\"particle_9 geometry\"]=s\ns= marker_sets[\"particle_9 geometry\"]\nmark=s.place_marker((8565.6, 1179.39, 2598.93), (0.7, 0.7, 0.7), 885.508)\nif \"particle_10 geometry\" not in marker_sets:\n  s=new_marker_set('particle_10 geometry')\n  marker_sets[\"particle_10 geometry\"]=s\ns= marker_sets[\"particle_10 geometry\"]\nmark=s.place_marker((7241.9, 1843.25, 1632.03), (0.7, 0.7, 0.7), 778.489)\nif \"particle_11 geometry\" not in marker_sets:\n  s=new_marker_set('particle_11 geometry')\n  marker_sets[\"particle_11 geometry\"]=s\ns= marker_sets[\"particle_11 geometry\"]\nmark=s.place_marker((7011.72, 1012.49, -305.89), (0.7, 0.7, 0.7), 790.333)\nif \"particle_12 geometry\" not in marker_sets:\n  s=new_marker_set('particle_12 geometry')\n  marker_sets[\"particle_12 geometry\"]=s\ns= marker_sets[\"particle_12 geometry\"]\nmark=s.place_marker((6903.61, 98.5444, -2158.28), (0.7, 0.7, 0.7), 707.721)\nif \"particle_13 geometry\" not in marker_sets:\n  s=new_marker_set('particle_13 geometry')\n  marker_sets[\"particle_13 geometry\"]=s\ns= marker_sets[\"particle_13 geometry\"]\nmark=s.place_marker((8201.9, 1002.62, -1834.4), (0.7, 0.7, 0.7), 651.166)\nif \"particle_14 geometry\" not in marker_sets:\n  s=new_marker_set('particle_14 geometry')\n  marker_sets[\"particle_14 geometry\"]=s\ns= marker_sets[\"particle_14 geometry\"]\nmark=s.place_marker((7414.09, -129.02, -854.322), (0.7, 0.7, 0.7), 708.61)\nif \"particle_15 geometry\" not in marker_sets:\n  s=new_marker_set('particle_15 geometry')\n  marker_sets[\"particle_15 geometry\"]=s\ns= marker_sets[\"particle_15 geometry\"]\nmark=s.place_marker((7122.12, -299.946, 714.323), (0.7, 0.7, 0.7), 490.595)\nif \"particle_16 geometry\" not in marker_sets:\n  s=new_marker_set('particle_16 geometry')\n  marker_sets[\"particle_16 geometry\"]=s\ns= marker_sets[\"particle_16 geometry\"]\nmark=s.place_marker((7708.18, 230.276, 1947.9), (0.7, 0.7, 0.7), 591.565)\nif \"particle_17 geometry\" not in marker_sets:\n  s=new_marker_set('particle_17 geometry')\n  marker_sets[\"particle_17 geometry\"]=s\ns= marker_sets[\"particle_17 geometry\"]\nmark=s.place_marker((8140.74, 861.511, 3347.95), (0.7, 0.7, 0.7), 581.287)\nif \"particle_18 geometry\" not in marker_sets:\n  s=new_marker_set('particle_18 geometry')\n  marker_sets[\"particle_18 geometry\"]=s\ns= marker_sets[\"particle_18 geometry\"]\nmark=s.place_marker((9916.15, 691.375, 3641.12), (0.7, 0.7, 0.7), 789.529)\nif \"particle_19 geometry\" not in marker_sets:\n  s=new_marker_set('particle_19 geometry')\n  marker_sets[\"particle_19 geometry\"]=s\ns= marker_sets[\"particle_19 geometry\"]\nmark=s.place_marker((10028.7, 610.495, 5184.02), (0.7, 0.7, 0.7), 623.587)\nif \"particle_20 geometry\" not in marker_sets:\n  s=new_marker_set('particle_20 geometry')\n  marker_sets[\"particle_20 geometry\"]=s\ns= marker_sets[\"particle_20 geometry\"]\nmark=s.place_marker((9947.99, 101.655, 6937.75), (0.7, 0.7, 0.7), 1083.56)\nif \"particle_21 geometry\" not in marker_sets:\n  s=new_marker_set('particle_21 geometry')\n  marker_sets[\"particle_21 geometry\"]=s\ns= marker_sets[\"particle_21 geometry\"]\nmark=s.place_marker((10141.9, -1122.2, 8124.17), (0.7, 0.7, 0.7), 504.258)\nif \"particle_22 geometry\" not in marker_sets:\n  s=new_marker_set('particle_22 geometry')\n  marker_sets[\"particle_22 geometry\"]=s\ns= marker_sets[\"particle_22 geometry\"]\nmark=s.place_marker((9427.43, 62.891, 7740), (0.7, 0.7, 0.7), 805.519)\nif \"particle_23 geometry\" not in marker_sets:\n  s=new_marker_set('particle_23 geometry')\n  marker_sets[\"particle_23 geometry\"]=s\ns= marker_sets[\"particle_23 geometry\"]\nmark=s.place_marker((7792.4, 1113.02, 6928.08), (0.7, 0.7, 0.7), 631.708)\nif \"particle_24 geometry\" not in marker_sets:\n  s=new_marker_set('particle_24 geometry')\n  marker_sets[\"particle_24 geometry\"]=s\ns= marker_sets[\"particle_24 geometry\"]\nmark=s.place_marker((5839.87, 1426.25, 6096.11), (0.7, 0.7, 0.7), 805.942)\nif \"particle_25 geometry\" not in marker_sets:\n  s=new_marker_set('particle_25 geometry')\n  marker_sets[\"particle_25 geometry\"]=s\ns= marker_sets[\"particle_25 geometry\"]\nmark=s.place_marker((4866.74, 1500.33, 5695.34), (1, 0.7, 0), 672.697)\nif \"particle_26 geometry\" not in marker_sets:\n  s=new_marker_set('particle_26 geometry')\n  marker_sets[\"particle_26 geometry\"]=s\ns= marker_sets[\"particle_26 geometry\"]\nmark=s.place_marker((4653.24, 3936.53, 6840.6), (0.7, 0.7, 0.7), 797.863)\nif \"particle_27 geometry\" not in marker_sets:\n  s=new_marker_set('particle_27 geometry')\n  marker_sets[\"particle_27 geometry\"]=s\ns= marker_sets[\"particle_27 geometry\"]\nmark=s.place_marker((3660.54, 5240.38, 7746.49), (1, 0.7, 0), 735.682)\nif \"particle_28 geometry\" not in marker_sets:\n  s=new_marker_set('particle_28 geometry')\n  marker_sets[\"particle_28 geometry\"]=s\ns= marker_sets[\"particle_28 geometry\"]\nmark=s.place_marker((4273.05, 5438.27, 8823.37), (0.7, 0.7, 0.7), 602.14)\nif \"particle_29 geometry\" not in marker_sets:\n  s=new_marker_set('particle_29 geometry')\n  marker_sets[\"particle_29 geometry\"]=s\ns= marker_sets[\"particle_29 geometry\"]\nmark=s.place_marker((4960.15, 5382.51, 11055.5), (0.7, 0.7, 0.7), 954.796)\nif \"particle_30 geometry\" not in marker_sets:\n  s=new_marker_set('particle_30 geometry')\n  marker_sets[\"particle_30 geometry\"]=s\ns= marker_sets[\"particle_30 geometry\"]\nmark=s.place_marker((4538.04, 5433.08, 10570.7), (0.7, 0.7, 0.7), 1021.88)\nif \"particle_31 geometry\" not in marker_sets:\n  s=new_marker_set('particle_31 geometry')\n  marker_sets[\"particle_31 geometry\"]=s\ns= marker_sets[\"particle_31 geometry\"]\nmark=s.place_marker((4142.42, 6699.7, 10521.4), (0.7, 0.7, 0.7), 909.323)\nif \"particle_32 geometry\" not in marker_sets:\n  s=new_marker_set('particle_32 geometry')\n  marker_sets[\"particle_32 geometry\"]=s\ns= marker_sets[\"particle_32 geometry\"]\nmark=s.place_marker((3794.77, 8574.69, 11766.8), (0.7, 0.7, 0.7), 621.049)\nif \"particle_33 geometry\" not in marker_sets:\n  s=new_marker_set('particle_33 geometry')\n  marker_sets[\"particle_33 geometry\"]=s\ns= marker_sets[\"particle_33 geometry\"]\nmark=s.place_marker((4229.33, 9771.99, 11046.9), (0.7, 0.7, 0.7), 525.154)\nif \"particle_34 geometry\" not in marker_sets:\n  s=new_marker_set('particle_34 geometry')\n  marker_sets[\"particle_34 geometry\"]=s\ns= marker_sets[\"particle_34 geometry\"]\nmark=s.place_marker((5420.81, 10555.7, 10510.1), (0.7, 0.7, 0.7), 890.246)\nif \"particle_35 geometry\" not in marker_sets:\n  s=new_marker_set('particle_35 geometry')\n  marker_sets[\"particle_35 geometry\"]=s\ns= marker_sets[\"particle_35 geometry\"]\nmark=s.place_marker((6615.4, 11834.4, 10784.8), (0.7, 0.7, 0.7), 671.216)\nif \"particle_36 geometry\" not in marker_sets:\n  s=new_marker_set('particle_36 geometry')\n  marker_sets[\"particle_36 geometry\"]=s\ns= marker_sets[\"particle_36 geometry\"]\nmark=s.place_marker((8123.43, 12065.3, 11499), (0.7, 0.7, 0.7), 662.672)\nif \"particle_37 geometry\" not in marker_sets:\n  s=new_marker_set('particle_37 geometry')\n  marker_sets[\"particle_37 geometry\"]=s\ns= marker_sets[\"particle_37 geometry\"]\nmark=s.place_marker((8008.57, 10546.9, 12037.1), (0.7, 0.7, 0.7), 646.682)\nif \"particle_38 geometry\" not in marker_sets:\n  s=new_marker_set('particle_38 geometry')\n  marker_sets[\"particle_38 geometry\"]=s\ns= marker_sets[\"particle_38 geometry\"]\nmark=s.place_marker((6588.29, 10507.4, 12644.8), (0.7, 0.7, 0.7), 769.945)\nif \"particle_39 geometry\" not in marker_sets:\n  s=new_marker_set('particle_39 geometry')\n  marker_sets[\"particle_39 geometry\"]=s\ns= marker_sets[\"particle_39 geometry\"]\nmark=s.place_marker((5333.97, 9838.61, 11243.6), (0.7, 0.7, 0.7), 606.92)\nif \"particle_40 geometry\" not in marker_sets:\n  s=new_marker_set('particle_40 geometry')\n  marker_sets[\"particle_40 geometry\"]=s\ns= marker_sets[\"particle_40 geometry\"]\nmark=s.place_marker((4610.87, 10843.4, 11069.5), (0.7, 0.7, 0.7), 622.571)\nif \"particle_41 geometry\" not in marker_sets:\n  s=new_marker_set('particle_41 geometry')\n  marker_sets[\"particle_41 geometry\"]=s\ns= marker_sets[\"particle_41 geometry\"]\nmark=s.place_marker((5113.26, 9718.7, 10484.3), (0.7, 0.7, 0.7), 466.865)\nif \"particle_42 geometry\" not in marker_sets:\n  s=new_marker_set('particle_42 geometry')\n  marker_sets[\"particle_42 geometry\"]=s\ns= marker_sets[\"particle_42 geometry\"]\nmark=s.place_marker((5912.2, 10033, 10071.9), (0.7, 0.7, 0.7), 682.933)\nif \"particle_43 geometry\" not in marker_sets:\n  s=new_marker_set('particle_43 geometry')\n  marker_sets[\"particle_43 geometry\"]=s\ns= marker_sets[\"particle_43 geometry\"]\nmark=s.place_marker((5196.91, 9912.15, 10527.5), (0.7, 0.7, 0.7), 809.326)\nif \"particle_44 geometry\" not in marker_sets:\n  s=new_marker_set('particle_44 geometry')\n  marker_sets[\"particle_44 geometry\"]=s\ns= marker_sets[\"particle_44 geometry\"]\nmark=s.place_marker((4146.77, 8424.44, 10674.7), (0.7, 0.7, 0.7), 796.72)\nif \"particle_45 geometry\" not in marker_sets:\n  s=new_marker_set('particle_45 geometry')\n  marker_sets[\"particle_45 geometry\"]=s\ns= marker_sets[\"particle_45 geometry\"]\nmark=s.place_marker((3517.66, 6984.69, 8251.86), (0.7, 0.7, 0.7), 870.026)\nif \"particle_46 geometry\" not in marker_sets:\n  s=new_marker_set('particle_46 geometry')\n  marker_sets[\"particle_46 geometry\"]=s\ns= marker_sets[\"particle_46 geometry\"]\nmark=s.place_marker((2724.62, 7296.95, 6580.21), (0.7, 0.7, 0.7), 909.577)\nif \"particle_47 geometry\" not in marker_sets:\n  s=new_marker_set('particle_47 geometry')\n  marker_sets[\"particle_47 geometry\"]=s\ns= marker_sets[\"particle_47 geometry\"]\nmark=s.place_marker((2710.77, 7987.75, 5648.79), (0, 1, 0), 500.536)\nif \"particle_48 geometry\" not in marker_sets:\n  s=new_marker_set('particle_48 geometry')\n  marker_sets[\"particle_48 geometry\"]=s\ns= marker_sets[\"particle_48 geometry\"]\nmark=s.place_marker((1852.63, 9762.8, 5359.63), (0.7, 0.7, 0.7), 725.276)\nif \"particle_49 geometry\" not in marker_sets:\n  s=new_marker_set('particle_49 geometry')\n  marker_sets[\"particle_49 geometry\"]=s\ns= marker_sets[\"particle_49 geometry\"]\nmark=s.place_marker((41.5662, 11727.3, 5413.74), (0.7, 0.7, 0.7), 570.331)\nif \"particle_50 geometry\" not in marker_sets:\n  s=new_marker_set('particle_50 geometry')\n  marker_sets[\"particle_50 geometry\"]=s\ns= marker_sets[\"particle_50 geometry\"]\nmark=s.place_marker((412.316, 12008.6, 7020.04), (0.7, 0.7, 0.7), 492.203)\nif \"particle_51 geometry\" not in marker_sets:\n  s=new_marker_set('particle_51 geometry')\n  marker_sets[\"particle_51 geometry\"]=s\ns= marker_sets[\"particle_51 geometry\"]\nmark=s.place_marker((304.441, 9258.71, 7961.56), (0, 1, 0), 547.7)\nif \"particle_52 geometry\" not in marker_sets:\n  s=new_marker_set('particle_52 geometry')\n  marker_sets[\"particle_52 geometry\"]=s\ns= marker_sets[\"particle_52 geometry\"]\nmark=s.place_marker((1047.31, 9521.91, 7963.95), (0.7, 0.7, 0.7), 581.921)\nif \"particle_53 geometry\" not in marker_sets:\n  s=new_marker_set('particle_53 geometry')\n  marker_sets[\"particle_53 geometry\"]=s\ns= marker_sets[\"particle_53 geometry\"]\nmark=s.place_marker((1974.71, 10863.2, 8973.95), (0.7, 0.7, 0.7), 555.314)\nif \"particle_54 geometry\" not in marker_sets:\n  s=new_marker_set('particle_54 geometry')\n  marker_sets[\"particle_54 geometry\"]=s\ns= marker_sets[\"particle_54 geometry\"]\nmark=s.place_marker((3220.08, 11733.6, 9246.88), (0.7, 0.7, 0.7), 404.219)\nif \"particle_55 geometry\" not in marker_sets:\n  s=new_marker_set('particle_55 geometry')\n  marker_sets[\"particle_55 geometry\"]=s\ns= marker_sets[\"particle_55 geometry\"]\nmark=s.place_marker((4736.09, 11001.1, 8495.51), (0.7, 0.7, 0.7), 764.234)\nfor k in surf_sets.keys():\n  chimera.openModels.add([surf_sets[k]])\n","label":0}
{"content":"# Copyright (C) 2010 Matthew McGowan\n#\n# Authors:\n#   Matthew McGowan\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n\nfrom gi.repository import Gtk, Gdk\nfrom gi.repository import GObject\nfrom gi.repository import Pango\n\nfrom softwarecenter.utils import normalize_package_description\nfrom softwarecenter.ui.gtk3.drawing import color_to_hex\nfrom softwarecenter.ui.gtk3.utils import point_in\n\n_PS = Pango.SCALE\n\n\nclass _SpecialCasePreParsers(object):\n\n    def preparse(self, k, desc):\n        if k is None:\n            return desc\n        func_name = '_%s_preparser' % k.lower().replace('-', '_')\n        if not hasattr(self, func_name):\n            return desc\n        f = getattr(self, func_name)\n        return f(desc)\n\n    # special case pre-parsers\n    def _skype_preparser(self, desc):\n        return desc.replace('. *', '.\\n*')\n\n    def _texlive_fonts_extra_preparser(self, desc):\n        return desc.replace(')\\n', ').\\n').replace('--\\n', '--\\n\\n')\n\n\nclass EventHelper(dict):\n\n    # FIXME: workaround for broken event.copy()\n    class ButtonEvent(object):\n        def __init__(self, event):\n            self.x = event.x\n            self.y = event.y\n            self.type = event.type\n            self.button = event.button\n\n    VALID_KEYS = (\n        'event',\n        'layout',\n        'index',\n        'within-selection',\n        'drag-active',\n        'drag-context')\n\n    def __init__(self):\n        dict.__init__(self)\n        self.new_press(None, None, None, False)\n\n    def __setitem__(self, k, v):\n        if k not in EventHelper.VALID_KEYS:\n            raise KeyError('\\\"%s\\\" is not a valid key' % k)\n            return False\n        return dict.__setitem__(self, k, v)\n\n    def new_press(self, event, layout, index, within_sel):\n        if event is None:\n            self['event'] = None\n        else:\n            # this should be simply event.copy() but that appears broken\n            # currently(?)\n            self['event'] = EventHelper.ButtonEvent(event)\n\n        self['layout'] = layout\n        self['index'] = index\n        self['within-selection'] = within_sel\n        self['drag-active'] = False\n        self['drag-context'] = None\n\n\nclass PangoLayoutProxy(object):\n\n    \"\"\" Because i couldn't figure out how to inherit from\n        pygi's Pango.Layout... \"\"\"\n\n    def __init__(self, context):\n        self._layout = Pango.Layout.new(context)\n\n    def xy_to_index(self, x, y):\n        return self._layout.xy_to_index(x, y)\n\n    def index_to_pos(self, *args):\n        return self._layout.index_to_pos(*args)\n\n    # setter proxies\n    def set_attributes(self, attrs):\n        return self._layout.set_attributes(attrs)\n\n    def set_markup(self, markup):\n        return self._layout.set_markup(markup, -1)\n\n    def set_font_description(self, font_desc):\n        return self._layout.set_font_description(font_desc)\n\n    def set_wrap(self, wrap_mode):\n        return self._layout.set_wrap(wrap_mode)\n\n    def set_width(self, width):\n        return self._layout.set_width(width)\n\n    # getter proxies\n    def get_text(self):\n        return self._layout.get_text()\n\n    def get_pixel_extents(self):\n        return self._layout.get_pixel_extents()[1]\n\n    def get_cursor_pos(self, index):\n        return self._layout.get_cursor_pos(index)\n\n    def get_iter(self):\n        return self._layout.get_iter()\n\n    def get_extents(self):\n        return self._layout.get_extents()\n\n\nclass Layout(PangoLayoutProxy):\n\n    def __init__(self, widget, text=\"\"):\n        PangoLayoutProxy.__init__(self, widget.get_pango_context())\n\n        self.widget = widget\n        self.length = 0\n        self.indent = 0\n        self.vspacing = None\n        self.is_bullet = False\n        self.index = 0\n        self.allocation = Gdk.Rectangle()\n        self._default_attrs = True\n        self.set_markup(text)\n\n    def __len__(self):\n        return self.length\n\n    def set_text(self, text):\n        PangoLayoutProxy.set_markup(self, text)\n        self.length = len(self.get_text())\n\n    def set_allocation(self, x, y, w, h):\n        a = self.allocation\n        a.x = x\n        a.y = y\n        a.width = w\n        a.height = h\n\n    def get_position(self):\n        return self.allocation.x, self.allocation.y\n\n    def cursor_up(self, cursor, target_x=-1):\n        layout = self.widget.order[cursor.paragraph]\n        pos = layout.index_to_pos(cursor.index)\n        x, y = pos.x, pos.y\n\n        if target_x >= 0:\n            x = target_x\n\n        y -= _PS * self.widget.line_height\n        return layout.xy_to_index(x, y), (x, y)\n\n    def cursor_down(self, cursor, target_x=-1):\n        layout = self.widget.order[cursor.paragraph]\n        pos = layout.index_to_pos(cursor.index)\n        x, y = pos.x, pos.y\n\n        if target_x >= 0:\n            x = target_x\n\n        y += _PS * self.widget.line_height\n        return layout.xy_to_index(x, y), (x, y)\n\n    def index_at(self, px, py):\n        #wa = self.widget.get_allocation()\n        x, y = self.get_position()  # layout allocation\n        (_, index, k) = self.xy_to_index((px - x) * _PS, (py - y) * _PS)\n        return point_in(self.allocation, px, py), index + k\n\n    def reset_attrs(self):\n        #~ self.set_attributes(Pango.AttrList())\n        self.set_markup(self.get_text())\n        self._default_attrs = True\n\n    def highlight(self, start, end, bg, fg):\n        # FIXME: AttrBackground doesnt seem to be expose by gi yet??\n        #~ attrs = Pango.AttrList()\n        #~ attrs.insert(Pango.AttrBackground(bg.red, bg.green, bg.blue, start,\n        #~     end))\n        #~ attrs.insert(Pango.AttrForeground(fg.red, fg.green, fg.blue, start,\n        #~     end))\n        #~ self.set_attributes(attrs)\n\n        # XXX: workaround\n        text = self.get_text()\n        new_text = (text[:start] + '<span background=\"%s\" foreground=\"%s\">' %\n            (bg, fg))\n        new_text += text[start:end]\n        new_text += '<\/span>' + text[end:]\n        self.set_markup(new_text)\n        self._default_attrs = False\n\n    def highlight_all(self, bg, fg):\n        # FIXME: AttrBackground doesnt seem to be expose by gi yet??\n        #~ attrs = Pango.AttrList()\n        #~ attrs.insert(Pango.AttrBackground(bg.red, bg.green, bg.blue, 0, -1))\n        #~ attrs.insert(Pango.AttrForeground(fg.red, fg.green, fg.blue, 0, -1))\n        #~ self.set_attributes(attrs)\n\n        # XXX: workaround\n        text = self.get_text()\n        self.set_markup('<span background=\"%s\" foreground=\"%s\">%s<\/span>' %\n            (bg, fg, text))\n        self._default_attrs = False\n\n\nclass Cursor(object):\n\n    WORD_TERMINATORS = (' ',)   # empty space. suggestions recommended...\n\n    def __init__(self, parent):\n        self.parent = parent\n        self.index = 0\n        self.paragraph = 0\n\n    def is_min(self, cursor):\n        return self.get_position() <= cursor.get_position()\n\n    def is_max(self, cursor):\n        return self.get_position() >= cursor.get_position()\n\n    def switch(self, cursor):\n        this_pos = self.get_position()\n        other_pos = cursor.get_position()\n        self.set_position(*other_pos)\n        cursor.set_position(*this_pos)\n\n    def same_line(self, cursor):\n        return self.get_current_line()[0] == cursor.get_current_line()[0]\n\n    def get_current_line(self):\n        keep_going = True\n        i, it = self.index, self.parent.order[self.paragraph].get_iter()\n        ln = 0\n        while keep_going:\n            l = it.get_line()\n            ls = l.start_index\n            le = ls + l.length\n\n            if i >= ls and i <= le:\n                if not it.at_last_line():\n                    le -= 1\n                return (self.paragraph, ln), (ls, le)\n            ln += 1\n            keep_going = it.next_line()\n        return None, None, None\n\n    def get_current_word(self):\n        keep_going = True\n        layout = self.parent.order[self.paragraph]\n        text = layout.get_text()\n        i, it = self.index, layout.get_iter()\n        start = 0\n        while keep_going:\n            j = it.get_index()\n            if j >= i and text[j] in self.WORD_TERMINATORS:\n                return self.paragraph, (start, j)\n\n            elif text[j] in self.WORD_TERMINATORS:\n                start = j + 1\n\n            keep_going = it.next_char()\n        return self.paragraph, (start, len(layout))\n\n    def set_position(self, paragraph, index):\n        self.index = index\n        self.paragraph = paragraph\n\n    def get_position(self):\n        return self.paragraph, self.index\n\n\nclass PrimaryCursor(Cursor):\n\n    def __init__(self, parent):\n        Cursor.__init__(self, parent)\n\n    def __repr__(self):\n        return 'Cursor: ' + str((self.paragraph, self.index))\n\n    def get_rectangle(self, layout, a):\n        if self.index < len(layout):\n            pos = layout.get_cursor_pos(self.index)[1]\n        else:\n            pos = layout.get_cursor_pos(len(layout))[1]\n\n        x = layout.allocation.x + pos.x \/ _PS\n        y = layout.allocation.y + pos.y \/ _PS\n        return x, y, 1, pos.height \/ _PS\n\n    def draw(self, cr, layout, a):\n        cr.set_source_rgb(0, 0, 0)\n        cr.rectangle(*self.get_rectangle(layout, a))\n        cr.fill()\n\n    def zero(self):\n        self.index = 0\n        self.paragraph = 0\n\n\nclass SelectionCursor(Cursor):\n\n    def __init__(self, cursor):\n        Cursor.__init__(self, cursor.parent)\n        self.cursor = cursor\n        self.target_x = None\n        self.target_x_indent = 0\n        self.restore_point = None\n\n    def __repr__(self):\n        return 'Selection: ' + str(self.get_range())\n\n    def __nonzero__(self):\n        c = self.cursor\n        return (self.paragraph, self.index) != (c.paragraph, c.index)\n\n    @property\n    def min(self):\n        c = self.cursor\n        return min((self.paragraph, self.index), (c.paragraph, c.index))\n\n    @property\n    def max(self):\n        c = self.cursor\n        return max((self.paragraph, self.index), (c.paragraph, c.index))\n\n    def clear(self, key=None):\n        self.index = self.cursor.index\n        self.paragraph = self.cursor.paragraph\n        self.restore_point = None\n\n        if key not in (Gdk.KEY_uparrow, Gdk.KEY_downarrow):\n            self.target_x = None\n            self.target_x_indent = 0\n\n    def set_target_x(self, x, indent):\n        self.target_x = x\n        self.target_x_indent = indent\n\n    def get_range(self):\n        return self.min, self.max\n\n    def within_selection(self, pos):\n        l = list(self.get_range())\n        l.append(pos)\n        l.sort()\n        # sort the list, see if pos is in between the extents of the selection\n        # range, if it is, pos is within the selection\n        if pos in l:\n            return l.index(pos) == 1\n        return False\n\n\nclass TextBlock(Gtk.EventBox):\n\n    PAINT_PRIMARY_CURSOR = False\n    DEBUG_PAINT_BBOXES = False\n\n    BULLET_POINT = u' \\u2022  '\n\n    def __init__(self):\n        Gtk.EventBox.__init__(self)\n        self.set_visible_window(False)\n        self.set_size_request(200, -1)\n\n        self.set_can_focus(True)\n        self.set_events(Gdk.EventMask.KEY_PRESS_MASK |\n                        Gdk.EventMask.ENTER_NOTIFY_MASK |\n                        Gdk.EventMask.LEAVE_NOTIFY_MASK |\n                        Gdk.EventMask.BUTTON_RELEASE_MASK |\n                        Gdk.EventMask.POINTER_MOTION_MASK)\n\n        self._is_new = False\n\n        self.order = []\n        self.cursor = cur = PrimaryCursor(self)\n        self.selection = sel = SelectionCursor(self.cursor)\n        self.clipboard = None\n\n        #~ event_helper = EventHelper()\n        self._update_cached_layouts()\n        self._test_layout = self.create_pango_layout('')\n\n        #self._xterm = Gdk.Cursor.new(Gdk.XTERM)\n\n        # popup menu and menuitem's\n        self.copy_menuitem = Gtk.ImageMenuItem.new_from_stock(\n                                            Gtk.STOCK_COPY, None)\n        self.select_all_menuitem = Gtk.ImageMenuItem.new_from_stock(\n                                            Gtk.STOCK_SELECT_ALL, None)\n        self.menu = Gtk.Menu()\n        self.menu.attach_to_widget(self, None)\n        self.menu.append(self.copy_menuitem)\n        self.menu.append(self.select_all_menuitem)\n        self.menu.show_all()\n        self.copy_menuitem.connect('select', self._menu_do_copy, sel)\n        self.select_all_menuitem.connect('select', self._menu_do_select_all,\n            cur, sel)\n\n        #~ Gtk.drag_source_set(self, Gdk.ModifierType.BUTTON1_MASK,\n                            #~ None, Gdk.DragAction.COPY)\n        #~ Gtk.drag_source_add_text_targets(self)\n        #~ self.connect('drag-begin', self._on_drag_begin)\n        #~ self.connect('drag-data-get', self._on_drag_data_get, sel)\n\n        event_helper = EventHelper()\n\n        self.connect('button-press-event', self._on_press, event_helper, cur,\n            sel)\n        self.connect('button-release-event', self._on_release, event_helper,\n            cur, sel)\n        self.connect('motion-notify-event', self._on_motion, event_helper,\n            cur, sel)\n        self.connect('key-press-event', self._on_key_press, cur, sel)\n        self.connect('key-release-event', self._on_key_release, cur, sel)\n        self.connect('focus-in-event', self._on_focus_in)\n        self.connect('focus-out-event', self._on_focus_out)\n\n        self.connect(\"size-allocate\", self.on_size_allocate)\n        self.connect('style-updated', self._on_style_updated)\n\n    def on_size_allocate(self, *args):\n        allocation = self.get_allocation()\n        width = allocation.width\n\n        x = y = 0\n        for layout in self.order:\n            layout.set_width(_PS * (width - layout.indent))\n            if layout.index > 0:\n                y += (layout.vspacing or self.line_height)\n\n            e = layout.get_pixel_extents()\n            if self.get_direction() != Gtk.TextDirection.RTL:\n                layout.set_allocation(e.x + layout.indent, y + e.y,\n                                      width - layout.indent, e.height)\n            else:\n                layout.set_allocation(x + width - e.x - e.width -\n                    layout.indent - 1, y + e.y, width - layout.indent,\n                    e.height)\n\n            y += e.y + e.height\n\n    # overrides\n    def do_get_request_mode(self):\n        return Gtk.SizeRequestMode.HEIGHT_FOR_WIDTH\n\n    def do_get_preferred_height_for_width(self, width):\n        height = 0\n        layout = self._test_layout\n        for l in self.order:\n            layout.set_text(l.get_text(), -1)\n            layout.set_width(_PS * (width - l.indent))\n            lh = layout.get_pixel_extents()[1].height\n            height += lh + (l.vspacing or self.line_height)\n\n        height = max(50, height)\n        return height, height\n\n    def do_draw(self, cr):\n        self.render(self, cr)\n\n    def _config_colors(self):\n        context = self.get_style_context()\n        context.save()\n        context.add_class(Gtk.STYLE_CLASS_HIGHLIGHT)\n        state = self.get_state_flags()\n        if self.has_focus():\n            state |= Gtk.StateFlags.FOCUSED\n        context.set_state(state)\n        self._bg = color_to_hex(context.get_background_color(state))\n        self._fg = color_to_hex(context.get_color(state))\n        context.restore()\n\n    def _on_style_updated(self, widget):\n        self._config_colors()\n        self._update_cached_layouts()\n\n#    def _on_drag_begin(self, widgets, context, event_helper):\n#        print 'drag: begin'\n\n    def _on_drag_data_get(self, widget, context, selection, info, timestamp,\n        sel):\n#        print 'drag: get data'\n        text = self.get_selected_text(sel)\n        selection.set_text(text, -1)\n\n    def _on_focus_in(self, widget, event):\n        self._config_colors()\n\n    def _on_focus_out(self, widget, event):\n        self._config_colors()\n\n    def _on_motion(self, widget, event, event_helper, cur, sel):\n\n        if not (event.state == Gdk.ModifierType.BUTTON1_MASK):\n            # or not self.has_focus():\n            return\n\n        # check if we have moved enough to count as a drag\n        press = event_helper['event']\n        # mvo: how can this be?\n        if not press:\n            return\n\n        start_x, start_y = int(press.x), int(press.y)\n        cur_x, cur_y = int(event.x), int(event.y)\n\n        if (not event_helper['drag-active'] and\n            self.drag_check_threshold(start_x, start_y, cur_x, cur_y)):\n            event_helper['drag-active'] = True\n\n        if not event_helper['drag-active']:\n            return\n\n        #~ if (event_helper['within-selection'] and\n            #~ not event_helper['drag-context']):\n            #~ target_list = Gtk.TargetList()\n            #~ target_list.add_text_targets(80)\n            #~ ctx = self.drag_begin(target_list,           # target list\n                                  #~ Gdk.DragAction.COPY,   # action\n                                  #~ 1,                     # initiating button\n                                  #~ event)                 # event\n#~\n            #~ event_helper['drag-context'] = ctx\n            #~ return\n\n        for layout in self.order:\n            point_in, index = layout.index_at(cur_x, cur_y)\n            if point_in:\n                cur.set_position(layout.index, index)\n                self.queue_draw()\n                break\n\n    def _on_press(self, widget, event, event_helper, cur, sel):\n        if sel and not self.has_focus():\n            self.grab_focus()\n            return  # spot the difference\n\n        if not self.has_focus():\n            self.grab_focus()\n\n        if event.button == 3:\n            self._button3_action(cur, sel, event)\n            return\n\n        elif event.button != 1:\n            return\n\n        for layout in self.order:\n            x, y = int(event.x), int(event.y)\n            point_in, index = layout.index_at(x, y)\n\n            if point_in:\n                within_sel = False\n                #~ within_sel = sel.within_selection((layout.index, index))\n\n                if not within_sel:\n                    cur.set_position(layout.index, index)\n                    sel.clear()\n\n                #~ event_helper.new_press(event.copy(), layout, index,\n                #~     within_sel)\n                event_helper.new_press(event, layout, index, within_sel)\n                break\n\n    def _on_release(self, widget, event, event_helper, cur, sel):\n        if not event_helper['event']:\n            return\n\n        # check if a drag occurred\n        if event_helper['drag-active']:\n            # if so, do not handle release\n            return\n\n        # else, handle release, do click\n        cur.set_position(event_helper['layout'].index,\n                         event_helper['index'])\n        sel.clear()\n\n        press = event_helper['event']\n\n        if (press.type == Gdk.EventType._2BUTTON_PRESS):\n            self._2click_select(cur, sel)\n        elif (press.type == Gdk.EventType._3BUTTON_PRESS):\n            self._3click_select(cur, sel)\n\n        self.queue_draw()\n\n    def _menu_do_copy(self, item, sel):\n        self._copy_text(sel)\n\n    def _menu_do_select_all(self, item, cur, sel):\n        self._select_all(cur, sel)\n\n    def _button3_action(self, cur, sel, event):\n        start, end = sel.get_range()\n\n        self.copy_menuitem.set_sensitive(True)\n        self.select_all_menuitem.set_sensitive(True)\n\n        if not sel:\n            self.copy_menuitem.set_sensitive(False)\n        elif start == (0, 0) and \\\n            end == (len(self.order) - 1, len(self.order[-1])):\n            self.select_all_menuitem.set_sensitive(False)\n\n        self.menu.popup(None,  # parent_menu_shell,\n                        None,  # parent_menu_item,\n                        None,  # GtkMenuPositionFunc func,\n                        None,  # data,\n                        event.button,\n                        event.time)\n\n    def _on_key_press(self, widget, event, cur, sel):\n        kv = event.keyval\n        s, i = cur.paragraph, cur.index\n\n        handled_keys = True\n        ctrl = (event.state & Gdk.ModifierType.CONTROL_MASK) > 0\n        shift = (event.state & Gdk.ModifierType.SHIFT_MASK) > 0\n\n        if not self.PAINT_PRIMARY_CURSOR and \\\n            kv in (Gdk.KEY_uparrow, Gdk.KEY_downarrow) and not sel:\n            return False\n\n        if kv == Gdk.KEY_Tab:\n            handled_keys = False\n\n        elif kv == Gdk.KEY_Left:\n            if ctrl:\n                self._select_left_word(cur, sel, s, i)\n            else:\n                self._select_left(cur, sel, s, i, shift)\n\n            if shift:\n                layout = self._get_cursor_layout()\n                pos = layout.index_to_pos(cur.index)\n                sel.set_target_x(pos.x, layout.indent)\n\n        elif kv == Gdk.KEY_Right:\n            if ctrl:\n                self._select_right_word(cur, sel, s, i)\n            else:\n                self._select_right(cur, sel, s, i, shift)\n\n            if shift:\n                layout = self._get_cursor_layout()\n                pos = layout.index_to_pos(cur.index)\n                sel.set_target_x(pos.x, layout.indent)\n\n        elif kv == Gdk.KEY_Up:\n            if ctrl:\n                if i == 0:\n                    if s > 0:\n                        cur.paragraph -= 1\n                cur.set_position(cur.paragraph, 0)\n            elif sel and not shift:\n                cur.set_position(*sel.min)\n            else:\n                self._select_up(cur, sel)\n\n        elif kv == Gdk.KEY_Down:\n            if ctrl:\n                if i == len(self._get_layout(cur)):\n                    if s + 1 < len(self.order):\n                        cur.paragraph += 1\n                i = len(self._get_layout(cur))\n                cur.set_position(cur.paragraph, i)\n            elif sel and not shift:\n                cur.set_position(*sel.max)\n            else:\n                self._select_down(cur, sel)\n\n        elif kv == Gdk.KEY_Home:\n            if shift:\n                self._select_home(cur, sel, self.order[cur.paragraph])\n            else:\n                cur.set_position(0, 0)\n\n        elif kv == Gdk.KEY_End:\n            if shift:\n                self._select_end(cur, sel, self.order[cur.paragraph])\n            else:\n                cur.paragraph = len(self.order) - 1\n                cur.index = len(self._get_layout(cur))\n\n        else:\n            handled_keys = False\n\n        if not shift and handled_keys:\n            sel.clear(kv)\n\n        self.queue_draw()\n        return handled_keys\n\n    def _on_key_release(self, widget, event, cur, sel):\n        ctrl = (event.state & Gdk.ModifierType.CONTROL_MASK) > 0\n        if ctrl:\n            if event.keyval == Gdk.KEY_a:\n                self._select_all(cur, sel)\n\n            elif event.keyval == Gdk.KEY_c:\n                self._copy_text(sel)\n\n            self.queue_draw()\n\n    def _select_up(self, cur, sel):\n        #~ if sel and not cur.is_min(sel) and cur.same_line(sel):\n            #~ cur.switch(sel)\n\n        s = cur.paragraph\n        layout = self._get_layout(cur)\n\n        if sel.target_x:\n            x = sel.target_x\n            if sel.target_x_indent:\n                x += (sel.target_x_indent - layout.indent) * _PS\n            (_, j, k), (x, y) = layout.cursor_up(cur, x)\n            j += k\n\n        else:\n            (_, j, k), (x, y) = layout.cursor_up(cur)\n            j += k\n            sel.set_target_x(x, layout.indent)\n\n        if (s, j) != cur.get_position():\n            cur.set_position(s, j)\n\n        elif s > 0:\n            cur.paragraph = s - 1\n            layout = self._get_layout(cur)\n            if sel.target_x_indent:\n                x += (sel.target_x_indent - layout.indent) * _PS\n            y = layout.get_extents()[0].height\n            (_, j, k) = layout.xy_to_index(x, y)\n            cur.set_position(s - 1, j + k)\n\n        else:\n            return False\n        return True\n\n    def _select_down(self, cur, sel):\n        #~ if sel and not cur.is_max(sel) and cur.same_line(sel):\n            #~ cur.switch(sel)\n\n        s = cur.paragraph\n        layout = self._get_layout(cur)\n\n        if sel.target_x:\n            x = sel.target_x\n            if sel.target_x_indent:\n                x += (sel.target_x_indent - layout.indent) * _PS\n            (_, j, k), (x, y) = layout.cursor_down(cur, x)\n            j += k\n\n        else:\n            (_, j, k), (x, y) = layout.cursor_down(cur)\n            j += k\n            sel.set_target_x(x, layout.indent)\n\n        if (s, j) != cur.get_position():\n            cur.set_position(s, j)\n\n        elif s < len(self.order) - 1:\n            cur.paragraph = s + 1\n            layout = self._get_layout(cur)\n            if sel.target_x_indent:\n                x += (sel.target_x_indent - layout.indent) * _PS\n            y = 0\n            (_, j, k) = layout.xy_to_index(x, y)\n            cur.set_position(s + 1, j + k)\n\n        else:\n            return False\n        return True\n\n    def _2click_select(self, cursor, sel):\n        self._select_word(cursor, sel)\n\n    def _3click_select(self, cursor, sel):\n        # XXX:\n        # _select_line seems to expose the following Pango issue:\n        # (description.py:3892): Pango-CRITICAL **:\n        # pango_layout_line_unref: assertion `private->ref_count > 0'\n        # failed\n        # ... which can result in a segfault\n        #~ self._select_line(cursor, sel)\n        self._select_all(cursor, sel)\n\n    def _copy_text(self, sel):\n\n        text = self.get_selected_text(sel)\n\n        if not self.clipboard:\n            display = Gdk.Display.get_default()\n            selection = Gdk.Atom.intern(\"CLIPBOARD\", False)\n            self.clipboard = Gtk.Clipboard.get_for_display(display, selection)\n\n        self.clipboard.clear()\n        self.clipboard.set_text(text.strip(), -1)\n\n    def _select_end(self, cur, sel, layout):\n        if not cur.is_max(sel):\n            cur.switch(sel)\n\n        n, r, line = cur.get_current_line()\n        cur_pos = cur.get_position()\n\n        if cur_pos == (len(self.order) - 1, len(self.order[-1])):  # abs end\n            if sel.restore_point:\n                # reinstate restore point\n                cur.set_position(*sel.restore_point)\n            else:\n                # reselect the line end\n                n, r, line = sel.get_current_line()\n                cur.set_position(n[0], r[1])\n\n        elif cur_pos[1] == len(self.order[n[0]]):   # para end\n            # select abs end\n            cur.set_position(len(self.order) - 1, len(self.order[-1]))\n\n        elif cur_pos == (n[0], r[1]):   # line end\n            # select para end\n            cur.set_position(n[0], len(self.order[n[0]]))\n\n        else:   # not at any end, within line somewhere\n            # select line end\n            if sel:\n                sel.restore_point = cur_pos\n            cur.set_position(n[0], r[1])\n\n    def _select_home(self, cur, sel, layout):\n        if not cur.is_min(sel):\n            cur.switch(sel)\n\n        n, r, line = cur.get_current_line()\n        cur_pos = cur.get_position()\n\n        if cur_pos == (0, 0):   # absolute home\n            if sel.restore_point:\n                cur.set_position(*sel.restore_point)\n            else:\n                n, r, line = sel.get_current_line()\n                cur.set_position(n[0], r[0])\n\n        elif cur_pos[1] == 0:   # para home\n            cur.set_position(0, 0)\n\n        elif cur_pos == (n[0], r[0]):      # line home\n            cur.set_position(n[0], 0)\n\n        else:                   # not at any home, within line somewhere\n            if sel:\n                sel.restore_point = cur_pos\n            cur.set_position(n[0], r[0])\n\n    def _select_left(self, cur, sel, s, i, shift):\n        if not shift and not cur.is_min(sel):\n            cur.switch(sel)\n            return\n        if i > 0:\n            cur.set_position(s, i - 1)\n        elif cur.paragraph > 0:\n            cur.paragraph -= 1\n            cur.set_position(s - 1, len(self._get_layout(cur)))\n\n    def _select_right(self, cur, sel, s, i, shift):\n        if not shift and not cur.is_max(sel):\n            cur.switch(sel)\n            return\n        if i < len(self._get_layout(cur)):\n            cur.set_position(s, i + 1)\n        elif s < len(self.order) - 1:\n            cur.set_position(s + 1, 0)\n\n    def _select_left_word(self, cur, sel, s, i):\n        if i > 0:\n            cur.index -= 1\n        elif s > 0:\n            cur.paragraph -= 1\n            cur.index = len(self._get_layout(cur))\n\n        paragraph, word = cur.get_current_word()\n        if not word:\n            return\n        cur.set_position(paragraph, max(0, word[0] - 1))\n\n    def _select_right_word(self, cur, sel, s, i):\n        ll = len(self._get_layout(cur))\n        if i < ll:\n            cur.index += 1\n        elif s + 1 < len(self.order):\n            cur.paragraph += 1\n            cur.index = 0\n\n        paragraph, word = cur.get_current_word()\n        if not word:\n            return\n        cur.set_position(paragraph, min(word[1] + 1, ll))\n\n    def _select_word(self, cursor, sel):\n        paragraph, word = cursor.get_current_word()\n        if word:\n            cursor.set_position(paragraph, word[1] + 1)\n            sel.set_position(paragraph, word[0])\n            if self.get_direction() == Gtk.TextDirection.RTL:\n                cursor.switch(sel)\n\n    def _select_line(self, cursor, sel):\n        n, r = self.cursor.get_current_line()\n        sel.set_position(n[0], r[0])\n        cursor.set_position(n[0], r[1])\n        if self.get_direction() == Gtk.TextDirection.RTL:\n            cursor.switch(sel)\n\n    def _select_all(self, cursor, sel):\n        layout = self.order[-1]\n        sel.set_position(0, 0)\n        cursor.set_position(layout.index, len(layout))\n        if self.get_direction() == Gtk.TextDirection.RTL:\n            cursor.switch(sel)\n\n    def _selection_copy(self, layout, sel, new_para=True):\n        i = layout.index\n        start, end = sel.get_range()\n\n        if new_para:\n            text = '\\n\\n'\n        else:\n            text = ''\n\n        if sel and i >= start[0] and i <= end[0]:\n\n            if i == start[0]:\n                if end[0] > i:\n                    return text + layout.get_text()[start[1]: len(layout)]\n                else:\n                    return text + layout.get_text()[start[1]: end[1]]\n\n            elif i == end[0]:\n                if start[0] < i:\n                    return text + layout.get_text()[0: end[1]]\n                else:\n                    return text + layout.get_text()[start[1]: end[1]]\n\n            else:\n                return text + layout.get_text()\n        return ''\n\n    def _new_layout(self, text=''):\n        layout = Layout(self, text)\n        layout.set_wrap(Pango.WrapMode.WORD_CHAR)\n        return layout\n\n    def _update_cached_layouts(self):\n        self._bullet = self._new_layout()\n        self._bullet.set_markup(self.BULLET_POINT)\n        font_desc = Pango.FontDescription()\n\n        font_desc.set_weight(Pango.Weight.BOLD)\n        self._bullet.set_font_description(font_desc)\n\n        e = self._bullet.get_pixel_extents()\n        self.indent, self.line_height = e.width, e.height\n\n    def _selection_highlight(self, layout, sel, bg, fg):\n        i = layout.index\n        start, end = sel.get_range()\n        if sel and i >= start[0] and i <= end[0]:\n            if i == start[0]:\n                if end[0] > i:\n                    layout.highlight(start[1], len(layout), bg, fg)\n                else:\n                    layout.highlight(start[1], end[1], bg, fg)\n\n            elif i == end[0]:\n                if start[0] < i:\n                    layout.highlight(0, end[1], bg, fg)\n                else:\n                    layout.highlight(start[1], end[1], bg, fg)\n\n            else:\n                layout.highlight_all(bg, fg)\n\n        elif not layout._default_attrs:\n            layout.reset_attrs()\n\n    def _paint_bullet_point(self, cr, x, y):\n        # draw the layout\n        Gtk.render_layout(self.get_style_context(),\n                            cr,             # state\n                            x,           # x coord\n                            y,           # y coord\n                            self._bullet._layout)   # a Pango.Layout()\n\n    def _get_layout(self, cursor):\n        return self.order[cursor.paragraph]\n\n    def _get_cursor_layout(self):\n        return self.order[self.cursor.paragraph]\n\n    def _get_selection_layout(self):\n        return self.order[self.selection.paragraph]\n\n    def render(self, widget, cr):\n        if not self.order:\n            return\n\n        a = self.get_allocation()\n        for layout in self.order:\n            lx, ly = layout.get_position()\n\n            self._selection_highlight(layout,\n                                      self.selection,\n                                      self._bg, self._fg)\n\n            if layout.is_bullet:\n                if self.get_direction() != Gtk.TextDirection.RTL:\n                    indent = layout.indent - self.indent\n                else:\n                    indent = a.width - layout.indent\n                self._paint_bullet_point(cr, indent, ly)\n\n            if self.DEBUG_PAINT_BBOXES:\n                la = layout.allocation\n                cr.rectangle(la.x, la.y, la.width, la.height)\n                cr.set_source_rgb(1, 0, 0)\n                cr.stroke()\n\n            # draw the layout\n            Gtk.render_layout(self.get_style_context(),\n                                cr,\n                                lx,             # x coord\n                                ly,             # y coord\n                                layout._layout)           # a Pango.Layout()\n\n        # draw the cursor\n        if self.PAINT_PRIMARY_CURSOR and self.has_focus():\n            self.cursor.draw(cr, self._get_layout(self.cursor), a)\n\n    def append_paragraph(self, p, vspacing=None):\n        l = self._new_layout()\n        l.index = len(self.order)\n        l.vspacing = vspacing\n        l.set_text(p)\n        self.order.append(l)\n\n    def append_bullet(self, point, indent_level, vspacing=None):\n        l = self._new_layout()\n        l.index = len(self.order)\n        l.indent = self.indent * (indent_level + 1)\n        l.vspacing = vspacing\n        l.is_bullet = True\n\n        l.set_text(point)\n        self.order.append(l)\n\n    def copy_clipboard(self):\n        self._copy_text(self.selection)\n\n    def get_selected_text(self, sel=None):\n        text = ''\n        if not sel:\n            sel = self.selection\n        for layout in self.order:\n            text += self._selection_copy(layout, sel, (layout.index > 0))\n        return text\n\n    def select_all(self):\n        self._select_all(self.cursor, self.selection)\n        self.queue_draw()\n\n    def finished(self):\n        self.queue_resize()\n\n    def clear(self, key=None):\n        self.cursor.zero()\n        self.selection.clear(key)\n        self.order = []\n\n\nclass AppDescription(Gtk.VBox):\n\n    TYPE_PARAGRAPH = 0\n    TYPE_BULLET = 1\n\n    _preparser = _SpecialCasePreParsers()\n\n    def __init__(self):\n        Gtk.VBox.__init__(self)\n        self.description = TextBlock()\n        self.pack_start(self.description, False, False, 0)\n        self._prev_type = None\n\n    def _part_is_bullet(self, part):\n        # normalize_description() ensures that we only have \"* \" bullets\n        i = part.find(\"* \")\n        return i > -1, i\n\n    def _parse_desc(self, desc, pkgname):\n        \"\"\" Attempt to maintain original fixed width layout, while\n            reconstructing the description into text blocks\n            (either paragraphs or bullets) which are line-wrap friendly.\n        \"\"\"\n        # pre-parse descrition if special case exists for the given pkgname\n        desc = self._preparser.preparse(pkgname, desc)\n\n        parts = normalize_package_description(desc).split('\\n')\n        for part in parts:\n            if not part:\n                continue\n            is_bullet, indent = self._part_is_bullet(part)\n            if is_bullet:\n                self.append_bullet(part, indent)\n            else:\n                self.append_paragraph(part)\n\n        self.description.finished()\n\n    def clear(self):\n        self.description.clear()\n\n    def append_paragraph(self, p):\n        vspacing = self.description.line_height\n        self.description.append_paragraph(p.strip(), vspacing)\n        self._prev_type = self.TYPE_PARAGRAPH\n\n    def append_bullet(self, point, indent_level):\n        if self._prev_type == self.TYPE_BULLET:\n            vspacing = int(0.4 * self.description.line_height)\n        else:\n            vspacing = self.description.line_height\n\n        self.description.append_bullet(\n                        point[indent_level + 2:], indent_level, vspacing)\n        self._prev_type = self.TYPE_BULLET\n\n    def set_description(self, raw_desc, pkgname):\n        self.clear()\n        if type(raw_desc) == str:\n            encoded_desc = unicode(raw_desc, 'utf8').encode('utf8')\n        else:\n            encoded_desc = raw_desc.encode('utf8')\n        self._text = GObject.markup_escape_text(encoded_desc)\n        self._parse_desc(self._text, pkgname)\n        self.show_all()\n\n    # easy access to some TextBlock methods\n    def copy_clipboard(self):\n        return TextBlock.copy_clipboard(self.description)\n\n    def get_selected_text(self):\n        return TextBlock.get_selected_text(self.description)\n\n    def select_all(self):\n        return TextBlock.select_all(self.description)\n\n\ndef get_test_description_window():\n    EXAMPLE0 = \"\"\"p7zip is the Unix port of 7-Zip, a file archiver that \\\narchives with very high compression ratios.\n\np7zip-full provides:\n\n - \/usr\/bin\/7za a standalone version of the 7-zip tool that handles\n   7z archives (implementation of the LZMA compression algorithm) and some \\\nother formats.\n\n - \/usr\/bin\/7z not only does it handle 7z but also ZIP, Zip64, CAB, RAR, \\\nARJ, GZIP,\n   BZIP2, TAR, CPIO, RPM, ISO and DEB archives. 7z compression is 30-50% \\\nbetter than ZIP compression.\n\np7zip provides 7zr, a light version of 7za, and p7zip a gzip like wrapper \\\naround 7zr.\"\"\"\n\n    EXAMPLE1 = \"\"\"Transmageddon supports almost any format as its input and \\\ncan generate a very large host of output files. The goal of the application \\\nwas to help people to create the files they need to be able to play on their \\\nmobile devices and for people not hugely experienced with multimedia to \\\ngenerate a multimedia file without having to resort to command line tools \\\nwith ungainly syntaxes.\nThe currently supported codecs are:\n * Containers:\n  - Ogg\n  - Matroska\n  - AVI\n  - MPEG TS\n  - flv\n  - QuickTime\n  - MPEG4\n  - 3GPP\n  - MXT\n * Audio encoders:\n  - Vorbis\n  - FLAC\n  - MP3\n  - AAC\n  - AC3\n  - Speex\n  - Celt\n * Video encoders:\n  - Theora\n  - Dirac\n  - H264\n  - MPEG2\n  - MPEG4\/DivX5\n  - xvid\n  - DNxHD\nIt also provide the support for the GStreamer's plugins auto-search.\"\"\"\n\n    EXAMPLE2 = \"\"\"File-roller is an archive manager for the GNOME \\\nenvironment. It allows you to:\n * Create and modify archives.\n * View the content of an archive.\n * View a file contained in an archive.\n * Extract files from the archive.\nFile-roller supports the following formats:\n * Tar (.tar) archives, including those compressed with\n   gzip (.tar.gz, .tgz), bzip (.tar.bz, .tbz), bzip2 (.tar.bz2, .tbz2),\n   compress (.tar.Z, .taz), lzip (.tar.lz, .tlz), lzop (.tar.lzo, .tzo),\n   lzma (.tar.lzma) and xz (.tar.xz)\n * Zip archives (.zip)\n * Jar archives (.jar, .ear, .war)\n * 7z archives (.7z)\n * iso9660 CD images (.iso)\n * Lha archives (.lzh)\n * Single files compressed with gzip (.gz), bzip (.bz), bzip2 (.bz2),\n   compress (.Z), lzip (.lz), lzop (.lzo), lzma (.lzma) and xz (.xz)\nFile-roller doesn't perform archive operations by itself, but relies on \\\nstandard tools for this.\"\"\"\n\n    EXAMPLE3 = \"\"\"This package includes the following CTAN packages:\n Asana-Math -- A font to typeset maths in Xe(La)TeX.\n albertus --\n allrunes -- Fonts and LaTeX package for almost all runes.\n antiqua -- the URW Antiqua Condensed Font.\n antp -- Antykwa Poltawskiego: a Type 1 family of Polish traditional type.\n antt -- Antykwa Torunska: a Type 1 family of a Polish traditional type.\n apl -- Fonts for typesetting APL programs.\n ar -- Capital A and capital R ligature for Apsect Ratio.\n archaic -- A collection of archaic fonts.\n arev -- Fonts and LaTeX support files for Arev Sans.\n ascii -- Support for IBM \"standard ASCII\" font.\n astro -- Astronomical (planetary) symbols.\n atqolive --\n augie -- Calligraphic font for typesetting handwriting.\n auncial-new -- Artificial Uncial font and LaTeX support macros.\n aurical -- Calligraphic fonts for use with LaTeX in T1 encoding.\n barcodes -- Fonts for making barcodes.\n bayer -- Herbert Bayers Universal Font For Metafont.\n bbding -- A symbol (dingbat) font and LaTeX macros for its use.\n bbm -- \"Blackboard-style\" cm fonts.\n bbm-macros -- LaTeX support for \"blackboard-style\" cm fonts.\n bbold -- Sans serif blackboard bold.\n belleek -- Free replacement for basic MathTime fonts.\n bera -- Bera fonts.\n blacklettert1 -- T1-encoded versions of Haralambous old German fonts.\n boisik -- A font inspired by Baskerville design.\n bookhands -- A collection of book-hand fonts.\n braille -- Support for braille.\n brushscr -- A handwriting script font.\n calligra -- Calligraphic font.\n carolmin-ps -- Adobe Type 1 format of Carolingian Minuscule fonts.\n cherokee -- A font for the Cherokee script.\n clarendo --\n cm-lgc -- Type 1 CM-based fonts for Latin, Greek and Cyrillic.\n cmbright -- Computer Modern Bright fonts.\n cmll -- Symbols for linear logic.\n cmpica -- A Computer Modern Pica variant.\n coronet --\n courier-scaled -- Provides a scaled Courier font.\n cryst -- Font for graphical symbols used in crystallography.\n cyklop -- The Cyclop typeface.\n dancers -- Font for Conan Doyle's \"The Dancing Men\".\n dice -- A font for die faces.\n dictsym -- DictSym font and macro package\n dingbat -- Two dingbat symbol fonts.\n doublestroke -- Typeset mathematical double stroke symbols.\n dozenal -- Typeset documents using base twelve numbering (also called\n  \"dozenal\")\n duerer -- Computer Duerer fonts.\n duerer-latex -- LaTeX support for the Duerer fonts.\n ean -- Macros for making EAN barcodes.\n ecc -- Sources for the European Concrete fonts.\n eco -- Oldstyle numerals using EC fonts.\n eiad -- Traditional style Irish fonts.\n eiad-ltx -- LaTeX support for the eiad font.\n elvish -- Fonts for typesetting Tolkien Elvish scripts.\n epigrafica -- A Greek and Latin font.\n epsdice -- A scalable dice \"font\".\n esvect -- Vector arrows.\n eulervm -- Euler virtual math fonts.\n euxm --\n feyn -- A font for in-text Feynman diagrams.\n fge -- A font for Frege's Grundgesetze der Arithmetik.\n foekfont -- The title font of the Mads Fok magazine.\n fonetika -- Support for the danish \"Dania\" phonetic system.\n fourier -- Using Utopia fonts in LaTeX documents.\n fouriernc -- Use New Century Schoolbook text with Fourier maths fonts.\n frcursive -- French cursive hand fonts.\n garamond --\n genealogy -- A compilation genealogy font.\n gfsartemisia -- A modern Greek font design.\n gfsbodoni -- A Greek and Latin font based on Bodoni.\n gfscomplutum -- A Greek font with a long history.\n gfsdidot -- A Greek font based on Didot's work.\n gfsneohellenic -- A Greek font in the Neo-Hellenic style.\n gfssolomos -- A Greek-alphabet font.\n gothic -- A collection of old German-style fonts.\n greenpoint -- The Green Point logo.\n groff --\n grotesq -- the URW Grotesk Bold Font.\n hands -- Pointing hand font.\n hfbright -- The hfbright fonts.\n hfoldsty -- Old style numerals with EC fonts.\n ifsym -- A collection of symbols.\n inconsolata -- A monospaced font, with support files for use with TeX.\n initials -- Adobe Type 1 decorative initial fonts.\n iwona -- A two-element sans-serif font.\n junicode -- A TrueType font for mediaevalists.\n kixfont -- A font for KIX codes.\n knuthotherfonts --\n kpfonts -- A complete set of fonts for text and mathematics.\n kurier -- A two-element sans-serif typeface.\n lettrgth --\n lfb -- A Greek font with normal and bold variants.\n libertine -- Use the font Libertine with LaTeX.\n libris -- Libris ADF fonts, with LaTeX support.\n linearA -- Linear A script fonts.\n logic -- A font for electronic logic design.\n lxfonts -- Set of slide fonts based on CM.\n ly1 -- Support for LY1 LaTeX encoding.\n marigold --\n mathabx -- Three series of mathematical symbols.\n mathdesign -- Mathematical fonts to fit with particular text fonts.\n mnsymbol -- Mathematical symbol font for Adobe MinionPro.\n nkarta -- A \"new\" version of the karta cartographic fonts.\n ocherokee -- LaTeX Support for the Cherokee language.\n ogham -- Fonts for typesetting Ogham script.\n oinuit -- LaTeX Support for the Inuktitut Language.\n optima --\n orkhun -- A font for orkhun script.\n osmanian -- Osmanian font for writing Somali.\n pacioli -- Fonts designed by Fra Luca de Pacioli in 1497.\n pclnfss -- Font support for current PCL printers.\n phaistos -- Disk of Phaistos font.\n phonetic -- MetaFont Phonetic fonts, based on Computer Modern.\n pigpen -- A font for the pigpen (or masonic) cipher.\n psafm --\n punk -- Donald Knuth's punk font.\n recycle -- A font providing the \"recyclable\" logo.\n sauter -- Wide range of design sizes for CM fonts.\n sauterfonts -- Use sauter fonts in LaTeX.\n semaphor -- Semaphore alphabet font.\n simpsons -- MetaFont source for Simpsons characters.\n skull -- A font to draw a skull.\n staves -- Typeset Icelandic staves and runic letters.\n tapir -- A simple geometrical font.\n tengwarscript -- LaTeX support for using Tengwar fonts.\n trajan -- Fonts from the Trajan column in Rome.\n umtypewriter -- Fonts to typeset with the xgreek package.\n univers --\n universa -- Herbert Bayer's 'universal' font.\n venturisadf -- Venturis ADF fonts collection.\n wsuipa -- International Phonetic Alphabet fonts.\n yfonts -- Support for old German fonts.\n zefonts -- Virtual fonts to provide T1 encoding from existing fonts.\"\"\"\n\n    EXAMPLE4 = \"\"\"Arista is a simple multimedia transcoder, it focuses on \\\nbeing easy to use by making complex task of encoding for various devices \\\nsimple.\nUsers should pick an input and a target device, choose a file to save to and \\\ngo. Features:\n* Presets for iPod, computer, DVD player, PSP, Playstation 3, and more.\n* Live preview to see encoded quality.\n* Automatically discover available DVD media and Video 4 Linux (v4l) devices.\n* Rip straight from DVD media easily (requires libdvdcss).\n* Rip straight from v4l devices.\n* Simple terminal client for scripting.\n* Automatic preset updating.\"\"\"\n\n    def on_clicked(widget, desc_widget, descs):\n        widget.position += 1\n        if widget.position >= len(descs):\n            widget.position = 0\n        desc_widget.set_description(*descs[widget.position])\n\n    descs = ((EXAMPLE0, ''),\n             (EXAMPLE1, ''),\n             (EXAMPLE2, ''),\n             (EXAMPLE3, 'texlive-fonts-extra'),\n             (EXAMPLE4, ''))\n\n    win = Gtk.Window()\n    win.set_default_size(300, 400)\n    win.set_has_resize_grip(True)\n    vb = Gtk.VBox()\n    win.add(vb)\n    b = Gtk.Button('Next test description >>')\n    b.position = 0\n    vb.pack_start(b, False, False, 0)\n    scroll = Gtk.ScrolledWindow()\n    vb.add(scroll)\n    d = AppDescription()\n    #~ d.description.DEBUG_PAINT_BBOXES = True\n    d.set_description(EXAMPLE0, pkgname='')\n    scroll.add_with_viewport(d)\n    win.show_all()\n\n    b.connect(\"clicked\", on_clicked, d, descs)\n    win.connect('destroy', lambda x: Gtk.main_quit())\n    return win\n\nif __name__ == '__main__':\n    win = get_test_description_window()\n    win.show_all()\n    Gtk.main()\n","label":0}
{"content":"'''CTS: Cluster Testing System: AIS dependent modules...\n'''\n\n__copyright__ = '''\nCopyright (C) 2007 Andrew Beekhof <andrew@suse.de>\n\n'''\n\n#\n# This program is free software; you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA.\n\nfrom cts.CTSvars import *\nfrom cts.CM_lha  import crm_lha\nfrom cts.CTS     import Process\nfrom cts.patterns    import PatternSelector\n\n#######################################################################\n#\n#  LinuxHA v2 dependent modules\n#\n#######################################################################\n\n\nclass crm_ais(crm_lha):\n    '''\n    The crm version 3 cluster manager class.\n    It implements the things we need to talk to and manipulate\n    crm clusters running on top of openais\n    '''\n    def __init__(self, Environment, randseed=None, name=None):\n        if not name: name=\"crm-ais\"\n        crm_lha.__init__(self, Environment, randseed=randseed, name=name)\n\n        self.fullcomplist = {}\n        self.templates = PatternSelector(self.name)\n\n    def NodeUUID(self, node):\n        return node\n\n    def ais_components(self, extra={}):\n\n        complist = []\n        if not len(self.fullcomplist.keys()):\n            for c in [\"cib\", \"lrmd\", \"crmd\", \"attrd\" ]:\n                self.fullcomplist[c] = Process(\n                    self, c, \n                    pats = self.templates.get_component(self.name, c),\n                    badnews_ignore = self.templates.get_component(self.name, \"%s-ignore\" % c),\n                    common_ignore = self.templates.get_component(self.name, \"common-ignore\"))\n\n            # pengine uses dc_pats instead of pats\n            self.fullcomplist[\"pengine\"] = Process(\n                self, \"pengine\", \n                dc_pats = self.templates.get_component(self.name, \"pengine\"),\n                badnews_ignore = self.templates.get_component(self.name, \"pengine-ignore\"),\n                common_ignore = self.templates.get_component(self.name, \"common-ignore\"))\n\n            # stonith-ng's process name is different from its component name\n            self.fullcomplist[\"stonith-ng\"] = Process(\n                self, \"stonith-ng\", process=\"stonithd\", \n                pats = self.templates.get_component(self.name, \"stonith\"),\n                badnews_ignore = self.templates.get_component(self.name, \"stonith-ignore\"),\n                common_ignore = self.templates.get_component(self.name, \"common-ignore\"))\n\n            # add (or replace) any extra components passed in\n            self.fullcomplist.update(extra)\n\n        # Processes running under valgrind can't be shot with \"killall -9 processname\",\n        # so don't include them in the returned list\n        vgrind = self.Env[\"valgrind-procs\"].split()\n        for key in list(self.fullcomplist.keys()):\n            if self.Env[\"valgrind-tests\"]:\n                if key in vgrind:\n                    self.log(\"Filtering %s from the component list as it is being profiled by valgrind\" % key)\n                    continue\n            if key == \"stonith-ng\" and not self.Env[\"DoFencing\"]:\n                continue\n            complist.append(self.fullcomplist[key])\n\n        return complist\n\n\nclass crm_cs_v0(crm_ais):\n    '''\n    The crm version 3 cluster manager class.\n    It implements the things we need to talk to and manipulate\n\n    crm clusters running against version 0 of our plugin\n    '''\n    def __init__(self, Environment, randseed=None, name=None):\n        if not name: name=\"crm-plugin-v0\"\n        crm_ais.__init__(self, Environment, randseed=randseed, name=name)\n\n    def Components(self):\n        extra = {}\n        extra[\"corosync\"] = Process(\n            self, \"corosync\", \n            pats = self.templates.get_component(self.name, \"corosync\"),\n            badnews_ignore = self.templates.get_component(self.name, \"corosync-ignore\"),\n            common_ignore = self.templates.get_component(self.name, \"common-ignore\")\n        )\n        return self.ais_components(extra=extra)\n\n\nclass crm_cs_v1(crm_cs_v0):\n    '''\n    The crm version 3 cluster manager class.\n    It implements the things we need to talk to and manipulate\n\n    crm clusters running on top of version 1 of our plugin\n    '''\n    def __init__(self, Environment, randseed=None, name=None):\n        if not name: name=\"crm-plugin-v1\"\n        crm_cs_v0.__init__(self, Environment, randseed=randseed, name=name)\n\n\nclass crm_mcp(crm_cs_v0):\n    '''\n    The crm version 4 cluster manager class.\n    It implements the things we need to talk to and manipulate\n    crm clusters running on top of native corosync (no plugins)\n    '''\n    def __init__(self, Environment, randseed=None, name=None):\n        if not name: name=\"crm-mcp\"\n        crm_cs_v0.__init__(self, Environment, randseed=randseed, name=name)\n\n        if self.Env[\"have_systemd\"]:\n            self.update({\n                # When systemd is in use, we can look for this instead\n                \"Pat:We_stopped\"   : \"%s.*Corosync Cluster Engine exiting normally\",\n            })\n\n\nclass crm_cman(crm_cs_v0):\n    '''\n    The crm version 3 cluster manager class.\n    It implements the things we need to talk to and manipulate\n    crm clusters running on top of openais\n    '''\n    def __init__(self, Environment, randseed=None, name=None):\n        if not name: name=\"crm-cman\"\n        crm_cs_v0.__init__(self, Environment, randseed=randseed, name=name)\n","label":0}
{"content":"# from yowsup.layers.protocol_media import mediacipher\nimport tempfile\nimport os\nclass DownloadableMediaMessageBuilder(object):\n    def __init__(self, downloadbleMediaMessageClass, jid, filepath):\n        self.jid = jid\n        self.filepath = filepath\n        self.encryptedFilepath = None\n        self.cls = downloadbleMediaMessageClass\n        self.mediaKey = None\n        self.attributes = {}\n        self.mediaType = self.cls.__name__.split(\"DownloadableMediaMessageProtocolEntity\")[0].lower() #ugly ?\n\n    # def encrypt(self):\n    #     fd, encpath = tempfile.mkstemp()\n    #     mediaKey = os.urandom(112)\n    #     keys = mediacipher.getDerivedKeys(mediaKey)\n    #     out = mediacipher.encryptImage(self.filepath, keys)\n    #     with open(encImagePath, 'w') as outF:\n    #         outF.write(out)\n    #\n    #     self.mediaKey = mediaKey\n    #     self.encryptedFilepath = encpath\n\n    # def decrypt(self):\n    #     self.mediaKey = None\n    #     self.encryptedFilePath = None\n\n\n    def setEncryptionData(self, mediaKey, encryptedFilepath):\n        self.mediaKey = mediaKey\n        self.encryptedFilepath = encryptedFilepath\n\n    def isEncrypted(self):\n        return self.encryptedFilepath is not None\n\n    def getFilepath(self):\n        return self.encryptedFilepath or self.filepath\n\n    def getOriginalFilepath(self):\n        return self.filepath\n\n    def set(self, key, val):\n        self.attributes[key] = val\n\n    def get(self, key, default = None):\n        if key in self.attributes and self.attributes[key] is not None:\n            return self.attributes[key]\n\n        return default\n\n    def getOrSet(self, key, func):\n        if not self.get(key):\n            self.set(key, func())\n\n    def build(self, url = None, ip = None):\n        if url:\n            self.set(\"url\", url)\n        if ip:\n            self.set(\"ip\", ip)\n        return self.cls.fromBuilder(self)\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n##\n## This file is part of Invenio.\n## Copyright (C) 2012 CERN.\n##\n## Invenio is free software; you can redistribute it and\/or\n## modify it under the terms of the GNU General Public License as\n## published by the Free Software Foundation; either version 2 of the\n## License, or (at your option) any later version.\n##\n## Invenio is distributed in the hope that it will be useful, but\n## WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n## General Public License for more details.\n##\n## You should have received a copy of the GNU General Public License\n## along with Invenio; if not, write to the Free Software Foundation, Inc.,\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\nfrom invenio.dbquery import run_sql\n\ndepends_on = ['invenio_release_1_1_0']\n\ndef info():\n    return \"New selfcite tables\"\n\ndef do_upgrade():\n    run_sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS `rnkRECORDSCACHE` (\n      `id_bibrec` int(10) unsigned NOT NULL,\n      `authorid` bigint(10) NOT NULL,\n      PRIMARY KEY (`id_bibrec`,`authorid`)\n    ) ENGINE=MyISAM\"\"\")\n\n    run_sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS `rnkEXTENDEDAUTHORS` (\n      `id` int(10) unsigned NOT NULL,\n      `authorid` bigint(10) NOT NULL,\n      PRIMARY KEY (`id`,`authorid`)\n    ) ENGINE=MyISAM\"\"\")\n\n    run_sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS `rnkSELFCITES` (\n      `id_bibrec` int(10) unsigned NOT NULL,\n      `count` int(10) unsigned NOT NULL,\n      `references` text NOT NULL,\n      `last_updated` datetime NOT NULL,\n      PRIMARY KEY (`id_bibrec`)\n    ) ENGINE=MyISAM\"\"\")\n\ndef estimate():\n    return 1\n","label":0}
{"content":"#!\/usr\/bin\/python -u\r\n# -*- coding: utf-8 -*-\r\n\r\nfrom django.db import models\r\nfrom datetime import datetime\r\nfrom django.contrib.auth.models import User\r\nfrom mapas.models import *\r\nfrom actores.models import *\r\n\r\nclass PerfilPublico(models.Model):\r\n    user = models.OneToOneField(User,verbose_name='Usuario')\r\n    persona = models.OneToOneField(Directorios)\r\n    class Meta:\r\n        db_table = u'perfilpublico'\r\n        verbose_name_plural='Perfil P\u00fablico'\r\n        verbose_name='Perfil P\u00fablico'\r\n        unique_together=('user','persona')\r\n        #app_label = 'Sistematizacion_de_modulos_publicos'\r\n    def __unicode__(self):\r\n        return u\"%s\" %(self.persona.nombre)\r\n\t\t\r\nclass SeccionesPanelPublico(models.Model):\r\n    panel = models.CharField(max_length=180,verbose_name='Modulo')\r\n    descripcion = models.TextField()\r\n#    modulos = models.ManyToManyField(ModulosPublicos,related_name='Modulos Principales',verbose_name='Modulos',blank=True)\r\n    activo = models.BooleanField(verbose_name=\"Activo\")\r\n    is_admmin = models.BooleanField(verbose_name=\"Solo para Administradores\")\r\n    posicion = models.IntegerField(verbose_name=\"Posicion\")\r\n    class Meta:\r\n        verbose_name_plural='Secciones del Panel Publico'\r\n        verbose_name='Secciones del Panel Publico'\r\n    def __unicode__(self):\r\n        return u\"%s\" %(self.panel)\r\n\r\nclass ModulosPublicos(models.Model):\r\n    paneles = models.ForeignKey(SeccionesPanelPublico)\r\n    modulo = models.CharField(max_length=180,verbose_name='Modulo')\r\n    url = models.CharField(max_length=180,verbose_name='URL',blank=True,null=True)\r\n    boton = models.ImageField(upload_to='modulos')\r\n#    submodulos = models.ManyToManyField(SubModulosPublicos,related_name='Submodulos',verbose_name='Sub Modulos',blank=True)\r\n    descripcion = models.TextField()\r\n    is_admmin = models.BooleanField(verbose_name=\"Solo para Administradores\")\r\n    activo = models.BooleanField(verbose_name=\"Activo\")\r\n    posicion = models.IntegerField(verbose_name=\"Posicion\")\r\n    target = models.CharField(max_length=40,choices=(('_blank',u'Abre el documento vinculado en una nueva ventana o pesta\u00f1a'),('_self',u'Abre el documento vinculado en el mismo marco que se ha hecho clic'),('_parent',u'Abre el documento vinculado en el marco padre'),('_top',u'Abre el documento vinculado en el pleno de la ventana')),verbose_name='Target del Vinculo')\r\n    class Meta:\r\n        verbose_name_plural='M\u00f3dulos P\u00fablicos'\r\n        verbose_name='M\u00f3dulos P\u00fablicos'\r\n        #app_label = 'Sistematizacion_de_modulos_publicos'\r\n    def __unicode__(self):\r\n        return u\"%s - %s\" %(self.paneles.panel, self.modulo)\r\n\t\t\r\n    def logo(self):\r\n        logo = \"\"\r\n        if self.boton:\r\n           esta = \"<img src='\" + self.boton.url +\"' alt='Activo' height='150px'>\"\r\n        else:\r\n           esta = \"<img src='\/media\/imgs\/icon-pendiente.gif' alt='Pendiente'> sin imagen\"\r\n        return u\"%s\"%(esta)\r\n    logo.allow_tags = True\r\n\r\nclass SubModulosPublicos(models.Model):\r\n    modulos = models.ForeignKey(ModulosPublicos)\r\n    titulo = models.CharField(max_length=180,verbose_name='Modulo')\r\n    url = models.CharField(max_length=180,verbose_name='URL',blank=True,null=True)\r\n    boton = models.ImageField(upload_to='modulos') \t  \r\n    descripcion = models.TextField()\r\n    is_admmin = models.BooleanField(verbose_name=\"Solo para Administradores\")\r\n    activo = models.BooleanField(verbose_name=\"Activo\")\r\n    posicion = models.IntegerField(verbose_name=\"Posicion\")\r\n    target = models.CharField(max_length=40,choices=(('_blank',u'Abre el documento vinculado en una nueva ventana o pesta\u00f1a'),('_self',u'Abre el documento vinculado en el mismo marco que se ha hecho clic'),('_parent',u'Abre el documento vinculado en el marco padre'),('_top',u'Abre el documento vinculado en el pleno de la ventana')),verbose_name='Target del Vinculo')\r\n    class Meta:\r\n        verbose_name_plural='Sub M\u00f3dulos P\u00fablicos'\r\n        verbose_name='Sub M\u00f3dulos P\u00fablicos'\r\n    def __unicode__(self):\r\n        return u\"%s %s %s\" %(self.modulos.paneles.panel, self.modulos.modulo,self.titulo)\r\n    def logo(self):\r\n        logo = \"\"\r\n        if self.boton:\r\n           esta = \"<img src='\" + self.boton.url +\"' alt='Activo' height='150px'>\"\r\n        else:\r\n           esta = \"<img src='\/media\/imgs\/icon-pendiente.gif' alt='Pendiente'> sin imagen\"\r\n        return u\"%s\"%(esta)\r\n    logo.allow_tags = True\r\n\r\n\r\nclass PerfilModulos(models.Model):\r\n    perfil = models.ForeignKey(PerfilPublico)\r\n    modulos = models.ForeignKey(ModulosPublicos,verbose_name='Modulos')\r\n    ver = models.BooleanField(verbose_name=\"Ver\")\r\n    add = models.BooleanField(verbose_name=\"Agregar\")\r\n    edit = models.BooleanField(verbose_name=\"Modificar\")\r\n    activo = models.BooleanField(verbose_name=\"Activo\")\r\n    class Meta:\r\n        db_table = u'perfilmodulos'\r\n        verbose_name_plural='Permisos Perfiles M\u00f3dulos'\r\n        unique_together=('perfil','modulos','activo')\r\n        verbose_name='Permisos Perfiles M\u00f3dulos'\r\n        #app_label = 'Sistematizacion_de_modulos_publicos'\r\n    def __unicode__(self):\r\n        return u\"%s %s\" %(self.perfil.persona.nombre,self.modulos.modulo)\r\n\t\t\r\nclass PerfilSubModulos(models.Model):\r\n    perfil = models.ForeignKey(PerfilPublico)\r\n    submodulos = models.ForeignKey(SubModulosPublicos,verbose_name='SubModulos')\r\n    ver = models.BooleanField(verbose_name=\"Ver\")\r\n    add = models.BooleanField(verbose_name=\"Agregar\")\r\n    edit = models.BooleanField(verbose_name=\"Modificar\")\r\n    activo = models.BooleanField(verbose_name=\"Activo\")\r\n    class Meta:\r\n        verbose_name_plural='Permisos Perfiles Sub M\u00f3dulos'\r\n        verbose_name='Permisos Perfil Sub M\u00f3dulos'\r\n        unique_together=('perfil','submodulos','activo')\r\n        #app_label = 'Sistematizacion_de_modulos_publicos'\r\n    def __unicode__(self):\r\n        return u\"%s %s\" %(self.perfil.persona.nombre,self.submodulos.titulo)\r\n\t\t\r\n#class PerfilPaneles(models.Model):\r\n#    perfil = models.ForeignKey(PerfilPublico)\r\n#    modulos = models.ManyToManyField(SeccionesPanelPublico,verbose_name='Paneles')\r\n#    class Meta:\r\n#        verbose_name_plural='Perfil Paneles'\r\n#        verbose_name='Perfil Paneles'\r\n#    def __unicode__(self):\r\n#        return u\"%s %s\" %(self.perfil.persona.nombre,self.perfil.persona.documentoidentidad)\r\n\r\nclass TipoSolicitud(models.Model):\r\n    tipo = models.CharField(max_length=180,verbose_name='Tipo')\r\n    descripcion = models.TextField()\r\n    class Meta:\r\n        verbose_name_plural='Tipo de Solicitud'\r\n        verbose_name='Tipo de Solicitud'\r\n    def __unicode__(self):\r\n        return u\"%s\" %(self.tipo)\r\n\t\t\r\n\t\t\r\nclass SistemaSolicitudes(models.Model):\r\n    remi = models.ForeignKey(Directorios,verbose_name='Remitente')\r\n    tipoSolicitud = models.ForeignKey(TipoSolicitud,verbose_name='Tipo de Solicitud',blank=True, null = True)\r\n    destino = models.ManyToManyField(Directorios, related_name='destinodirect',verbose_name='Destinatarios',blank=True, null = True)\r\n    destinoinst = models.ManyToManyField(Actores, related_name='destinoactor',verbose_name='Destinatarios Instituciones',blank=True, null = True)\r\n    asunto = models.CharField(max_length=120,blank=True,null=True)\r\n    mensaje = models.TextField(blank=True,null=True)\r\n    fecha = models.DateTimeField(default=datetime.now(),editable = False)\r\n    fechainicio = models.DateTimeField(verbose_name='Fecha de Inicio',blank=True,null=True)\r\n    fechaentrega = models.DateTimeField(verbose_name='Fecha de Entrega',blank=True,null=True)\r\n    fechaculminacion = models.DateTimeField(verbose_name='Fecha de Culminaci\u00f3n',blank=True,null=True)\r\n    fechaprorroga = models.DateTimeField(verbose_name='Prorroga',blank=True,null=True)\r\n    proyect = models.BooleanField(verbose_name='Es Proyectable?')\r\n    estrucorg = models.TextField(verbose_name='Recursos', blank=True, null=True)\r\n\r\n    personasinvol = models.ManyToManyField(Directorios, related_name='persoinvol',verbose_name='Personas Involucradas',blank=True, null = True)\r\n    personasinvoltext = models.TextField(verbose_name='Personas Involucradas, no registradas', blank=True, null=True)\r\n\r\n    instituinvol = models.ManyToManyField(Actores, related_name='instiinvol',verbose_name='Instituciones Involucradas',blank=True, null = True)\r\n    instituinvoltext = models.TextField(verbose_name='Institutos Involucrados, no registrados', blank=True, null=True)\r\n\t\r\n    especies = models.ManyToManyField(Taxon, related_name='tax',verbose_name='Especies Involucradas',blank=True, null = True)\r\n    especiestext = models.TextField(verbose_name='Especies Involucradas, no registradas', blank=True, null=True)\r\n\r\n    areas = models.ManyToManyField(Areas, related_name='ar',verbose_name='Areas Involucradas',blank=True, null = True)\r\n    areastext = models.TextField(verbose_name='Areas Involucradas, no registradas', blank=True, null=True)\r\n\t\r\n    datos = models.FileField(upload_to='solicitudes',verbose_name='Datos Adjuntos',blank=True,null=True)\r\n    prioridad = models.IntegerField(choices=((0,'Urgente'),(1,'Normal'),(2,'Especial')),verbose_name='Prioridad',null=True,blank=True)\r\n    estatu = models.IntegerField(choices=((0,'Abierto'),(1,'Cerrado'),(2,'Pausado')),verbose_name='Estatus',null=True,blank=True,db_column='estatu_id')\r\n    class Meta:\r\n        verbose_name_plural='Sistema de Solicitudes'\r\n        #app_label = 'Datos_Transversales' \r\n        verbose_name = 'Sistema de Solicitudes'\r\n    def __unicode__(self):\r\n        return u\" %s %s\"%(self.remi,self.estatu)\r\n\t\t\r\n#    def VerEspecies(self):\r\n#        try:\r\n#           espe = Taxon.objects.get(detalletaxon=self)\r\n#        except Taxon.DoesNotExist:\r\n#           espe = None\r\n#        return u\"<a href='\/manager\/especies\/taxon\/%s'>Ver Taxon<\/a>\"%(tax.id)\r\n#    VerTaxon.allow_tags = True\r\n\r\nclass Seguimiento(models.Model):\r\n    solicitud = models.ForeignKey(SistemaSolicitudes,verbose_name='Solicitud',blank=True, null = True)\r\n    persona = models.ForeignKey(Directorios,verbose_name='Persona',blank=True, null = True,editable = False)\r\n    mensaje = models.TextField()\r\n    fecha = models.DateTimeField(default=datetime.now(),editable = False)\r\n    class Meta:\r\n        verbose_name_plural='Seguimiento'\r\n        verbose_name='Seguimiento'\r\n    def __unicode__(self):\r\n        return u\"%s\" %(self.solicitud)\r\n\r\nclass validaciones(models.Model):\r\n    usuario = models.ForeignKey(PerfilPublico,verbose_name='Usuario')\r\n    codigo = models.CharField(max_length=120)\r\n    estatu = models.IntegerField(choices=((0,'Validacion'),(1,'Recuperacion'),(2,'Eliminacion')),verbose_name='Tipo',null=True,blank=True)\r\n    fecha = models.DateTimeField(default=datetime.now(),editable = False)\r\n    estado = models.BooleanField(verbose_name=\"Activo\")\r\n    class Meta:\r\n        verbose_name_plural='Validacion de Cuentas'\r\n        #app_label = 'Datos_Transversales' \r\n        verbose_name = 'Validacion de Cuentas'\r\n    def __unicode__(self):\r\n        return u\" %s %s\"%(self.usuario,self.estatu)\r\n\t\t\r\nclass GruposPermisos(models.Model):\r\n    nombre = models.CharField(max_length=120)\r\n    estado = models.BooleanField(verbose_name=\"Activo\")\r\n    class Meta:\r\n        verbose_name_plural='Grupos de Permisos de Perfil'\r\n        verbose_name = 'Grupos de Permisos de Perfil'\r\n    def __unicode__(self):\r\n        return u\" %s %s\"%(self.nombre,self.estado)\r\n\t\t\r\nclass DetalleGruposPermisos(models.Model):\r\n    grupo = models.ForeignKey(GruposPermisos,verbose_name='Grupo')\r\n    seccion = models.ForeignKey(SeccionesPanelPublico,verbose_name='Panel')\r\n    modulo = ChainedForeignKey(ModulosPublicos,chained_field=\"seccion\",chained_model_field=\"paneles\",show_all=False,auto_choose=True,verbose_name='Modulo',null=True,blank=True)\r\n    #modulo = models.ForeignKey(ModulosPublicos,verbose_name='Modulo')\r\n    submodulo = ChainedForeignKey(SubModulosPublicos,chained_field=\"modulo\",chained_model_field=\"modulos\",show_all=False,auto_choose=True,verbose_name='SubModulo',null=True,blank=True)\r\n    #submodulo = models.ForeignKey(SubModulosPublicos,verbose_name='SubModulo')\r\n    estado = models.BooleanField(verbose_name=\"Activo\")\r\n    class Meta:\r\n        verbose_name_plural='Detalle Grupos de Permisos de Perfil'\r\n        verbose_name = 'Detalle Grupos de Permisos de Perfil'\r\n    def __unicode__(self):\r\n        return u\" %s %s\"%(self.grupo,self.estado)\r\n\r\n","label":0}
{"content":"\"\"\"\nTests for courseware middleware\n\"\"\"\n\nfrom django.http import Http404\nfrom django.test.client import RequestFactory\nfrom nose.plugins.attrib import attr\n\nfrom lms.djangoapps.courseware.exceptions import Redirect\nfrom lms.djangoapps.courseware.middleware import RedirectMiddleware\nfrom xmodule.modulestore.tests.django_utils import SharedModuleStoreTestCase\nfrom xmodule.modulestore.tests.factories import CourseFactory\n\n\n@attr(shard=1)\nclass CoursewareMiddlewareTestCase(SharedModuleStoreTestCase):\n    \"\"\"Tests that courseware middleware is correctly redirected\"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        super(CoursewareMiddlewareTestCase, cls).setUpClass()\n        cls.course = CourseFactory.create()\n\n    def test_process_404(self):\n        \"\"\"A 404 should not trigger anything\"\"\"\n        request = RequestFactory().get(\"dummy_url\")\n        response = RedirectMiddleware().process_exception(\n            request, Http404()\n        )\n        self.assertIsNone(response)\n\n    def test_redirect_exceptions(self):\n        \"\"\"\n        Unit tests for handling of Redirect exceptions.\n        \"\"\"\n        request = RequestFactory().get(\"dummy_url\")\n        test_url = '\/test_url'\n        exception = Redirect(test_url)\n        response = RedirectMiddleware().process_exception(\n            request, exception\n        )\n        self.assertEqual(response.status_code, 302)\n        target_url = response._headers['location'][1]\n        self.assertTrue(target_url.endswith(test_url))\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    Copyright (C) 2016 Compassion CH (http:\/\/www.compassion.ch)\n#    Releasing children from poverty in Jesus' name\n#    @author: Emanuel Cino <ecino@compassion.ch>\n#\n#    The licence is in the file __manifest__.py\n#\n##############################################################################\nfrom odoo import api, models, fields\n\n\nclass Email(models.Model):\n    \"\"\" Add relation to communication configuration to track generated\n    e-mails.\n    \"\"\"\n    _inherit = 'mail.mail'\n\n    ##########################################################################\n    #                                 FIELDS                                 #\n    ##########################################################################\n    communication_config_id = fields.Many2one('partner.communication.config')\n\n    @api.multi\n    def send(self, auto_commit=False, raise_exception=False):\n        \"\"\" Create communication for partner, if not already existing.\n        \"\"\"\n        comm_obj = self.env['partner.communication.job'].with_context(\n            {}).with_context(no_print=True)\n        config = self.env.ref(\n            'partner_communication.default_communication')\n        for email in self.exists().filtered(\n                lambda e: e.mail_message_id.model !=\n                'partner.communication.job'):\n            communication = comm_obj.search([('email_id', '=', email.id)])\n            if not communication:\n                for partner in email.recipient_ids.filtered(\n                        lambda p: not p.user_ids or reduce(\n                            lambda u1, u2: u1 and u2,\n                            p.user_ids.mapped('share'))):\n                    comm_obj.create({\n                        'config_id': config.id,\n                        'partner_id': partner.id,\n                        'user_id': email.author_id.user_ids.id,\n                        'object_ids': email.recipient_ids.ids,\n                        'state': 'done',\n                        'auto_send': False,\n                        'email_id': email.id,\n                        'sent_date': fields.Datetime.now(),\n                        'body_html': email.body_html,\n                        'subject': email.subject,\n                        'ir_attachment_ids': [(6, 0, email.attachment_ids.ids)]\n                    })\n        return super(Email, self).send(auto_commit, raise_exception)\n","label":0}
{"content":"# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU Affero Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU Affero Public License for more details.\n#\n# You should have received a copy of the GNU Affero Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses.\n#\n# http:\/\/numenta.org\/licenses\/\n# ----------------------------------------------------------------------\n\nimport copy\n\n\n# TODO: Note the functions 'rUpdate' are duplicated in\n# the swarming.hypersearch.utils.py module\n\n\nclass DictObj(dict):\n  \"\"\"Dictionary that allows attribute-like access to its elements.\n  Attributes are read-only.\"\"\"\n\n  def __getattr__(self, name):\n    if name == '__deepcopy__':\n      return super(DictObj, self).__getattribute__(\"__deepcopy__\")\n    return self[name]\n\n  def __setstate__(self, state):\n    for k, v in state.items():\n      self[k] = v\n\n\ndef rUpdate(original, updates):\n  \"\"\"Recursively updates the values in original with the values from updates.\"\"\"\n  # Keep a list of the sub-dictionaries that need to be updated to avoid having\n  # to use recursion (which could fail for dictionaries with a lot of nesting.\n  dictPairs = [(original, updates)]\n  while len(dictPairs) > 0:\n    original, updates = dictPairs.pop()\n    for k, v in updates.iteritems():\n      if k in original and isinstance(original[k], dict) and isinstance(v, dict):\n        dictPairs.append((original[k], v))\n      else:\n        original[k] = v\n\n\ndef rApply(d, f):\n  \"\"\"Recursively applies f to the values in dict d.\n\n  Args:\n    d: The dict to recurse over.\n    f: A function to apply to values in d that takes the value and a list of\n        keys from the root of the dict to the value.\n  \"\"\"\n  remainingDicts = [(d, ())]\n  while len(remainingDicts) > 0:\n    current, prevKeys = remainingDicts.pop()\n    for k, v in current.iteritems():\n      keys = prevKeys + (k,)\n      if isinstance(v, dict):\n        remainingDicts.insert(0, (v, keys))\n      else:\n        f(v, keys)\n\n\ndef find(d, target):\n  remainingDicts = [d]\n  while len(remainingDicts) > 0:\n    current = remainingDicts.pop()\n    for k, v in current.iteritems():\n      if k == target:\n        return v\n      if isinstance(v, dict):\n        remainingDicts.insert(0, v)\n  return None\n\n\ndef get(d, keys):\n  for key in keys:\n    d = d[key]\n  return d\n\n\ndef set(d, keys, value):\n  for key in keys[:-1]:\n    d = d[key]\n  d[keys[-1]] = value\n\n\ndef dictDiffAndReport(da, db):\n  \"\"\" Compares two python dictionaries at the top level and report differences,\n  if any, to stdout\n\n  da:             first dictionary\n  db:             second dictionary\n\n  Returns:        The same value as returned by dictDiff() for the given args\n  \"\"\"\n  differences = dictDiff(da, db)\n\n  if not differences:\n    return differences\n\n  if differences['inAButNotInB']:\n    print \">>> inAButNotInB: %s\" % differences['inAButNotInB']\n\n  if differences['inBButNotInA']:\n    print \">>> inBButNotInA: %s\" % differences['inBButNotInA']\n\n  for key in differences['differentValues']:\n    print \">>> da[%s] != db[%s]\" % (key, key)\n    print \"da[%s] = %r\" % (key, da[key])\n    print \"db[%s] = %r\" % (key, db[key])\n\n  return differences\n\n\ndef dictDiff(da, db):\n  \"\"\" Compares two python dictionaries at the top level and return differences\n\n  da:             first dictionary\n  db:             second dictionary\n\n  Returns:        None if dictionaries test equal; otherwise returns a\n                  dictionary as follows:\n                  {\n                    'inAButNotInB':\n                        <sequence of keys that are in da but not in db>\n                    'inBButNotInA':\n                        <sequence of keys that are in db but not in da>\n                    'differentValues':\n                        <sequence of keys whose corresponding values differ\n                         between da and db>\n                  }\n  \"\"\"\n  different = False\n\n  resultDict = dict()\n\n  resultDict['inAButNotInB'] = set(da) - set(db)\n  if resultDict['inAButNotInB']:\n    different = True\n\n  resultDict['inBButNotInA'] = set(db) - set(da)\n  if resultDict['inBButNotInA']:\n    different = True\n\n  resultDict['differentValues'] = []\n  for key in (set(da) - resultDict['inAButNotInB']):\n    comparisonResult = da[key] == db[key]\n    if isinstance(comparisonResult, bool):\n      isEqual = comparisonResult\n    else:\n      # This handles numpy arrays (but only at the top level)\n      isEqual = comparisonResult.all()\n    if not isEqual:\n      resultDict['differentValues'].append(key)\n      different = True\n\n  assert (((resultDict['inAButNotInB'] or resultDict['inBButNotInA'] or\n          resultDict['differentValues']) and different) or not different)\n\n  return resultDict if different else None\n","label":0}
{"content":"from contextlib import contextmanager\n\nfrom bok_choy.javascript import wait_for_js\nfrom bok_choy.page_object import PageObject\nfrom bok_choy.promise import EmptyPromise, Promise\n\nfrom .course_page import CoursePage\n\n\nclass DiscussionPageMixin(object):\n\n    def is_ajax_finished(self):\n        return self.browser.execute_script(\"return jQuery.active\") == 0\n\n\nclass DiscussionThreadPage(PageObject, DiscussionPageMixin):\n    url = None\n\n    def __init__(self, browser, thread_selector):\n        super(DiscussionThreadPage, self).__init__(browser)\n        self.thread_selector = thread_selector\n\n    def _find_within(self, selector):\n        \"\"\"\n        Returns a query corresponding to the given CSS selector within the scope\n        of this thread page\n        \"\"\"\n        return self.q(css=self.thread_selector + \" \" + selector)\n\n    def is_browser_on_page(self):\n        return self.q(css=self.thread_selector).present\n\n    def _get_element_text(self, selector):\n        \"\"\"\n        Returns the text of the first element matching the given selector, or\n        None if no such element exists\n        \"\"\"\n        text_list = self._find_within(selector).text\n        return text_list[0] if text_list else None\n\n    def _is_element_visible(self, selector):\n        query = self._find_within(selector)\n        return query.present and query.visible\n\n    @contextmanager\n    def _secondary_action_menu_open(self, ancestor_selector):\n        \"\"\"\n        Given the selector for an ancestor of a secondary menu, return a context\n        manager that will open and close the menu\n        \"\"\"\n        self._find_within(ancestor_selector + \" .action-more\").click()\n        EmptyPromise(\n            lambda: self._is_element_visible(ancestor_selector + \" .actions-dropdown\"),\n            \"Secondary action menu opened\"\n        ).fulfill()\n        yield\n        if self._is_element_visible(ancestor_selector + \" .actions-dropdown\"):\n            self._find_within(ancestor_selector + \" .action-more\").click()\n            EmptyPromise(\n                lambda: not self._is_element_visible(ancestor_selector + \" .actions-dropdown\"),\n                \"Secondary action menu closed\"\n            ).fulfill()\n\n    def get_group_visibility_label(self):\n        \"\"\"\n        Returns the group visibility label shown for the thread.\n        \"\"\"\n        return self._get_element_text(\".group-visibility-label\")\n\n    def get_response_total_text(self):\n        \"\"\"Returns the response count text, or None if not present\"\"\"\n        return self._get_element_text(\".response-count\")\n\n    def get_num_displayed_responses(self):\n        \"\"\"Returns the number of responses actually rendered\"\"\"\n        return len(self._find_within(\".discussion-response\"))\n\n    def get_shown_responses_text(self):\n        \"\"\"Returns the shown response count text, or None if not present\"\"\"\n        return self._get_element_text(\".response-display-count\")\n\n    def get_load_responses_button_text(self):\n        \"\"\"Returns the load more responses button text, or None if not present\"\"\"\n        return self._get_element_text(\".load-response-button\")\n\n    def load_more_responses(self):\n        \"\"\"Clicks the load more responses button and waits for responses to load\"\"\"\n        self._find_within(\".load-response-button\").click()\n\n        EmptyPromise(\n            self.is_ajax_finished,\n            \"Loading more Responses\"\n        ).fulfill()\n\n    def has_add_response_button(self):\n        \"\"\"Returns true if the add response button is visible, false otherwise\"\"\"\n        return self._is_element_visible(\".add-response-btn\")\n\n    def click_add_response_button(self):\n        \"\"\"\n        Clicks the add response button and ensures that the response text\n        field receives focus\n        \"\"\"\n        self._find_within(\".add-response-btn\").first.click()\n        EmptyPromise(\n            lambda: self._find_within(\".discussion-reply-new textarea:focus\").present,\n            \"Response field received focus\"\n        ).fulfill()\n\n    @wait_for_js\n    def is_response_editor_visible(self, response_id):\n        \"\"\"Returns true if the response editor is present, false otherwise\"\"\"\n        return self._is_element_visible(\".response_{} .edit-post-body\".format(response_id))\n\n    @wait_for_js\n    def is_discussion_body_visible(self):\n        return self._is_element_visible(\".post-body\")\n\n    def is_mathjax_preview_available(self):\n        return self.q(css=\".MathJax_Preview\").text[0] == \"\"\n\n    def is_mathjax_rendered(self):\n        return self._is_element_visible(\".MathJax\")\n\n    def is_response_visible(self, comment_id):\n        \"\"\"Returns true if the response is viewable onscreen\"\"\"\n        return self._is_element_visible(\".response_{} .response-body\".format(comment_id))\n\n    def is_response_editable(self, response_id):\n        \"\"\"Returns true if the edit response button is present, false otherwise\"\"\"\n        with self._secondary_action_menu_open(\".response_{} .discussion-response\".format(response_id)):\n            return self._is_element_visible(\".response_{} .discussion-response .action-edit\".format(response_id))\n\n    def get_response_body(self, response_id):\n        return self._get_element_text(\".response_{} .response-body\".format(response_id))\n\n    def start_response_edit(self, response_id):\n        \"\"\"Click the edit button for the response, loading the editing view\"\"\"\n        with self._secondary_action_menu_open(\".response_{} .discussion-response\".format(response_id)):\n            self._find_within(\".response_{} .discussion-response .action-edit\".format(response_id)).first.click()\n            EmptyPromise(\n                lambda: self.is_response_editor_visible(response_id),\n                \"Response edit started\"\n            ).fulfill()\n\n    def get_link_href(self):\n        \"\"\"Extracts href attribute of the referenced link\"\"\"\n        link_href = self._find_within(\".post-body p a\").attrs('href')\n        return link_href[0] if link_href else None\n\n    def get_response_vote_count(self, response_id):\n        return self._get_element_text(\".response_{} .discussion-response .action-vote .vote-count\".format(response_id))\n\n    def vote_response(self, response_id):\n        current_count = self._get_element_text(\".response_{} .discussion-response .action-vote .vote-count\".format(response_id))\n        self._find_within(\".response_{} .discussion-response .action-vote\".format(response_id)).first.click()\n        self.wait_for_ajax()\n        EmptyPromise(\n            lambda: current_count != self.get_response_vote_count(response_id),\n            \"Response is voted\"\n        ).fulfill()\n\n    def is_response_reported(self, response_id):\n        return self._is_element_visible(\".response_{} .discussion-response .post-label-reported\".format(response_id))\n\n    def report_response(self, response_id):\n        with self._secondary_action_menu_open(\".response_{} .discussion-response\".format(response_id)):\n            self._find_within(\".response_{} .discussion-response .action-report\".format(response_id)).first.click()\n            self.wait_for_ajax()\n            EmptyPromise(\n                lambda: self.is_response_reported(response_id),\n                \"Response is reported\"\n            ).fulfill()\n\n    def is_response_endorsed(self, response_id):\n        return \"endorsed\" in self._get_element_text(\".response_{} .discussion-response .posted-details\".format(response_id))\n\n    def endorse_response(self, response_id):\n        self._find_within(\".response_{} .discussion-response .action-endorse\".format(response_id)).first.click()\n        self.wait_for_ajax()\n        EmptyPromise(\n            lambda: self.is_response_endorsed(response_id),\n            \"Response edit started\"\n        ).fulfill()\n\n    def set_response_editor_value(self, response_id, new_body):\n        \"\"\"Replace the contents of the response editor\"\"\"\n        self._find_within(\".response_{} .discussion-response .wmd-input\".format(response_id)).fill(new_body)\n\n    def submit_response_edit(self, response_id, new_response_body):\n        \"\"\"Click the submit button on the response editor\"\"\"\n        self._find_within(\".response_{} .discussion-response .post-update\".format(response_id)).first.click()\n        EmptyPromise(\n            lambda: (\n                not self.is_response_editor_visible(response_id) and\n                self.is_response_visible(response_id) and\n                self.get_response_body(response_id) == new_response_body\n            ),\n            \"Comment edit succeeded\"\n        ).fulfill()\n\n    def is_show_comments_visible(self, response_id):\n        \"\"\"Returns true if the \"show comments\" link is visible for a response\"\"\"\n        return self._is_element_visible(\".response_{} .action-show-comments\".format(response_id))\n\n    def show_comments(self, response_id):\n        \"\"\"Click the \"show comments\" link for a response\"\"\"\n        self._find_within(\".response_{} .action-show-comments\".format(response_id)).first.click()\n        EmptyPromise(\n            lambda: self._is_element_visible(\".response_{} .comments\".format(response_id)),\n            \"Comments shown\"\n        ).fulfill()\n\n    def is_add_comment_visible(self, response_id):\n        \"\"\"Returns true if the \"add comment\" form is visible for a response\"\"\"\n        return self._is_element_visible(\"#wmd-input-comment-body-{}\".format(response_id))\n\n    def is_comment_visible(self, comment_id):\n        \"\"\"Returns true if the comment is viewable onscreen\"\"\"\n        return self._is_element_visible(\"#comment_{} .response-body\".format(comment_id))\n\n    def get_comment_body(self, comment_id):\n        return self._get_element_text(\"#comment_{} .response-body\".format(comment_id))\n\n    def is_comment_deletable(self, comment_id):\n        \"\"\"Returns true if the delete comment button is present, false otherwise\"\"\"\n        with self._secondary_action_menu_open(\"#comment_{}\".format(comment_id)):\n            return self._is_element_visible(\"#comment_{} .action-delete\".format(comment_id))\n\n    def delete_comment(self, comment_id):\n        with self.handle_alert():\n            with self._secondary_action_menu_open(\"#comment_{}\".format(comment_id)):\n                self._find_within(\"#comment_{} .action-delete\".format(comment_id)).first.click()\n        EmptyPromise(\n            lambda: not self.is_comment_visible(comment_id),\n            \"Deleted comment was removed\"\n        ).fulfill()\n\n    def is_comment_editable(self, comment_id):\n        \"\"\"Returns true if the edit comment button is present, false otherwise\"\"\"\n        with self._secondary_action_menu_open(\"#comment_{}\".format(comment_id)):\n            return self._is_element_visible(\"#comment_{} .action-edit\".format(comment_id))\n\n    def is_comment_editor_visible(self, comment_id):\n        \"\"\"Returns true if the comment editor is present, false otherwise\"\"\"\n        return self._is_element_visible(\".edit-comment-body[data-id='{}']\".format(comment_id))\n\n    def _get_comment_editor_value(self, comment_id):\n        return self._find_within(\"#wmd-input-edit-comment-body-{}\".format(comment_id)).text[0]\n\n    def start_comment_edit(self, comment_id):\n        \"\"\"Click the edit button for the comment, loading the editing view\"\"\"\n        old_body = self.get_comment_body(comment_id)\n        with self._secondary_action_menu_open(\"#comment_{}\".format(comment_id)):\n            self._find_within(\"#comment_{} .action-edit\".format(comment_id)).first.click()\n            EmptyPromise(\n                lambda: (\n                    self.is_comment_editor_visible(comment_id) and\n                    not self.is_comment_visible(comment_id) and\n                    self._get_comment_editor_value(comment_id) == old_body\n                ),\n                \"Comment edit started\"\n            ).fulfill()\n\n    def set_comment_editor_value(self, comment_id, new_body):\n        \"\"\"Replace the contents of the comment editor\"\"\"\n        self._find_within(\"#comment_{} .wmd-input\".format(comment_id)).fill(new_body)\n\n    def submit_comment_edit(self, comment_id, new_comment_body):\n        \"\"\"Click the submit button on the comment editor\"\"\"\n        self._find_within(\"#comment_{} .post-update\".format(comment_id)).first.click()\n        EmptyPromise(\n            lambda: (\n                not self.is_comment_editor_visible(comment_id) and\n                self.is_comment_visible(comment_id) and\n                self.get_comment_body(comment_id) == new_comment_body\n            ),\n            \"Comment edit succeeded\"\n        ).fulfill()\n\n    def cancel_comment_edit(self, comment_id, original_body):\n        \"\"\"Click the cancel button on the comment editor\"\"\"\n        self._find_within(\"#comment_{} .post-cancel\".format(comment_id)).first.click()\n        EmptyPromise(\n            lambda: (\n                not self.is_comment_editor_visible(comment_id) and\n                self.is_comment_visible(comment_id) and\n                self.get_comment_body(comment_id) == original_body\n            ),\n            \"Comment edit was canceled\"\n        ).fulfill()\n\n\nclass DiscussionSortPreferencePage(CoursePage):\n    \"\"\"\n    Page that contain the discussion board with sorting options\n    \"\"\"\n    def __init__(self, browser, course_id):\n        super(DiscussionSortPreferencePage, self).__init__(browser, course_id)\n        self.url_path = \"discussion\/forum\"\n\n    def is_browser_on_page(self):\n        \"\"\"\n        Return true if the browser is on the right page else false.\n        \"\"\"\n        return self.q(css=\"body.discussion .forum-nav-sort-control\").present\n\n    def get_selected_sort_preference(self):\n        \"\"\"\n        Return the text of option that is selected for sorting.\n        \"\"\"\n        options = self.q(css=\"body.discussion .forum-nav-sort-control option\")\n        return options.filter(lambda el: el.is_selected())[0].get_attribute(\"value\")\n\n    def change_sort_preference(self, sort_by):\n        \"\"\"\n        Change the option of sorting by clicking on new option.\n        \"\"\"\n        self.q(css=\"body.discussion .forum-nav-sort-control option[value='{0}']\".format(sort_by)).click()\n\n    def refresh_page(self):\n        \"\"\"\n        Reload the page.\n        \"\"\"\n        self.browser.refresh()\n\n\nclass DiscussionTabSingleThreadPage(CoursePage):\n    def __init__(self, browser, course_id, discussion_id, thread_id):\n        super(DiscussionTabSingleThreadPage, self).__init__(browser, course_id)\n        self.thread_page = DiscussionThreadPage(\n            browser,\n            \"body.discussion .discussion-article[data-id='{thread_id}']\".format(thread_id=thread_id)\n        )\n        self.url_path = \"discussion\/forum\/{discussion_id}\/threads\/{thread_id}\".format(\n            discussion_id=discussion_id, thread_id=thread_id\n        )\n\n    def is_browser_on_page(self):\n        return self.thread_page.is_browser_on_page()\n\n    def __getattr__(self, name):\n        return getattr(self.thread_page, name)\n\n    def close_open_thread(self):\n        with self.thread_page._secondary_action_menu_open(\".forum-thread-main-wrapper\"):\n            self._find_within(\".forum-thread-main-wrapper .action-close\").first.click()\n\n    @wait_for_js\n    def is_window_on_top(self):\n        \"\"\"\n        Check if window's scroll is at top\n        \"\"\"\n        return self.browser.execute_script(\"return $('html, body').offset().top\") == 0\n\n    def _thread_is_rendered_successfully(self, thread_id):\n        return self.q(css=\".discussion-article[data-id='{}']\".format(thread_id)).visible\n\n    def click_and_open_thread(self, thread_id):\n        \"\"\"\n        Click specific thread on the list.\n        \"\"\"\n        thread_selector = \"li[data-id='{}']\".format(thread_id)\n        self.q(css=thread_selector).first.click()\n        EmptyPromise(\n            lambda: self._thread_is_rendered_successfully(thread_id),\n            \"Thread has been rendered\"\n        ).fulfill()\n\n    def check_threads_rendered_successfully(self, thread_count):\n        \"\"\"\n        Count the number of threads available on page.\n        \"\"\"\n        return len(self.q(css=\".forum-nav-thread\").results) == thread_count\n\n    def check_window_is_on_top(self):\n        \"\"\"\n        Check window is on top of the page\n        \"\"\"\n        EmptyPromise(\n            self.is_window_on_top,\n            \"Window is on top\"\n        ).fulfill()\n\n\nclass InlineDiscussionPage(PageObject):\n    url = None\n\n    def __init__(self, browser, discussion_id):\n        super(InlineDiscussionPage, self).__init__(browser)\n        self._discussion_selector = (\n            \".discussion-module[data-discussion-id='{discussion_id}'] \".format(\n                discussion_id=discussion_id\n            )\n        )\n\n    def _find_within(self, selector):\n        \"\"\"\n        Returns a query corresponding to the given CSS selector within the scope\n        of this discussion page\n        \"\"\"\n        return self.q(css=self._discussion_selector + \" \" + selector)\n\n    def is_browser_on_page(self):\n        self.wait_for_ajax()\n        return self.q(css=self._discussion_selector).present\n\n    def is_discussion_expanded(self):\n        return self._find_within(\".discussion\").present\n\n    def expand_discussion(self):\n        \"\"\"Click the link to expand the discussion\"\"\"\n        self._find_within(\".discussion-show\").first.click()\n        EmptyPromise(\n            self.is_discussion_expanded,\n            \"Discussion expanded\"\n        ).fulfill()\n\n    def get_num_displayed_threads(self):\n        return len(self._find_within(\".discussion-thread\"))\n\n    def has_thread(self, thread_id):\n        \"\"\"Returns true if this page is showing the thread with the specified id.\"\"\"\n        return self._find_within('.discussion-thread#thread_{}'.format(thread_id)).present\n\n    def element_exists(self, selector):\n        return self.q(css=self._discussion_selector + \" \" + selector).present\n\n    def is_new_post_opened(self):\n        return self._find_within(\".new-post-article\").visible\n\n    def click_element(self, selector):\n        self.wait_for_element_presence(\n            \"{discussion} {selector}\".format(discussion=self._discussion_selector, selector=selector),\n            \"{selector} is visible\".format(selector=selector)\n        )\n        self._find_within(selector).click()\n\n    def click_cancel_new_post(self):\n        self.click_element(\".cancel\")\n        EmptyPromise(\n            lambda: not self.is_new_post_opened(),\n            \"New post closed\"\n        ).fulfill()\n\n    def click_new_post_button(self):\n        self.click_element(\".new-post-btn\")\n        EmptyPromise(\n            self.is_new_post_opened,\n            \"New post opened\"\n        ).fulfill()\n\n    @wait_for_js\n    def _is_element_visible(self, selector):\n        query = self._find_within(selector)\n        return query.present and query.visible\n\n\nclass InlineDiscussionThreadPage(DiscussionThreadPage):\n    def __init__(self, browser, thread_id):\n        super(InlineDiscussionThreadPage, self).__init__(\n            browser,\n            \"body.courseware .discussion-module #thread_{thread_id}\".format(thread_id=thread_id)\n        )\n\n    def expand(self):\n        \"\"\"Clicks the link to expand the thread\"\"\"\n        self._find_within(\".forum-thread-expand\").first.click()\n        EmptyPromise(\n            lambda: bool(self.get_response_total_text()),\n            \"Thread expanded\"\n        ).fulfill()\n\n    def is_thread_anonymous(self):\n        return not self.q(css=\".posted-details > .username\").present\n\n    @wait_for_js\n    def check_if_selector_is_focused(self, selector):\n        \"\"\"\n        Check if selector is focused\n        \"\"\"\n        return self.browser.execute_script(\"return $('{}').is(':focus')\".format(selector))\n\n\nclass DiscussionUserProfilePage(CoursePage):\n\n    TEXT_NEXT = u'Next >'\n    TEXT_PREV = u'< Previous'\n    PAGING_SELECTOR = \"a.discussion-pagination[data-page-number]\"\n\n    def __init__(self, browser, course_id, user_id, username, page=1):\n        super(DiscussionUserProfilePage, self).__init__(browser, course_id)\n        self.url_path = \"discussion\/forum\/dummy\/users\/{}?page={}\".format(user_id, page)\n        self.username = username\n\n    def is_browser_on_page(self):\n        return (\n            self.q(css='section.discussion-user-threads[data-course-id=\"{}\"]'.format(self.course_id)).present\n            and\n            self.q(css='section.user-profile a.learner-profile-link').present\n            and\n            self.q(css='section.user-profile a.learner-profile-link').text[0] == self.username\n        )\n\n    @wait_for_js\n    def is_window_on_top(self):\n        return self.browser.execute_script(\"return $('html, body').offset().top\") == 0\n\n    def get_shown_thread_ids(self):\n        elems = self.q(css=\"article.discussion-thread\")\n        return [elem.get_attribute(\"id\")[7:] for elem in elems]\n\n    def get_current_page(self):\n        def check_func():\n            try:\n                current_page = int(self.q(css=\"nav.discussion-paginator li.current-page\").text[0])\n            except:\n                return False, None\n            return True, current_page\n\n        return Promise(\n            check_func, 'discussion-paginator current page has text', timeout=5,\n        ).fulfill()\n\n    def _check_pager(self, text, page_number=None):\n        \"\"\"\n        returns True if 'text' matches the text in any of the pagination elements.  If\n        page_number is provided, only return True if the element points to that result\n        page.\n        \"\"\"\n        elems = self.q(css=self.PAGING_SELECTOR).filter(lambda elem: elem.text == text)\n        if page_number:\n            elems = elems.filter(lambda elem: int(elem.get_attribute('data-page-number')) == page_number)\n        return elems.present\n\n    def get_clickable_pages(self):\n        return sorted([\n            int(elem.get_attribute('data-page-number'))\n            for elem in self.q(css=self.PAGING_SELECTOR)\n            if str(elem.text).isdigit()\n        ])\n\n    def is_prev_button_shown(self, page_number=None):\n        return self._check_pager(self.TEXT_PREV, page_number)\n\n    def is_next_button_shown(self, page_number=None):\n        return self._check_pager(self.TEXT_NEXT, page_number)\n\n    def _click_pager_with_text(self, text, page_number):\n        \"\"\"\n        click the first pagination element with whose text is `text` and ensure\n        the resulting page number matches `page_number`.\n        \"\"\"\n        targets = [elem for elem in self.q(css=self.PAGING_SELECTOR) if elem.text == text]\n        targets[0].click()\n        EmptyPromise(\n            lambda: self.get_current_page() == page_number,\n            \"navigated to desired page\"\n        ).fulfill()\n\n    def click_prev_page(self):\n        self._click_pager_with_text(self.TEXT_PREV, self.get_current_page() - 1)\n        EmptyPromise(\n            self.is_window_on_top,\n            \"Window is on top\"\n        ).fulfill()\n\n    def click_next_page(self):\n        self._click_pager_with_text(self.TEXT_NEXT, self.get_current_page() + 1)\n        EmptyPromise(\n            self.is_window_on_top,\n            \"Window is on top\"\n        ).fulfill()\n\n    def click_on_page(self, page_number):\n        self._click_pager_with_text(unicode(page_number), page_number)\n        EmptyPromise(\n            self.is_window_on_top,\n            \"Window is on top\"\n        ).fulfill()\n\n    def click_on_sidebar_username(self):\n        self.wait_for_page()\n        self.q(css='.learner-profile-link').first.click()\n\n\nclass DiscussionTabHomePage(CoursePage, DiscussionPageMixin):\n\n    ALERT_SELECTOR = \".discussion-body .forum-nav .search-alert\"\n\n    def __init__(self, browser, course_id):\n        super(DiscussionTabHomePage, self).__init__(browser, course_id)\n        self.url_path = \"discussion\/forum\/\"\n\n    def is_browser_on_page(self):\n        return self.q(css=\".discussion-body section.home-header\").present\n\n    def perform_search(self, text=\"dummy\"):\n        self.q(css=\".forum-nav-search-input\").fill(text + chr(10))\n        EmptyPromise(\n            self.is_ajax_finished,\n            \"waiting for server to return result\"\n        ).fulfill()\n\n    def get_search_alert_messages(self):\n        return self.q(css=self.ALERT_SELECTOR + \" .message\").text\n\n    def get_search_alert_links(self):\n        return self.q(css=self.ALERT_SELECTOR + \" .link-jump\")\n\n    def dismiss_alert_message(self, text):\n        \"\"\"\n        dismiss any search alert message containing the specified text.\n        \"\"\"\n        def _match_messages(text):\n            return self.q(css=\".search-alert\").filter(lambda elem: text in elem.text)\n\n        for alert_id in _match_messages(text).attrs(\"id\"):\n            self.q(css=\"{}#{} a.dismiss\".format(self.ALERT_SELECTOR, alert_id)).click()\n        EmptyPromise(\n            lambda: _match_messages(text).results == [],\n            \"waiting for dismissed alerts to disappear\"\n        ).fulfill()\n\n    def click_new_post_button(self):\n        \"\"\"\n        Clicks the 'New Post' button.\n        \"\"\"\n        self.new_post_button.click()\n        EmptyPromise(\n            lambda: (\n                self.new_post_form\n            ),\n            \"New post action succeeded\"\n        ).fulfill()\n\n    @property\n    def new_post_button(self):\n        \"\"\"\n        Returns the new post button.\n        \"\"\"\n        elements = self.q(css=\"ol.course-tabs .new-post-btn\")\n        return elements.first if elements.visible and len(elements) == 1 else None\n\n    @property\n    def new_post_form(self):\n        \"\"\"\n        Returns the new post form.\n        \"\"\"\n        elements = self.q(css=\".forum-new-post-form\")\n        return elements[0] if elements.visible and len(elements) == 1 else None\n","label":0}
{"content":"\"\"\"Provides access to stored IDLE configuration information.\r\n\r\nRefer to the comments at the beginning of config-main.def for a description of\r\nthe available configuration files and the design implemented to update user\r\nconfiguration information.  In particular, user configuration choices which\r\nduplicate the defaults will be removed from the user's configuration files,\r\nand if a file becomes empty, it will be deleted.\r\n\r\nThe contents of the user files may be altered using the Options\/Configure IDLE\r\nmenu to access the configuration GUI (configDialog.py), or manually.\r\n\r\nThroughout this module there is an emphasis on returning useable defaults\r\nwhen a problem occurs in returning a requested configuration value back to\r\nidle. This is to allow IDLE to continue to function in spite of errors in\r\nthe retrieval of config information. When a default is returned instead of\r\na requested config value, a message is printed to stderr to aid in\r\nconfiguration problem notification and resolution.\r\n\r\n\"\"\"\r\nimport os\r\nimport sys\r\nimport string\r\nfrom ConfigParser import ConfigParser, NoOptionError, NoSectionError\r\n\r\nclass InvalidConfigType(Exception): pass\r\nclass InvalidConfigSet(Exception): pass\r\nclass InvalidFgBg(Exception): pass\r\nclass InvalidTheme(Exception): pass\r\n\r\nclass IdleConfParser(ConfigParser):\r\n    \"\"\"\r\n    A ConfigParser specialised for idle configuration file handling\r\n    \"\"\"\r\n    def __init__(self, cfgFile, cfgDefaults=None):\r\n        \"\"\"\r\n        cfgFile - string, fully specified configuration file name\r\n        \"\"\"\r\n        self.file=cfgFile\r\n        ConfigParser.__init__(self,defaults=cfgDefaults)\r\n\r\n    def Get(self, section, option, type=None, default=None):\r\n        \"\"\"\r\n        Get an option value for given section\/option or return default.\r\n        If type is specified, return as type.\r\n        \"\"\"\r\n        if type=='bool':\r\n            getVal=self.getboolean\r\n        elif type=='int':\r\n            getVal=self.getint\r\n        else:\r\n            getVal=self.get\r\n        if self.has_option(section,option):\r\n            #return getVal(section, option, raw, vars, default)\r\n            return getVal(section, option)\r\n        else:\r\n            return default\r\n\r\n    def GetOptionList(self,section):\r\n        \"\"\"\r\n        Get an option list for given section\r\n        \"\"\"\r\n        if self.has_section(section):\r\n            return self.options(section)\r\n        else:  #return a default value\r\n            return []\r\n\r\n    def Load(self):\r\n        \"\"\"\r\n        Load the configuration file from disk\r\n        \"\"\"\r\n        self.read(self.file)\r\n\r\nclass IdleUserConfParser(IdleConfParser):\r\n    \"\"\"\r\n    IdleConfigParser specialised for user configuration handling.\r\n    \"\"\"\r\n\r\n    def AddSection(self,section):\r\n        \"\"\"\r\n        if section doesn't exist, add it\r\n        \"\"\"\r\n        if not self.has_section(section):\r\n            self.add_section(section)\r\n\r\n    def RemoveEmptySections(self):\r\n        \"\"\"\r\n        remove any sections that have no options\r\n        \"\"\"\r\n        for section in self.sections():\r\n            if not self.GetOptionList(section):\r\n                self.remove_section(section)\r\n\r\n    def IsEmpty(self):\r\n        \"\"\"\r\n        Remove empty sections and then return 1 if parser has no sections\r\n        left, else return 0.\r\n        \"\"\"\r\n        self.RemoveEmptySections()\r\n        if self.sections():\r\n            return 0\r\n        else:\r\n            return 1\r\n\r\n    def RemoveOption(self,section,option):\r\n        \"\"\"\r\n        If section\/option exists, remove it.\r\n        Returns 1 if option was removed, 0 otherwise.\r\n        \"\"\"\r\n        if self.has_section(section):\r\n            return self.remove_option(section,option)\r\n\r\n    def SetOption(self,section,option,value):\r\n        \"\"\"\r\n        Sets option to value, adding section if required.\r\n        Returns 1 if option was added or changed, otherwise 0.\r\n        \"\"\"\r\n        if self.has_option(section,option):\r\n            if self.get(section,option)==value:\r\n                return 0\r\n            else:\r\n                self.set(section,option,value)\r\n                return 1\r\n        else:\r\n            if not self.has_section(section):\r\n                self.add_section(section)\r\n            self.set(section,option,value)\r\n            return 1\r\n\r\n    def RemoveFile(self):\r\n        \"\"\"\r\n        Removes the user config file from disk if it exists.\r\n        \"\"\"\r\n        if os.path.exists(self.file):\r\n            os.remove(self.file)\r\n\r\n    def Save(self):\r\n        \"\"\"Update user configuration file.\r\n\r\n        Remove empty sections. If resulting config isn't empty, write the file\r\n        to disk. If config is empty, remove the file from disk if it exists.\r\n\r\n        \"\"\"\r\n        if not self.IsEmpty():\r\n            cfgFile=open(self.file,'w')\r\n            self.write(cfgFile)\r\n        else:\r\n            self.RemoveFile()\r\n\r\nclass IdleConf:\r\n    \"\"\"\r\n    holds config parsers for all idle config files:\r\n    default config files\r\n        (idle install dir)\/config-main.def\r\n        (idle install dir)\/config-extensions.def\r\n        (idle install dir)\/config-highlight.def\r\n        (idle install dir)\/config-keys.def\r\n    user config  files\r\n        (user home dir)\/.idlerc\/config-main.cfg\r\n        (user home dir)\/.idlerc\/config-extensions.cfg\r\n        (user home dir)\/.idlerc\/config-highlight.cfg\r\n        (user home dir)\/.idlerc\/config-keys.cfg\r\n    \"\"\"\r\n    def __init__(self):\r\n        self.defaultCfg={}\r\n        self.userCfg={}\r\n        self.cfg={}\r\n        self.CreateConfigHandlers()\r\n        self.LoadCfgFiles()\r\n        #self.LoadCfg()\r\n\r\n    def CreateConfigHandlers(self):\r\n        \"\"\"\r\n        set up a dictionary of config parsers for default and user\r\n        configurations respectively\r\n        \"\"\"\r\n        #build idle install path\r\n        if __name__ != '__main__': # we were imported\r\n            idleDir=os.path.dirname(__file__)\r\n        else: # we were exec'ed (for testing only)\r\n            idleDir=os.path.abspath(sys.path[0])\r\n        userDir=self.GetUserCfgDir()\r\n        configTypes=('main','extensions','highlight','keys')\r\n        defCfgFiles={}\r\n        usrCfgFiles={}\r\n        for cfgType in configTypes: #build config file names\r\n            defCfgFiles[cfgType]=os.path.join(idleDir,'config-'+cfgType+'.def')\r\n            usrCfgFiles[cfgType]=os.path.join(userDir,'config-'+cfgType+'.cfg')\r\n        for cfgType in configTypes: #create config parsers\r\n            self.defaultCfg[cfgType]=IdleConfParser(defCfgFiles[cfgType])\r\n            self.userCfg[cfgType]=IdleUserConfParser(usrCfgFiles[cfgType])\r\n\r\n    def GetUserCfgDir(self):\r\n        \"\"\"\r\n        Creates (if required) and returns a filesystem directory for storing\r\n        user config files.\r\n        \"\"\"\r\n        cfgDir='.idlerc'\r\n        userDir=os.path.expanduser('~')\r\n        if userDir != '~': #'HOME' exists as a key in os.environ\r\n            if not os.path.exists(userDir):\r\n                warn=('\\n Warning: HOME environment variable points to\\n '+\r\n                        userDir+'\\n but the path does not exist.\\n')\r\n                sys.stderr.write(warn)\r\n                userDir='~'\r\n        if userDir=='~': #we still don't have a home directory\r\n            #traditionally idle has defaulted to os.getcwd(), is this adeqate?\r\n            userDir = os.getcwd() #hack for no real homedir\r\n        userDir=os.path.join(userDir,cfgDir)\r\n        if not os.path.exists(userDir):\r\n            try: #make the config dir if it doesn't exist yet\r\n                os.mkdir(userDir)\r\n            except IOError:\r\n                warn=('\\n Warning: unable to create user config directory\\n '+\r\n                        userDir+'\\n')\r\n                sys.stderr.write(warn)\r\n        return userDir\r\n\r\n    def GetOption(self, configType, section, option, default=None, type=None):\r\n        \"\"\"\r\n        Get an option value for given config type and given general\r\n        configuration section\/option or return a default. If type is specified,\r\n        return as type. Firstly the user configuration is checked, with a\r\n        fallback to the default configuration, and a final 'catch all'\r\n        fallback to a useable passed-in default if the option isn't present in\r\n        either the user or the default configuration.\r\n        configType must be one of ('main','extensions','highlight','keys')\r\n        If a default is returned a warning is printed to stderr.\r\n        \"\"\"\r\n        if self.userCfg[configType].has_option(section,option):\r\n            return self.userCfg[configType].Get(section, option, type=type)\r\n        elif self.defaultCfg[configType].has_option(section,option):\r\n            return self.defaultCfg[configType].Get(section, option, type=type)\r\n        else: #returning default, print warning\r\n            warning=('\\n Warning: configHandler.py - IdleConf.GetOption -\\n'+\r\n                       ' problem retrieving configration option '+`option`+'\\n'+\r\n                       ' from section '+`section`+'.\\n'+\r\n                       ' returning default value: '+`default`+'\\n')\r\n            sys.stderr.write(warning)\r\n            return default\r\n\r\n    def GetSectionList(self, configSet, configType):\r\n        \"\"\"\r\n        Get a list of sections from either the user or default config for\r\n        the given config type.\r\n        configSet must be either 'user' or 'default'\r\n        configType must be one of ('main','extensions','highlight','keys')\r\n        \"\"\"\r\n        if not (configType in ('main','extensions','highlight','keys')):\r\n            raise InvalidConfigType, 'Invalid configType specified'\r\n        if configSet == 'user':\r\n            cfgParser=self.userCfg[configType]\r\n        elif configSet == 'default':\r\n            cfgParser=self.defaultCfg[configType]\r\n        else:\r\n            raise InvalidConfigSet, 'Invalid configSet specified'\r\n        return cfgParser.sections()\r\n\r\n    def GetHighlight(self, theme, element, fgBg=None):\r\n        \"\"\"\r\n        return individual highlighting theme elements.\r\n        fgBg - string ('fg'or'bg') or None, if None return a dictionary\r\n        containing fg and bg colours (appropriate for passing to Tkinter in,\r\n        e.g., a tag_config call), otherwise fg or bg colour only as specified.\r\n        \"\"\"\r\n        if self.defaultCfg['highlight'].has_section(theme):\r\n            themeDict=self.GetThemeDict('default',theme)\r\n        else:\r\n            themeDict=self.GetThemeDict('user',theme)\r\n        fore=themeDict[element+'-foreground']\r\n        if element=='cursor': #there is no config value for cursor bg\r\n            back=themeDict['normal-background']\r\n        else:\r\n            back=themeDict[element+'-background']\r\n        highlight={\"foreground\": fore,\"background\": back}\r\n        if not fgBg: #return dict of both colours\r\n            return highlight\r\n        else: #return specified colour only\r\n            if fgBg == 'fg':\r\n                return highlight[\"foreground\"]\r\n            if fgBg == 'bg':\r\n                return highlight[\"background\"]\r\n            else:\r\n                raise InvalidFgBg, 'Invalid fgBg specified'\r\n\r\n    def GetThemeDict(self,type,themeName):\r\n        \"\"\"\r\n        type - string, 'default' or 'user' theme type\r\n        themeName - string, theme name\r\n        Returns a dictionary which holds {option:value} for each element\r\n        in the specified theme. Values are loaded over a set of ultimate last\r\n        fallback defaults to guarantee that all theme elements are present in\r\n        a newly created theme.\r\n        \"\"\"\r\n        if type == 'user':\r\n            cfgParser=self.userCfg['highlight']\r\n        elif type == 'default':\r\n            cfgParser=self.defaultCfg['highlight']\r\n        else:\r\n            raise InvalidTheme, 'Invalid theme type specified'\r\n        #foreground and background values are provded for each theme element\r\n        #(apart from cursor) even though all these values are not yet used\r\n        #by idle, to allow for their use in the future. Default values are\r\n        #generally black and white.\r\n        theme={ 'normal-foreground':'#000000',\r\n                'normal-background':'#ffffff',\r\n                'keyword-foreground':'#000000',\r\n                'keyword-background':'#ffffff',\r\n                'comment-foreground':'#000000',\r\n                'comment-background':'#ffffff',\r\n                'string-foreground':'#000000',\r\n                'string-background':'#ffffff',\r\n                'definition-foreground':'#000000',\r\n                'definition-background':'#ffffff',\r\n                'hilite-foreground':'#000000',\r\n                'hilite-background':'gray',\r\n                'break-foreground':'#ffffff',\r\n                'break-background':'#000000',\r\n                'hit-foreground':'#ffffff',\r\n                'hit-background':'#000000',\r\n                'error-foreground':'#ffffff',\r\n                'error-background':'#000000',\r\n                #cursor (only foreground can be set)\r\n                'cursor-foreground':'#000000',\r\n                #shell window\r\n                'stdout-foreground':'#000000',\r\n                'stdout-background':'#ffffff',\r\n                'stderr-foreground':'#000000',\r\n                'stderr-background':'#ffffff',\r\n                'console-foreground':'#000000',\r\n                'console-background':'#ffffff' }\r\n        for element in theme.keys():\r\n            if not cfgParser.has_option(themeName,element):\r\n                #we are going to return a default, print warning\r\n                warning=('\\n Warning: configHandler.py - IdleConf.GetThemeDict'+\r\n                           ' -\\n problem retrieving theme element '+`element`+\r\n                           '\\n from theme '+`themeName`+'.\\n'+\r\n                           ' returning default value: '+`theme[element]`+'\\n')\r\n                sys.stderr.write(warning)\r\n            colour=cfgParser.Get(themeName,element,default=theme[element])\r\n            theme[element]=colour\r\n        return theme\r\n\r\n    def CurrentTheme(self):\r\n        \"\"\"\r\n        Returns the name of the currently active theme\r\n        \"\"\"\r\n        return self.GetOption('main','Theme','name',default='')\r\n\r\n    def CurrentKeys(self):\r\n        \"\"\"\r\n        Returns the name of the currently active key set\r\n        \"\"\"\r\n        return self.GetOption('main','Keys','name',default='')\r\n\r\n    def GetExtensions(self, activeOnly=1):\r\n        \"\"\"\r\n        Gets a list of all idle extensions declared in the config files.\r\n        activeOnly - boolean, if true only return active (enabled) extensions\r\n        \"\"\"\r\n        extns=self.RemoveKeyBindNames(\r\n                self.GetSectionList('default','extensions'))\r\n        userExtns=self.RemoveKeyBindNames(\r\n                self.GetSectionList('user','extensions'))\r\n        for extn in userExtns:\r\n            if extn not in extns: #user has added own extension\r\n                extns.append(extn)\r\n        if activeOnly:\r\n            activeExtns=[]\r\n            for extn in extns:\r\n                if self.GetOption('extensions',extn,'enable',default=1,\r\n                    type='bool'):\r\n                    #the extension is enabled\r\n                    activeExtns.append(extn)\r\n            return activeExtns\r\n        else:\r\n            return extns\r\n\r\n    def RemoveKeyBindNames(self,extnNameList):\r\n        #get rid of keybinding section names\r\n        names=extnNameList\r\n        kbNameIndicies=[]\r\n        for name in names:\r\n            if name.endswith('_bindings') or name.endswith('_cfgBindings'):\r\n                kbNameIndicies.append(names.index(name))\r\n        kbNameIndicies.sort()\r\n        kbNameIndicies.reverse()\r\n        for index in kbNameIndicies: #delete each keybinding section name\r\n            del(names[index])\r\n        return names\r\n\r\n    def GetExtnNameForEvent(self,virtualEvent):\r\n        \"\"\"\r\n        Returns the name of the extension that virtualEvent is bound in, or\r\n        None if not bound in any extension.\r\n        virtualEvent - string, name of the virtual event to test for, without\r\n                       the enclosing '<< >>'\r\n        \"\"\"\r\n        extName=None\r\n        vEvent='<<'+virtualEvent+'>>'\r\n        for extn in self.GetExtensions(activeOnly=0):\r\n            for event in self.GetExtensionKeys(extn).keys():\r\n                if event == vEvent:\r\n                    extName=extn\r\n        return extName\r\n\r\n    def GetExtensionKeys(self,extensionName):\r\n        \"\"\"\r\n        returns a dictionary of the configurable keybindings for a particular\r\n        extension,as they exist in the dictionary returned by GetCurrentKeySet;\r\n        that is, where previously used bindings are disabled.\r\n        \"\"\"\r\n        keysName=extensionName+'_cfgBindings'\r\n        activeKeys=self.GetCurrentKeySet()\r\n        extKeys={}\r\n        if self.defaultCfg['extensions'].has_section(keysName):\r\n            eventNames=self.defaultCfg['extensions'].GetOptionList(keysName)\r\n            for eventName in eventNames:\r\n                event='<<'+eventName+'>>'\r\n                binding=activeKeys[event]\r\n                extKeys[event]=binding\r\n        return extKeys\r\n\r\n    def __GetRawExtensionKeys(self,extensionName):\r\n        \"\"\"\r\n        returns a dictionary of the configurable keybindings for a particular\r\n        extension, as defined in the configuration files, or an empty dictionary\r\n        if no bindings are found\r\n        \"\"\"\r\n        keysName=extensionName+'_cfgBindings'\r\n        extKeys={}\r\n        if self.defaultCfg['extensions'].has_section(keysName):\r\n            eventNames=self.defaultCfg['extensions'].GetOptionList(keysName)\r\n            for eventName in eventNames:\r\n                binding=self.GetOption('extensions',keysName,\r\n                        eventName,default='').split()\r\n                event='<<'+eventName+'>>'\r\n                extKeys[event]=binding\r\n        return extKeys\r\n\r\n    def GetExtensionBindings(self,extensionName):\r\n        \"\"\"\r\n        Returns a dictionary of all the event bindings for a particular\r\n        extension. The configurable keybindings are returned as they exist in\r\n        the dictionary returned by GetCurrentKeySet; that is, where re-used\r\n        keybindings are disabled.\r\n        \"\"\"\r\n        bindsName=extensionName+'_bindings'\r\n        extBinds=self.GetExtensionKeys(extensionName)\r\n        #add the non-configurable bindings\r\n        if self.defaultCfg['extensions'].has_section(bindsName):\r\n            eventNames=self.defaultCfg['extensions'].GetOptionList(bindsName)\r\n            for eventName in eventNames:\r\n                binding=self.GetOption('extensions',bindsName,\r\n                        eventName,default='').split()\r\n                event='<<'+eventName+'>>'\r\n                extBinds[event]=binding\r\n\r\n        return extBinds\r\n\r\n    def GetKeyBinding(self, keySetName, eventStr):\r\n        \"\"\"\r\n        returns the keybinding for a specific event.\r\n        keySetName - string, name of key binding set\r\n        eventStr - string, the virtual event we want the binding for,\r\n                   represented as a string, eg. '<<event>>'\r\n        \"\"\"\r\n        eventName=eventStr[2:-2] #trim off the angle brackets\r\n        binding=self.GetOption('keys',keySetName,eventName,default='').split()\r\n        return binding\r\n\r\n    def GetCurrentKeySet(self):\r\n        return self.GetKeySet(self.CurrentKeys())\r\n\r\n    def GetKeySet(self,keySetName):\r\n        \"\"\"\r\n        Returns a dictionary of: all requested core keybindings, plus the\r\n        keybindings for all currently active extensions. If a binding defined\r\n        in an extension is already in use, that binding is disabled.\r\n        \"\"\"\r\n        keySet=self.GetCoreKeys(keySetName)\r\n        activeExtns=self.GetExtensions(activeOnly=1)\r\n        for extn in activeExtns:\r\n            extKeys=self.__GetRawExtensionKeys(extn)\r\n            if extKeys: #the extension defines keybindings\r\n                for event in extKeys.keys():\r\n                    if extKeys[event] in keySet.values():\r\n                        #the binding is already in use\r\n                        extKeys[event]='' #disable this binding\r\n                    keySet[event]=extKeys[event] #add binding\r\n        return keySet\r\n\r\n    def IsCoreBinding(self,virtualEvent):\r\n        \"\"\"\r\n        returns true if the virtual event is bound in the core idle keybindings.\r\n        virtualEvent - string, name of the virtual event to test for, without\r\n                       the enclosing '<< >>'\r\n        \"\"\"\r\n        return ('<<'+virtualEvent+'>>') in self.GetCoreKeys().keys()\r\n\r\n    def GetCoreKeys(self, keySetName=None):\r\n        \"\"\"\r\n        returns the requested set of core keybindings, with fallbacks if\r\n        required.\r\n        Keybindings loaded from the config file(s) are loaded _over_ these\r\n        defaults, so if there is a problem getting any core binding there will\r\n        be an 'ultimate last resort fallback' to the CUA-ish bindings\r\n        defined here.\r\n        \"\"\"\r\n        keyBindings={\r\n            '<<copy>>': ['<Control-c>', '<Control-C>'],\r\n            '<<cut>>': ['<Control-x>', '<Control-X>'],\r\n            '<<paste>>': ['<Control-v>', '<Control-V>'],\r\n            '<<beginning-of-line>>': ['<Control-a>', '<Home>'],\r\n            '<<center-insert>>': ['<Control-l>'],\r\n            '<<close-all-windows>>': ['<Control-q>'],\r\n            '<<close-window>>': ['<Alt-F4>'],\r\n            '<<do-nothing>>': ['<Control-x>'],\r\n            '<<end-of-file>>': ['<Control-d>'],\r\n            '<<python-docs>>': ['<F1>'],\r\n            '<<python-context-help>>': ['<Shift-F1>'],\r\n            '<<history-next>>': ['<Alt-n>'],\r\n            '<<history-previous>>': ['<Alt-p>'],\r\n            '<<interrupt-execution>>': ['<Control-c>'],\r\n            '<<view-restart>>': ['<F6>'],\r\n            '<<restart-shell>>': ['<Control-F6>'],\r\n            '<<open-class-browser>>': ['<Alt-c>'],\r\n            '<<open-module>>': ['<Alt-m>'],\r\n            '<<open-new-window>>': ['<Control-n>'],\r\n            '<<open-window-from-file>>': ['<Control-o>'],\r\n            '<<plain-newline-and-indent>>': ['<Control-j>'],\r\n            '<<print-window>>': ['<Control-p>'],\r\n            '<<redo>>': ['<Control-y>'],\r\n            '<<remove-selection>>': ['<Escape>'],\r\n            '<<save-copy-of-window-as-file>>': ['<Alt-Shift-S>'],\r\n            '<<save-window-as-file>>': ['<Alt-s>'],\r\n            '<<save-window>>': ['<Control-s>'],\r\n            '<<select-all>>': ['<Alt-a>'],\r\n            '<<toggle-auto-coloring>>': ['<Control-slash>'],\r\n            '<<undo>>': ['<Control-z>'],\r\n            '<<find-again>>': ['<Control-g>', '<F3>'],\r\n            '<<find-in-files>>': ['<Alt-F3>'],\r\n            '<<find-selection>>': ['<Control-F3>'],\r\n            '<<find>>': ['<Control-f>'],\r\n            '<<replace>>': ['<Control-h>'],\r\n            '<<goto-line>>': ['<Alt-g>'],\r\n            '<<smart-backspace>>': ['<Key-BackSpace>'],\r\n            '<<newline-and-indent>>': ['<Key-Return> <Key-KP_Enter>'],\r\n            '<<smart-indent>>': ['<Key-Tab>'],\r\n            '<<indent-region>>': ['<Control-Key-bracketright>'],\r\n            '<<dedent-region>>': ['<Control-Key-bracketleft>'],\r\n            '<<comment-region>>': ['<Alt-Key-3>'],\r\n            '<<uncomment-region>>': ['<Alt-Key-4>'],\r\n            '<<tabify-region>>': ['<Alt-Key-5>'],\r\n            '<<untabify-region>>': ['<Alt-Key-6>'],\r\n            '<<toggle-tabs>>': ['<Alt-Key-t>'],\r\n            '<<change-indentwidth>>': ['<Alt-Key-u>']\r\n            }\r\n        if keySetName:\r\n            for event in keyBindings.keys():\r\n                binding=self.GetKeyBinding(keySetName,event)\r\n                if binding:\r\n                    keyBindings[event]=binding\r\n                else: #we are going to return a default, print warning\r\n                    warning=('\\n Warning: configHandler.py - IdleConf.GetCoreKeys'+\r\n                               ' -\\n problem retrieving key binding for event '+\r\n                               `event`+'\\n from key set '+`keySetName`+'.\\n'+\r\n                               ' returning default value: '+`keyBindings[event]`+'\\n')\r\n                    sys.stderr.write(warning)\r\n        return keyBindings\r\n\r\n    def GetExtraHelpSourceList(self,configSet):\r\n        \"\"\"Fetch list of extra help sources from a given configSet.\r\n\r\n        Valid configSets are 'user' or 'default'.  Return a list of tuples of\r\n        the form (menu_item , path_to_help_file , option), or return the empty\r\n        list.  'option' is the sequence number of the help resource.  'option'\r\n        values determine the position of the menu items on the Help menu,\r\n        therefore the returned list must be sorted by 'option'.\r\n\r\n        \"\"\"\r\n        helpSources=[]\r\n        if configSet=='user':\r\n            cfgParser=self.userCfg['main']\r\n        elif configSet=='default':\r\n            cfgParser=self.defaultCfg['main']\r\n        else:\r\n            raise InvalidConfigSet, 'Invalid configSet specified'\r\n        options=cfgParser.GetOptionList('HelpFiles')\r\n        for option in options:\r\n            value=cfgParser.Get('HelpFiles',option,default=';')\r\n            if value.find(';')==-1: #malformed config entry with no ';'\r\n                menuItem='' #make these empty\r\n                helpPath='' #so value won't be added to list\r\n            else: #config entry contains ';' as expected\r\n                value=string.split(value,';')\r\n                menuItem=value[0].strip()\r\n                helpPath=value[1].strip()\r\n            if menuItem and helpPath: #neither are empty strings\r\n                helpSources.append( (menuItem,helpPath,option) )\r\n        helpSources.sort(self.__helpsort)\r\n        return helpSources\r\n\r\n    def __helpsort(self, h1, h2):\r\n        if int(h1[2]) < int(h2[2]):\r\n            return -1\r\n        elif int(h1[2]) > int(h2[2]):\r\n            return 1\r\n        else:\r\n            return 0\r\n\r\n    def GetAllExtraHelpSourcesList(self):\r\n        \"\"\"\r\n        Returns a list of tuples containing the details of all additional help\r\n        sources configured, or an empty list if there are none. Tuples are of\r\n        the format returned by GetExtraHelpSourceList.\r\n        \"\"\"\r\n        allHelpSources=( self.GetExtraHelpSourceList('default')+\r\n                self.GetExtraHelpSourceList('user') )\r\n        return allHelpSources\r\n\r\n    def LoadCfgFiles(self):\r\n        \"\"\"\r\n        load all configuration files.\r\n        \"\"\"\r\n        for key in self.defaultCfg.keys():\r\n            self.defaultCfg[key].Load()\r\n            self.userCfg[key].Load() #same keys\r\n\r\n    def SaveUserCfgFiles(self):\r\n        \"\"\"\r\n        write all loaded user configuration files back to disk\r\n        \"\"\"\r\n        for key in self.userCfg.keys():\r\n            self.userCfg[key].Save()\r\n\r\nidleConf=IdleConf()\r\n\r\n### module test\r\nif __name__ == '__main__':\r\n    def dumpCfg(cfg):\r\n        print '\\n',cfg,'\\n'\r\n        for key in cfg.keys():\r\n            sections=cfg[key].sections()\r\n            print key\r\n            print sections\r\n            for section in sections:\r\n                options=cfg[key].options(section)\r\n                print section\r\n                print options\r\n                for option in options:\r\n                    print option, '=', cfg[key].Get(section,option)\r\n    dumpCfg(idleConf.defaultCfg)\r\n    dumpCfg(idleConf.userCfg)\r\n    print idleConf.userCfg['main'].Get('Theme','name')\r\n    #print idleConf.userCfg['highlight'].GetDefHighlight('Foo','normal')\r\n","label":0}
{"content":"from Plugins.Extensions.CutListEditor.plugin import CutListEditor\nfrom Components.ServiceEventTracker import ServiceEventTracker\nfrom enigma import iPlayableService, iServiceInformation\nfrom Tools.Directories import fileExists\n\nclass TitleCutter(CutListEditor):\n\tdef __init__(self, session, t):\n\t\tCutListEditor.__init__(self, session, t.source)\n\t\tself.skin = CutListEditor.skin\n\t\tself.session = session\n\t\tself.t = t\n\t\tself.__event_tracker = ServiceEventTracker(screen=self, eventmap=\n\t\t\t{\n\t\t\t\tiPlayableService.evUpdatedInfo: self.getPMTInfo,\n\t\t\t\tiPlayableService.evCuesheetChanged: self.refillList\n\t\t\t})\n\t\tself.onExecBegin.remove(self.showTutorial)\n\n\tdef getPMTInfo(self):\n\t\tservice = self.session.nav.getCurrentService()\n\t\taudio = service and service.audioTracks()\n\t\tn = audio and audio.getNumberOfTracks() or 0\n\t\tif n > 0:\n\t\t\tfrom Title import ConfigFixedText\n\t\t\tfrom Project import iso639language\n\t\t\tfrom Components.config import config, ConfigSubsection, ConfigSubList, ConfigSelection, ConfigYesNo\n\t\t\tself.t.properties.audiotracks = ConfigSubList()\n\t\t\tfor x in range(n):\n\t\t\t\ti = audio.getTrackInfo(x)\n\t\t\t\tDVB_lang = i.getLanguage()\n\t\t\t\tdescription = i.getDescription()\n\t\t\t\tpid = str(i.getPID())\n\t\t\t\tif description == \"MPEG\":\n\t\t\t\t\tdescription = \"MP2\"\n\t\t\t\tprint \"[audiotrack] pid:\", pid, \"description:\", description, \"language:\", DVB_lang, \"count:\", x, \"active:\", (x < 8)\n\t\t\t\tself.t.properties.audiotracks.append(ConfigSubsection())\n\t\t\t\tself.t.properties.audiotracks[-1].active = ConfigYesNo(default = (x < 8))\n\t\t\t\tself.t.properties.audiotracks[-1].format = ConfigFixedText(description)\n\t\t\t\tchoicelist = iso639language.getChoices()\n\t\t\t\tdetermined_language = iso639language.determineLanguage(DVB_lang)\n\t\t\t\tself.t.properties.audiotracks[-1].language = ConfigSelection(choices = choicelist, default=determined_language)\n\t\t\t\tself.t.properties.audiotracks[-1].pid = ConfigFixedText(pid)\n\t\t\t\tself.t.properties.audiotracks[-1].DVB_lang = ConfigFixedText(DVB_lang)\n\t\tsAspect = service.info().getInfo(iServiceInformation.sAspect)\n\t\tif sAspect in ( 1, 2, 5, 6, 9, 0xA, 0xD, 0xE ):\n\t\t\taspect = \"4:3\"\n\t\telse:\n\t\t\taspect = \"16:9\"\n\t\tself.t.properties.aspect.setValue(aspect)\n\t\tself.t.VideoType = service.info().getInfo(iServiceInformation.sVideoType)\n\t\tself.t.VideoPID = service.info().getInfo(iServiceInformation.sVideoPID)\n\t\txres = service.info().getInfo(iServiceInformation.sVideoWidth)\n\t\tyres = service.info().getInfo(iServiceInformation.sVideoHeight)\n\t\tself.t.resolution = (xres, yres)\n\t\tself.t.framerate = service.info().getInfo(iServiceInformation.sFrameRate)\n\t\tself.t.progressive = service.info().getInfo(iServiceInformation.sProgressive)\n\n\tdef checkAndGrabThumb(self):\n\t\tif not fileExists(self.t.inputfile.rsplit('.',1)[0] + \".png\"):\n\t\t\tCutListEditor.grabFrame(self)\n\n\tdef exit(self):\n\t\tif self.t.VideoType == -1:\n\t\t\tself.getPMTInfo()\n\t\tself.checkAndGrabThumb()\n\t\tself.session.nav.stopService()\n\t\tself.close(self.cut_list[:])\n\nclass CutlistReader(TitleCutter):\n\tskin = \"\"\"\n\t\t<screen position=\"0,0\" size=\"720,576\">\n\t\t<eLabel position=\"0,0\" size=\"720,576\" zPosition=\"1\" backgroundColor=\"#000000\" \/>\n\t\t<widget name=\"Video\" position=\"0,0\" size=\"100,75\" \/>\n\t\t<widget name=\"SeekState\" position=\"0,0\" \/>\n\t\t<widget source=\"cutlist\" position=\"0,0\" render=\"Listbox\" >\n\t\t\t<convert type=\"TemplatedMultiContent\">\n\t\t\t\t{\"template\": [\n\t\t\t\t\t\tMultiContentEntryText(text = 1),\n\t\t\t\t\t\tMultiContentEntryText(text = 2)\n\t\t\t\t\t],\n\t\t\t\t \"fonts\": [gFont(\"Regular\", 18)],\n\t\t\t\t \"itemHeight\": 20\n\t\t\t\t}\n\t\t\t<\/convert>\n\t\t<\/widget>\n\t\t<widget name=\"Timeline\" position=\"0,0\" \/>\n\t<\/screen>\"\"\"\n\n\tdef __init__(self, session, t):\n\t\tTitleCutter.__init__(self, session, t)\n\t\tself.skin = CutlistReader.skin\n\n\tdef getPMTInfo(self):\n\t\tTitleCutter.getPMTInfo(self)\n\t\tTitleCutter.checkAndGrabThumb(self)\n\t\tself.close(self.cut_list[:])\n","label":0}
{"content":"import unittest\nfrom ctypes import *\n\nclass StructFieldsTestCase(unittest.TestCase):\n    # Structure\/Union classes must get 'finalized' sooner or\n    # later, when one of these things happen:\n    #\n    # 1. _fields_ is set.\n    # 2. An instance is created.\n    # 3. The type is used as field of another Structure\/Union.\n    # 4. The type is subclassed\n    #\n    # When they are finalized, assigning _fields_ is no longer allowed.\n\n    def test_1_A(self):\n        class X(Structure):\n            pass\n        self.assertEqual(sizeof(X), 0) # not finalized\n        X._fields_ = [] # finalized\n        self.assertRaises(AttributeError, setattr, X, \"_fields_\", [])\n\n    def test_1_B(self):\n        class X(Structure):\n            _fields_ = [] # finalized\n        self.assertRaises(AttributeError, setattr, X, \"_fields_\", [])\n\n    def test_2(self):\n        class X(Structure):\n            pass\n        X()\n        self.assertRaises(AttributeError, setattr, X, \"_fields_\", [])\n\n    def test_3(self):\n        class X(Structure):\n            pass\n        class Y(Structure):\n            _fields_ = [(\"x\", X)] # finalizes X\n        self.assertRaises(AttributeError, setattr, X, \"_fields_\", [])\n\n    def test_4(self):\n        class X(Structure):\n            pass\n        class Y(X):\n            pass\n        self.assertRaises(AttributeError, setattr, X, \"_fields_\", [])\n        Y._fields_ = []\n        self.assertRaises(AttributeError, setattr, X, \"_fields_\", [])\n\nif __name__ == \"__main__\":\n    unittest.main()\n","label":0}
{"content":"# run doom process on a series of maps\n# can be used for regression testing, or to fetch media\n# keeps a log of each run ( see getLogfile )\n\n# currently uses a basic stdout activity timeout to decide when to move on\n# using a periodic check of \/proc\/<pid>\/status SleepAVG\n# when the sleep average is reaching 0, issue a 'quit' to stdout\n\n# keeps serialized run status in runner.pickle\n# NOTE: can be used to initiate runs on failed maps only for instance etc.\n\n# TODO: use the serialized and not the logs to sort the run order\n\n# TODO: better logging. Use idLogger?\n\n# TODO: configurable event when the process is found interactive\n# instead of emitting a quit, perform some warning action?\n\nimport sys, os, commands, string, time, traceback, pickle\n\nfrom twisted.application import internet, service\nfrom twisted.internet import protocol, reactor, utils, defer\nfrom twisted.internet.task import LoopingCall\n\nclass doomClientProtocol( protocol.ProcessProtocol ):\n\n\t# ProcessProtocol API\n\n\tdef connectionMade( self ):\n\t\tself.logfile.write( 'connectionMade\\n' )\n\t\t\n\tdef outReceived( self, data ):\n\t\tprint data\n\t\tself.logfile.write( data )\n\n\tdef errReceived( self, data ):\n\t\tprint 'stderr: ' + data\n\t\tself.logfile.write( 'stderr: ' + data )\n\t\t\n\tdef inConnectionLost( self ):\n\t\tself.logfile.write( 'inConnectionLost\\n' )\n\t\t\n\tdef outConnectionLost( self ):\n\t\tself.logfile.write( 'outConnectionLost\\n' )\n\t\t\n\tdef errConnectionLost( self ):\n\t\tself.logfile.write( 'errConnectionLost\\n' )\n\t\t\n\tdef processEnded( self, status_object ):\n\t\tself.logfile.write( 'processEnded %s\\n' % repr( status_object ) )\n\t\tself.logfile.write( time.strftime( '%H:%M:%S', time.localtime( time.time() ) ) + '\\n' )\n\t\tself.logfile.close()\n\t\tself.deferred.callback( None )\n\t\t\n\t# mac management\n\tdef __init__( self, logfilename, deferred ):\n\t\tself.logfilename = logfilename\n\t\tself.logfile = open( logfilename, 'a' )\n\t\tself.logfile.write( time.strftime( '%H:%M:%S', time.localtime( time.time() ) ) + '\\n' )\n\t\tself.deferred = deferred\n\nclass doomService( service.Service ):\n\n\t# current monitoring state\n\t# 0: nothing running\n\t# 1: we have a process running, we're monitoring it's CPU usage\n\t# 2: we issued a 'quit' to the process's stdin\n\t#   either going to get a processEnded, or a timeout\n\t# 3: we forced a kill because of error, timeout etc.\n\tstate = 0\n\n\t# load check period\n\tcheck_period = 10\n\n\t# pickled status file\n\tpickle_file = 'runner.pickle'\n\n\t# stores status indexed by filename\n\t# { 'mapname' : ( state, last_update ), .. }\n\tstatus = {}\n\n\t# start the maps as multiplayer server\n\tmultiplayer = 0\n\n\tdef __init__( self, bin, cmdline, maps, sort = 0, multiplayer = 0, blank_run = 0 ):\n\t\tself.p_transport = None\n\t\tself.multiplayer = multiplayer\n\t\tself.blank_run = blank_run\n\t\tif ( self.multiplayer ):\n\t\t\tprint 'Operate in multiplayer mode'\n\t\tself.bin = os.path.abspath( bin )\n\t\tif ( type( cmdline ) is type( '' ) ):\n\t\t\tself.cmdline = string.split( cmdline, ' ' )\n\t\telse:\n\t\t\tself.cmdline = cmdline\n\t\tself.maps = maps\n\t\tif ( os.path.exists( self.pickle_file ) ):\n\t\t\tprint 'Loading pickled status %s' % self.pickle_file\n\t\t\thandle = open( self.pickle_file, 'r' )\n\t\t\tself.status = pickle.load( handle )\n\t\t\thandle.close()\n\t\tif ( sort ):\n\t\t\tprint 'Sorting maps oldest runs first'\n\t\t\tmaps_sorted = [ ]\n\t\t\tfor i in self.maps:\n\t\t\t\ti_log = self.getLogfile( i )\n\t\t\t\tif ( os.path.exists( i_log ) ):\n\t\t\t\t\tmaps_sorted.append( ( i, os.path.getmtime( i_log ) ) )\n\t\t\t\telse:\n\t\t\t\t\tmaps_sorted.append( ( i, 0 ) )\n\t\t\tmaps_sorted.sort( lambda x,y : cmp( x[1], y[1] ) )\n\t\t\tself.maps = [ ]\n\t\t\tif ( blank_run ):\n\t\t\t\tself.maps.append( 'blankrun' )\n\t\t\tfor i in maps_sorted:\n\t\t\t\tself.maps.append( i[ 0 ] )\n\t\t\tprint 'Sorted as: %s\\n' % repr( self.maps )\n\n\tdef getLogfile( self, name ):\n\t\treturn 'logs\/' + string.translate( name, string.maketrans( '\/', '-' ) ) + '.log'\n\n\t# deferred call when child process dies\n\tdef processEnded( self, val ):\n\t\tprint 'child has died - state %d' % self.state\n\t\tself.status[ self.maps[ self.i_map ] ] = ( self.state, time.time() )\n\t\tself.i_map += 1\n\t\tif ( self.i_map >= len( self.maps ) ):\n\t\t\treactor.stop()\n\t\telse:\n\t\t\tself.nextMap()\n\n\tdef processTimeout( self ):\n\t\tself.p_transport.signalProcess( \"KILL\" )\n\n\tdef sleepAVGReply( self, val ):\n\t\ttry:\n\t\t\ts = val[10:][:-2]\n\t\t\tprint 'sleepAVGReply %s%%' % s\n\t\t\tif ( s == '0' ):\n\t\t\t\t# need twice in a row\n\t\t\t\tif ( self.state == 2 ):\t\t\t\t\t\n\t\t\t\t\tprint 'child process is interactive'\n\t\t\t\t\tself.p_transport.write( 'quit\\n' )\n\t\t\t\telse:\n\t\t\t\t\tself.state = 2\n\t\t\telse:\n\t\t\t\tself.state = 1\n#\t\t\telse:\n#\t\t\t\treactor.callLater( self.check_period, self.checkCPU )\n\t\texcept:\n\t\t\tprint traceback.format_tb( sys.exc_info()[2] )\n\t\t\tprint sys.exc_info()[0]\n\t\t\tprint 'exception raised in sleepAVGReply - killing process'\n\t\t\tself.state = 3\n\t\t\tself.p_transport.signalProcess( 'KILL' )\n\n\tdef sleepAVGTimeout( self ):\n\t\tprint 'sleepAVGTimeout - killing process'\n\t\tself.state = 3\n\t\tself.p_transport.signalProcess( 'KILL' )\n\n\t# called at regular intervals to monitor the sleep average of the child process\n\t# when sleep reaches 0, it means the map is loaded and interactive\n\tdef checkCPU( self ):\n\t\tif ( self.state == 0 or self.p_transport is None or self.p_transport.pid is None ):\n\t\t\tprint 'checkCPU: no child process atm'\n\t\t\treturn\n\t\tdefer = utils.getProcessOutput( '\/bin\/bash', [ '-c', 'cat \/proc\/%d\/status | grep SleepAVG' % self.p_transport.pid ] )\n\t\tdefer.addCallback( self.sleepAVGReply )\n\t\tdefer.setTimeout( 2, self.sleepAVGTimeout )\t\t\n\n\tdef nextMap( self ):\n\t\tself.state = 0\n\t\tname = self.maps[ self.i_map ]\n\t\tprint 'Starting map: ' + name\n\t\tlogfile = self.getLogfile( name )\n\t\tprint 'Logging to: ' + logfile\n\t\tif ( self.multiplayer ):\n\t\t\tcmdline = [ self.bin ] + self.cmdline + [ '+set', 'si_map', name ]\n\t\t\tif ( name != 'blankrun' ):\n\t\t\t\tcmdline.append( '+spawnServer' )\n\t\telse:\n\t\t\tcmdline = [ self.bin ] + self.cmdline\n\t\t\tif ( name != 'blankrun' ):\n\t\t\t\tcmdline += [ '+devmap', name ]\n\t\tprint 'Command line: ' + repr( cmdline )\t\t\n\t\tself.deferred = defer.Deferred()\n\t\tself.deferred.addCallback( self.processEnded )\n\t\tself.p_transport = reactor.spawnProcess( doomClientProtocol( logfile, self.deferred ), self.bin, cmdline , path = os.path.dirname( self.bin ), env = os.environ )\n\t\tself.state = 1\n#\t\t# setup the CPU usage loop\n#\t\treactor.callLater( self.check_period, self.checkCPU )\n\n\tdef startService( self ):\n\t\tprint 'doomService startService'\n\t\tloop = LoopingCall( self.checkCPU )\n\t\tloop.start( self.check_period )\n\t\tself.i_map = 0\n\t\tself.nextMap()\n\n\tdef stopService( self ):\n\t\tprint 'doomService stopService'\n\t\tif ( not self.p_transport.pid is None ):\t\t\t\n\t\t\tself.p_transport.signalProcess( 'KILL' )\n\t\t# serialize\n\t\tprint 'saving status to %s' % self.pickle_file\n\t\thandle = open( self.pickle_file, 'w+' )\n\t\tpickle.dump( self.status, handle )\n\t\thandle.close()\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n\nimport inspect\nimport re\nimport urlparse\n\nfrom module.network.HTTPRequest import BadHeader\n\nfrom ..captcha.ReCaptcha import ReCaptcha\nfrom ..internal.Addon import Addon\nfrom ..internal.misc import parse_html_header\n\n\ndef plugin_id(plugin):\n    return (\"<%(plugintype)s %(pluginname)s%(id)s>\" %\n            {'plugintype': plugin.__type__.upper(),\n             'pluginname': plugin.__name__,\n             'id': \"[%s]\" % plugin.pyfile.id if plugin.pyfile else \"\"})\n\n\ndef is_simple_plugin(obj):\n    return any(k.__name__ in (\"SimpleHoster\", \"SimpleCrypter\")\n               for k in inspect.getmro(type(obj)))\n\n\ndef get_plugin_last_header(plugin):\n    # @NOTE: req can be a HTTPRequest or a Browser object\n    return plugin.req.http.header if hasattr(plugin.req, \"http\") else plugin.req.header\n\n\nclass CloudFlare(object):\n\n    @staticmethod\n    def handle_function(addon_plugin, owner_plugin, func_name, orig_func, args):\n        addon_plugin.log_debug(\"Calling %s() of %s\" % (func_name, plugin_id(owner_plugin)))\n\n        try:\n            data = orig_func(*args[0], **args[1])\n            addon_plugin.log_debug(\"%s() returned successfully\" % func_name)\n            return data\n\n        except BadHeader, e:\n            addon_plugin.log_debug(\"%s(): got BadHeader exception %s\" % (func_name, e.code))\n\n            header = parse_html_header(e.header)\n\n            if \"cloudflare\" in header.get('server', \"\"):\n                if e.code == 403:\n                    data = CloudFlare._solve_cf_security_check(addon_plugin, owner_plugin, e.content)\n\n                elif e.code == 503:\n                    for _i in range(3):\n                        try:\n                            data = CloudFlare._solve_cf_ddos_challenge(addon_plugin, owner_plugin, e.content)\n                            break\n\n                        except BadHeader, e:  #: Possibly we got another ddos challenge\n                            addon_plugin.log_debug(\"%s(): got BadHeader exception %s\" % (func_name, e.code))\n\n                            header = parse_html_header(e.header)\n\n                            if e.code == 503 and \"cloudflare\" in header.get('server', \"\"):\n                                continue  #: Yes, it's a ddos challenge again..\n\n                            else:\n                                data = None  # Tell the exception handler to re-throw the exception\n                                break\n\n                    else:\n                        addon_plugin.log_error(\"%s(): Max solve retries reached\" % func_name)\n                        data = None  # Tell the exception handler to re-throw the exception\n\n                else:\n                    addon_plugin.log_warning(_(\"Unknown CloudFlare response code %s\") % e.code)\n                    raise\n\n                if data is None:\n                    raise e\n\n                else:\n                    return data\n\n            else:\n                raise\n\n    @staticmethod\n    def _solve_cf_ddos_challenge(addon_plugin, owner_plugin, data):\n        try:\n            addon_plugin.log_info(_(\"Detected CloudFlare's DDoS protection page\"))\n            # Cloudflare requires a delay before solving the challenge\n            wait_time = (int(re.search('submit\\(\\);\\r?\\n\\s*},\\s*([0-9]+)', data).group(1)) + 999) \/ 1000\n            owner_plugin.set_wait(wait_time)\n\n            last_url = owner_plugin.req.lastEffectiveURL\n            urlp = urlparse.urlparse(last_url)\n            domain = urlp.netloc\n            submit_url = \"%s:\/\/%s\/cdn-cgi\/l\/chk_jschl\" % (urlp.scheme, domain)\n\n            get_params = {}\n\n            try:\n                get_params['jschl_vc'] = re.search(r'name=\"jschl_vc\" value=\"(\\w+)\"', data).group(1)\n                get_params['pass'] = re.search(r'name=\"pass\" value=\"(.+?)\"', data).group(1)\n                get_params['s'] = re.search(r'name=\"s\" value=\"(.+?)\"', data).group(1)\n\n                # Extract the arithmetic operation\n                js = re.search(r'setTimeout\\(function\\(\\){\\s+(var s,t,o,p,b,r,e,a,k,i,n,g,f.+?\\r?\\n[\\s\\S]+?a\\.value =.+?)\\r?\\n',\n                               data).group(1)\n                js = re.sub(r'a\\.value = (.+\\.toFixed\\(10\\);).+', r'\\1', js)\n\n                solution_name = re.search(r's,t,o,p,b,r,e,a,k,i,n,g,f,\\s*(.+)\\s*=', js).group(1)\n                g = re.search(r'(.*};)\\n\\s*(t\\s*=(.+))\\n\\s*(;%s.*)' % (solution_name), js, re.M | re.I | re.S).groups()\n                js = g[0] + g[-1]\n                js = re.sub(r\"[\\n\\\\']\", \"\", js)\n\n            except Exception:\n                # Something is wrong with the page.\n                # This may indicate CloudFlare has changed their anti-bot\n                # technique.\n                owner_plugin.log_error(_(\"Unable to parse CloudFlare's DDoS protection page\"))\n                return None  # Tell the exception handler to re-throw the exception\n\n            if \"toFixed\" not in js:\n                owner_plugin.log_error(_(\"Unable to parse CloudFlare's DDoS protection page\"))\n                return None  # Tell the exception handler to re-throw the exception\n\n            atob = 'var atob = function(str) {return Buffer.from(str, \"base64\").toString(\"binary\");}'\n            try:\n                k = re.search(r'k\\s*=\\s*\\'(.+?)\\';', data).group(1)\n                v = re.search(r'<div(?:.*)id=\"%s\"(?:.*)>(.*)<\/div>' % k, data).group(1)\n                doc = 'var document= {getElementById: function(x) { return {innerHTML:\"%s\"};}}' % v\n            except (AttributeError, IndexError):\n                doc = ''\n            js = '%s;%s;var t=\"%s\";%s' % (doc, atob, domain, js)\n\n            # Safely evaluate the Javascript expression\n            res = owner_plugin.js.eval(js)\n\n            try:\n                get_params['jschl_answer'] = str(float(res))\n\n            except ValueError:\n                owner_plugin.log_error(_(\"Unable to parse CloudFlare's DDoS protection page\"))\n                return None  # Tell the exception handler to re-throw the exception\n\n            owner_plugin.wait()  # Do the actual wait\n\n            return owner_plugin.load(submit_url,\n                                     get=get_params,\n                                     ref=last_url)\n\n        except BadHeader, e:\n            raise e  #: Huston, we have a BadHeader!\n\n        except Exception, e:\n            addon_plugin.log_error(e)\n            return None  # Tell the exception handler to re-throw the exception\n\n    @staticmethod\n    def _solve_cf_security_check(addon_plugin, owner_plugin, data):\n        try:\n            last_url = owner_plugin.req.lastEffectiveURL\n\n            captcha = ReCaptcha(owner_plugin.pyfile)\n\n            captcha_key = captcha.detect_key(data)\n            if captcha_key:\n                addon_plugin.log_info(_(\"Detected CloudFlare's security check page\"))\n\n                response, challenge = captcha.challenge(captcha_key, data)\n                return owner_plugin.load(owner_plugin.fixurl(\"\/cdn-cgi\/l\/chk_captcha\"),\n                                         get={'g-recaptcha-response': response},\n                                         ref=last_url)\n\n            else:\n                addon_plugin.log_warning(_(\"Got unexpected CloudFlare html page\"))\n                return None  # Tell the exception handler to re-throw the exception\n\n        except Exception, e:\n            addon_plugin.log_error(e)\n            return None  # Tell the exception handler to re-throw the exception\n\n\nclass PreloadStub(object):\n\n    def __init__(self, addon_plugin, owner_plugin):\n        self.addon_plugin = addon_plugin\n        self.owner_plugin = owner_plugin\n        self.old_preload = owner_plugin._preload\n\n    def my_preload(self, *args, **kwargs):\n        data = CloudFlare.handle_function(self.addon_plugin, self.owner_plugin, \"_preload\", self.old_preload, (args, kwargs))\n        if data is not None:\n            self.owner_plugin.data = data\n\n    def __repr__(self):\n        return \"<PreloadStub object at %s>\" % hex(id(self))\n\n\nclass CloudFlareDdos(Addon):\n    __name__ = \"CloudFlareDdos\"\n    __type__ = \"hook\"\n    __version__ = \"0.16\"\n    __status__ = \"testing\"\n\n    __config__ = [(\"activated\", \"bool\", \"Activated\", False)]\n\n    __description__ = \"\"\"CloudFlare DDoS protection support\"\"\"\n    __license__ = \"GPLv3\"\n    __authors__ = [(\"GammaC0de\", \"nitzo2001[AT]yahoo[DOT]com\")]\n\n    def activate(self):\n        self.stubs = {}\n        self._override_get_url()\n\n    def deactivate(self):\n        while len(self.stubs):\n            stub = next(self.stubs.itervalues())\n            self._unoverride_preload(stub.owner_plugin)\n\n        self._unoverride_get_url()\n\n    def _unoverride_preload(self, plugin):\n        if id(plugin) in self.stubs:\n            self.log_debug(\"Unoverriding _preload() for %s\" % plugin_id(plugin))\n\n            stub = self.stubs.pop(id(plugin))\n            stub.owner_plugin._preload = stub.old_preload\n\n        else:\n            self.log_warning(_(\"No _preload() override found for %s, cannot un-override>\") %\n                plugin_id(plugin))\n\n    def _override_preload(self, plugin):\n        if id(plugin) not in self.stubs:\n            stub = PreloadStub(self, plugin)\n            self.stubs[id(plugin)] = stub\n\n            self.log_debug(\"Overriding _preload() for %s\" % plugin_id(plugin))\n            plugin._preload = stub.my_preload\n\n        else:\n            self.log_warning(_(\"Already overrided _preload() for %s\") % plugin_id(plugin))\n\n    def _override_get_url(self):\n        self.log_debug(\"Overriding get_url()\")\n\n        self.old_get_url = self.pyload.requestFactory.getURL\n        self.pyload.requestFactory.getURL = self.my_get_url\n\n    def _unoverride_get_url(self):\n        self.log_debug(\"Unoverriding get_url()\")\n\n        self.pyload.requestFactory.getURL = self.old_get_url\n\n    def _find_owner_plugin(self):\n        \"\"\"\n        Walk the callstack until we find SimpleHoster or SimpleCrypter class\n        Dirty but works.\n        \"\"\"\n        f = frame = inspect.currentframe()\n        try:\n            while True:\n                if f is None:\n                    return None\n\n                elif 'self' in f.f_locals and is_simple_plugin(f.f_locals['self']):\n                    return f.f_locals['self']\n\n                else:\n                    f = f.f_back\n\n        finally:\n            del frame\n\n    def download_preparing(self, pyfile):\n        #: Only SimpleHoster and SimpleCrypter based plugins are supported\n        if not is_simple_plugin(pyfile.plugin):\n            self.log_debug(\"Skipping plugin %s\" % plugin_id(pyfile.plugin))\n            return\n\n        attr = getattr(pyfile.plugin, \"_preload\", None)\n        if not attr and not callable(attr):\n            self.log_error(_(\"%s is missing _preload() function, cannot override!\") % plugin_id(pyfile.plugin))\n            return\n\n        self._override_preload(pyfile.plugin)\n\n    def download_processed(self, pyfile):\n        if id(pyfile.plugin) in self.stubs:\n            self._unoverride_preload(pyfile.plugin)\n\n    def my_get_url(self, *args, **kwargs):\n        owner_plugin = self._find_owner_plugin()\n        if owner_plugin is None:\n            self.log_warning(_(\"Owner plugin not found, cannot process\"))\n            return self.old_get_url(*args, **kwargs)\n\n        else:\n            #@NOTE: Better use owner_plugin.load() instead of get_url() so cookies are saved and so captcha credits\n            #@NOTE: Also that way we can use 'owner_plugin.req.header' to get the headers, otherwise we cannot get them\n            res = CloudFlare.handle_function(self, owner_plugin, \"get_url\", owner_plugin.load, (args, kwargs))\n            if kwargs.get('just_header', False):\n                # @NOTE: SimpleHoster\/SimpleCrypter returns a dict while get_url() returns raw headers string,\n                # make sure we return a string for get_url('just_header'=True)\n                res = get_plugin_last_header(owner_plugin)\n\n            return res\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n\"\"\" Tests for student profile views. \"\"\"\n\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.test import TestCase\nfrom django.test.client import RequestFactory\n\nfrom util.testing import UrlResetMixin\nfrom student.tests.factories import UserFactory\n\nfrom student_profile.views import learner_profile_context\n\n\nclass LearnerProfileViewTest(UrlResetMixin, TestCase):\n    \"\"\" Tests for the student profile view. \"\"\"\n\n    USERNAME = \"username\"\n    PASSWORD = \"password\"\n    CONTEXT_DATA = [\n        'default_public_account_fields',\n        'accounts_api_url',\n        'preferences_api_url',\n        'account_settings_page_url',\n        'has_preferences_access',\n        'own_profile',\n        'country_options',\n        'language_options',\n        'account_settings_data',\n        'preferences_data',\n    ]\n\n    def setUp(self):\n        super(LearnerProfileViewTest, self).setUp()\n        self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD)\n        self.client.login(username=self.USERNAME, password=self.PASSWORD)\n\n    def test_context(self):\n        \"\"\"\n        Verify learner profile page context data.\n        \"\"\"\n        request = RequestFactory().get('\/url')\n        request.user = self.user\n\n        context = learner_profile_context(request, self.USERNAME, self.user.is_staff)\n\n        self.assertEqual(\n            context['data']['default_public_account_fields'],\n            settings.ACCOUNT_VISIBILITY_CONFIGURATION['public_fields']\n        )\n\n        self.assertEqual(\n            context['data']['accounts_api_url'],\n            reverse(\"accounts_api\", kwargs={'username': self.user.username})\n        )\n\n        self.assertEqual(\n            context['data']['preferences_api_url'],\n            reverse('preferences_api', kwargs={'username': self.user.username})\n        )\n\n        self.assertEqual(\n            context['data']['profile_image_upload_url'],\n            reverse(\"profile_image_upload\", kwargs={'username': self.user.username})\n        )\n\n        self.assertEqual(\n            context['data']['profile_image_remove_url'],\n            reverse('profile_image_remove', kwargs={'username': self.user.username})\n        )\n\n        self.assertEqual(\n            context['data']['profile_image_max_bytes'],\n            settings.PROFILE_IMAGE_MAX_BYTES\n        )\n\n        self.assertEqual(\n            context['data']['profile_image_min_bytes'],\n            settings.PROFILE_IMAGE_MIN_BYTES\n        )\n\n        self.assertEqual(context['data']['account_settings_page_url'], reverse('account_settings'))\n\n        for attribute in self.CONTEXT_DATA:\n            self.assertIn(attribute, context['data'])\n\n    def test_view(self):\n        \"\"\"\n        Verify learner profile page view.\n        \"\"\"\n        profile_path = reverse('learner_profile', kwargs={'username': self.USERNAME})\n        response = self.client.get(path=profile_path)\n\n        for attribute in self.CONTEXT_DATA:\n            self.assertIn(attribute, response.content)\n\n    def test_undefined_profile_page(self):\n        \"\"\"\n        Verify that a 404 is returned for a non-existent profile page.\n        \"\"\"\n        profile_path = reverse('learner_profile', kwargs={'username': \"no_such_user\"})\n        response = self.client.get(path=profile_path)\n        self.assertEqual(404, response.status_code)\n","label":0}
{"content":"# encoding: utf-8\n# module PyKDE4.kdeui\n# from \/usr\/lib\/python3\/dist-packages\/PyKDE4\/kdeui.cpython-34m-x86_64-linux-gnu.so\n# by generator 1.135\n# no doc\n\n# imports\nimport PyKDE4.kdecore as __PyKDE4_kdecore\nimport PyQt4.QtCore as __PyQt4_QtCore\nimport PyQt4.QtGui as __PyQt4_QtGui\nimport PyQt4.QtSvg as __PyQt4_QtSvg\n\n\nclass KShortcutWidget(__PyQt4_QtGui.QWidget):\n    # no doc\n    def applyStealShortcut(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def clearShortcut(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def isModifierlessAllowed(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def setCheckActionCollections(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def setCheckActionList(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def setClearButtonsShown(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def setModifierlessAllowed(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def setShortcut(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def shortcutChanged(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def __init__(self, *args, **kwargs): # real signature unknown\n        pass\n\n\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2011\n#     Per Ove Ringdal\n#\n# Copyright (C) 2004\n#     Wido Depping, <widod@users.sourceforge.net>\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses\/\nimport os.path\nimport copy\n\nimport PyQt4\nfrom PyQt4.QtCore import QString, pyqtSlot\nfrom PyQt4.QtGui import QWizard\nfrom .gui.AddAttributeWizardDesign import Ui_AddAttributeWizardDesign\nfrom base.backend.ObjectClassAttributeInfo import ObjectClassAttributeInfo\nfrom base.util.IconTheme import pixmapFromTheme\n\nclass AddAttributeWizard(QWizard, Ui_AddAttributeWizardDesign):\n\n    def __init__(self, parent = None, flags = PyQt4.QtCore.Qt.Widget):\n        QWizard.__init__(self, parent, flags)\n        self.setupUi(self)\n        # need to initialize the pages before connecting signals\n        self.restart()\n\n        attributePixmap = pixmapFromTheme(\n            \"addattribute\", \":\/icons\/64\/add-attribute\")\n        objectclassPixmap = pixmapFromTheme(\n            \"objectclass\", \":\/icons\/64\/objectclass\")\n        self.imageLabel.setPixmap(attributePixmap)\n        self.objectclassLabel.setPixmap(objectclassPixmap)\n\n        self.enableAllBox.toggled.connect(self.initAttributeBox)\n        self.attributeBox.activated[str].connect(self.newSelection)\n        self.classBox.itemSelectionChanged.connect(self.classSelection)\n\n        # attribute values of the current ldap object\n        self.OBJECTVALUES = None\n\n        # schema information for the ldap server\n        self.SCHEMAINFO = None\n\n        # set of attributes which are possible with the current objectclasses\n        self.possibleAttributes = None\n\n        # set of all attributes which are supported by the server\n        self.allPossibleAttributes = None\n\n###############################################################################\n\n    def setData(self, smartObject):\n        \"\"\" Sets the current object data, schema information and initializes\n        the attribute box and wizard buttons.\n        \"\"\"\n\n        self.smartObject = smartObject\n\n        self.SCHEMAINFO = ObjectClassAttributeInfo(self.smartObject.getServerMeta())\n        self.processData()\n        self.initAttributeBox()\n\n        currentPageWidget = self.page(0)\n        #self.button(QWizard.FinishButton).setDisabled(False)\n        #self.button(QWizard.NextButton).setDisabled(True)\n\n###############################################################################\n\n    def processData(self):\n        \"\"\" Compute all attributes which can be added according to the data of\n        the object. Single values which are already given are sorted out.\n        \"\"\"\n\n        possibleMust, possibleMay = self.smartObject.getPossibleAttributes()\n\n        # attributes used by the current objectClass\n        #usedAttributes = set(objectAttributes).difference(set(['objectClass']))\n        usedAttributes = self.smartObject.getAttributeList()\n\n        # set of attribute which are used and have to be single\n        singleAttributes = set(filter(self.SCHEMAINFO.isSingle, usedAttributes))\n\n        # create a set of attributes which may be added\n        self.possibleAttributes = (possibleMust.union(possibleMay)).difference(singleAttributes)\n        self.possibleAttributes = map(lambda x: x.lower(), self.possibleAttributes)\n\n        # create a set of attributes which are supported by the server\n        self.allPossibleAttributes = set(self.SCHEMAINFO.attributeDict.keys()).difference(singleAttributes)\n\n###############################################################################\n\n    def initAttributeBox(self):\n        self.attributeBox.clear()\n\n        currentPageWidget = self.currentPage()\n\n        showAll = self.enableAllBox.isChecked()\n        currentPageWidget.setFinalPage(True)\n        currentPageWidget.setCommitPage(False)\n        #self.button(QWizard.FinishButton).setDisabled(False)\n\n        tmpList = None\n        if showAll:\n            tmpList = copy.deepcopy(self.allPossibleAttributes)\n        else:\n            tmpList = copy.deepcopy(self.possibleAttributes)\n\n        structuralClass = self.smartObject.getStructuralClasses()\n\n        # only show attributes whose objectclass combinations don't violate\n        # the objectclass chain (not two structural classes)\n        if len(structuralClass) > 0:\n            classList = filter(lambda x: not self.SCHEMAINFO.isStructural(x), self.SCHEMAINFO.getObjectClasses())\n            for x in structuralClass:\n                classList += self.SCHEMAINFO.getParents(x)\n\n            for x in self.smartObject.getObjectClasses():\n                if not (x in classList):\n                    classList.append(x)\n\n            mustAttributes, mayAttributes = self.SCHEMAINFO.getAllAttributes(classList)\n            attributeList = mustAttributes.union(mayAttributes)\n\n            cleanList = filter(lambda x: x.lower() in tmpList, attributeList)\n            tmpList = cleanList\n        else:\n            self.enableAllBox.setChecked(True)\n            self.enableAllBox.setEnabled(False)\n            tmpList = sorted(self.allPossibleAttributes)\n        tmpList.sort()\n        tmpList = filter(lambda x: not (x.lower() == \"objectclass\"), tmpList)\n        map(self.attributeBox.addItem, tmpList)\n\n        self.newSelection(self.attributeBox.currentText())\n\n\n###############################################################################\n\n    @pyqtSlot(int)\n    def newSelection(self, attribute):\n        pass\n\n    @pyqtSlot(\"QString\")\n    def newSelection(self, attribute):\n        attribute = str(attribute).lower()\n\n        currentPageWidget = self.currentPage()\n\n        mustSet, maySet = self.SCHEMAINFO.getAllObjectclassesForAttr(attribute)\n        tmpSet = mustSet.union(maySet)\n\n        if (attribute in self.possibleAttributes) or (len(tmpSet) == 0):\n            currentPageWidget.setFinalPage(True)\n            #self.button(QWizard.FinishButton).setDisabled(False)\n            self.button(QWizard.NextButton).setDisabled(True)\n        else:\n            currentPageWidget.setFinalPage(False)\n            #self.button(QWizard.FinishButton).setDisabled(True)\n            self.button(QWizard.NextButton).setDisabled(False)\n\n###############################################################################\n\n    def initClassPage(self):\n        currentPageWidget = self.currentPage()\n        #self.button(QWizard.FinishButton).setDisabled(True)\n\n        self.classBox.clear()\n        self.mustAttributeBox.clear()\n\n        attribute = str(self.attributeBox.currentText())\n        mustSet, maySet = self.SCHEMAINFO.getAllObjectclassesForAttr(attribute)\n        classList = mustSet.union(maySet)\n\n        if self.smartObject.hasStructuralClass():\n            structList = filter(lambda x: self.SCHEMAINFO.isStructural(x), classList)\n            classList = filter(lambda x: not self.SCHEMAINFO.isStructural(x), classList)\n\n            for x in structList:\n                for y in self.smartObject.getObjectClasses():\n                    if self.SCHEMAINFO.sameObjectClassChain(x, y):\n                        classList.append(x)\n        else:\n            classList = sorted(classList)\n\n\n        classList.sort()\n\n        map(self.classBox.addItem, classList)\n        self.classBox.setCurrentRow(0)\n\n###############################################################################\n\n    def classSelection(self):\n        self.mustAttributeBox.clear()\n\n        objectclass = str(self.classBox.currentItem().text())\n\n        mustAttributes = self.SCHEMAINFO.getAllMusts([objectclass])\n\n        attribute = set([str(self.attributeBox.currentText())])\n\n        map(self.mustAttributeBox.addItem, mustAttributes.difference(attribute))\n\n        currentPageWidget = self.currentPage()\n        #self.button(QWizard.FinishButton).setDisabled(False)\n\n###############################################################################\n\n    def initializePage(self, id):\n        if id == 1:\n            self.initClassPage()\n\n# vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4\n","label":0}
{"content":"# Copyright (C) 2003-2007  Robey Pointer <robey@lag.net>\n#\n# This file is part of paramiko.\n#\n# Paramiko is free software; you can redistribute it and\/or modify it under the\n# terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation; either version 2.1 of the License, or (at your option)\n# any later version.\n#\n# Paramiko is distrubuted in the hope that it will be useful, but WITHOUT ANY\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\n# A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with Paramiko; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA.\n\n\nimport socket\nimport sys\n\n# windows does not have termios...\ntry:\n    import termios\n    import tty\n    has_termios = True\nexcept ImportError:\n    has_termios = False\n\n\ndef interactive_shell(chan):\n    if has_termios:\n        posix_shell(chan)\n    else:\n        windows_shell(chan)\n\n\ndef posix_shell(chan):\n    import select\n    \n    oldtty = termios.tcgetattr(sys.stdin)\n    try:\n        tty.setraw(sys.stdin.fileno())\n        tty.setcbreak(sys.stdin.fileno())\n        chan.settimeout(0.0)\n\n        while True:\n            r, w, e = select.select([chan, sys.stdin], [], [])\n            if chan in r:\n                try:\n                    x = chan.recv(1024)\n                    if len(x) == 0:\n                        print '\\r\\n*** EOF\\r\\n',\n                        break\n                    sys.stdout.write(x)\n                    sys.stdout.flush()\n                except socket.timeout:\n                    pass\n            if sys.stdin in r:\n                x = sys.stdin.read(1)\n                if len(x) == 0:\n                    break\n                chan.send(x)\n\n    finally:\n        termios.tcsetattr(sys.stdin, termios.TCSADRAIN, oldtty)\n\n    \n# thanks to Mike Looijmans for this code\ndef windows_shell(chan):\n    import threading\n\n    sys.stdout.write(\"Line-buffered terminal emulation. Press F6 or ^Z to send EOF.\\r\\n\\r\\n\")\n        \n    def writeall(sock):\n        while True:\n            data = sock.recv(256)\n            if not data:\n                sys.stdout.write('\\r\\n*** EOF ***\\r\\n\\r\\n')\n                sys.stdout.flush()\n                break\n            sys.stdout.write(data)\n            sys.stdout.flush()\n        \n    writer = threading.Thread(target=writeall, args=(chan,))\n    writer.start()\n        \n    try:\n        while True:\n            d = sys.stdin.read(1)\n            if not d:\n                break\n            chan.send(d)\n    except EOFError:\n        # user hit ^Z or F6\n        pass\n","label":0}
{"content":"# -*- coding: utf-8 -*-\n\"\"\"\nTests for the newer CyberSource API implementation.\n\"\"\"\nfrom mock import patch\nfrom django.test import TestCase\nfrom django.conf import settings\nimport ddt\n\nfrom student.tests.factories import UserFactory\nfrom shoppingcart.models import Order, OrderItem\nfrom shoppingcart.processors.CyberSource2 import (\n    processor_hash,\n    process_postpay_callback,\n    render_purchase_form_html,\n    get_signed_purchase_params,\n    _get_processor_exception_html\n)\nfrom shoppingcart.processors.exceptions import (\n    CCProcessorSignatureException,\n    CCProcessorDataException,\n    CCProcessorWrongAmountException\n)\n\n\n@ddt.ddt\nclass CyberSource2Test(TestCase):\n    \"\"\"\n    Test the CyberSource API implementation.  As much as possible,\n    this test case should use ONLY the public processor interface\n    (defined in shoppingcart.processors.__init__.py).\n\n    Some of the tests in this suite rely on Django settings\n    to be configured a certain way.\n\n    \"\"\"\n\n    COST = \"10.00\"\n    CALLBACK_URL = \"\/test_callback_url\"\n    FAILED_DECISIONS = [\"DECLINE\", \"CANCEL\", \"ERROR\"]\n\n    def setUp(self):\n        \"\"\" Create a user and an order. \"\"\"\n        super(CyberSource2Test, self).setUp()\n\n        self.user = UserFactory()\n        self.order = Order.get_cart_for_user(self.user)\n        self.order_item = OrderItem.objects.create(\n            order=self.order,\n            user=self.user,\n            unit_cost=self.COST,\n            line_cost=self.COST\n        )\n\n    def assert_dump_recorded(self, order):\n        \"\"\"\n        Verify that this order does have a dump of information from the\n        payment processor.\n        \"\"\"\n        self.assertNotEqual(order.processor_reply_dump, '')\n\n    def test_render_purchase_form_html(self):\n        # Verify that the HTML form renders with the payment URL specified\n        # in the test settings.\n        # This does NOT test that all the form parameters are correct;\n        # we verify that by testing `get_signed_purchase_params()` directly.\n        html = render_purchase_form_html(self.order, callback_url=self.CALLBACK_URL)\n        self.assertIn('<form action=\"\/shoppingcart\/payment_fake\" method=\"post\">', html)\n        self.assertIn('transaction_uuid', html)\n        self.assertIn('signature', html)\n        self.assertIn(self.CALLBACK_URL, html)\n\n    def test_get_signed_purchase_params(self):\n        params = get_signed_purchase_params(self.order, callback_url=self.CALLBACK_URL)\n\n        # Check the callback URL override\n        self.assertEqual(params['override_custom_receipt_page'], self.CALLBACK_URL)\n\n        # Parameters determined by the order model\n        self.assertEqual(params['amount'], '10.00')\n        self.assertEqual(params['currency'], 'usd')\n        self.assertEqual(params['orderNumber'], 'OrderId: {order_id}'.format(order_id=self.order.id))\n        self.assertEqual(params['reference_number'], self.order.id)\n\n        # Parameters determined by the Django (test) settings\n        self.assertEqual(params['access_key'], '0123456789012345678901')\n        self.assertEqual(params['profile_id'], 'edx')\n\n        # Some fields will change depending on when the test runs,\n        # so we just check that they're set to a non-empty string\n        self.assertGreater(len(params['signed_date_time']), 0)\n        self.assertGreater(len(params['transaction_uuid']), 0)\n\n        # Constant parameters\n        self.assertEqual(params['transaction_type'], 'sale')\n        self.assertEqual(params['locale'], 'en')\n        self.assertEqual(params['payment_method'], 'card')\n        self.assertEqual(\n            params['signed_field_names'],\n            \",\".join([\n                'amount',\n                'currency',\n                'orderNumber',\n                'access_key',\n                'profile_id',\n                'reference_number',\n                'transaction_type',\n                'locale',\n                'signed_date_time',\n                'signed_field_names',\n                'unsigned_field_names',\n                'transaction_uuid',\n                'payment_method',\n                'override_custom_receipt_page',\n                'override_custom_cancel_page',\n            ])\n        )\n        self.assertEqual(params['unsigned_field_names'], '')\n\n        # Check the signature\n        self.assertEqual(params['signature'], self._signature(params))\n\n    # We patch the purchased callback because\n    # we're using the OrderItem base class, which throws an exception\n    # when item doest not have a course id associated\n    @patch.object(OrderItem, 'purchased_callback')\n    def test_process_payment_raises_exception(self, purchased_callback):  # pylint: disable=unused-argument\n        self.order.clear()\n        OrderItem.objects.create(\n            order=self.order,\n            user=self.user,\n            unit_cost=self.COST,\n            line_cost=self.COST,\n        )\n        params = self._signed_callback_params(self.order.id, self.COST, self.COST)\n        process_postpay_callback(params)\n\n    # We patch the purchased callback because\n    # (a) we're using the OrderItem base class, which doesn't implement this method, and\n    # (b) we want to verify that the method gets called on success.\n    @patch.object(OrderItem, 'purchased_callback')\n    @patch.object(OrderItem, 'pdf_receipt_display_name')\n    def test_process_payment_success(self, pdf_receipt_display_name, purchased_callback):  # pylint: disable=unused-argument\n        # Simulate a callback from CyberSource indicating that payment was successful\n        params = self._signed_callback_params(self.order.id, self.COST, self.COST)\n        result = process_postpay_callback(params)\n\n        # Expect that we processed the payment successfully\n        self.assertTrue(\n            result['success'],\n            msg=\"Payment was not successful: {error}\".format(error=result.get('error_html'))\n        )\n        self.assertEqual(result['error_html'], '')\n\n        # Expect that the item's purchased callback was invoked\n        purchased_callback.assert_called_with()\n\n        # Expect that the order has been marked as purchased\n        self.assertEqual(result['order'].status, 'purchased')\n        self.assert_dump_recorded(result['order'])\n\n    def test_process_payment_rejected(self):\n        # Simulate a callback from CyberSource indicating that the payment was rejected\n        params = self._signed_callback_params(self.order.id, self.COST, self.COST, decision='REJECT')\n        result = process_postpay_callback(params)\n\n        # Expect that we get an error message\n        self.assertFalse(result['success'])\n        self.assertIn(u\"did not accept your payment\", result['error_html'])\n        self.assert_dump_recorded(result['order'])\n\n    def test_process_payment_invalid_signature(self):\n        # Simulate a callback from CyberSource indicating that the payment was rejected\n        params = self._signed_callback_params(self.order.id, self.COST, self.COST, signature=\"invalid!\")\n        result = process_postpay_callback(params)\n\n        # Expect that we get an error message\n        self.assertFalse(result['success'])\n        self.assertIn(u\"corrupted message regarding your charge\", result['error_html'])\n\n    def test_process_payment_invalid_order(self):\n        # Use an invalid order ID\n        params = self._signed_callback_params(\"98272\", self.COST, self.COST)\n        result = process_postpay_callback(params)\n\n        # Expect an error\n        self.assertFalse(result['success'])\n        self.assertIn(u\"inconsistent data\", result['error_html'])\n\n    def test_process_invalid_payment_amount(self):\n        # Change the payment amount (no longer matches the database order record)\n        params = self._signed_callback_params(self.order.id, \"145.00\", \"145.00\")\n        result = process_postpay_callback(params)\n\n        # Expect an error\n        self.assertFalse(result['success'])\n        self.assertIn(u\"different amount than the order total\", result['error_html'])\n        # refresh data for current order\n        order = Order.objects.get(id=self.order.id)\n        self.assert_dump_recorded(order)\n\n    def test_process_amount_paid_not_decimal(self):\n        # Change the payment amount to a non-decimal\n        params = self._signed_callback_params(self.order.id, self.COST, \"abcd\")\n        result = process_postpay_callback(params)\n\n        # Expect an error\n        self.assertFalse(result['success'])\n        self.assertIn(u\"badly-typed value\", result['error_html'])\n\n    def test_process_user_cancelled(self):\n        # Change the payment amount to a non-decimal\n        params = self._signed_callback_params(self.order.id, self.COST, \"abcd\")\n        params['decision'] = u'CANCEL'\n        result = process_postpay_callback(params)\n\n        # Expect an error\n        self.assertFalse(result['success'])\n        self.assertIn(u\"you have cancelled this transaction\", result['error_html'])\n\n    @patch.object(OrderItem, 'purchased_callback')\n    @patch.object(OrderItem, 'pdf_receipt_display_name')\n    def test_process_no_credit_card_digits(self, pdf_receipt_display_name, purchased_callback):  # pylint: disable=unused-argument\n        # Use a credit card number with no digits provided\n        params = self._signed_callback_params(\n            self.order.id, self.COST, self.COST,\n            card_number='nodigits'\n        )\n        result = process_postpay_callback(params)\n\n        # Expect that we processed the payment successfully\n        self.assertTrue(\n            result['success'],\n            msg=\"Payment was not successful: {error}\".format(error=result.get('error_html'))\n        )\n        self.assertEqual(result['error_html'], '')\n        self.assert_dump_recorded(result['order'])\n\n        # Expect that the order has placeholders for the missing credit card digits\n        self.assertEqual(result['order'].bill_to_ccnum, '####')\n\n    @ddt.data('req_reference_number', 'req_currency', 'decision', 'auth_amount')\n    def test_process_missing_parameters(self, missing_param):\n        # Remove a required parameter\n        params = self._signed_callback_params(self.order.id, self.COST, self.COST)\n        del params[missing_param]\n\n        # Recalculate the signature with no signed fields so we can get past\n        # signature validation.\n        params['signed_field_names'] = 'reason_code,message'\n        params['signature'] = self._signature(params)\n\n        result = process_postpay_callback(params)\n\n        # Expect an error\n        self.assertFalse(result['success'])\n        self.assertIn(u\"did not return a required parameter\", result['error_html'])\n\n    @patch.object(OrderItem, 'purchased_callback')\n    @patch.object(OrderItem, 'pdf_receipt_display_name')\n    def test_sign_then_verify_unicode(self, pdf_receipt_display_name, purchased_callback):  # pylint: disable=unused-argument\n        params = self._signed_callback_params(\n            self.order.id, self.COST, self.COST,\n            first_name=u'\\u2699'\n        )\n\n        # Verify that this executes without a unicode error\n        result = process_postpay_callback(params)\n        self.assertTrue(result['success'])\n        self.assert_dump_recorded(result['order'])\n\n    @ddt.data('string', u'\u00fc\u00f1\u00ee\u00e7\u00f8\u2202\u00e9')\n    def test_get_processor_exception_html(self, error_string):\n        \"\"\"\n        Tests the processor exception html message\n        \"\"\"\n        for exception_type in [CCProcessorSignatureException, CCProcessorWrongAmountException, CCProcessorDataException]:\n            error_msg = error_string\n            exception = exception_type(error_msg)\n            html = _get_processor_exception_html(exception)\n            self.assertIn(settings.PAYMENT_SUPPORT_EMAIL, html)\n            self.assertIn('Sorry!', html)\n            self.assertIn(error_msg, html)\n\n    def _signed_callback_params(\n        self, order_id, order_amount, paid_amount,\n        decision='ACCEPT', signature=None, card_number='xxxxxxxxxxxx1111',\n        first_name='John'\n    ):\n        \"\"\"\n        Construct parameters that could be returned from CyberSource\n        to our payment callback.\n\n        Some values can be overridden to simulate different test scenarios,\n        but most are fake values captured from interactions with\n        a CyberSource test account.\n\n        Args:\n            order_id (string or int): The ID of the `Order` model.\n            order_amount (string): The cost of the order.\n            paid_amount (string): The amount the user paid using CyberSource.\n\n        Keyword Args:\n\n            decision (string): Whether the payment was accepted or rejected or declined.\n            signature (string): If provided, use this value instead of calculating the signature.\n            card_numer (string): If provided, use this value instead of the default credit card number.\n            first_name (string): If provided, the first name of the user.\n\n        Returns:\n            dict\n\n        \"\"\"\n        # Parameters sent from CyberSource to our callback implementation\n        # These were captured from the CC test server.\n\n        signed_field_names = [\"transaction_id\",\n                              \"decision\",\n                              \"req_access_key\",\n                              \"req_profile_id\",\n                              \"req_transaction_uuid\",\n                              \"req_transaction_type\",\n                              \"req_reference_number\",\n                              \"req_amount\",\n                              \"req_currency\",\n                              \"req_locale\",\n                              \"req_payment_method\",\n                              \"req_override_custom_receipt_page\",\n                              \"req_bill_to_forename\",\n                              \"req_bill_to_surname\",\n                              \"req_bill_to_email\",\n                              \"req_bill_to_address_line1\",\n                              \"req_bill_to_address_city\",\n                              \"req_bill_to_address_state\",\n                              \"req_bill_to_address_country\",\n                              \"req_bill_to_address_postal_code\",\n                              \"req_card_number\",\n                              \"req_card_type\",\n                              \"req_card_expiry_date\",\n                              \"message\",\n                              \"reason_code\",\n                              \"auth_avs_code\",\n                              \"auth_avs_code_raw\",\n                              \"auth_response\",\n                              \"auth_amount\",\n                              \"auth_code\",\n                              \"auth_trans_ref_no\",\n                              \"auth_time\",\n                              \"bill_trans_ref_no\",\n                              \"signed_field_names\",\n                              \"signed_date_time\"]\n\n        # if decision is in FAILED_DECISIONS list then remove  auth_amount from\n        # signed_field_names list.\n        if decision in self.FAILED_DECISIONS:\n            signed_field_names.remove(\"auth_amount\")\n\n        params = {\n            # Parameters that change based on the test\n            \"decision\": decision,\n            \"req_reference_number\": str(order_id),\n            \"req_amount\": order_amount,\n            \"auth_amount\": paid_amount,\n            \"req_card_number\": card_number,\n\n            # Stub values\n            \"utf8\": u\"\u2713\",\n            \"req_bill_to_address_country\": \"US\",\n            \"auth_avs_code\": \"X\",\n            \"req_card_expiry_date\": \"01-2018\",\n            \"bill_trans_ref_no\": \"85080648RYI23S6I\",\n            \"req_bill_to_address_state\": \"MA\",\n            \"signed_field_names\": \",\".join(signed_field_names),\n            \"req_payment_method\": \"card\",\n            \"req_transaction_type\": \"sale\",\n            \"auth_code\": \"888888\",\n            \"req_locale\": \"en\",\n            \"reason_code\": \"100\",\n            \"req_bill_to_address_postal_code\": \"02139\",\n            \"req_bill_to_address_line1\": \"123 Fake Street\",\n            \"req_card_type\": \"001\",\n            \"req_bill_to_address_city\": \"Boston\",\n            \"signed_date_time\": \"2014-08-18T14:07:10Z\",\n            \"req_currency\": \"usd\",\n            \"auth_avs_code_raw\": \"I1\",\n            \"transaction_id\": \"4083708299660176195663\",\n            \"auth_time\": \"2014-08-18T140710Z\",\n            \"message\": \"Request was processed successfully.\",\n            \"auth_response\": \"100\",\n            \"req_profile_id\": \"0000001\",\n            \"req_transaction_uuid\": \"ddd9935b82dd403f9aa4ba6ecf021b1f\",\n            \"auth_trans_ref_no\": \"85080648RYI23S6I\",\n            \"req_bill_to_surname\": \"Doe\",\n            \"req_bill_to_forename\": first_name,\n            \"req_bill_to_email\": \"john@example.com\",\n            \"req_override_custom_receipt_page\": \"http:\/\/localhost:8000\/shoppingcart\/postpay_callback\/\",\n            \"req_access_key\": \"abcd12345\",\n        }\n\n        # if decision is in FAILED_DECISIONS list then remove the auth_amount from params dict\n\n        if decision in self.FAILED_DECISIONS:\n            del params[\"auth_amount\"]\n\n        # Calculate the signature\n        params['signature'] = signature if signature is not None else self._signature(params)\n        return params\n\n    def _signature(self, params):\n        \"\"\"\n        Calculate the signature from a dictionary of params.\n\n        NOTE: This method uses the processor's hashing method.  That method\n        is a thin wrapper of standard library calls, and it seemed overly complex\n        to rewrite that code in the test suite.\n\n        Args:\n            params (dict): Dictionary with a key 'signed_field_names',\n                which is a comma-separated list of keys in the dictionary\n                to include in the signature.\n\n        Returns:\n            string\n\n        \"\"\"\n        return processor_hash(\n            \",\".join([\n                u\"{0}={1}\".format(signed_field, params[signed_field])\n                for signed_field\n                in params['signed_field_names'].split(u\",\")\n            ])\n        )\n\n    def test_process_payment_declined(self):\n        # Simulate a callback from CyberSource indicating that the payment was declined\n        params = self._signed_callback_params(self.order.id, self.COST, self.COST, decision='DECLINE')\n        result = process_postpay_callback(params)\n\n        # Expect that we get an error message\n        self.assertFalse(result['success'])\n        self.assertIn(u\"payment was declined\", result['error_html'])\n","label":0}
{"content":"# SchedGui.py - Python extension for perf script, basic GUI code for\n#\t\ttraces drawing and overview.\n#\n# Copyright (C) 2010 by Frederic Weisbecker <fweisbec@gmail.com>\n#\n# This software is distributed under the terms of the GNU General\n# Public License (\"GPL\") version 2 as published by the Free Software\n# Foundation.\n\n\ntry:\n\timport wx\nexcept ImportError:\n\traise ImportError, \"You need to install the wxpython lib for this script\"\n\n\nclass RootFrame(wx.Frame):\n\tY_OFFSET = 100\n\tRECT_HEIGHT = 100\n\tRECT_SPACE = 50\n\tEVENT_MARKING_WIDTH = 5\n\n\tdef __init__(self, sched_tracer, title, parent = None, id = -1):\n\t\twx.Frame.__init__(self, parent, id, title)\n\n\t\t(self.screen_width, self.screen_height) = wx.GetDisplaySize()\n\t\tself.screen_width -= 10\n\t\tself.screen_height -= 10\n\t\tself.zoom = 0.5\n\t\tself.scroll_scale = 20\n\t\tself.sched_tracer = sched_tracer\n\t\tself.sched_tracer.set_root_win(self)\n\t\t(self.ts_start, self.ts_end) = sched_tracer.interval()\n\t\tself.update_width_virtual()\n\t\tself.nr_rects = sched_tracer.nr_rectangles() + 1\n\t\tself.height_virtual = RootFrame.Y_OFFSET + (self.nr_rects * (RootFrame.RECT_HEIGHT + RootFrame.RECT_SPACE))\n\n\t\t# whole window panel\n\t\tself.panel = wx.Panel(self, size=(self.screen_width, self.screen_height))\n\n\t\t# scrollable container\n\t\tself.scroll = wx.ScrolledWindow(self.panel)\n\t\tself.scroll.SetScrollbars(self.scroll_scale, self.scroll_scale, self.width_virtual \/ self.scroll_scale, self.height_virtual \/ self.scroll_scale)\n\t\tself.scroll.EnableScrolling(True, True)\n\t\tself.scroll.SetFocus()\n\n\t\t# scrollable drawing area\n\t\tself.scroll_panel = wx.Panel(self.scroll, size=(self.screen_width - 15, self.screen_height \/ 2))\n\t\tself.scroll_panel.Bind(wx.EVT_PAINT, self.on_paint)\n\t\tself.scroll_panel.Bind(wx.EVT_KEY_DOWN, self.on_key_press)\n\t\tself.scroll_panel.Bind(wx.EVT_LEFT_DOWN, self.on_mouse_down)\n\t\tself.scroll.Bind(wx.EVT_PAINT, self.on_paint)\n\t\tself.scroll.Bind(wx.EVT_KEY_DOWN, self.on_key_press)\n\t\tself.scroll.Bind(wx.EVT_LEFT_DOWN, self.on_mouse_down)\n\n\t\tself.scroll.Fit()\n\t\tself.Fit()\n\n\t\tself.scroll_panel.SetDimensions(-1, -1, self.width_virtual, self.height_virtual, wx.SIZE_USE_EXISTING)\n\n\t\tself.txt = None\n\n\t\tself.Show(True)\n\n\tdef us_to_px(self, val):\n\t\treturn val \/ (10 ** 3) * self.zoom\n\n\tdef px_to_us(self, val):\n\t\treturn (val \/ self.zoom) * (10 ** 3)\n\n\tdef scroll_start(self):\n\t\t(x, y) = self.scroll.GetViewStart()\n\t\treturn (x * self.scroll_scale, y * self.scroll_scale)\n\n\tdef scroll_start_us(self):\n\t\t(x, y) = self.scroll_start()\n\t\treturn self.px_to_us(x)\n\n\tdef paint_rectangle_zone(self, nr, color, top_color, start, end):\n\t\toffset_px = self.us_to_px(start - self.ts_start)\n\t\twidth_px = self.us_to_px(end - self.ts_start)\n\n\t\toffset_py = RootFrame.Y_OFFSET + (nr * (RootFrame.RECT_HEIGHT + RootFrame.RECT_SPACE))\n\t\twidth_py = RootFrame.RECT_HEIGHT\n\n\t\tdc = self.dc\n\n\t\tif top_color is not None:\n\t\t\t(r, g, b) = top_color\n\t\t\ttop_color = wx.Colour(r, g, b)\n\t\t\tbrush = wx.Brush(top_color, wx.SOLID)\n\t\t\tdc.SetBrush(brush)\n\t\t\tdc.DrawRectangle(offset_px, offset_py, width_px, RootFrame.EVENT_MARKING_WIDTH)\n\t\t\twidth_py -= RootFrame.EVENT_MARKING_WIDTH\n\t\t\toffset_py += RootFrame.EVENT_MARKING_WIDTH\n\n\t\t(r ,g, b) = color\n\t\tcolor = wx.Colour(r, g, b)\n\t\tbrush = wx.Brush(color, wx.SOLID)\n\t\tdc.SetBrush(brush)\n\t\tdc.DrawRectangle(offset_px, offset_py, width_px, width_py)\n\n\tdef update_rectangles(self, dc, start, end):\n\t\tstart += self.ts_start\n\t\tend += self.ts_start\n\t\tself.sched_tracer.fill_zone(start, end)\n\n\tdef on_paint(self, event):\n\t\tdc = wx.PaintDC(self.scroll_panel)\n\t\tself.dc = dc\n\n\t\twidth = min(self.width_virtual, self.screen_width)\n\t\t(x, y) = self.scroll_start()\n\t\tstart = self.px_to_us(x)\n\t\tend = self.px_to_us(x + width)\n\t\tself.update_rectangles(dc, start, end)\n\n\tdef rect_from_ypixel(self, y):\n\t\ty -= RootFrame.Y_OFFSET\n\t\trect = y \/ (RootFrame.RECT_HEIGHT + RootFrame.RECT_SPACE)\n\t\theight = y % (RootFrame.RECT_HEIGHT + RootFrame.RECT_SPACE)\n\n\t\tif rect < 0 or rect > self.nr_rects - 1 or height > RootFrame.RECT_HEIGHT:\n\t\t\treturn -1\n\n\t\treturn rect\n\n\tdef update_summary(self, txt):\n\t\tif self.txt:\n\t\t\tself.txt.Destroy()\n\t\tself.txt = wx.StaticText(self.panel, -1, txt, (0, (self.screen_height \/ 2) + 50))\n\n\n\tdef on_mouse_down(self, event):\n\t\t(x, y) = event.GetPositionTuple()\n\t\trect = self.rect_from_ypixel(y)\n\t\tif rect == -1:\n\t\t\treturn\n\n\t\tt = self.px_to_us(x) + self.ts_start\n\n\t\tself.sched_tracer.mouse_down(rect, t)\n\n\n\tdef update_width_virtual(self):\n\t\tself.width_virtual = self.us_to_px(self.ts_end - self.ts_start)\n\n\tdef __zoom(self, x):\n\t\tself.update_width_virtual()\n\t\t(xpos, ypos) = self.scroll.GetViewStart()\n\t\txpos = self.us_to_px(x) \/ self.scroll_scale\n\t\tself.scroll.SetScrollbars(self.scroll_scale, self.scroll_scale, self.width_virtual \/ self.scroll_scale, self.height_virtual \/ self.scroll_scale, xpos, ypos)\n\t\tself.Refresh()\n\n\tdef zoom_in(self):\n\t\tx = self.scroll_start_us()\n\t\tself.zoom *= 2\n\t\tself.__zoom(x)\n\n\tdef zoom_out(self):\n\t\tx = self.scroll_start_us()\n\t\tself.zoom \/= 2\n\t\tself.__zoom(x)\n\n\n\tdef on_key_press(self, event):\n\t\tkey = event.GetRawKeyCode()\n\t\tif key == ord(\"+\"):\n\t\t\tself.zoom_in()\n\t\t\treturn\n\t\tif key == ord(\"-\"):\n\t\t\tself.zoom_out()\n\t\t\treturn\n\n\t\tkey = event.GetKeyCode()\n\t\t(x, y) = self.scroll.GetViewStart()\n\t\tif key == wx.WXK_RIGHT:\n\t\t\tself.scroll.Scroll(x + 1, y)\n\t\telif key == wx.WXK_LEFT:\n\t\t\tself.scroll.Scroll(x - 1, y)\n\t\telif key == wx.WXK_DOWN:\n\t\t\tself.scroll.Scroll(x, y + 1)\n\t\telif key == wx.WXK_UP:\n\t\t\tself.scroll.Scroll(x, y - 1)\n","label":0}
{"content":"# -*- coding: utf-8 -*-\nimport xbmc\nimport xbmcaddon\n\nADDON = xbmcaddon.Addon(id='screensaver.weather')\nADDON_ID = ADDON.getAddonInfo('id')\n\n\n# Common logging module\ndef log(txt, loglevel=xbmc.LOGDEBUG):\n    if (ADDON.getSetting(\"logEnabled\") == \"true\") or (loglevel != xbmc.LOGDEBUG):\n        if isinstance(txt, str):\n            txt = txt.decode(\"utf-8\")\n        message = u'%s: %s' % (ADDON_ID, txt)\n        xbmc.log(msg=message.encode(\"utf-8\"), level=loglevel)\n\n\n##############################\n# Stores Various Settings\n##############################\nclass Settings():\n    DIM_LEVEL = (\n        '00000000',\n        '11000000',\n        '22000000',\n        '33000000',\n        '44000000',\n        '55000000',\n        '66000000',\n        '77000000',\n        '88000000',\n        '99000000',\n        'AA000000',\n        'BB000000',\n        'CC000000',\n        'DD000000',\n        'EE000000'\n    )\n\n    @staticmethod\n    def getDimValue():\n        # The actual dim level (Hex) is one of\n        # Where 00000000 is not changed\n        # So that is a total of 15 different options\n        # FF000000 would be completely black, so we do not use that one\n        if ADDON.getSetting(\"dimLevel\"):\n            return Settings.DIM_LEVEL[int(ADDON.getSetting(\"dimLevel\"))]\n        else:\n            return '00000000'\n","label":0}
{"content":"# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='OpenIDNonce',\n            fields=[\n                ('id', models.AutoField(primary_key=True, verbose_name='ID', serialize=False, auto_created=True)),\n                ('server_url', models.CharField(max_length=255)),\n                ('timestamp', models.IntegerField()),\n                ('salt', models.CharField(max_length=255)),\n                ('date_created', models.DateTimeField(auto_now_add=True)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='OpenIDStore',\n            fields=[\n                ('id', models.AutoField(primary_key=True, verbose_name='ID', serialize=False, auto_created=True)),\n                ('server_url', models.CharField(max_length=255)),\n                ('handle', models.CharField(max_length=255)),\n                ('secret', models.TextField()),\n                ('issued', models.IntegerField()),\n                ('lifetime', models.IntegerField()),\n                ('assoc_type', models.TextField()),\n            ],\n        ),\n    ]\n","label":0}
{"content":"'''\nAuthor: Jon Tsai    \nCreated: May 29 2016\n'''\n\nimport numpy as np \nimport theano\nfrom time import sleep\nimport sys\n\ndef progress_bar(percent, speed):\n    i = int(percent)\/2\n    sys.stdout.write('\\r')\n    # the exact output you're looking for:\n    sys.stdout.write(\"[%-50s] %d%% %f instances\/s\" % ('='*i, percent, speed))\n    sys.stdout.flush()\n    \n\n\n\ndef combine_sents(sent_set):\n    '''\n    parameter: sent_set ==> 2D sentences set\n                        ==> type: list[list[list]]\n\n    return: sents1D ==> 1D sentences set\n                    ==> type: list[list]\n\n    This function will combine 2D sentence set \n    into 1D sentence set. \n    e.g.\n    [\n        [[sent1], [sent2], [sent3], ..., [sentn]]\n        ...\n        [[sent1], [sent2], [sent3], ..., [sentn]]\n    ]\n    ==> \n    [\n        [sentences1],\n        ...\n        [sentencesn]\n    ]\n    '''\n    sents1D = []\n    for doc in sent_set:\n        combine_sent = np.array([])\n        for sent in doc:\n            combine_sent = np.concatenate((combine_sent,sent))\n        sents1D.append(combine_sent)\n\n    return sents1D\n\ndef shuffle_index(length_of_indices_ls):\n    '''\n    ----------\n    parameter: \n    ----------\n    length_of_indices_ls: type = int \n\n    ----------\n    return: \n    ----------\n    a shuffled numpy array of indices \n    '''\n    ls = np.arange(length_of_indices_ls)\n    np.random.shuffle(ls)\n    return ls\n\ndef padding(batch_input_list):\n    '''\n    ----------\n    parameter: \n    ----------\n    batch_input_list: type = list(list) \n\n    ----------\n    return: \n    ----------\n    numpy.ndarray: shape == (n_batch, max_time_step) \n    '''\n    n_batch = len(batch_input_list)\n    max_time_step = max([len(batch_input_list[i]) for i in range(n_batch)])\n\n    padding_result = np.zeros((n_batch, max_time_step))\n    for batch in range(n_batch):\n        padding_result[batch] = np.concatenate((np.asarray(batch_input_list[batch]),\n                                                np.zeros(max_time_step - len(batch_input_list[batch]))))\n    return padding_result.astype('int64')\n\n\n\ndef mask_generator(indices_matrix):\n    '''\n    ----------\n    parameter: \n    ----------\n    indices_matrix: type = list[list] \n\n    ----------\n    return: \n    ----------\n    mask : type = np.ndarray\n    a mask matrix of a batch of varied length instances\n    '''\n\n    n_batch = len(indices_matrix)\n    len_ls = [len(sent) for sent in indices_matrix]\n    max_len = max(len_ls)\n    mask = np.zeros((n_batch, max_len))\n    for i in range(n_batch):\n        for j in range(len(indices_matrix[i])):\n            mask[i][j] = 1 \n\n    return mask\n\ndef mlp_mask_generator(indices_matrix, wemb_size):\n    '''\n    ----------\n    parameter: \n    ----------\n    indices_matrix: type = list[list] \n\n    ----------\n    return: \n    ----------\n    mask : type = np.ndarray\n           mask.shape = (n_batch, wemb_size)\n    '''\n\n    n_batch = len(indices_matrix)\n    len_ls = [len(sent) for sent in indices_matrix]\n    \n    mask = np.ones((n_batch, wemb_size))\n    for i in range(n_batch):\n        mask[i] = mask[i] * len_ls[i]\n\n    return mask\n\ndef fake_input_generator(max_index, batch_number, length_range):\n    '''\n    ----------\n    parameter: \n    ----------\n    max_index: type = int \n    batch_number: type = int \n    length_range: tuple(int), len(length_range) = 2 \n                  e.g. (50, 70)\n\n    ----------\n    return: \n    ----------\n    fake_data: type = list[list]\n               format: fake_data.shape[0] = batch_number\n                       length_range[0] <= len(fake_data[i]) <= length_range[1]\n                       0 <= fake_data[i][j] <= max_index\n    '''    \n    max_time_step = length_range[0] + np.random.randint(length_range[1] - length_range[0] + 1)\n    \n    fake_data = np.zeros((batch_number, max_time_step))\n    \n    mask = np.zeros((batch_number, max_time_step)).astype(theano.config.floatX)\n\n    len_range = max_time_step - length_range[0]\n    assert len_range >= 0\n    #pick a row to be the max length row\n    row = np.random.randint(batch_number)\n    fake_data[row] = np.random.randint(max_index+1, size = (max_time_step,))\n    mask[row] = np.ones(max_time_step)\n\n    for batch in range(batch_number):\n        if batch == row:\n            continue\n        length = length_range[0]+np.random.randint(len_range)\n\n        fake_data[batch] = np.concatenate((np.random.randint(max_index+1 ,size = (length,)), \n                                       np.zeros(max_time_step - length)))\n        mask[batch] = np.concatenate((np.ones(length), np.zeros(max_time_step - length)))\n\n    return (fake_data.astype('int32'), mask)\n\ndef fake_data(max_index, batch_number, max_time_step, min_time_step):\n    \n    fake_data = np.zeros((batch_number, max_time_step))\n    \n    mask = np.zeros((batch_number, max_time_step)).astype(theano.config.floatX)\n\n    len_range = max_time_step - min_time_step\n    assert len_range >= 0\n    #pick a row to be the max length row\n    row = np.random.randint(batch_number)\n    fake_data[row] = np.random.randint(max_index+1, size = (max_time_step,))\n    mask[row] = np.ones(max_time_step)\n\n    for batch in range(batch_number):\n        if batch == row:\n            continue\n        length = min_time_step+np.random.randint(len_range)\n\n        fake_data[batch] = np.concatenate((np.random.randint(max_index+1 ,size = (length,)), \n                                       np.zeros(max_time_step - length)))\n        mask[batch] = np.concatenate((np.ones(length), np.zeros(max_time_step - length)))\n\n    return (fake_data.astype('int32'), mask)","label":0}
{"content":"#!\/usr\/bin\/env python\n\"\"\"\nRender Django templates.\nUseful for generating fixtures for the JavaScript unit test suite.\n\nUsage:\n    python render_templates.py path\/to\/templates.json\n\nwhere \"templates.json\" is a JSON file of the form:\n    [\n        {\n            \"template\": \"openassessmentblock\/oa_base.html\",\n            \"context\": {\n                \"title\": \"Lorem\",\n                \"question\": \"Ipsum?\"\n            },\n            \"output\": \"oa_base.html\"\n        },\n        ...\n    ]\n\nThe rendered templates are saved to \"output\" relative to the\ntemplates.json file's directory.\n\"\"\"\nimport sys\nimport os.path\nimport json\nimport re\nimport dateutil.parser\nimport pytz\n\n# This is a bit of a hack to ensure that the root repo directory\n# is in the Python path, so Django can find the settings module.\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\nfrom django.template.context import Context\nfrom django.template.loader import get_template\n\n\nUSAGE = u\"{prog} TEMPLATE_DESC\"\n\n\nDATETIME_REGEX = re.compile(\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}$\")\n\ndef parse_dates(context):\n    \"\"\"\n    Transform datetime strings into Python datetime objects.\n\n    JSON does not provide a standard way to serialize datetime objects,\n    but some of the templates expect that the context contains\n    Python datetime objects.\n\n    This (somewhat hacky) solution recursively searches the context\n    for formatted datetime strings of the form \"2014-01-02T12:34\"\n    and converts them to Python datetime objects with the timezone\n    set to UTC.\n\n    Args:\n        context (JSON-serializable): The context (or part of the context)\n            that will be passed to the template.  Dictionaries and lists\n            will be recursively searched and transformed.\n\n    Returns:\n        JSON-serializable of the same type as the `context` argument.\n\n    \"\"\"\n    if isinstance(context, dict):\n        return {\n            key: parse_dates(value)\n            for key, value in context.iteritems()\n        }\n    elif isinstance(context, list):\n        return [\n            parse_dates(item)\n            for item in context\n        ]\n    elif isinstance(context, basestring):\n        if DATETIME_REGEX.match(context) is not None:\n            return dateutil.parser.parse(context).replace(tzinfo=pytz.utc)\n\n    return context\n\n\ndef render_templates(root_dir, template_json):\n    \"\"\"\n    Create rendered templates.\n\n    Args:\n        root_dir (str): The directory in which to write the rendered templates.\n        template_json (dict): Description of which templates to render.  Must be a list\n            of dicts, each containing keys \"template\" (str), \"context\" (dict), and \"output\" (str).\n\n    Returns:\n        None\n\n    \"\"\"\n    for template_dict in template_json:\n        template = get_template(template_dict['template'])\n        context = parse_dates(template_dict['context'])\n        rendered = template.render(Context(context))\n        output_path = os.path.join(root_dir, template_dict['output'])\n\n        try:\n            with open(output_path, 'w') as output_file:\n                output_file.write(rendered.encode('utf-8'))\n        except IOError:\n            print \"Could not write rendered template to file: {}\".format(output_path)\n            sys.exit(1)\n\n\ndef main():\n    \"\"\"\n    Main entry point for the script.\n    \"\"\"\n    if len(sys.argv) < 2:\n        print USAGE.format(sys.argv[0])\n        sys.exit(1)\n\n    try:\n        with open(sys.argv[1]) as template_json:\n            root_dir = os.path.dirname(sys.argv[1])\n            render_templates(root_dir, json.load(template_json))\n    except IOError as ex:\n        print u\"Could not open template description file: {}\".format(sys.argv[1])\n        print(ex)\n        sys.exit(1)\n    except ValueError as ex:\n        print u\"Could not parse template description as JSON: {}\".format(sys.argv[1])\n        print(ex)\n        sys.exit(1)\n\n\nif __name__ == '__main__':\n    main()\n","label":0}
